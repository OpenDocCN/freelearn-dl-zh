- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Demystifying Large Language Models: Theory, Design, and Langchain Implementation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delve deep into the intricate world of **large language
    models** (**LLMs**) and the underpinning mathematical concepts that fuel their
    performance. The advent of these models has revolutionized the field of **natural
    language processing** (**NLP**), offering unparalleled proficiency in understanding,
    generating, and interacting with human language.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are a subset of **artificial intelligence** (**AI**) models that can understand
    and generate human-like text. They achieve this by being trained on a diverse
    range of internet text, thus learning an extensive array of facts about the world.
    They also learn to predict what comes next in a piece of text, which enables them
    to generate creative, fluent, and contextually coherent sentences.
  prefs: []
  type: TYPE_NORMAL
- en: As we explore the operations of LLMs, we will introduce the key metric of **perplexity**,
    a measurement of uncertainty that is pivotal in determining the performance of
    these models. A lower perplexity indicates the confidence that a **language model**
    (**LM**) has in predicting the next word in a sequence, thus showcasing its proficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter draws on multiple insightful publications that delve into the
    mathematical insights of LLMs. Some of these include *A Neural Probabilistic Language
    Model*, *Attention is All You Need*, and *PaLM: Scaling Language Modeling with
    Pathways*. These sources will guide us in understanding the robust mechanisms
    that underpin LLMs and their exceptional capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: We will also explore the emerging field of **reinforcement learning from human
    feedback** (**RLHF**) in the context of LMs. RLHF has proven to be a powerful
    tool in fine-tuning the performance of LLMs, thereby leading to more accurate
    and meaningful generated texts.
  prefs: []
  type: TYPE_NORMAL
- en: With a comprehensive understanding of the mathematical foundations of LLMs and
    a deep dive into RLHF, we will gain a robust knowledge of these advanced AI systems,
    paving the way for future innovations and advancements in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will discuss the detailed architecture and design of recent models,
    such as **Pathways Language Model** (**PaLM**), **Large Language Model Meta AI**
    (**LLaMA**), and GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the topics covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What are LLMs and how are they different from LMs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivations for developing and using LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in developing LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you are expected to possess a solid foundation in **machine
    learning** (**ML**) concepts, particularly in the areas of **Transformers** and
    **reinforcement learning**. An understanding of Transformer-based models, which
    underpin many of today’s LLMs, is vital. This includes familiarity with concepts
    such as self-attention mechanisms, positional encoding, and the structure of encoder-decoder
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge of reinforcement learning principles is also essential, as we will
    delve into the application of RLHF in the fine-tuning of LMs. Familiarity with
    concepts such as policy gradients, reward functions, and Q-learning will greatly
    enhance your comprehension of this content.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, coding proficiency, specifically in Python, is crucial. This is because
    many of the concepts will be demonstrated and explored through the lens of programming.
    Experience with PyTorch or TensorFlow, popular ML libraries, and Hugging Face’s
    Transformers library, a key resource for working with transformer models, will
    also be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: However, don’t be discouraged if you feel you’re lacking in some areas. This
    chapter aims to walk you through the complexities of these subjects, bridging
    any knowledge gaps along the way. So, come prepared with a mindset for learning,
    and let’s delve into the fascinating world of LLMs!
  prefs: []
  type: TYPE_NORMAL
- en: What are LLMs and how are they different from LMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An LM is a type of ML model that is trained to predict the next word (or character
    or subword, depending on the granularity of the model) in a sequence, given the
    words that came before it (or in some models, the surrounding words). It’s a probabilistic
    model that is capable of generating text that follows a certain linguistic style
    or pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Before the advent of Transformer-based models such as **generative pretrained
    Transformers** (**GPTs**) and **Bidirectional Encoder Representations from Transformers**
    (**BERT**), there were several other types of LMs widely used in NLP tasks. The
    following subsections discuss a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: n-gram models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are some of the simplest LMs. An *n*-gram model uses the (*n*-1) previous
    words to predict the *n*th word in a sentence. For example, in a bigram (2-gram)
    model, we would use the previous word to predict the next word. These models are
    easy to implement and computationally efficient, but they typically don’t perform
    as well as more complex models because they don’t capture long-range dependencies
    between words. Their performance also degrades as *n* increases, as they suffer
    from data sparsity issues (not having enough data to accurately estimate the probabilities
    for all possible *n*-grams).
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov models (HMMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These models consider the “hidden” states that generate the observed data. In
    the context of language modeling, each word would be an observed state, and the
    “hidden” state would be some kind of linguistic feature that’s not directly observable
    (such as the part of speech of the word). However, like *n*-gram models, HMMs
    struggle to capture long-range dependencies between words.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks (RNNs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are a type of neural network where connections between nodes form a directed
    graph along a temporal sequence. This allows them to use their internal state
    (memory) to process sequences of inputs, making them ideal for language modeling.
    They can capture long-range dependencies between words, but they struggle with
    the so-called vanishing gradient problem, which makes it difficult to learn these
    dependencies in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory (LSTM) networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An LSTM network is a special kind of RNN that is designed to learn long-term
    dependencies. They do this by using a series of “gates” that control the flow
    of information in and out of the memory state of the network. LSTMs were a big
    step forward in the state of the art of language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent unit (GRU) networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These are a variation of LSTMs that use a slightly different set of gates in
    their architecture. They’re often simpler and faster to train than LSTMs, but
    whether they perform better or worse than LSTMs tends to depend on the specific
    task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these models has its own strengths and weaknesses, and none of them
    are inherently better or worse than the others – it all depends on the specific
    task and dataset. However, Transformer-based models have generally outperformed
    all of these models in a wide range of tasks, leading to their current popularity
    in the field of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: How LLMs stand out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLMs, such as GPT-3 and GPT-4, are simply LMs that are trained on a very large
    amount of text and have a very large number of parameters. The larger the model
    (in terms of parameters and training data), the more capable it is of understanding
    and generating complex and varied texts. Here are some key ways in which LLMs
    differ from smaller LMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: LLMs are trained on vast amounts of data. This allows them to learn
    from a wide range of linguistic patterns, styles, and topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameters**: LLMs have a huge number of parameters. Parameters in an ML
    model are the parts of the model that are learned from the training data. The
    more parameters a model has, the more complex patterns it can learn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Because they’re trained on more data and have more parameters,
    LLMs generally perform better than smaller ones. They’re capable of generating
    more coherent and diverse texts, and they’re better at understanding context,
    making inferences, and even answering questions or generating texts on a wide
    range of topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute resources**: LLMs require a significant amount of computational resources
    to train, both in terms of processing power and memory. They also take longer
    to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage and inference time**: Large models also require more storage space,
    and it takes longer to generate predictions (although this inference time is still
    typically quite fast on modern hardware).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, we can say that LLMs are essentially scaled-up versions of smaller LMs.
    They’re trained on more data, have more parameters, and are generally capable
    of producing higher-quality results, but they also require more resources to train
    and use. Besides that, an important advantage of an LLM is that we can train them
    unsupervised on a large corpus of data and then fine-tune them with a limited
    amount of data for different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Motivations for developing and using LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The motivation to develop and use LLMs arises from several factors related to
    the capabilities of these models, and the potential benefits they can bring in
    diverse applications. The following subsections detail a few of these key motivations.
  prefs: []
  type: TYPE_NORMAL
- en: Improved performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs, when trained with sufficient data, generally demonstrate better performance
    compared to smaller models. They are more capable of understanding context, identifying
    nuances, and generating coherent and contextually relevant responses. This performance
    gain applies to a wide range of tasks in NLP, including text classification, named
    entity recognition, sentiment analysis, machine translation, question answering,
    and text generation. As shown in *Table 7.1*, the performance of BERT – one of
    the first well-known LLMs – and GPT is compared to the previous models on the
    **General Language Understanding Evaluation** (**GLUE**) benchmark. The GLUE benchmark
    is a collection of diverse **natural language understanding** (**NLU**) tasks
    designed to evaluate the performance of models across multiple linguistic challenges.
    The benchmark encompasses tasks such as sentiment analysis, question answering,
    and textual entailment, among others. It’s a widely recognized standard in the
    field of NLU, providing a comprehensive suite for comparing and improving language
    understanding models. It can be seen that its performance is better in all tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Average (in** **all tasks)** | **Sentiment analysis** | **Grammatical**
    | **Similarity** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BERT large | 82.1 | 94.9 | 60.5 | 86.5 |'
  prefs: []
  type: TYPE_TB
- en: '| BERT base | 79.6 | 93.5 | 52.1 | 85.8 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI GPT | 75.1 | 91.3 | 45.4 | 80.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Pre-open AI **State of the** **Art** (**STOA**) | 74.0 | 93.2 | 35.0 | 81.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bidirectional Long Short-Term memory (BiLSTM) + Embeddings from Language
    Model (ELMo) + Attention | 71.0 | 90.4 | 36.0 | 73.3 |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Comparing different models’ performance on GLUE (this comparison
    is based on 2018 when BERT and GPT were released)
  prefs: []
  type: TYPE_NORMAL
- en: Broad generalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs trained on diverse datasets can generalize better across different tasks,
    domains, or styles of language. They can effectively learn from the training data
    to identify and understand a wide range of linguistic patterns, styles, and topics.
    This broad generalization capability makes them versatile for various applications,
    from chatbots to content creation to information retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: When an LM is bigger, it means it has more parameters. These parameters allow
    the model to capture and encode more complex relationships and nuances within
    the data. In other words, a bigger model can learn and retain more information
    from the training data. As such, it is better equipped to handle a wider array
    of tasks and contexts post-training. It is this increased complexity and capacity
    that makes bigger LMs more generalizable across different tasks. As we can see
    in *Figure 7**.1*, the bigger LMs perform better in different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – LLMs performance based on their size and training](img/B18949_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – LLMs performance based on their size and training
  prefs: []
  type: TYPE_NORMAL
- en: We can also see the progress in the development of the LLMs within the last
    three years in *Figure 7**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.2 – The released LMs within 2019 \uFEFFto 2023  (the publicly available\
    \ models are highlighted) \uFEFF](img/B18949_07_2.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – The released LMs within 2019 to 2023 (the publicly available models
    are highlighted)
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that while larger models tend to be more generalizable,
    they also pose challenges such as increased computational requirements and the
    risk of overfitting. It is also essential to ensure that the training data is
    representative of the tasks and domains the model is expected to perform in, as
    models might carry over any biases present in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs such as GPT-3, GPT-3.5, and GPT-4 have demonstrated impressive few-shot
    learning capabilities. Given a few examples (the “shots”), these models can generalize
    to complete similar tasks effectively. This makes adjusting and deploying these
    models in real-world applications more efficient. The prompts can be designed
    to include information for the model to refer to, such as example questions and
    their respective answers.
  prefs: []
  type: TYPE_NORMAL
- en: The model temporarily learns from given examples and refers to given information
    as an additional source. For example, when the LLM is used as a personal assistant
    or advisor, background information about the user can be appended to the prompt,
    allowing the model to “get to know you,” as it uses your personal information
    prompts as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding complex contexts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have the advantage of understanding complex contexts due to their extensive
    training on a wide range of data, including various topics, literary styles, and
    nuances as well as their deep architecture and large parameter space. This capacity
    allows them to comprehend and generate appropriate responses even in complex or
    nuanced situations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a scenario where a user asks the model to summarize a
    complicated scientific article. An LLM can understand the context and the technical
    language used in the article and generate a coherent and simplified summary.
  prefs: []
  type: TYPE_NORMAL
- en: Multilingual capabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs can handle multiple languages effectively, making them suitable for global
    applications. Here are a few well-known multilingual LMs.
  prefs: []
  type: TYPE_NORMAL
- en: mBERT (multilingual BERT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An extension of BERT, mBERT is pretrained on the top 104 languages with the
    largest Wikipedia using a masked LM objective.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-lingual language model (XLM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is trained in 100 languages. It extends the BERT model to include several
    methods for cross-lingual model training.
  prefs: []
  type: TYPE_NORMAL
- en: XLM-RoBERTa
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XLM-RoBERTa extends RoBERTa, which itself is an optimized version of BERT, and
    is trained on a much larger multilingual corpus covering more languages.
  prefs: []
  type: TYPE_NORMAL
- en: MarianMT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Part of Hugging Face’s Transformers library, MarianMT is a state-of-the-art
    Transformer-based model optimized for translation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: DistilBERT Multilingual
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a smaller and faster version of mBERT, achieved through a distillation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: T2T (T5) Multilingual
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a variant of the **Text-to-Text Transfer Transformer** (**T5**) model,
    which is fine-tuned for translation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: These models have achieved significant results in a variety of tasks, such as
    translation, named entity recognition, part-of-speech tagging, and sentiment analysis
    in multiple languages.
  prefs: []
  type: TYPE_NORMAL
- en: Human-like text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs have shown a remarkable capability in generating human-like text. They
    can create contextually appropriate responses in conversations, write essays,
    and generate creative content such as poetry and stories. Models such as GPT-3,
    ChatGPT, and GPT-4 have shown good results in text generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: While the advantages are many, it’s important to note that there are also challenges
    and potential risks associated with the use of LLMs. They require significant
    computational resources to train and deploy, and there are ongoing concerns related
    to their potential to generate harmful or biased content, their interpretability,
    and their environmental impact. Researchers are actively working on ways to mitigate
    these issues while leveraging the powerful capabilities of these models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to these reasons, companies are trying to implement and train larger LMs
    (*Figure 7**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.3 – Newer LMs and their size, as well a\uFEFFs the developers](img/B18949_07_3.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Newer LMs and their size, as well as the developers
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in developing LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing LLMs poses a unique set of challenges, including but not limited
    to handling massive amounts of data, requiring vast computational resources, and
    the risk of introducing or perpetuating bias. The following subsections outline
    the detailed explanations of these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Amounts of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs require enormous amounts of data for training. As the model size grows,
    so does the need for diverse, high-quality training data. However, collecting
    and curating such large datasets is a challenging task. It can be time - consuming
    and expensive. There’s also the risk of inadvertently including sensitive or inappropriate
    data in the training set. To have more of an idea, BERT has been trained using
    3.3 billion words from Wikipedia and BookCorpus. GPT-2 has been trained on 40
    GB of text data, and GPT-3 has been trained on 570 GB of text data. *Table 7.2*
    shows the number of parameters and size of training data of a few recent LMs.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Parameters** | **Size of** **training data** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | 175 B | 300 billion tokens |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 | 175 B | 300 billion tokens |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM | 540 B | 780 billion tokens |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA | 65 B | 1.4 trillion tokens |'
  prefs: []
  type: TYPE_TB
- en: '| Bloom | 176 B | 366 billion tokens |'
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – Number of parameters and training data of a few recent LMs
  prefs: []
  type: TYPE_NORMAL
- en: Computational resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training LLMs requires substantial computational resources. These models often
    have billions or even trillions of parameters and need to process vast amounts
    of data during training, which requires high-performance hardware (such as GPUs
    or TPUs) and a significant amount of time. This can be costly and could limit
    the accessibility of developing such models to only those who have these resources.
    For example, training GPT-3 took 1 million GPU hours, which cost around 4.6 million
    dollars (in 2020). *Table 7.3* shows the computational resources and training
    time of a few recent LMs.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Hardware** | **Training time** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM | 6144 TPU v4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA | 2048 80G A100 | 21 days |'
  prefs: []
  type: TYPE_TB
- en: '| Bloom | 384 80G A100 | 105 days |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 | 1024x A100 | 34 days |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 25000 A100 | 90–100 days |'
  prefs: []
  type: TYPE_TB
- en: Table 7.3 – The hardware and training time of a few recent LMs
  prefs: []
  type: TYPE_NORMAL
- en: Risk of bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs can learn and perpetuate biases present in their training data. This could
    be explicit bias, such as racial or gender bias in the way language is used, or
    more subtle forms of bias, such as the underrepresentation of certain topics or
    perspectives. This issue can be challenging to address because bias in language
    is a deeply rooted societal issue, and it’s often not easy to even identify what
    might be considered bias in a given context.
  prefs: []
  type: TYPE_NORMAL
- en: Model robustness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s challenging to ensure that LLMs will perform well in all possible scenarios,
    particularly on inputs that differ from their training data. This includes dealing
    with ambiguous queries, handling out-of-distribution data, and ensuring a level
    of consistency in the responses. Making sure that the model is not overtrained
    can help to have a more robust model, but much more is needed to have a robust
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability and debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs, like most **deep learning** (**DL**) models, are often described as “black
    boxes.” It’s not easy to understand why they’re making a particular prediction
    or how they’re arriving at a conclusion. This makes debugging challenging if the
    model starts to produce incorrect or inappropriate outputs. Improving interpretability
    is an active area of research. For example, some libraries attempt to elucidate
    the decision-making process of an LM by employing techniques such as feature importance
    analysis, which involves removing some words and analyzing the change in gradients.
  prefs: []
  type: TYPE_NORMAL
- en: One such method is the input perturbation technique. In this approach, a word
    (or words) from the input text is perturbed or removed, and the change in the
    model’s output is analyzed. The rationale behind this is to understand the influence
    of a specific input word on the model’s output prediction. If the removal of a
    certain word significantly changes the model’s prediction, it can be inferred
    that the model deemed this word as important for its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing gradient changes is another popular method. By investigating how the
    gradient of the output with respect to the input changes when a certain word is
    removed, one can gain insight into how the model’s decision-making process is
    influenced by that specific word.
  prefs: []
  type: TYPE_NORMAL
- en: These interpretation techniques provide a more transparent view into the complex
    decision-making process of LLMs, enabling researchers to better understand and
    improve their models. Libraries such as LIME and SHAP offer tools for model interpretation
    tasks, thus making the process more accessible to researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Environmental impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The high computational resources needed for training LLMs can have significant
    environmental implications. The energy required for training these models can
    contribute to carbon emissions, which is a concern from a sustainability perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Besides that, there are concerns about privacy and security in LLMs. For example,
    it is recommended not to share models that are trained using patients’ medical
    information, or not to feed sensitive information into publicly available LLMs
    such as ChatGPT, since it can return it to other users as the answer to their
    questions.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are generally neural network architectures that are trained on a large
    corpus of text data. The term “large” refers to the size of these models in terms
    of the number of parameters and the scale of training data. Here are some examples
    of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformer models have been at the forefront of the recent wave of LLMs. They
    are based on the “Transformer” architecture, which uses self-attention mechanisms
    to weigh the relevance of different words in the input when making predictions.
    Transformers are a type of neural network architecture introduced in the paper
    *Attention is All You Need* by Vaswani et al. One of their significant advantages,
    particularly for training LLMs, is their suitability for parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional RNN models, such as LSTM and GRU, the sequence of tokens (words,
    subwords, or characters in the text) must be processed sequentially. That’s because
    each token’s representation depends not only on the token itself but also on the
    previous tokens in the sequence. The inherent sequential nature of these models
    makes it difficult to parallelize their operations, which can limit the speed
    and efficiency of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers, in contrast, eliminate the necessity for sequential processing
    by using a mechanism called self-attention (or scaled dot-product attention).
    In the self-attention process, each token’s representation is computed as a weighted
    sum of all tokens in the sequence, with the weights determined by the attention
    mechanism. Importantly, these computations for each token are independent of the
    computations for other tokens, and thus they can be performed in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: This parallelization capability brings several advantages for training LLMs
    as we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By parallelizing the computations, Transformers can process large amounts of
    data more quickly than RNNs. This speed can significantly reduce the training
    time of LLMs, which often need to process vast amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers’ parallelization makes it easier to scale up the model size and
    the amount of training data. This capability is crucial for developing LLMs, as
    these models often benefit from being trained on larger datasets and having a
    larger number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Long-range dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers can better capture long-range dependencies between tokens because
    they consider all tokens in the sequence simultaneously, rather than processing
    them one at a time. This capability is valuable in many language tasks and can
    improve the performance of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these models has its own strengths and weaknesses, and the best choice
    of model can depend on the specific task, the amount and type of available training
    data, and the computational resources available.
  prefs: []
  type: TYPE_NORMAL
- en: Example designs of state-of-the-art LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we are going to dig more into the design and architecture of some
    of the newest LLMs at the time of writing this book.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 and ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of ChatGPT is a Transformer, a type of model architecture that uses
    self-attention mechanisms to weigh the relevance of different words in the input
    when making predictions. It allows the model to consider the full context of the
    input when generating a response.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ChatGPT is based on the GPT version of the Transformer. The GPT models are trained
    to predict the next word in a sequence of words, given all the previous words.
    They process text from left to right (unidirectional context), which makes them
    well-suited for text generation tasks. For instance, GPT-3, one of the versions
    of GPT on which ChatGPT is based, contains 175 billion parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Two-step training process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training process for ChatGPT is done in two steps: pretraining and fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this step, the model is trained on a large corpus of publicly available text
    from the internet. However, it’s worth noting that it does not know specifics
    about which documents were in its training set or have access to any specific
    documents or sources.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After pretraining, the base model is further trained (fine-tuned) on custom
    datasets created by OpenAI, which include demonstrations of correct behavior as
    well as comparisons to rank different responses. Some prompts are from users of
    the Playground and ChatGPT apps, but they are anonymized and stripped of personally
    identifiable information.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Part of the fine-tuning process involves RLHF, where human AI trainers provide
    feedback on model outputs for a range of example inputs, and this feedback is
    used to improve the model’s responses. RLHF is a key component of the fine-tuning
    process used to train ChatGPT. It’s a technique for refining the performance of
    the model by learning from feedback provided by human evaluators. Here, we first
    explain the general idea of RLHF, and in the next section, we explain it step
    by step.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in RLHF is to collect human feedback. For ChatGPT, this often
    involves having human AI trainers participate in conversations where they play
    both sides (the user and the AI assistant). The trainers also have access to model-written
    suggestions to help them compose responses. This dialogue, in which AI trainers
    are essentially having a conversation with themselves, is added to the dataset
    for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the dialogues, comparison data is created where multiple model
    responses are ranked by quality. This is done by taking a conversation turn, generating
    several different completions (responses), and having human evaluators rank them.
    The evaluators don’t just rank the responses on factual correctness but also on
    how useful and safe they judged the response to be.
  prefs: []
  type: TYPE_NORMAL
- en: The model is then fine-tuned using **proximal policy optimization** (**PPO**),
    a reinforcement learning algorithm. PPO attempts to improve the model’s responses
    based on human feedback, making small adjustments to the model’s parameters to
    increase the likelihood of better-rated responses and decrease the likelihood
    of worse-rated responses.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF is an iterative process. The procedure of collecting human feedback, creating
    comparison data, and fine-tuning the model using PPO is repeated multiple times
    to incrementally improve the model. Next, we will explain in more detail how PPO
    works.
  prefs: []
  type: TYPE_NORMAL
- en: PPO is a reinforcement learning algorithm used to optimize the π policy of an
    agent. The policy defines how the agent selects actions based on its current state.
    PPO aims to optimize this policy to maximize the expected cumulative rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into PPO, it’s important to define the reward model. In the context
    of reinforcement learning, the reward model is a `R`(`s`, `a`) function, which
    assigns a reward value to every state-action pair (`s`, `a`). The goal of the
    agent is to learn a policy π that maximizes the expected sum of these rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the objective of reinforcement learning can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>J</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>R</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfenced></mml:math>](img/307.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, *E*π[.] is the expectation over trajectories (sequences of
    state-action pairs) generated by following policy *π*, *s*_t is the state at time
    *t*, *a*_t is the action taken at time *t*, and *R(s*_t*, a*_t*)* is the reward
    received at time *t*.
  prefs: []
  type: TYPE_NORMAL
- en: 'PPO modifies this objective to encourage exploration of the policy space while
    preventing too drastic changes in the policy at each update. This is done by introducing
    a ratio, *r*_t(θ), which represents the ratio of the probabilities of the current
    policy *π*_θ to the old policy *π_θ_old*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mi>r</mi><mi>t</mi></msub><mo>(</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo>(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>)</mo></mrow><mrow><msub><mi>π</mi><mrow><mi>θ</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo>(</mo><msub><mi>a</mi><mi>t</mi></msub><mo>|</mo><msub><mi>s</mi><mi>t</mi></msub><mo>)</mo></mrow></mfrac></mrow></mrow></mrow></math>](img/308.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The objective of PPO is then defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>J</mi><mrow><mi>P</mi><mi>P</mi><mi>O</mi></mrow></msub><mfenced
    open="(" close=")"><mi>π</mi></mfenced><mo>=</mo><msub><mi>E</mi><mrow><mi>π</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mfenced
    open="(" close=")"><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi
    mathvariant="normal">n</mi><mo>(</mo><msub><mi>r</mi><mi>t</mi></msub><mfenced
    open="(" close=")"><mi>θ</mi></mfenced><msub><mi>A</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi><mo>(</mo><msub><mi>r</mi><mi>t</mi></msub><mfenced
    open="(" close=")"><mi>θ</mi></mfenced><mo>,</mo><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo>,</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo>)</mo><msub><mi>A</mi><mi>t</mi></msub></mrow></mfenced></mrow></mrow></math>](img/309.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *A_t* is the advantage function that measures how much better the taking
    action *a_t* is compared to the average action at state *s_*t, and *clip(r_*t*(θ),
    1 - ε, 1 + ε)* is a clipped version of *r_*t*(θ)* that discourages too large policy
    updates.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm then optimizes this objective using stochastic gradient ascent,
    adjusting the policy parameters *θ* to increase *J_*PPO*(π)*.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of ChatGPT and RLHF, the states correspond to the conversation
    histories, the actions correspond to the model-generated messages, and the rewards
    correspond to the human feedback on these messages. PPO is used to adjust the
    model parameters to improve the quality of the generated messages as judged by
    the human feedback.
  prefs: []
  type: TYPE_NORMAL
- en: The human rankings are used to create a reward model, which quantifies how good
    each response is. The reward model is a function that takes in a state and an
    action (in this case, a conversation context and a model-generated message), and
    outputs a scalar reward. During training, the model tries to maximize its expected
    cumulative reward.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of RLHF is to align the model’s behavior with human values and to improve
    its ability to generate useful and safe responses. By learning from human feedback,
    ChatGPT can adapt to a wider range of conversational contexts and provide more
    appropriate and helpful responses. It’s worth noting that despite these efforts,
    the system might still make mistakes, and handling these errors and improving
    the RLHF process is an area of ongoing research.
  prefs: []
  type: TYPE_NORMAL
- en: Generating responses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When generating a response, ChatGPT takes as input a conversation history, which
    includes previous messages in the conversation along with the most recent user
    message and produces a model-generated message as output. The conversation history
    is tokenized and fed into the model, which generates a sequence of tokens in response,
    and these tokens are then detokenized to form the final output text.
  prefs: []
  type: TYPE_NORMAL
- en: System-level controls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenAI has also implemented some system-level controls to mitigate harmful or
    untruthful outputs from ChatGPT. This includes a Moderation API that warns or
    blocks certain types of unsafe content.
  prefs: []
  type: TYPE_NORMAL
- en: Step by step process of RLHF in ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since RLHF is an important part of ChatGPT and several other **State of the
    Art** (**SOTA**) models, understanding it better is useful to the you. In recent
    years, LMs have demonstrated remarkable abilities, creating varied and compelling
    text based on human-generated prompts. Nonetheless, it’s challenging to precisely
    define what constitutes “good” text as it is inherently subjective and depends
    on the context. For instance, while crafting stories demands creativity, informative
    pieces require accuracy, and code snippets need to be executable.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a loss function to encapsulate these attributes seems virtually impossible,
    hence most LMs are trained using a basic next-token prediction loss, such as cross-entropy.
    To overcome the limitations of the loss function, individuals have developed metrics
    that better align with human preferences, such as BLEU or **ROUGE**. The **BLEU**
    score, or Bilingual evaluation understudy, is a metric which is used to measure
    how well machine-translated text compares to a set of reference translations.
    Although these metrics are more effective at assessing performance, they are inherently
    limited as they merely compare the generated text to references using basic rules.
  prefs: []
  type: TYPE_NORMAL
- en: Wouldn’t it be transformative if we could use human feedback on generated text
    as a performance measure, or even better, as a loss to optimize the model? This
    is the concept behind RLHF – leveraging reinforcement learning techniques to directly
    optimize an LM using human feedback. RLHF has begun to enable LMs to align a model
    trained on a general text corpus with intricate human values.
  prefs: []
  type: TYPE_NORMAL
- en: One of the recent successful applications of RLHF has been in the development
    of ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of RLHF presents a formidable challenge due to its multifaceted
    model training process and various deployment phases. Here, we’ll dissect the
    training procedure into its three essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: Initial pretraining of an LM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection and reward model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refining the LM using reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll begin by examining the pretraining phase for LMs.
  prefs: []
  type: TYPE_NORMAL
- en: LM pretraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a foundation, RLHF utilizes an LM that’s already been pretrained using traditional
    pretraining objectives, which means that we create the tokenizer based on our
    training data, design model architecture, and then pretrain the model using the
    training data. For its initial well-received RLHF model, InstructGPT, OpenAI employed
    a smaller version of GPT-3\. On the other hand, Anthropic used transformer models
    ranging from 10 million to 52 billion parameters trained for this task, and DeepMind
    utilized its 280 billion parameter model, Gopher.
  prefs: []
  type: TYPE_NORMAL
- en: This preliminary model may be further refined on extra text or particular conditions,
    although it’s not always necessary. As an example, OpenAI chose to refine its
    model using human-generated text identified as “preferable.” This dataset is used
    to further fine-tune the model using the RLHF model, distilling the original LM
    model based on contextual hints from humans.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, there’s no definitive answer to the question of “which model”
    serves as the best launching point for RLHF. The array of options available for
    RLHF training has not been extensively explored.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, once an LM is in place, it’s necessary to generate data to train
    a reward model. This step is crucial for integrating human preferences into the
    system.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figur\uFEFFe 7.4 – Pretraining LM](img/B18949_07_4.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Pretraining LM
  prefs: []
  type: TYPE_NORMAL
- en: Training the reward model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the newly proposed method, RLHF is being used as the RM, which is known as
    a preference model as well. The main idea here is to get a text and return a scalar
    reward that reflects human preferences. This approach can be implemented in two
    ways. First, implement an end-to-end LLM, which gives us the preferred output.
    This process can be performed by fine-tuning a LLM or training a LLM from scratch.
    Second, have an extra component that ranks different outputs of the LLM and returns
    the best one.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used for training the RM is a set of prompt-generation pairs. Prompts
    are sampled from a predetermined dataset (Anthropic’s data). These prompts undergo
    processing by the initial LM to generate fresh text.
  prefs: []
  type: TYPE_NORMAL
- en: Human annotators rank the text outputs generated by the LM. It might seem intuitive
    to have humans directly assign a scalar score to each text piece to generate a
    reward model, but it proves challenging in reality. Varied human values render
    these scores unstandardized and unreliable. Consequently, rankings are employed
    to compare multiple model outputs, thereby creating a substantially better regularized
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There are several strategies for text ranking. One successful approach involves
    users comparing the text produced by two LMs given the same prompt. By evaluating
    model outputs in direct comparison, an **Elo rating system**, which we will soon
    describe, can generate a ranking of models and outputs relative to each other.
    These varying ranking methods are then normalized into a scalar reward signal
    for training. The Elo rating system, originally developed for chess, is also applicable
    to RLHF for LMs.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LMs, each model or variant (e.g., models at different stages
    of training) can be seen as a “player.” Its Elo rating reflects how well it performs
    in terms of generating human-preferred outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fundamental mechanics of the Elo rating system remain the same. Here’s
    how it can be adapted for RLHF in LMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: All models start with the same Elo rating, often 1,000
    or 1,500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparison**: For a given prompt, two models (A and B) generate their outputs.
    A human evaluator then ranks these outputs. If the evaluator considers the output
    from model A to be better, model A “wins” the match, and model B “loses.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Elo ratings are updated in this way after each evaluation. Over time, they
    provide an ongoing, dynamic ranking of the models based on human preferences.
    This is useful for tracking progress over the course of training and for comparing
    different models or model variants.
  prefs: []
  type: TYPE_NORMAL
- en: Successful RLHF systems have employed diverse-sized reward LMs relative to text
    generation. For example, OpenAI used a 175 B LM with a 6 B reward model, Anthropic
    utilized LM and reward models ranging from 10 B to 52 B, and DeepMind employed
    70 B Chinchilla models for both the LM and reward model. This is because preference
    models must match the capacity needed to understand a text as a model would need
    to generate it. At this juncture in the RLHF process, we possess an initial LM
    capable of text generation and a preference model that assigns a score to any
    text based on human perception. We next apply reinforcement learning to optimize
    the original LM concerning the reward model.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.5 – The reward model f\uFEFFor reinforcement learning](img/B18949_07_5.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – The reward model for reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: How to fine-tune the model using reinforcement learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a considerable period, the prospect of training an LM using reinforcement
    learning was considered unattainable due to both technical and algorithmic challenges.
    However, several organizations have achieved fine-tuning some or all parameters
    of a replica of the initial LM with a policy-gradient reinforcement learning algorithm
    – namely, PPO. Parameters of the LM are kept static because fine-tuning an entire
    model with 10 B or 100 B+ parameters is prohibitively expensive (for further details,
    refer to **Low-Rank Adaptation** (**LoRA**) for LMs or DeepMind’s Sparrow LM).
    PPO, an established method for some time now, has abundant available guides explaining
    its functioning. This maturity made it an attractive choice for scaling up to
    the novel application of distributed training for RLHF. It appears that significant
    strides in RLHF have been made by determining how to update such a colossal model
    with a known algorithm (more on that later).
  prefs: []
  type: TYPE_NORMAL
- en: We can articulate this fine-tuning task as a reinforcement learning problem.
    Initially, the policy is an LM that accepts a prompt and produces a sequence of
    text (or merely probability distributions over text). The action space of this
    policy is all the tokens aligning with the LM’s vocabulary (typically around 50
    K tokens), and the observation space is the distribution of possible input token
    sequences, which is also notably large in light of reinforcement learning’s prior
    uses (the dimension approximates the vocabulary size power (`^`) length of the
    input token sequence). The reward function melds the preference model with a constraint
    on policy shift.
  prefs: []
  type: TYPE_NORMAL
- en: The reward function is the juncture where the system integrates all the models
    discussed into a single RLHF process. Given a prompt, *x*, from the dataset, the
    text, *y*, is created by the current iteration of the fine-tuned policy. This
    text, coupled with the original prompt, is passed to the preference model, which
    returns a scalar measure of “preferability”,![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi>r</mi><mi>θ</mi></msub></mrow></math>](img/310.png)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, per-token probability distributions from the reinforcement learning
    policy are contrasted with those from the initial model to compute a penalty on
    their difference. In several papers from OpenAI, Anthropic, and DeepMind, this
    penalty has been constructed as a scaled version of the **Kullback–Leibler** (**KL**)
    divergence between these sequences of distributions over tokens, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub></mml:math>](img/311.png)
    . The KL divergence term penalizes the reinforcement learning policy from veering
    significantly from the initial pretrained model with each training batch, ensuring
    the production of reasonably coherent text snippets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without this penalty, the optimization might start generating gibberish text
    that somehow deceives the reward model into granting a high reward. In practical
    terms, the KL divergence is approximated via sampling from both distributions.
    The final reward transmitted to the reinforcement learning update rule is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>θ</mi></msub><mo>−</mo><mi>λ</mi><msub><mi>r</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></mrow></math>](img/312.png)'
  prefs: []
  type: TYPE_IMG
- en: Additional terms have been incorporated into the reward function by some RLHF
    systems. For instance, OpenAI’s InstructGPT successfully tried the blending of
    additional pretraining gradients (from the human annotation set) into the update
    rule for PPO. It is anticipated that as RLHF continues to be studied, the formulation
    of this reward function will continue to evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the update rule is the parameter update from PPO that optimizes the
    reward metrics in the current data batch (PPO is on-policy, meaning the parameters
    are only updated with the current batch of prompt-generation pairs). PPO is a
    trust region optimization algorithm that employs constraints on the gradient to
    ensure the update step does not destabilize the learning process. DeepMind utilized
    a similar reward setup for Gopher but employed a synchronous advantage actor.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Fine-tuning the model using reinforcement learning](img/B18949_07_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Fine-tuning the model using reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram may suggest that both models produce different responses
    for the same prompt, but what actually occurs is that the reinforcement learning
    policy generates text, which is then supplied to the initial model to derive its
    relative probabilities for the KL penalty.
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, RLHF can advance from this stage by cyclically updating both the
    reward model and the policy. As the reinforcement learning policy evolves, users
    can maintain the ranking of these outputs against the model’s previous versions.
    However, most papers haven’t yet addressed the implementation of this operation
    since the mode of deployment required to collect this type of data only works
    for dialogue agents who can access an active user base. Anthropic mentions this
    alternative as **iterated online RLHF** (as referred to in the original paper),
    where iterations of the policy are incorporated into the Elo ranking system across
    models. This brings about complex dynamics of the policy and reward model evolving,
    representing a complex and unresolved research question. In the next section,
    we will explain some well-known open-source tools for RLHF.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the time of writing this book, we know very little about the GPT-4 model
    design. As OpenAI is slow to reveal, it is assumed that GPT-4 is not a single
    model but a combination of eight 220-billion-parameter models, an assumption that
    is confirmed by key figures in the AI community. This assumption suggests OpenAI
    used a “mixture of experts” strategy, an ML design tactic that dates even before
    LLMs, to create the model. However, while we, the authors, support this assumption,
    it has not been officially confirmed by OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the speculation, GPT-4’s impressive performance is undeniable, regardless
    of its internal structure. Its capabilities in writing and coding tasks are remarkable,
    and the specifics of whether it’s one model or eight bundled together does not
    change its impact.
  prefs: []
  type: TYPE_NORMAL
- en: A common narrative suggests that OpenAI has managed expectations around GPT-4
    deftly, focusing on its power and opting not to disclose specifications due to
    competitive pressures. The secrecy surrounding GPT-4 has led many to believe it
    to be a scientific marvel.
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Meta has publicly launched LLaMA, a high-performing LLM aimed at aiding researchers
    in AI. This move allows individuals with limited access to extensive infrastructure
    to examine these models, thus broadening access in this rapidly evolving field.
  prefs: []
  type: TYPE_NORMAL
- en: LLaMA models are attractive because they require significantly less computational
    power and resources, allowing for the exploration of new approaches and use cases.
    Available in several sizes, these models are designed to be fine-tuned for various
    tasks and have been developed with responsible AI practices.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, despite their advancements, have limited research accessibility due to
    the resources required to train and run them. Smaller models, such as LLaMA, trained
    on more tokens, are simpler to retrain and adjust for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to other models, LLaMA takes a sequence of words as input to predict
    the next word and generate text. Despite its capabilities, LLaMA shares the same
    challenges as other models regarding bias, toxic comments, and hallucinations.
    By sharing LLaMA’s code, Meta enables researchers to test new ways of addressing
    these issues in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Meta emphasizes the need for cooperation across the AI community to establish
    guidelines around responsible AI and LLMs. They anticipate that LLaMA will facilitate
    new learning and development in the field.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PaLM is a 540-billion-parameter, densely-activated Transformer LM that was trained
    on 6,144 TPU v4 chips using Pathways, a new ML system, that enables highly efficient
    training across multiple TPU pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'PaLM has been shown to achieve breakthrough performance on a variety of natural
    language tasks, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step reasoning tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recently released **Beyond the Imitation Game** **Benchmark** (**BIG-bench**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilingual tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source code generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BIG-bench benchmark is worth expanding on as it serves as a recognized collection
    of benchmarks to measure against. The BIG-bench is an extensive assessment mechanism
    specifically designed for large-scale LMs. It is a broad-based, community-focused
    benchmark that presents a diversity of tasks to evaluate a model’s performance
    in different disciplines and its competence in natural language comprehension,
    problem solving, and reasoning. With a total of 204 tasks from 450 contributors
    across 132 institutions, BIG-bench covers an eclectic mix of subjects including
    linguistics, childhood development, mathematics, common-sense reasoning, biology,
    physics, software development, and even social bias. It concentrates on challenges
    believed to be currently beyond the reach of existing LMs. The primary goal of
    BIG-bench extends beyond mere imitation or Turing test-style evaluations, aiming
    instead for a deeper, more nuanced appraisal of the abilities and constraints
    of these large models. This initiative is founded on the conviction that an open,
    collaborative approach to evaluation paves the way for a more comprehensive understanding
    of these LMs and their potential societal ramifications.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM 540B excels beyond the fine-tuned state-of-the-art across various multi-step
    reasoning tasks and surpasses average human performance on the BIG-bench benchmark.
    Many BIG-bench tasks exhibit significant leaps in performance as PaLM scales to
    its largest size, demonstrating discontinuous improvements from the model scale.
    PaLM also has strong capabilities in multilingual tasks and source code generation.
    For example, PaLM can translate between 50 languages, and it can generate code
    in a variety of programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the PaLM paper also discuss the ethical considerations related
    to LLMs, and they discuss potential mitigation strategies. For example, they suggest
    that it is important to be aware of the potential for bias in LLMs and that it
    is important to develop techniques for detecting and mitigating bias.
  prefs: []
  type: TYPE_NORMAL
- en: PaLM architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PaLM employs the conventional Transformer model architecture in a decoder-exclusive
    setup, which allows each timestep to attend only to itself and preceding timesteps.
    Several modifications were applied to this setup, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SwiGLU activation**: Instead of standard ReLU, GeLU, or Swish activations,
    PaLM utilizes SwiGLU activations (*Swish(xW) · xV*) for the **multilayer perceptron**
    (**MLP**) intermediate activations due to their superior performance in enhancing
    quality. This approach, however, requires three matrix multiplications in the
    MLP as opposed to the conventional two.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallel layers**: Rather than the typical “serialized” approach, PaLM uses
    a “parallel” formulation for each Transformer block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The standard structure is given by the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo>(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/313.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The parallel structure is instead the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo>(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/314.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: This leads to roughly 15% quicker training speed at larger scales due to the
    fusion of MLP and attention input matrix multiplications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Multi-query attention**: In the conventional Transformer formulation, *k*
    attention heads are employed. For each timestep, the input vector is linearly
    projected into query, key, and value tensors, which have a shape of [*k, h*]*,*
    where *h* denotes the size of the attention head. In the new approach, the projections
    for “key” and “value” are shared across all heads, meaning “key” and “value” are
    projected to [*1, h*], while “query” maintains the shape [*k, h*]. The authors
    claimed that this approach doesn’t notably affect model quality or training speed
    while it does result in significant cost reductions during autoregressive decoding.
    The reason for this lies in the inefficiency of standard multi-headed attention
    on accelerator hardware during autoregressive decoding, as the key/value tensors
    are not shared across examples and only one token is decoded at each moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rotary Position Embedding (RoPE) embeddings**: RoPE embeddings, shown to
    perform better on longer sequence lengths, are preferred over absolute or relative
    position embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shared input-output embeddings**: The input and output embedding matrices
    are shared, a practice that is common, though not universal, in previous work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No biases**: The model abstains from using biases in any dense kernels or
    layer norms, which enhances training stability for larger models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vocabulary**: PaLM uses a 256k-token SentencePiece vocabulary designed for
    diverse languages in the training corpus, ensuring efficient training without
    excessive tokenization. This preserves all whitespaces and out-of-vocabulary Unicode
    characters while splitting numbers into individual digit tokens for clarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, PaLM is a powerful LM that has the potential to be used for a wide
    variety of applications. It is still under development, but it has already demonstrated
    the ability to achieve breakthrough performance on a number of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Open-source tools for RLHF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenAI released the first open-source code to perform RLHF in 2019\. They have
    implemented this approach to improve GPT-2 for different use cases such as summarization.
    Based on human feedback, the model is optimized to have outputs similar to humans,
    such as copying parts of the note. More information about this project can be
    found at the following link: [https://openai.com/research/fine-tuning-gpt-2](https://openai.com/research/fine-tuning-gpt-2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is also available at the following link: [https://github.com/openai/lm-human-preferences](https://github.com/openai/lm-human-preferences).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers Reinforcement Learning** (**TRL**) is a tool crafted for fine-tuning
    pretrained LMs using PPO within the Hugging Face ecosystem. TRLX, an enhanced
    fork developed by CarperAI, is capable of handling larger models for both online
    and offline training. Currently, TRLX is equipped with a production-ready API
    supporting RLHF with PPO and **implicit language Q-learning** (**ILQL**) for deploying
    LLMs of up to 33 billion parameters. Future versions of TRLX aim to accommodate
    LMs of up to 200 billion parameters, making it ideal for ML engineers working
    at such scales.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for TRL is available at the following link: [https://github.com/lvwerra/trl](https://github.com/lvwerra/trl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for TRLX can be found at the following link: [https://github.com/CarperAI/trlx](https://github.com/CarperAI/trlx).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another good library is **Reinforcement Learning for Language Models** (**RL4LMs**).
    The RL4LMs project addresses the challenge of training LLMs to align with human
    preference metrics. It recognizes that many NLP tasks can be seen as sequence
    learning problems, but their application is limited due to issues such as reinforcement
    learning training instability, high variance in automated NLP metrics, and reward
    hacking. The project offers solutions by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Giving guidelines on when to use reinforcement learning and suggesting suitable
    NLP tasks/metrics via a continually updated benchmark called GRUE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing a new reinforcement learning algorithm, **Natural Language Policy
    Optimization** (**NLPO**), designed to handle large language action spaces and
    reward variance better
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offering practical advice with high-quality implementations and hyperparameters
    of reinforcement learning, as well as other reinforcement learning algorithms,
    for training Transformers in the Hugging Face library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this project can be found at the following link: [https://github.com/allenai/RL4LMs](https://github.com/allenai/RL4LMs).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve delved into the dynamic and complex world of state-of-the-art
    LLMs. We’ve discussed their remarkable generalization capabilities, making them
    versatile tools for a wide range of tasks. We also highlighted the crucial aspect
    of understanding complex contexts, where these models excel by grasping the nuances
    of language and the intricacies of various subject matters.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we explored the paradigm of RLHF and how it is being employed
    to enhance LMs. RLHF leverages scalar feedback to improve LMs by mimicking human
    judgments, thereby helping to mitigate some of the common pitfalls encountered
    in NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed technical requirements for working with these models, emphasizing
    the need for foundational knowledge in areas such as Transformers, reinforcement
    learning, and coding skills.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also touched on some prominent LMs such as GPT-4 and LLaMA, discussing
    their architecture, methods, and performance. We highlighted the strategies some
    libraries employ to interpret LM predictions, such as the removal of certain words
    and analyzing gradient changes.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, this chapter offers a comprehensive overview of the current state
    of LLMs, exploring their capabilities, challenges, the methods used to refine
    them, and the evolving tools and measures for their evaluation and interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Hugging* *Face*: [huggingface.co](http://huggingface.co)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Large language* *model*: [https://en.m.wikipedia.org/wiki/Large_language_model#](https://en.m.wikipedia.org/wiki/Large_language_model#)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zhao, Wayne Xin, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
    Yingqian Min et al. “A survey of large language models.”* arXiv preprint arXiv:2303.18223
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introducing LLaMA: A foundational, 65-billion-parameter large language* *model*:
    [https://ai.facebook.com/blog/large-language-model-llama-meta-ai/](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model* *Details*: [https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière et al. “Llama: Open and efficient
    foundation language models.” arXiv* preprint arXiv:2302.13971 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Elo rating* *system*: [https://en.wikipedia.org/wiki/Elo_rating_system](https://en.wikipedia.org/wiki/Elo_rating_system)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
    Adam Roberts, Paul Barham et al. “Palm: Scaling language modeling with pathways.”*
    arXiv preprint arXiv:2204.02311 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*BIG-bench*: [https://github.com/google/BIG-bench](https://github.com/google/BIG-bench)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Srivastava, Aarohi, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar
    Abid, Adam Fisch, Adam R. Brown et al. “Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models.”* arXiv preprint arXiv:2206.04615
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
