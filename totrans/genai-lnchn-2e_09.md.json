["```py\nfrom langchain.evaluation import load_evaluator, ExactMatchStringEvaluator\nprompt = \"What is the current Federal Reserve interest rate?\"\nreference_answer = \"0.25%\" # Suppose this is the correct answer.\n# Example predictions from your LLM:\nprediction_correct = \"0.25%\"\nprediction_incorrect = \"0.50%\"\n# Initialize an Exact Match evaluator that ignores case differences.\nexact_evaluator = ExactMatchStringEvaluator(ignore_case=True)\n# Evaluate the correct prediction.\nexact_result_correct = exact_evaluator.evaluate_strings(\n    prediction=prediction_correct, reference=reference_answer\n)\nprint(\"Exact match result (correct answer):\", exact_result_correct)\n# Expected output: score of 1 (or 'Y') indicating a perfect match.\n# Evaluate an incorrect prediction.\nexact_result_incorrect = exact_evaluator.evaluate_strings(\n    prediction=prediction_incorrect, reference=reference_answer\n)\nprint(\"Exact match result (incorrect answer):\", exact_result_incorrect)\n# Expected output: score of 0 (or 'N') indicating a mismatch.\n```", "```py\nfrom langchain_mistralai import ChatMistralAI\nfrom langchain.evaluation.scoring import ScoreStringEvalChain\n```", "```py\n# Initialize the evaluator LLM\nllm = ChatMistralAI(\n    model=\"mistral-large-latest\",\n    temperature=0,\n    max_retries=2\n)\n# Create the ScoreStringEvalChain from the LLM\nchain = ScoreStringEvalChain.from_llm(llm=llm)\n# Define the finance-related input, prediction, and reference answer\nfinance_input = \"What is the current Federal Reserve interest rate?\"\nfinance_prediction = \"The current interest rate is 0.25%.\"\nfinance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n# Evaluate the prediction using the scoring chain\nresult_finance = chain.evaluate_strings(\n input=finance_input,\n    prediction=finance_prediction,\n)\nprint(\"Finance Evaluation Result:\")\nprint(result_finance)\n```", "```py\nFinance Evaluation Result:\n{'reasoning': \"The assistant's response is not verifiable as it does not provide a date or source for the information. The Federal Reserve interest rate changes over time and is not static. Therefore, without a specific date or source, the information provided could be incorrect. The assistant should have advised the user to check the Federal Reserve's official website or a reliable financial news source for the most current rate. The response lacks depth and accuracy. Rating: [[3]]\", 'score': 3}\n```", "```py\nfrom langchain_mistralai import ChatMistralAI\nfrom langchain.evaluation.scoring import LabeledScoreStringEvalChain\n# Initialize the evaluator LLM with deterministic output (temperature 0.)\nllm = ChatMistralAI(\n    model=\"mistral-large-latest\",\n    temperature=0,\n    max_retries=2\n)\n# Create the evaluation chain that can use reference answers\nlabeled_chain = LabeledScoreStringEvalChain.from_llm(llm=llm)\n# Define the finance-related input, prediction, and reference answer\nfinance_input = \"What is the current Federal Reserve interest rate?\"\nfinance_prediction = \"The current interest rate is 0.25%.\"\nfinance_reference = \"The Federal Reserve's current interest rate is 0.25%.\"\n# Evaluate the prediction against the reference\nlabeled_result = labeled_chain.evaluate_strings(\n input=finance_input,\n    prediction=finance_prediction,\n    reference=finance_reference,\n)\nprint(\"Finance Evaluation Result (with reference):\")\nprint(labeled_result)\n```", "```py\n{'reasoning': \"The assistant's response is helpful, relevant, and correct. It directly answers the user's question about the current Federal Reserve interest rate. However, it lacks depth as it does not provide any additional information or context about the interest rate, such as how it is determined or what it means for the economy. Rating: [[8]]\", 'score': 8}\n```", "```py\nfrom langchain.evaluation import load_evaluator\nfrom langchain.chat_models import ChatOpenAI\nevaluation_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n```", "```py\nprompt_health = \"What is a healthy blood pressure range for adults?\"\n# A sample LLM output from your healthcare assistant:\nprediction_health = (\n    \"A normal blood pressure reading is typically around 120/80 mmHg. \"\n    \"It's important to follow your doctor's advice for personal health management!\"\n)\n```", "```py\nconciseness_evaluator = load_evaluator(\n \"criteria\", criteria=\"conciseness\", llm=evaluation_llm\n)\nconciseness_result = conciseness_evaluator.evaluate_strings(\n    prediction=prediction_health, input=prompt_health\n)\nprint(\"Conciseness evaluation result:\", conciseness_result)\n```", "```py\nConciseness evaluation result: {'reasoning': \"The criterion is conciseness. This means the submission should be brief, to the point, and not contain unnecessary information.\\n\\nLooking at the submission, it provides a direct answer to the question, stating that a normal blood pressure reading is around 120/80 mmHg. This is a concise answer to the question.\\n\\nThe submission also includes an additional sentence advising to follow a doctor's advice for personal health management. While this information is not directly related to the question, it is still relevant and does not detract from the conciseness of the answer.\\n\\nTherefore, the submission meets the criterion of conciseness.\\n\\nY\", 'value': 'Y', 'score': 1}\n```", "```py\ncustom_friendliness = {\n \"friendliness\": \"Is the response written in a friendly and approachable tone?\"\n}\n# Load a criteria evaluator with this custom criterion.\nfriendliness_evaluator = load_evaluator(\n```", "```py\n \"criteria\", criteria=custom_friendliness, llm=evaluation_llm\n)\nfriendliness_result = friendliness_evaluator.evaluate_strings(\n    prediction=prediction_health, input=prompt_health\n)\nprint(\"Friendliness evaluation result:\", friendliness_result)\n```", "```py\nFriendliness evaluation result: {'reasoning': \"The criterion is to assess whether the response is written in a friendly and approachable tone. The submission provides the information in a straightforward manner and ends with a suggestion to follow doctor's advice for personal health management. This suggestion can be seen as a friendly advice, showing concern for the reader's health. Therefore, the submission can be considered as written in a friendly and approachable tone.\\n\\nY\", 'value': 'Y', 'score': 1}\n```", "```py\nfrom langchain.evaluation import JsonValidityEvaluator\n# Initialize the JSON validity evaluator.\n```", "```py\njson_validator = JsonValidityEvaluator()\nvalid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000}'\ninvalid_json_output = '{\"company\": \"Acme Corp\", \"revenue\": 1000000, \"profit\": 200000,}'\n# Evaluate the valid JSON.\nvalid_result = json_validator.evaluate_strings(prediction=valid_json_output)\nprint(\"JSON validity result (valid):\", valid_result)\n# Evaluate the invalid JSON.\ninvalid_result = json_validator.evaluate_strings(prediction=invalid_json_output)\nprint(\"JSON validity result (invalid):\", invalid_result)\n```", "```py\nJSON validity result (valid): {'score': 1}\n```", "```py\nJSON validity result (invalid): {'score': 0, 'reasoning': 'Expecting property name enclosed in double quotes: line 1 column 63 (char 62)'}\n```", "```py\nfrom langsmith import Client\n# Custom trajectory subsequence evaluator\ndef trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:\n \"\"\"Check how many of the desired steps the agent took.\"\"\"\n if len(reference_outputs['trajectory']) > len(outputs['trajectory']):\n return False\n\n    i = j = 0\n while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']):\n if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:\n            i += 1\n        j += 1\n\n return i / len(reference_outputs['trajectory'])\n# Create example dataset with expected trajectories\nclient = Client()\ntrajectory_dataset = client.create_dataset(\n \"Healthcare Agent Trajectory Evaluation\",\n    description=\"Evaluates agent trajectory for medication queries\"\n)\n# Add example with expected trajectory\n```", "```py\nclient.create_example(\n    inputs={\n \"question\": \"What is the recommended dosage of ibuprofen for an adult?\"\n    },\n    outputs={\n \"trajectory\": [\n \"intent_classifier\",\n \"healthcare_agent\",\n \"MedicalDatabaseSearch\",\n \"format_response\"\n        ],\n \"response\": \"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day.\"\n    },\n    dataset_id=trajectory_dataset.id\n)\n```", "```py\n# Function to run graph with trajectory tracking (example implementation)\nasync def run_graph_with_trajectory(inputs: dict) -> dict:\n \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n    trajectory = []\n    final_response = \"\"\n\n # Here you would implement your actual graph execution\n # For the example, we'll just return a sample result\n    trajectory = [\"intent_classifier\", \"healthcare_agent\", \"MedicalDatabaseSearch\", \"format_response\"]\n```", "```py\n    final_response = \"Typically, 200-400mg every 4-6 hours, not exceeding 3200mg per day.\"\n return {\n \"trajectory\": trajectory,\n \"response\": final_response\n    }\n# Note: This is an async function, so in a notebook you'd need to use await\nexperiment_results = await client.aevaluate(\n    run_graph_with_trajectory,\n    data=trajectory_dataset.id,\n    evaluators=[trajectory_subsequence],\n    experiment_prefix=\"healthcare-agent-trajectory\",\n    num_repetitions=1,\n    max_concurrency=4,\n)\n```", "```py\nresults_df = experiment_results.to_pandas()\nprint(f\"Average trajectory match score: {results_df['feedback.trajectory_subsequence'].mean()}\")\n```", "```py\nfrom langchain.evaluation import load_evaluator\n# Simulated chain-of-thought reasoning provided by the agent:\nagent_reasoning = (\n \"The current interest rate is 0.25%. I determined this by recalling that recent monetary policies have aimed \"\n \"to stimulate economic growth by keeping borrowing costs low. A rate of 0.25% is consistent with the ongoing \"\n \"trend of low rates, which encourages consumer spending and business investment.\"\n)\n```", "```py\n# Expected reasoning reference:\nexpected_reasoning = (\n \"An ideal reasoning should mention that the Federal Reserve has maintained a low interest rate—around 0.25%—to \"\n \"support economic growth, and it should briefly explain the implications for borrowing costs and consumer spending.\"\n)\n# Load the chain-of-thought evaluator.\ncot_evaluator = load_evaluator(\"cot_qa\")\nresult_reasoning = cot_evaluator.evaluate_strings(\n input=\"What is the current Federal Reserve interest rate and why does it matter?\",\n    prediction=agent_reasoning,\n    reference=expected_reasoning,\n)\nprint(\"\\nChain-of-Thought Reasoning Evaluation:\")\nprint(result_reasoning)\n```", "```py\nChain-of-Thought Reasoning Evaluation:\n{'reasoning': \"The student correctly identified the current Federal Reserve interest rate as 0.25%. They also correctly explained why this rate matters, stating that it is intended to stimulate economic growth by keeping borrowing costs low, which in turn encourages consumer spending and business investment. This explanation aligns with the context provided, which asked for a brief explanation of the implications for borrowing costs and consumer spending. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", 'value': 'CORRECT', 'score': 1}\n```", "```py\n# Define structured examples with queries, reference answers, and contexts\nfinancial_examples = [\n    {\n \"inputs\": {\n \"question\": \"What are the tax implications of early 401(k) withdrawal?\",\n \"context_needed\": [\"retirement\", \"taxation\", \"penalties\"]\n        },\n \"outputs\": {\n \"answer\": \"Early withdrawals from a 401(k) typically incur a 10% penalty if you're under 59½ years old, in addition to regular income taxes. However, certain hardship withdrawals may qualify for penalty exemptions.\",\n \"key_points\": [\"10% penalty\", \"income tax\", \"hardship exemptions\"],\n \"documents\": [\"IRS publication 575\", \"Retirement plan guidelines\"]\n        }\n    },\n    {\n \"inputs\": {\n \"question\": \"How does dollar-cost averaging compare to lump-sum investing?\",\n \"context_needed\": [\"investment strategy\", \"risk management\", \"market timing\"]\n        },\n \"outputs\": {\n \"answer\": \"Dollar-cost averaging spreads investments over time to reduce timing risk, while lump-sum investing typically outperforms in rising markets due to longer market exposure. DCA may provide psychological benefits through reduced volatility exposure.\",\n \"key_points\": [\"timing risk\", \"market exposure\", \"psychological benefits\"],\n```", "```py\n \"documents\": [\"Investment strategy comparisons\", \"Market timing research\"]\n        }\n    },\n # Additional examples would be added here\n]\n```", "```py\n# Basic LangSmith Integration Example\nimport os\n# Set up environment variables for LangSmith tracing\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"LLM Evaluation Example\"\nprint(\"Setting up LangSmith tracing...\")\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langsmith import Client\n# Create a simple LLM call that will be traced in LangSmith\nllm = ChatOpenAI()\nresponse = llm.invoke(\"Hello, world!\")\nprint(f\"Model response: {response.content}\")\nprint(\"\\nThis run has been logged to LangSmith.\")\n```", "```py\nfrom langsmith import Client\nclient = Client()\n# Create dataset in LangSmith\ndataset_name = \"Financial Advisory RAG Evaluation\"\ndataset = client.create_dataset(\n    dataset_name=dataset_name,\n    description=\"Evaluation dataset for financial advisory RAG systems covering retirement, investments, and tax planning.\"\n)\n# Add examples to the dataset\nfor example in financial_examples:\n    client.create_example(\n        inputs=example[\"inputs\"],\n        outputs=example[\"outputs\"],\n        dataset_id=dataset.id\n    )\nprint(f\"Created evaluation dataset with {len(financial_examples)} examples\")\n```", "```py\ndef construct_chain():\n return None\n```", "```py\nfrom langchain.smith import RunEvalConfig\n# Define evaluation criteria specific to RAG systems\nevaluation_config = RunEvalConfig(\n    evaluators=[\n # Correctness: Compare response to reference answer\n        RunEvalConfig.LLM(\n            criteria={\n \"factual_accuracy\": \"Does the response contain only factually correct information consistent with the reference answer?\"\n            }\n        ),\n # Groundedness: Ensure response is supported by retrieved context\n        RunEvalConfig.LLM(\n            criteria={\n \"groundedness\": \"Is the response fully supported by the retrieved documents without introducing unsupported information?\"\n            }\n        ),\n # Retrieval quality: Assess relevance of retrieved documents\n        RunEvalConfig.LLM(\n            criteria={\n```", "```py\n \"retrieval_relevance\": \"Are the retrieved documents relevant to answering the question?\"\n            }\n        )\n    ]\n)\n```", "```py\nfrom langchain.smith import run_on_dataset\nresults = run_on_dataset(\n    client=client,\n    dataset_name=dataset_name,\n    dataset=dataset,\n    llm_or_chain_factory=construct_chain,\n    evaluation=evaluation_config\n)\n```", "```py\nfrom datasets import load_dataset\nfrom evaluate import load\nfrom langchain_core.messages import HumanMessage\nhuman_eval = load_dataset(\"openai_humaneval\", split=\"test\")\ncode_eval_metric = load(\"code_eval\")\ntest_cases = [\"assert add(2,3)==5\"]\ncandidates = [[\"def add(a,b): return a*b\", \"def add(a, b): return a+b\"]]\npass_at_k, results = code_eval_metric.compute(references=test_cases, predictions=candidates, k=[1, 2])\nprint(pass_at_k)\n```", "```py\n{'pass@1': 0.5, 'pass@2': 1.0}\n```", "```py\nfrom langsmith import Client\n# Define a list of synthetic insurance claim examples\nexample_inputs = [\n    (\n \"I was involved in a car accident on 2023-08-15\\. My name is Jane Smith, Claim ID INS78910, \"\n \"Policy Number POL12345, and the damage is estimated at $3500.\",\n        {\n \"claimant_name\": \"Jane Smith\",\n \"claim_id\": \"INS78910\",\n \"policy_number\": \"POL12345\",\n \"claim_amount\": \"$3500\",\n \"accident_date\": \"2023-08-15\",\n \"accident_description\": \"Car accident causing damage\",\n \"status\": \"pending\"\n        }\n    ),\n    (\n \"My motorcycle was hit in a minor collision on 2023-07-20\\. I am John Doe, with Claim ID INS112233 \"\n \"and Policy Number POL99887\\. The estimated damage is $1500.\",\n        {\n \"claimant_name\": \"John Doe\",\n \"claim_id\": \"INS112233\",\n \"policy_number\": \"POL99887\",\n \"claim_amount\": \"$1500\",\n \"accident_date\": \"2023-07-20\",\n \"accident_description\": \"Minor motorcycle collision\",\n```", "```py\n \"status\": \"pending\"\n        }\n    )\n]\n```", "```py\nclient = Client()\ndataset_name = \"Insurance Claims\"\n# Create the dataset in LangSmith\ndataset = client.create_dataset(\n    dataset_name=dataset_name,\n    description=\"Synthetic dataset for insurance claim extraction tasks\",\n)\n# Store examples in the dataset\nfor input_text, expected_output in example_inputs:\n    client.create_example(\n        inputs={\"input\": input_text},\n        outputs={\"output\": expected_output},\n        metadata={\"source\": \"Synthetic\"},\n        dataset_id=dataset.id,\n    )\n```", "```py\n# Define the extraction schema\nfrom pydantic import BaseModel, Field\nclass InsuranceClaim(BaseModel):\n    claimant_name: str = Field(..., description=\"The name of the claimant\")\n    claim_id: str = Field(..., description=\"The unique insurance claim identifier\")\n    policy_number: str = Field(..., description=\"The policy number associated with the claim\")\n    claim_amount: str = Field(..., description=\"The claimed amount (e.g., '$5000')\")\n```", "```py\n    accident_date: str = Field(..., description=\"The date of the accident (YYYY-MM-DD)\")\n    accident_description: str = Field(..., description=\"A brief description of the accident\")\n    status: str = Field(\"pending\", description=\"The current status of the claim\")\n```", "```py\n# Create extraction chain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\ninstructions = (\n \"Extract the following structured information from the insurance claim text: \"\n \"claimant_name, claim_id, policy_number, claim_amount, accident_date, \"\n \"accident_description, and status. Return the result as a JSON object following \"\n \"this schema: \" + InsuranceClaim.schema_json()\n)\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0).bind_functions(\n    functions=[InsuranceClaim.schema()],\n    function_call=\"InsuranceClaim\"\n)\noutput_parser = JsonOutputFunctionsParser()\nextraction_chain = instructions | llm | output_parser | (lambda x: {\"output\": x})\n```", "```py\n# Test the extraction chain\nsample_claim_text = (\n \"I was involved in a car accident on 2023-08-15\\. My name is Jane Smith, \"\n```", "```py\n \"Claim ID INS78910, Policy Number POL12345, and the damage is estimated at $3500\\. \"\n \"Please process my claim.\"\n)\nresult = extraction_chain.invoke({\"input\": sample_claim_text})\nprint(\"Extraction Result:\")\nprint(result)\n```"]