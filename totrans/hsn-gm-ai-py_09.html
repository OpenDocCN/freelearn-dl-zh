<html><head></head><body>
        

                            
                    <h1 class="header-title">Going Deeper with DDQN</h1>
                
            
            
                
<p>Deep learning is the evolution of raw computational learning and it is quickly evolving and starting to dominate all areas of data science, <strong>machine learning</strong> (<strong>ML</strong>), and <strong>artificial intelligence</strong> (<strong>AI</strong>) in general. In turn, these enhancements have brought about incredible innovation in <strong>deep reinforcement learning</strong> (<strong>DRL</strong>) that have allowed it to play games, previously thought to be impossible. DRL is now able to tackle game environments such as the classic Atari 2600 series and play them better than a human. In this chapter, we'll look at what new features in DL allow DRL to play visual state games, such as Atari games. First, we'll look at how a game screen can be used as a visual state. Then, we'll understand how DL can consume a visual state with a new component called <strong>convolutional neural networks</strong> (<strong>CNNs</strong>). After, we'll use that knowledge to build a modified DQN agent to tackle the Atari environment. Building on that, we'll look at an enhancement of DQN called <strong>DDQN</strong>, or <strong>double (dueling) DQN</strong>. Finally, we'll finish the chapter by playing other visual environments.</p>
<p>In summary, in this chapter we'll look at how extensions to DL, called CNNs, can be used to observe visual states. Then, we'll use that knowledge to play Atari games and implement further enhancements as we go. The following is what we will cover in this chapter:</p>
<ul>
<li>Understanding visual state</li>
<li>Introducing CNNs</li>
<li>Working with a DQN on Atari</li>
<li>Introducing DDQN</li>
<li>Extending replay with prioritized experience replay</li>
</ul>
<p>We will continue using the same virtual environment we constructed in <a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>, in this chapter. You will need that environment set up and configured properly in order to use the examples in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding visual state</h1>
                
            
            
                
<p>Up until now, we have observed state as an encoded value or values. These values may have been the cell number in a grid or the x,y location in an area. Either way, these values have been encoded with respect to some reference. In the case of the grid environment, we may use a number to denote the square or a pair of numbers. For x,y coordinates, we still need to denote an origin, and examples of these three types of encoding mechanism are as follows:</p>
<div><img class="aligncenter size-full wp-image-598 image-border" src="img/b90d1fe2-5c07-428d-a721-5ba6484829e8.png" style="width:44.50em;height:14.67em;"/></div>
<p>Three types of encoding state for an agent</p>
<p>In the preceding diagram, there are three examples of encoding state for an environment. For the first example, which is on the left, we just use a number to represent that state. Moving right to the next grid, the state is now represented as a pair of digits, row by column. On the far right, we can see our old friend the Lunar Lander and how part of its state, the location, is taken with respect to the landing pad, which is the origin. With all these cases, the state is always represented as some form of encoding, whether a single-digit or eight like in the Lander environment. By encoding, we mean that we are using a value, that is, a number, to represent that state of the environment. In <a href="3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml">Chapter 5</a>, <em>Exploring SARSA</em>, we learned how discretization of state is a type of transformation of that encoding into simpler forms but that transforming this encoding would need to be tweaked or learned and we realized there needed to be a better way to do this. Fortunately, we did devise a better way, but before we get to that, let's consider what state it is itself.</p>
<p>State is just a numeric representation or index of our policy that lets our agent determine its choice of next actions. The important thing to remember here is that state needs to be an index into the policy or, in the case of DRL, the model. Therefore, our agent will always need to transform that state into a numeric index in that model. This is made substantially simpler with DL, as we have already seen. What would be ideal is for the agent to be able to visually consume the same visible state – the game area – as we humans do and learn to encode the state on its own. 10 years ago, that statement would have sounded like science fiction. Today, it is a science fact, and we will learn how that is done in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Encoding visual state</h1>
                
            
            
                
<p>Fortunately for DRL, the concept of learning from an image has been the center of ongoing research into DL for over 30 years. DL has taken this concept from being able to recognize handwritten digits to being able to detect object position and rotation to understanding the human pose. All of this is done by feeding raw pixels into a deep learning network and it being taught (or teaching itself) how to encode those images to some answer. We will use these same tools in this chapter, but before we do, let's understand the fundamentals of taking an image and feeding it into a network. An example of how you may do this is shown in the following diagram:</p>
<div><img class="aligncenter size-full wp-image-599 image-border" src="img/0a2d2581-3f16-4722-86ac-39237e1cdbd0.png" style="width:28.00em;height:24.83em;"/></div>
<p>Dissecting an image for input into DL</p>
<p>In the preceding diagram, the image was split into four sections and each section was fed as a piece into the network. One thing to note is how each piece is fed into each neuron on the input layer. Now, we could use four pieces, like in the preceding diagram, or 100 pieces, perhaps breaking the image apart pixel by pixel. Either way, we are still blindly discretizing the space, that is, an image, and trying to make sense of it. Funnily enough, this problem that we recognized in RL with discretization is the same type of problem we encounter in deep learning. It is perhaps further compounded in DL because we would often just flatten the image, a 2D matrix of data, into a 1D vector of numbers. In the preceding example, for instance, we can see two eyes being entered into the network but no indication of a relationship, such as spacing and orientation, between them. This information is completely lost when we flatten an image and is more significant the more we discretize the input image. What we need, and what DL discovered, was a way to extract particular features from a set of data, such as an image, and preserve those features in order to classify the entire image in some manner. DL did, in fact, solve this problem very well and we will discover how in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing CNNs</h1>
                
            
            
                
<p>In September 2012, a team supervised by Dr. Geoffrey Hinton from the University of Toronto, considered the godfather of deep learning, competed to build AlexNet. AlexNet was training against a behemoth image test set called ImageNet. ImageNet consisted of more than 14 million images in over 20,000 different classes. AlexNet handily beat its competition, a non-deep learning solution, by more than 10 points that year and achieved what many thought impossible – that is, the recognition of objects in images done as well or perhaps even better than humans. Since that time, the component that made this possible – CNN – has in some cases surpassed human cognition levels in image recognition.</p>
<p>The component that made this possible, CNN, works by dissecting an image into features – features that it learns to detect by learning to detect those features. This sounds a bit recursive and it is, but it is also the reason it works so well. So, let's repeat that again. CNN works by detecting features in an image, except we don't specify those features – what we specify is whether the answer is right or wrong. By using that answer, we can then use backpropagation to push any errors back through the network and correct the way the network detects those features.</p>
<p>In order to detect features, we use filters, much the same way you may use a filter in Photoshop. These filters are the pieces that we now train and do so by introducing them in a new type of layer called CNN, convolution, or CONV. What we find is that we can then also stack those layers on top of each other to extract further features. These concepts likely still remain abstract. Fortunately, there are plenty of great tools that we can use to explore these concepts in the next exercise. Let's take a look at one:</p>
<ol>
<li>Open and point a web browser to <a href="https://tensorspace.org">tensorspace.org</a>.</li>
<li>Find the link for the <strong>Playground</strong> and click on it.</li>
<li>On the <strong>TensorSpace Playground</strong> page, note the various model names on the left-hand side. Click on the <strong>AlexNet</strong> example, as shown in the following screenshot:</li>
</ol>
<div><img class="aligncenter size-full wp-image-864 image-border" src="img/6a62226d-3d38-4446-98b7-f1c66579887a.png" style="width:32.75em;height:32.75em;"/></div>
<p>TensorSpace Playground – AlexNet</p>
<p style="padding-left: 60px">Playground allows you to interactively explore the various deep learning models, such as AlexNet, right down to the layer.  </p>
<ol start="4">
<li>Move through and click on the various layers in the diagram. You can zoom in and out and explore the model in 3D. You will be able to look at all the layers in the network model. Each layer type is color coded. This includes CNN layers (yellow), as well as special pooling layers (blue). </li>
</ol>
<p>Pooling layers, which are layers that collect the learned features from a CNN layer, allow a network to learn quicker since the layers essentially reduce the size of the learning space. However, that reduction eliminates any spatial relationship between features. As such, we typically avoid using pooling layers in DRL and games.</p>
<ol start="5">
<li>If you zoom in, you can look at the way the image is broken by each color channel (red, green, and blue) and then fed into the network. The following screenshot shows this:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-601 image-border" src="img/60a88ff3-b244-4d61-918f-a9ccda3f98c9.png" style="width:44.58em;height:24.17em;"/></p>
<p>Inspecting the image separation and filter extraction</p>
<ol start="6">
<li>From the way the image is separated, we can see how the first layer of CNN, the filters, are extracting features. By doing this, it is possible to recognize the entire dog, but as you go through the layer, the features get smaller and smaller.</li>
<li>Finally, there is a final pooling layer in blue, followed by a green layer, which is a single line. This single line layer represents the input data being flattened so that it can be fed into further layers of your typical deep learning network.</li>
</ol>
<p>Of course, feel free to explore many of the other models in the Playground. Understanding how layers extract features is import to understanding how CNN works. In the next section, we'll look at upgrading our DQN agent so that it can play Atari games using CNN.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Working with a DQN on Atari</h1>
                
            
            
                
<p>Now that we've looked at the output CNNs produce in terms of filters, the best way to understand how this works is to look at the code that constructs them. Before we get to that, though, let's begin a new exercise where we use a new form of DQN to solve Atari:</p>
<ol>
<li>Open this chapter's sample code, which can be found in the <kbd>Chapter_7_DQN_CNN.py</kbd> file. The code is fairly similar to <kbd>Chapter_6_lunar.py</kbd> but with some critical differences. We will just focus on the differences in this exercise. If you need a better explanation of the code, review <a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>:</li>
</ol>
<pre style="padding-left: 60px">from wrappers import *</pre>
<ol start="2">
<li>Starting at the top, the only change is a new import from a local file called <kbd>wrappers.py</kbd>. We will examine what this does by creating the environment:</li>
</ol>
<pre style="padding-left: 60px">env_id = 'PongNoFrameskip-v4'<br/>env = make_atari(env_id)<br/>env = wrap_deepmind(env)<br/>env = wrap_pytorch(env)</pre>
<ol start="3">
<li>We create the environment quite differently here for a few reasons. The three functions, <kbd>make_atari</kbd>, <kbd>wrap_deepmind</kbd>, and <kbd>wrap_pytorch</kbd>, are all located in the new <kbd>wrappers.py</kbd> file we imported earlier. These wrappers are based on the OpenAI specification for creating wrappers around the Gym environment. We will spend more time on wrappers later but for now, the three functions do the following:
<ul>
<li><kbd>make_atari</kbd>:<strong> </strong>This prepares the environment so that we can capture visual input in a form we can encode with CNN. We are setting this up so we can take screenshots of the environment at set intervals.</li>
<li><kbd>wrap_deepmind</kbd>: This is another wrapper that allows for some helper tools. We will look at this later.</li>
<li><kbd>wrap_pytorch</kbd>:<strong> </strong>This is a helper library that converts the visual input image we load into the CNN network into a special form for PyTorch. The various deep learning frameworks have different input styles for CNN layers, so until all the DL frameworks are standardized, you have to be aware of which way the channels appear in your input image. In PyTorch, image channels need to be first. For other frameworks, such as Keras, it is the exact opposite.</li>
</ul>
</li>
<li>After that, we need to alter some of the other code that sets the hyperparameters, as follows:</li>
</ol>
<pre style="padding-left: 60px">epsilon_start = 1.0<br/>epsilon_final = 0.01<br/><strong>epsilon_decay = 30000</strong><br/><br/>epsilon_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)<br/><br/><strong>plt.plot([epsilon_by_episode(i) for i in range(1000000)])</strong><br/>plt.show()</pre>
<ol start="5">
<li>The highlighted lines show the changes we made. The main thing we are changing is just increasing values – a lot. The Pong Atari environment is the simplest and still may require 1 million iterations to solve. On some systems, that may take days:</li>
</ol>
<pre style="padding-left: 60px">model = CnnDQN(env.observation_space.shape, env.action_space.n)<br/>optimizer = optim.Adam(model.parameters(), lr=0.00001)<br/><br/>replay_start = 10000<br/>replay_buffer = ReplayBuffer(100000)</pre>
<ol start="6">
<li>In the preceding block of code, we can see that we are constructing a new class called <kbd>CnnDQN</kbd>. We will get to that shortly. After that, the code is mostly the same except for a new variable, <kbd>replay_start</kbd>, and how large the replay buffer is now set to. Our buffer has increased in size 100 times from 1,000 to 100,000 entries. However, we want to be able to train the agent before the entire buffer fills now. After all, that is a lot of entries. Due to this, we're using <kbd>replay_start</kbd> to denote a training starting point for when the buffer will be used to train the agent:</li>
</ol>
<pre style="padding-left: 60px">episodes = 1400000</pre>
<ol start="7">
<li>Next, we update the episode count to a much higher number. This is because we can expect this environment requires at least a million episodes to train an agent:</li>
</ol>
<pre style="padding-left: 60px">if episode % 200000 == 0:<br/>  plot(episode, all_rewards, losses) </pre>
<ol start="8">
<li> All of the other code remains the same aside from the last part of the training loop, which can be seen in the preceding code. This code shows that we plot iterations every 200,000 episodes. Previously, we did this every 2,000 episodes. You can, of course, increase this or remove it altogether if it gets annoying when training for long hours.</li>
</ol>
<p>This environment and many of the others we will look at may now take hours or days to train. In fact, DeepMind recently estimated that it would take a regular desktop system somewhere near 45 years to train its top RL algorithms. And in case you are wondering, most of the other environments take 40 million iterations to converge. Pong is the easiest at 1 million iterations.</p>
<ol start="9">
<li>Run the example as you normally do. Wait for a while and perhaps move on to the rest of this book. This sample will take hours to train, so we will continue exploring other sections of code while it runs. To confirm the sample is running correctly though, just confirm that the environment is rendering, as shown in the following image:</li>
</ol>
<div><img class="aligncenter size-full wp-image-602 image-border" src="img/715cbba5-c487-4daa-9775-2fff2a6632a4.png" style="width:13.33em;height:12.33em;"/></div>
<p>Running the code example</p>
<p>Keep the sample running. In the next section, we will look at how the CNN layers are built into the new model.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding CNN layers</h1>
                
            
            
                
<p class="mce-root">Now that we understand the basic premise behind CNN layers, it's time to take an in-depth look at how they work. Open up code example, which can be found in the <kbd>Chapter_7_DQN_CNN.py</kbd> file, and follow these steps:</p>
<ol>
<li>At this point, the only code we need to focus on is for a new class called <kbd>CnnDQN</kbd>, as shown here:</li>
</ol>
<pre style="padding-left: 60px">class CnnDQN(nn.Module):<br/> def __init__(self, input_shape, num_actions):<br/>   super(CnnDQN, self).__init__()<br/><br/>   self.input_shape = input_shape<br/>   self.num_actions = num_actions<br/><br/>   self.features = nn.Sequential(<br/>     nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),<br/>     nn.ReLU(),<br/>     nn.Conv2d(32, 64, kernel_size=4, stride=2),<br/>    nn.ReLU(),<br/>     nn.Conv2d(64, 64, kernel_size=3, stride=1),<br/>     nn.ReLU())<br/>   self.fc = nn.Sequential(<br/>     nn.Linear(self.feature_size(), 512),<br/>     nn.ReLU(),<br/>     nn.Linear(512, self.num_actions))<br/><br/>  def forward(self, x):<br/>    x = self.features(x)<br/>    x = x.view(x.size(0), -1)<br/>    x = self.fc(x)<br/>    return x<br/><br/> def feature_size(self): <br/>   return self.features(autograd.Variable(torch.zeros(1,<br/>     *self.input_shape))).view(1, -1).size(1)<br/><br/>  def act(self, state, epsilon):<br/>    if random.random() &gt; epsilon:<br/>      state = autograd.Variable(torch.FloatTensor(<br/>        np.float32(state)).unsqueeze(0), volatile=True)<br/>      q_value = self.forward(state)<br/>      action = q_value.max(1)[1].data[0]<br/>    else:<br/>      action = random.randrange(env.action_space.n)<br/>    return action</pre>
<ol start="2">
<li>The preceding class replaces our previous vanilla DQN version. A number of key differences exist between both, so let's start with the network setup and building the first convolution layer, as shown here:</li>
</ol>
<pre style="padding-left: 60px">self.features = nn.Sequential(<br/>     nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),</pre>
<ol start="3">
<li>The first thing to notice is that we are constructing a new model and putting it in <kbd>self.features</kbd>. <kbd>features</kbd> will be our model for performing convolution and separating features. The first layer is constructed by passing in <kbd>input_shape</kbd>, the number of filters (32), <kbd>kernel_size</kbd> (8), and the <kbd>stride</kbd> (4). All of these inputs are described in more detail here:
<ul>
<li><kbd>input_shape[0]</kbd>:<strong> </strong>The input shape refers to the observation space. With the wrappers we looked at earlier, we transformed the input space to (1, 84,84). Remember that we needed to order the channels first. With 1 channel, we can see our image is grayscale (no RGB). 1 channel is also the number we input as the first value into <kbd>Conv2d.</kbd></li>
<li><strong> The </strong>number of filters (<kbd>32</kbd>): The next input represents the number of filter patches we want to construct in this layer. Each filter is applied across the image and is determined by a window size (kernel size) and movement (stride). We observed the results of these patches earlier when we used TensorSpace Playground to view CNN models in detail.</li>
<li><kbd>kernel_size</kbd> (<kbd>8</kbd>):<strong> </strong>This represents the window size. In this case, since we are using a 2D convolution, Conv2d, that size actually represents a value of 8x8. Passing the window or kernel over the image and applying the learned filter is the convolving operation.</li>
<li><kbd>stride</kbd> (<kbd>4</kbd>):<strong> </strong>Stride indicates how much the window or kernel moves between operations. A stride of 4 means that the window is moved 4 pixels or units which, as it turns out, is half the window size of 8. </li>
</ul>
</li>
<li>An example of how convolution works can be seen in the following image. The upper area is a single output patch. Each element in the kernel, that is, the 3x3 patch in the following image, is the part that is being learned:</li>
</ol>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-603 image-border" src="img/ad2c12b1-37c5-42e7-ae9e-86b1633ea357.png" style="width:34.67em;height:14.42em;"/></p>
<p>The strided convolution process explained</p>
<ol start="5">
<li>The process of applying the kernel on the image is done by simply multiplying the values in the patch with each value in the image. All these values are summed and then output as a single element in the resulting output filter operation:</li>
</ol>
<pre style="padding-left: 60px">self.fc = nn.Sequential(<br/>     nn.Linear(self.feature_size(), 512),</pre>
<ol start="6">
<li>Using the code that constructs the model for the convolution layers, we build another Linear model, just like we constructed in our previous examples. This model will flatten the output from the convolution layers and use that flattened model to predict actions from. We end up with two models for the network in this case but note that we will pass the output from one to the other, as well as backpropagate errors back from one model to the other. The <kbd>feature_size</kbd> function is just a helper so that we can calculate the input from the CNN model to the <kbd>Linear</kbd> model:</li>
</ol>
<pre style="padding-left: 60px">def forward(self, x):<br/>  x = self.features(x)<br/>  x = x.view(x.size(0), -1)<br/>  x = self.fc(x)<br/>  return x</pre>
<ol start="7">
<li>Inside the <kbd>forward</kbd> function, we can see that the prediction of our model has changed. Now, we will break up the prediction by passing it to the <kbd>self.features</kbd> or the CNN part of our model. Then, we need to flatten the data and feed it into the Linear portion with <kbd>self.fc</kbd>.  </li>
<li>The <kbd>action</kbd> function remains the same as our previous DQN implementation.</li>
</ol>
<p>If the agent is still running, see if you can wait for it to finish. It can take a while but it can be both rewarding and interesting to see the final results. Like almost anything in RL, there have been various improvements to the DQN model and we will look at those in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introducing DDQN</h1>
                
            
            
                
<p><strong>DDQN</strong> stands for <strong>dueling DQN</strong> and is different from the double DQN, although people often confuse them. Both variations assume some form of duality, but in the first case, the model is assumed to be split at the base, while in the second case, double DQN, the model is assumed to be split into two entirely different DQN models. </p>
<p>The following diagram shows the difference between DDQN and DQN, which is not to be confused with dueling DQN:</p>
<div><img class="aligncenter size-full wp-image-604 image-border" src="img/ab231724-a4a6-41ea-ac48-ed0003d71459.png" style="width:24.17em;height:19.67em;"/></div>
<p>The difference between DQN and DDQN</p>
<p>In the preceding diagram, CNN layers are being used in both models but in the upcoming exercises, we will just use linear fully connected layers instead, just to simplify things.</p>
<p>Notice how the DDQN network separates into two parts that then converge back to an answer. This is the dueling part of the DDQN model we will get to shortly. Before that, though, let's explore the double DQN model.  </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Double DQN or the fixed Q targets</h1>
                
            
            
                
<p>In order to understand why we may use two networks in combination, or dueling, we first need to understand why we would need to do that. Let's go back to how we calculated the TD loss and used that as our way to estimate actions. As you may recall, we calculated loss based on estimations of the target. However, in the case of our DQN model, that target is now continually changing. The analogy we can use here is that our agent may chase its own tail at times, trying to find a target. Those of you who have been very observant may have viewed this during previous training by seeing an oscillating reward. What we can do here is create another target network that we will aim for and update as we go along. This sounds way more complicated than it is, so let's look at an example:</p>
<ol>
<li>Open the code example in the <kbd>Chapter_7_DoubleDQN.py</kbd> file. This example was built from the <kbd>Chapter_6_DQN_lunar.py</kbd> file that we looked at earlier. There are a number of subtle changes here, so we will review each of those in detail, starting with model construction:</li>
</ol>
<pre style="padding-left: 60px">current_model = DQN(env.observation_space.shape[0], env.action_space.n)<br/>target_model = DQN(env.observation_space.shape[0], env.action_space.n)<br/><br/>optimizer = optim.Adam(current_model.parameters())</pre>
<ol start="2">
<li>As its name suggests, we now construct two DQN models: one for online use and one as a target. We train the <kbd>current_model</kbd> value and then swap back to the target model every <em>x</em> number of iterations using the following code:</li>
</ol>
<pre style="padding-left: 60px">def update_target(current_model, target_model):<br/>  target_model.load_state_dict(current_model.state_dict())<br/><br/>update_target(current_model, target_model)</pre>
<ol start="3">
<li>The <kbd>update_target</kbd> function updates <kbd>target_model</kbd> so that it uses the <kbd>current_model</kbd> model. This assures us that the target Q values are always sufficiently enough ahead or behind since we are using skip traces and looking back.</li>
</ol>
<ol start="4">
<li>Right after that is the <kbd>compute_td_loss</kbd> function, which needs to be updated as follows:</li>
</ol>
<pre style="padding-left: 60px">def compute_td_loss(batch_size):<br/> state, action, reward, next_state, done = replay_buffer.sample(batch_size)<br/><br/> state = autograd.Variable(torch.FloatTensor(np.float32(state)))<br/> next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)),<br/>   volatile=True)<br/> action = autograd.Variable(torch.LongTensor(action))<br/> reward = autograd.Variable(torch.FloatTensor(reward))<br/> done = autograd.Variable(torch.FloatTensor(done))<br/><br/> <strong>q_values = current_model(state)</strong><br/><strong> next_q_values = current_model(next_state)</strong><br/><strong> next_q_state_values = target_model(next_state)</strong><br/><br/><strong> q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1) </strong><br/><strong> next_q_value = next_q_state_values.gather(1,</strong><br/><strong>   torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)</strong><br/> expected_q_value = reward + gamma * next_q_value * (1 - done)<br/> <br/> loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()<br/> <br/> optimizer.zero_grad()<br/> loss.backward()<br/> optimizer.step()<br/><br/> return loss</pre>
<ol start="5">
<li>The highlighted lines in the function show the lines that were changed. Notice how the new models, <kbd>current_model</kbd> and <kbd>target_model</kbd>, are used to predict the loss now and not just the individual model itself. Finally, in the training or trial and error loop, we can see a couple of final changes:</li>
</ol>
<pre style="padding-left: 60px">action = current_model.act(state, epsilon)</pre>
<ol start="6">
<li>The first change is that we are now taking the action from the <kbd>current_model</kbd> model:</li>
</ol>
<pre style="padding-left: 60px">if episode % 500 == 0:<br/> update_target(current_model, target_model)</pre>
<ol start="7">
<li>The second change is updating <kbd>target_model</kbd> with the weights from <kbd>current_model</kbd> using <kbd>update_target</kbd>:</li>
</ol>
<pre style="padding-left: 60px">def play_game():<br/> done = False<br/> state = env.reset()<br/> while(not done):<br/>   <strong>action = current_model.act(state, epsilon_final)</strong><br/>   next_state, reward, done, _ = env.step(action)<br/>   env.render()<br/>   state = next_state</pre>
<ol start="8">
<li>We also need to update the <kbd>play_game</kbd> function so that we can take the action from <kbd>current_model</kbd>. It may be interesting to see what happens if you change that to the target model instead.</li>
<li>At this point, run the code as you normally would and observe the results. </li>
</ol>
<p>Now that we understand why we may want to use a different model, we will move on and learn how we can use dueling DQN or DDQN to solve the same environment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Dueling DQN or the real DDQN</h1>
                
            
            
                
<p>Dueling DQN or DDQN extends the concept of a fixed target or fixed Q target and extends that to include a new concept called advantage. Advantage is a concept where we determine what additional value or advantage we may get by taking other actions. Ideally, we want to calculate advantage so that it includes all the other actions. We can do this with computational graphs by separating the layers into a calculation of state value and another that calculates the advantage from all the permutations of state and action. </p>
<p>This construction can be seen in the following diagram:</p>
<div><img class="aligncenter size-full wp-image-605 image-border" src="img/e94bf46b-2e18-484a-95d1-ac81d8b8330c.png" style="width:41.75em;height:20.92em;"/></div>
<p>DDQN visualized in detail</p>
<p>The preceding diagram once again shows CNN layers, but our example will just start with the linear flattened model. What we can see is how the model is split into two parts after it is flattened. The first part calculates the state value or value and the second lower part calculates the advantage or action values. This is then aggregated to output the Q values. This setup works because we can push the loss back through the entire network using optimization, also known as backpropagation. Therefore, the network learns how to calculate the advantage of each action. Let's look at how this comes together in a new code example. Open the same in the <kbd>Chapter_7_DDQN.py</kbd> file and follow these steps:</p>
<ol>
<li>This example uses the previous example as a source but differs in terms of a number of important details:</li>
</ol>
<pre style="padding-left: 60px">class DDQN(nn.Module):<br/> def __init__(self, num_inputs, num_outputs):<br/>   super(DDQN, self).__init__() <br/><br/>   self.feature = nn.Sequential(<br/>     nn.Linear(num_inputs, 128),<br/>     nn.ReLU())<br/><br/>   self.advantage = nn.Sequential(<br/>     nn.Linear(128, 128),<br/>     nn.ReLU(),<br/>     nn.Linear(128, num_outputs))<br/><br/>   self.value = nn.Sequential(<br/>     nn.Linear(128, 128),<br/>     nn.ReLU(),<br/>     nn.Linear(128, 1))<br/><br/> def forward(self, x):<br/>   x = self.feature(x)<br/>   advantage = self.advantage(x)<br/>   value = self.value(x)<br/>   return value + advantage - advantage.mean()<br/><br/> def act(self, state, epsilon):<br/>   if random.random() &gt; epsilon:<br/>     state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0),<br/>       volatile=True)<br/>     q_value = self.forward(state)<br/>     action = q_value.max(1)[1].item() <br/>  else:<br/>    action = random.randrange(env.action_space.n)<br/>  return action</pre>
<ol start="2">
<li>The DDQN class is almost entirely new aside from the <kbd>act</kbd> function. Inside the init function, we can see the construction of the three submodels: <kbd>self.feature</kbd>, <kbd>self.value</kbd>, and <kbd>self.advantage</kbd>. Then, inside the <kbd>forward</kbd> function, we can see how the input, <strong>x</strong> is transformed by the first <strong>feature</strong> submodel, then fed into the advantage and value submodels. The outputs, <kbd>advantage</kbd> and <kbd>value</kbd>, are then used to calculate the predicted value, as follows:</li>
</ol>
<pre style="padding-left: 60px">return value + advantage - advantage.mean()</pre>
<ol start="3">
<li>What we can see is that the predicted value is the state value denoted by value. This is added to the advantage or combined state-action values and subtracted from the mean or average. The result is a prediction of the best advantage or what the agent learns may be an advantage:</li>
</ol>
<pre style="padding-left: 60px">current_model = DDQN(env.observation_space.shape[0], env.action_space.n)<br/>target_model = DDQN(env.observation_space.shape[0], env.action_space.n)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>The next change is that we now construct two instances of the DDQN model instead of a DQN in our last double DQN example. This means that we also continue to use two models in order to evaluate our targets. After all, we don't want to go backward.</li>
<li>The next major change occurs in the <kbd>compute_td_loss</kbd> function. The updated lines are as follows:</li>
</ol>
<pre style="padding-left: 60px">q_values = current_model(state)<br/>next_q_values = target_model(next_state)<br/><br/>q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)<br/>next_q_value = next_q_values.max(1)[0]<br/>expected_q_value = reward + gamma * next_q_value * (1 - done)<br/> <br/>loss = (q_value - expected_q_value.detach()).pow(2).mean()</pre>
<ol start="6">
<li>This actually simplifies the preceding code. Now, we can clearly see that our next_q_values are being taken from the <kbd>target_model</kbd>.</li>
<li>Run the code example as you always do and watch the agent play the Lander. Make sure you keep the agent training until it reaches some amount of positive reward. This may require you to increase the number of training iterations or episodes.</li>
</ol>
<p>As a reminder, we use the term episode to mean one training observation or iteration for one time step. Many examples will use the word frame and frames to denote the same thing. While frame can be appropriate in some contexts, it is less so in others, especially when we start to stack frames or input observations. If you find the name confusing, an alternative may be to use training iteration.</p>
<p>You will see that this algorithm does indeed converge faster, but as you may expect, there are improvements we can make to this algorithm as well. We will look at how we can improve on this in the next section.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Extending replay with prioritized experience replay</h1>
                
            
            
                
<p>So far, we've seen how using a replay buffer or experience replay mechanism allows us to pull values back in batches at a later time in order to train the network graph. These batches of data were composed of random samples, which works well, but of course, we can do better. Therefore, instead of storing just everything, we can make two decisions: what data to store and what data is a priority to use. In order to simplify things, we will just look at prioritizing what data we extract from the experience replay. By prioritizing the data we extract, we can hope this will dramatically improve the information we do feed to the network for learning and thus the whole performance of the agent.</p>
<p>Unfortunately, the idea behind prioritizing the replay buffer is quite simple to grasp but far more difficult in practice to derive and estimate. What we can do, though, is prioritize the return events by the TD error or loss from the prediction and the actual expected target of that event. Thus, we prioritize the values the agent predicts where the most amount of error is or where the agent is wrong the most. Another way to think of this is that we prioritize the events that surprised the agent the most. The replay buffer is structured so that it prioritizes those events by surprise level and then returns a sample of those, except it doesn't necessarily order the events by surprise. Here, it's better to randomly sample the events from a bucket or distribution ordered by surprise. This means the agent would then be more inclined to choose samples from the more average surprising events.</p>
<p>In this section, we'll use a Prioritized Experience Replay mechanism, which was first introduced in this paper: <a href="https://arxiv.org/pdf/1511.05952.pdf">https://arxiv.org/pdf/1511.05952.pdf</a>. It was then coded in PyTorch from this repository: <a href="https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb">https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb</a>. Our implementation has been modified to run outside a notebook and for Python 3.6 (<a href="https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb">https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb</a>).</p>
<p>We will work with an entirely new sample. Open up <kbd>Chapter_7_DDQN_wprority.py</kbd> and follow these steps:</p>
<ol>
<li>The first big change in this sample is an upgrade from the <kbd>ReplayBuffer</kbd> class to <kbd>NaivePrioritizedBuffer</kbd>, as shown here:</li>
</ol>
<pre style="padding-left: 60px">class NaivePrioritizedBuffer(object):<br/> def __init__(self, capacity, prob_alpha=0.6):<br/>   self.prob_alpha = prob_alpha<br/>   self.capacity = capacity<br/>   self.buffer = []<br/>   self.pos = 0<br/>   self.priorities = np.zeros((capacity,), dtype=np.float32)<br/><br/> def push(self, state, action, reward, next_state, done):<br/>   assert state.ndim == next_state.ndim<br/>   state = np.expand_dims(state, 0)<br/>   next_state = np.expand_dims(next_state, 0)<br/>   max_prio = self.priorities.max() if self.buffer else 1.0<br/><br/>   if len(self.buffer) &lt; self.capacity:<br/>     self.buffer.append((state, action, reward, next_state, done))<br/>   else:<br/>     self.buffer[self.pos] = (state, action, reward, next_state, done)<br/><br/>   self.priorities[self.pos] = max_prio<br/>   self.pos = (self.pos + 1) % self.capacity<br/> <br/> def sample(self, batch_size, beta=0.4):<br/>   if len(self.buffer) == self.capacity:<br/>     prios = self.priorities<br/>   else:<br/>     prios = self.priorities[:self.pos]<br/><br/>   probs = prios ** self.prob_alpha<br/>   probs /= probs.sum()<br/><br/>   indices = np.random.choice(len(self.buffer), batch_size, p=probs)<br/>   samples = [self.buffer[idx] for idx in indices]<br/><br/>   total = len(self.buffer)<br/>   weights = (total * probs[indices]) ** (-beta)<br/>   weights /= weights.max()<br/>   weights = np.array(weights, dtype=np.float32)<br/><br/>   batch = list(zip(*samples))<br/>   states = np.concatenate(batch[0])<br/>   actions = batch[1]<br/>   rewards = batch[2]<br/>   next_states = np.concatenate(batch[3])<br/>   dones = batch[4]<br/><br/>   return states, actions, rewards, next_states, dones, indices, weights<br/><br/>  def update_priorities(self, batch_indices, batch_priorities):<br/>    for idx, prio in zip(list(batch_indices), [batch_priorities]):<br/>      self.priorities[idx] = prio<br/><br/>  def __len__(self):<br/>    return len(self.buffer)</pre>
<ol start="2">
<li>This code naively assigns priorities based on observed error prediction. Then, it sorts those values based on priority order. It then randomly samples those events back. Again, since the sampling is random, but the samples are aligned by priority, random sampling will generally take the samples with an average error.  </li>
<li>What happens is that by reordering the samples, we reorder to expected actual distribution of data. Therefore, to account for this, we introduce a new factor called <strong>beta</strong>, or <strong>importance-sampling</strong>. <strong>Beta</strong> allows us to control the distribution of events and essentially reset them to their original placement:</li>
</ol>
<pre style="padding-left: 60px">beta_start = 0.4<br/>beta_episodes = episodes / 10 <br/>beta_by_episode = lambda episode: min(1.0,<br/>  beta_start + episode * (1.0 - beta_start) / beta_episodes)<br/><br/>plt.plot([beta_by_episode(i) for i in range(episodes)])</pre>
<ol start="4">
<li>Now, we will define a function to return an increasing beta over episodes using the preceding code. Then, the code plots beta much like we plot epsilon, as shown here:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-606 image-border" src="img/ae38a330-3d0d-49bf-aeab-886984d7adda.png" style="width:95.92em;height:36.75em;"/></p>
<p>Example of beta and epsilon plots</p>
<ol start="5">
<li>After modifying the sample function in the replay buffer, we also need to update the <kbd>compute_td_loss</kbd> function, as shown here:</li>
</ol>
<pre style="padding-left: 60px">def compute_td_loss(batch_size, beta):<br/>  <strong>state, action, reward, next_state, done, indices, </strong><br/><strong>    weights = replay_buffer.sample(batch_size, beta)</strong><br/><br/>  state = autograd.Variable(torch.FloatTensor(np.float32(state)))<br/>  next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)))<br/>  action = autograd.Variable(torch.LongTensor(action))<br/>  reward = autograd.Variable(torch.FloatTensor(reward))<br/>  done = autograd.Variable(torch.FloatTensor(done))<br/>  <strong>weights = autograd.Variable(torch.FloatTensor(weights))</strong><br/><br/>  q_values = current_model(state)<br/>  next_q_values = target_model(next_state)<br/><br/>  q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)<br/>  next_q_value = next_q_values.max(1)[0]<br/>  expected_q_value = reward + gamma * next_q_value * (1 - done)<br/> <br/>  loss = (q_value - expected_q_value.detach()).pow(2).mean()<br/>  prios = loss + 1e-5<br/>  loss = loss.mean()<br/> <br/>  optimizer.zero_grad()<br/>  loss.backward()<br/>  <strong>replay_buffer.update_priorities(indices, prios.data.cpu().numpy())</strong><br/>  optimizer.step()<br/><br/>  return loss</pre>
<ol start="6">
<li>Only the preceding highlighted lines are different from what we have seen already. The first difference is the return of two new values: <kbd>indices</kbd> and <kbd>weights</kbd>. Then, we can see that <kbd>replay_buffer</kbd> calls <kbd>update_priorities</kbd> based on the previously returned <kbd>indices</kbd>:</li>
</ol>
<pre style="padding-left: 60px">if done:<br/>  if episode &gt; buffer_size and avg_reward &gt; min_play_reward:<br/>    play_game() <br/>  state = env.reset()<br/>  all_rewards.append(episode_reward)<br/>  episode_reward = 0</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="7">
<li>Next, inside the training loop, we update the call to <kbd>play_game</kbd> and introduce a new <kbd>min_play_reward</kbd> threshold value. This allows us to set some minimum reward threshold before rendering the game. Rendering the game can be quite time-consuming and this will also speed up training:</li>
</ol>
<pre style="padding-left: 60px">if len(replay_buffer) &gt; batch_size:<br/>  beta = beta_by_episode(episode)<br/>  loss = compute_td_loss(batch_size, beta)<br/>  losses.append(loss.item())</pre>
<ol start="8">
<li>Continuing inside the training loop, we can see how we extract <kbd>beta</kbd> and use that in the <kbd>td_compute_loss</kbd> function.</li>
<li>Run the sample again. This time, you may have to wait to see the agent drive the Lander but when it does, it will do quite well, as shown here:</li>
</ol>
<div><img class="aligncenter size-full wp-image-607 image-border" src="img/da8e4389-45ac-40c0-8f9d-04de5d01e475.png" style="width:32.75em;height:14.17em;"/></div>
<p>The agent landing the Lander</p>
<p>Typically, in a reasonably short amount of time, the agent will be able to consistently land the Lander. The algorithm should converge to landing within 75,000 iterations. You can, of course, continue to tweak and play with the hyperparameters, but that is what our next section is for.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p>The further we progress in this book, the more valuable and expensive each of these exercises will become. By expensive, we mean the amount of time you need to invest in each will increase. That may mean you are inclined to do fewer exercises, but please continue to try and do two or three exercises on your own:</p>
<ol>
<li>Revisit TensorSpace Playground and see if you can understand the difference pooling makes in those models. Remember that we avoid the use of pooling in order to avoid losing spatial integrity.</li>
<li>Open <kbd>Chapter_7_DQN_CNN.py</kbd><strong> </strong>and alter some of the convolutional layer inputs such as the kernel or stride size. See what effect this has on training.</li>
<li>Tune the hyperparameters or create new ones for <kbd>Chapter_7_DoubleDQN.py</kbd>.</li>
<li>Tune the hyperparameters or create new ones for <kbd>Chapter_7_DDQN.py</kbd>.</li>
<li>Tune the hyperparameters or create new ones for <kbd>Chapter_7_DoubleDQN_wprority.py</kbd>.</li>
<li>Convert <kbd>Chapter_7_DoubleDQN.py</kbd> so that it uses convolutional layers and then upgrade the sample so that it works with an Atari environment such as Pong.</li>
<li>Convert <kbd>Chapter_7_DDQN.py</kbd><strong> </strong>so that it uses convolutional layers and then upgrade the sample so that it works with an Atari environment such as Pong.</li>
<li>Convert <kbd>Chapter_7_DDQN_wprority.py</kbd><strong> </strong>so that it uses convolutional layers and then upgrade the sample so that it works with an Atari environment such as Pong.</li>
<li>Add a Pooling layer in-between the convolutional layers in one of the examples. You will likely need to consult the PyTorch documentation to learn how to do this.</li>
<li>How else could you improve the experience replay buffer in the preceding example? Are there other forms of replay buffers you could use?</li>
</ol>
<p>As always, have fun working through the samples. After all, if you are not happy watching your code play the Lunar Lander or an Atari game, when will you be?</p>
<p>In the next section, we'll wrap up this chapter and look at what we'll learn about next.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Extending from where we left off with DQN, we looked at ways of extending this model with CNN and adding additional networks to create double DQN and dueling DQN, or DDQN. Before exploring CNN, we looked at what visual observation encoding is and why we need it. Then, we briefly introduced CNN and used the TensorSpace Playground to explore some well-known, state-of-the-art models. Next, we added CNN to a DQN model and used that to play the Atari game environment Pong. After, we took a closer look at how we could extend DQN by adding another network as the target and adding another network to duel against or to contradict the other network, also known as the dueling DQN or DDQN. This introduced the concept of advantage in choosing an action. Finally, we looked at extending the experience replay buffer so that we can prioritize events that get captured there. Using this framework, we were able to easily land the Lander with just a short amount of agent training.</p>
<p>In the next chapter, we'll look at new ways of selecting policy methods and no longer look at global averages. Instead, we will sample distributions using policy gradient methods.</p>


            

            
        
    </body></html>