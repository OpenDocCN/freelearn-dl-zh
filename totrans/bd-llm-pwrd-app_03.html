<html><head></head><body>
<div class="calibre1" id="_idContainer089">
<h1 class="chapternumber"><span class="kobospan" id="kobo.1.1">3</span></h1>
<h1 class="chaptertitle" id="_idParaDest-39"><span class="kobospan" id="kobo.2.1">Choosing an LLM for Your Application</span></h1>
<p class="normal"><span class="kobospan" id="kobo.3.1">In the last chapter, we saw how pivotal it is to properly orchestrate </span><strong class="screentext"><span class="kobospan" id="kobo.4.1">large language models</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.6.1">LLMs</span></strong><span class="kobospan" id="kobo.7.1">) and</span><a id="_idIndexMarker168" class="calibre3"/><span class="kobospan" id="kobo.8.1"> their components within applications. </span><span class="kobospan" id="kobo.8.2">In fact, we saw that not all LLMs are created equal. </span><span class="kobospan" id="kobo.8.3">The next key decision is which LLMs to actually use. </span><span class="kobospan" id="kobo.8.4">Different LLMs may have different architectures, sizes, training data, capabilities, and limitations. </span><span class="kobospan" id="kobo.8.5">Choosing the right LLM for your application is not a trivial decision, as it can have a significant impact on the performance, quality, and cost of your solution.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.9.1">In this chapter, we will guide you through the process of choosing the right LLM for your application. </span><span class="kobospan" id="kobo.9.2">We will cover the following topics:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.10.1">An overview of the most promising LLMs in the market</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.11.1">The main criteria and tools to use when comparing LLMs</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.12.1">Trade-offs between size and performance</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.13.1">By the end of this chapter, you should have a clear understanding of how to choose the right LLM for your application and how to use it effectively and responsibly.</span></p>
<h1 class="heading" id="_idParaDest-40"><span class="kobospan" id="kobo.14.1">The most promising LLMs in the market</span></h1>
<p class="normal"><span class="kobospan" id="kobo.15.1">The last year has </span><a id="_idIndexMarker169" class="calibre3"/><span class="kobospan" id="kobo.16.1">witnessed an unprecedented surge in the research and development of LLMs. </span><span class="kobospan" id="kobo.16.2">Several new models have been released or announced by different organizations, each with its own features and capabilities. </span><span class="kobospan" id="kobo.16.3">Some of these models are the largest and most advanced ever created, surpassing the previous </span><strong class="screentext"><span class="kobospan" id="kobo.17.1">state-of-the-art</span></strong><span class="kobospan" id="kobo.18.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.19.1">SOTA</span></strong><span class="kobospan" id="kobo.20.1">) by orders of magnitude. </span><span class="kobospan" id="kobo.20.2">Others are lighter yet more </span><a id="_idIndexMarker170" class="calibre3"/><span class="kobospan" id="kobo.21.1">specialized in specific tasks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.22.1">In this chapter, we will review some of the most promising LLMs in the market as of 2024. </span><span class="kobospan" id="kobo.22.2">We will introduce their background, key findings, and main techniques. </span><span class="kobospan" id="kobo.22.3">We will also compare their performance, strengths, and limitations on various benchmarks and tasks. </span><span class="kobospan" id="kobo.22.4">We will also </span><a id="_idIndexMarker171" class="calibre3"/><span class="kobospan" id="kobo.23.1">discuss their potential applications, challenges, and implications for the future of AI and society.</span></p>
<h2 class="heading1" id="_idParaDest-41"><span class="kobospan" id="kobo.24.1">Proprietary models</span></h2>
<p class="normal"><span class="kobospan" id="kobo.25.1">Proprietary LLMs are</span><a id="_idIndexMarker172" class="calibre3"/><span class="kobospan" id="kobo.26.1"> developed and</span><a id="_idIndexMarker173" class="calibre3"/><span class="kobospan" id="kobo.27.1"> owned by private companies, and they are not disclosed with code. </span><span class="kobospan" id="kobo.27.2">They are also typically subject to a fee for consumption.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.28.1">Proprietary models offer a series of advantages, including better support and maintenance as well as safety and alignment. </span><span class="kobospan" id="kobo.28.2">They also tend to outperform open-source models in terms of generalization, because of their complexity and training datasets. </span><span class="kobospan" id="kobo.28.3">On the other hand, they act as a “black box,” meaning that owners do not disclose the source code to developers.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.29.1">In the next sections, we will cover three of the most popular proprietary LLMs in the market, as of August 2023.</span></p>
<h3 class="heading2" id="_idParaDest-42"><span class="kobospan" id="kobo.30.1">GPT-4</span></h3>
<p class="normal"><span class="kobospan" id="kobo.31.1">Released in</span><a id="_idIndexMarker174" class="calibre3"/><span class="kobospan" id="kobo.32.1"> March 2023, GPT-4 is, together with its newly released “cousin” GPT-4 Turbo, one</span><a id="_idIndexMarker175" class="calibre3"/><span class="kobospan" id="kobo.33.1"> of the latest models </span><a id="_idIndexMarker176" class="calibre3"/><span class="kobospan" id="kobo.34.1">developed by </span><strong class="screentext"><span class="kobospan" id="kobo.35.1">OpenAI</span></strong><span class="kobospan" id="kobo.36.1">, is among the top performers in the market at the time of writing this book (while OpenAI, as confirmed by its CEO Sam Altman, is already working on GPT-5).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.37.1">It belongs to the </span><a id="_idIndexMarker177" class="calibre3"/><span class="kobospan" id="kobo.38.1">class of </span><strong class="screentext"><span class="kobospan" id="kobo.39.1">generative pretrained transformer</span></strong><span class="kobospan" id="kobo.40.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.41.1">GPT</span></strong><span class="kobospan" id="kobo.42.1">) models, a decoder-only transformer-based architecture introduced by OpenAI. </span><span class="kobospan" id="kobo.42.2">The following diagram shows the basic architecture:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.43.1"><img alt="" role="presentation" src="../Images/B21714_03_01.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.44.1">Figure 3.1: High-level architecture of a decoder-only transformer</span></p>
<p class="normal1"><span class="kobospan" id="kobo.45.1">As you can see </span><a id="_idIndexMarker178" class="calibre3"/><span class="kobospan" id="kobo.46.1">from the preceding diagram, the decoder-only</span><a id="_idIndexMarker179" class="calibre3"/><span class="kobospan" id="kobo.47.1"> architecture still includes the main elements that feature in transformer architecture that we covered in </span><em class="italic"><span class="kobospan" id="kobo.48.1">Chapter 1</span></em><span class="kobospan" id="kobo.49.1">, </span><em class="italic"><span class="kobospan" id="kobo.50.1">Positional Embeddings, Multi-Head Attention</span></em><span class="kobospan" id="kobo.51.1">, and </span><em class="italic"><span class="kobospan" id="kobo.52.1">Feed Forward </span></em><span class="kobospan" id="kobo.53.1">layers. </span><span class="kobospan" id="kobo.53.2">However, in this architecture, the model solely comprises a decoder, which is trained to predict the next token in a sequence based on the preceding tokens. </span><span class="kobospan" id="kobo.53.3">Unlike the encoder-decoder architecture, the decoder-only design lacks an explicit encoder for summarizing input information. </span><span class="kobospan" id="kobo.53.4">Instead, the information is implicitly encoded within the hidden state of the decoder, which is updated at each step during the generation process.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.54.1">Now, we’ll look at some of the improvements in GPT-4 over previous versions.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.55.1">GPT-4, like the previous models in the GPT series, has been trained on both publicly available and OpenAI-licensed datasets (OpenAI didn’t disclose the exact composition of the training set).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.56.1">Additionally, to make the</span><a id="_idIndexMarker180" class="calibre3"/><span class="kobospan" id="kobo.57.1"> model more aligned with the user’s intent, the training process</span><a id="_idIndexMarker181" class="calibre3"/><span class="kobospan" id="kobo.58.1"> also</span><a id="_idIndexMarker182" class="calibre3"/><span class="kobospan" id="kobo.59.1"> involved </span><strong class="screentext"><span class="kobospan" id="kobo.60.1">reinforcement learning from human feedback</span></strong><span class="kobospan" id="kobo.61.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.62.1">RLHF</span></strong><span class="kobospan" id="kobo.63.1">) training.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.64.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.65.1">RLHF is a technique that aims at using human feedback as an evaluating metric for LLMs’ generated output and then using that feedback to further optimize the model. </span><span class="kobospan" id="kobo.65.2">There are two main steps to achieve that goal:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.66.1">Training a reward model based on human preferences.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.67.1">Optimizing the LLM with respect to the reward model. </span><span class="kobospan" id="kobo.67.2">This step is done via reinforcement learning and it is a type of machine learning paradigm where an agent learns to make decisions by interacting with an environment. </span><span class="kobospan" id="kobo.67.3">The agent receives feedback in the form of rewards or penalties based on its actions, and its goal is to maximize the cumulative reward over time by continuously adapting its behavior through trial and error.</span></li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.68.1">With RLHF, thanks to the reward model, the LLM is able to learn from human preferences and be more aligned with users’ intents.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.69.1">As an example, think about ChatGPT. </span><span class="kobospan" id="kobo.69.2">This model integrates various training methods, including unsupervised pretraining, supervised fine-tuning, instruction tuning, and RLHF. </span><span class="kobospan" id="kobo.69.3">The RLHF component involves training the model to predict human preferences by using feedback from human trainers. </span><span class="kobospan" id="kobo.69.4">These trainers review the model’s responses and provide ratings or corrections, guiding the model to generate more helpful, accurate, and aligned responses.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.70.1">For instance, if a language model initially produces an output that is not quite helpful or accurate, human trainers can provide feedback that indicates the preferred output. </span><span class="kobospan" id="kobo.70.2">The model then uses this feedback to adjust its parameters and improve future responses. </span><span class="kobospan" id="kobo.70.3">This process iteratively continues, with the model learning from a series of human judgments to better align with what is considered helpful or appropriate by human standards.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.71.1">GPT-4 demonstrated outstanding capabilities in commonsense reasoning and analytical skills. </span><span class="kobospan" id="kobo.71.2">It has been benchmarked with SOTA systems, including the </span><strong class="screentext"><span class="kobospan" id="kobo.72.1">Massive Multitask Language Understanding</span></strong><span class="kobospan" id="kobo.73.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.74.1">MMLU</span></strong><span class="kobospan" id="kobo.75.1">) we </span><a id="_idIndexMarker183" class="calibre3"/><span class="kobospan" id="kobo.76.1">covered in </span><em class="italic"><span class="kobospan" id="kobo.77.1">Chapter 1</span></em><span class="kobospan" id="kobo.78.1">. </span><span class="kobospan" id="kobo.78.2">On MMLU, GPT-4 outperformed previous models not only in English, but also in other languages.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.79.1">The following</span><a id="_idIndexMarker184" class="calibre3"/><span class="kobospan" id="kobo.80.1"> is an </span><a id="_idIndexMarker185" class="calibre3"/><span class="kobospan" id="kobo.81.1">illustration that shows GPT-4’s performance on MMLU:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.82.1"><img alt="A graph with green and blue bars  Description automatically generated" src="../Images/B21714_03_02.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.83.1">Figure 3.2: GPT-4 3-shot accuracy on MMLU across languages (source: </span><a href="https://openai.com/research/gpt-4" class="calibre3"><span class="kobospan" id="kobo.84.1">https://openai.com/research/gpt-4</span></a><span class="kobospan" id="kobo.85.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.86.1">In addition to MMLU, GPT-4 has been benchmarked on a variety of SOTA systems and academic exams, as you can see from the following graph:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.87.1"><img alt="A graph of a performance  Description automatically generated" src="../Images/B21714_03_03.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.88.1">Figure 3.3: GPT performance on academic and professional exams (source: </span><a href="https://arxiv.org/pdf/2303.08774.pdf" class="calibre3"><span class="kobospan" id="kobo.89.1">https://arxiv.org/pdf/2303.08774.pdf</span></a><span class="kobospan" id="kobo.90.1">)</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.91.1">Note</span></strong><span class="kobospan" id="kobo.92.1">: in</span><a id="_idIndexMarker186" class="calibre3"/><span class="kobospan" id="kobo.93.1"> the</span><a id="_idIndexMarker187" class="calibre3"/><span class="kobospan" id="kobo.94.1"> preceding graph, you can see two versions of GPT-4, vision and no vision (along with the GPT-3.5 for benchmarking purposes). </span><span class="kobospan" id="kobo.94.2">This is because GPT-4 is a multi-modal model, meaning that it can take images as input, in addition to text. </span><span class="kobospan" id="kobo.94.3">However, in this chapter, we will benchmark only its textual capabilities.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.95.1">Another great improvement of GPT-4 with respect to its predecessors (GPT-3.5 and GPT-3) is its noticeable reduction in the risk of hallucination.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.96.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.97.1">Hallucination is a term that describes a phenomenon where LLMs generate text that is incorrect, nonsensical, or not real, but appears to be plausible or coherent. </span><span class="kobospan" id="kobo.97.2">For example, an LLM may hallucinate a fact that contradicts the source or common knowledge, a name that does not exist, or a sentence that does not make sense.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.98.1">Hallucination can happen because LLMs are not databases or search engines that store or retrieve factual information. </span><span class="kobospan" id="kobo.98.2">Rather, they are statistical models that learn from massive amounts of text data and produce outputs based on the patterns and probabilities they have learned. </span><span class="kobospan" id="kobo.98.3">However, these patterns and probabilities may not reflect the truth or the reality, as the data may be incomplete, noisy, or biased. </span><span class="kobospan" id="kobo.98.4">Moreover, LLMs have limited contextual understanding and memory, as they can only process a certain number of tokens at a time and abstract them into latent representations. </span><span class="kobospan" id="kobo.98.5">Therefore, LLMs may generate text that is not supported by any data or logic but is the most likely or correlated from the prompt.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.99.1">In fact, even</span><a id="_idIndexMarker188" class="calibre3"/><span class="kobospan" id="kobo.100.1"> though it is still not 100% reliable, GPT-4 made great improvements </span><a id="_idIndexMarker189" class="calibre3"/><span class="kobospan" id="kobo.101.1">with TruthfulQA benchmarks, which test the model’s ability to separate fact from incorrect statements (we covered TruthfulQA benchmarks in </span><em class="italic"><span class="kobospan" id="kobo.102.1">Chapter 1</span></em><span class="kobospan" id="kobo.103.1">, in the </span><em class="italic"><span class="kobospan" id="kobo.104.1">Model evaluation </span></em><span class="kobospan" id="kobo.105.1">section).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.106.1">Here, you can see an illustration that compares GPT-4 results in a TruthfulQA benchmark with those of GPT-3.5 (the model behind OpenAI’s ChatGPT) and Anthropic-LM (we will cover this latter model in the next sections).</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.107.1"><img alt="A graph of different colored squares  Description automatically generated" src="../Images/B21714_03_04.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.108.1">Figure 3.4: Model comparison in TruthfulQA benchmark (source: </span><a href="https://openai.com/research/gpt-4" class="calibre3"><span class="kobospan" id="kobo.109.1">https://openai.com/research/gpt-4</span></a><span class="kobospan" id="kobo.110.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.111.1">Finally, with GPT-4, OpenAI made an additional effort to make it safer and more aligned, engaging from the </span><a id="_idIndexMarker190" class="calibre3"/><span class="kobospan" id="kobo.112.1">beginning a team of over 50 experts in domains like AI </span><a id="_idIndexMarker191" class="calibre3"/><span class="kobospan" id="kobo.113.1">alignment risks, privacy, and cybersecurity, with the goal of understanding the extent of the risks of such a powerful model and how to prevent them.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.114.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.115.1">Alignment is a term that describes the degree to which LLMs behave in ways that are useful and harmless for their human users. </span><span class="kobospan" id="kobo.115.2">For example, an LLM may be aligned if it generates text that is accurate, relevant, coherent, and respectful. </span><span class="kobospan" id="kobo.115.3">An LLM may be misaligned if it generates text that is false, misleading, harmful, or offensive.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.116.1">Thanks to this</span><a id="_idIndexMarker192" class="calibre3"/><span class="kobospan" id="kobo.117.1"> analysis, further data have been collected and used while training GPT-4 to mitigate its potential risks, resulting in a reduced risk compared to its predecessor, GPT-3.5.</span></p>
<h3 class="heading2" id="_idParaDest-43"><span class="kobospan" id="kobo.118.1">Gemini 1.5</span></h3>
<p class="normal"><span class="kobospan" id="kobo.119.1">Gemini 1.5 is a SOTA generative</span><a id="_idIndexMarker193" class="calibre3"/><span class="kobospan" id="kobo.120.1"> AI model developed by Google</span><a id="_idIndexMarker194" class="calibre3"/><span class="kobospan" id="kobo.121.1"> and released in December 2023. </span><span class="kobospan" id="kobo.121.2">Like GPT-4, Gemini is designed to be multimodal, meaning that it can process and generate content across various modalities, including text, images, audio, video, and code. </span><span class="kobospan" id="kobo.121.3">It is based </span><a id="_idIndexMarker195" class="calibre3"/><span class="kobospan" id="kobo.122.1">on a </span><strong class="screentext"><span class="kobospan" id="kobo.123.1">mixture-of-expert</span></strong><span class="kobospan" id="kobo.124.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.125.1">MoE</span></strong><span class="kobospan" id="kobo.126.1">) transformer.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.127.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.128.1">In the context of transformer architecture, MoE refers to a model that incorporates multiple specialized sub-models, known as “experts,” within its layers. </span><span class="kobospan" id="kobo.128.2">Each expert is a neural network designed to handle different types of data or tasks more efficiently. </span><span class="kobospan" id="kobo.128.3">The MoE model uses a gating mechanism or router to determine which expert should process a given input, allowing the model to dynamically allocate resources and specialize in processing certain types of information. </span><span class="kobospan" id="kobo.128.4">This approach can lead to more efficient training and inference, as it enables the model to scale up in size and complexity without a proportional increase in computational cost.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.129.1">Gemini comes in various sizes, including Ultra, Pro, and Nano, to cater to different computational needs, from data centers to mobile devices. </span><span class="kobospan" id="kobo.129.2">To use Gemini, developers can access it via the APIs provided for different model variants, allowing the integration of its capabilities into applications.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.130.1">Compared to its previous version, Gemini 1.0, the current model outperforms it in text, vision, and audio tasks, as shown in the following screenshot:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.131.1"><img alt="" role="presentation" src="../Images/B21714_03_05.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.132.1">Figure 3.5: Gemini 1.5 Pro and Ultra compared to its previous version 1.0 (source: </span><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" class="calibre3"><span class="kobospan" id="kobo.133.1">https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf</span></a><span class="kobospan" id="kobo.134.1"> )</span></p>
<p class="normal1"><span class="kobospan" id="kobo.135.1">Similarly, it </span><a id="_idIndexMarker196" class="calibre3"/><span class="kobospan" id="kobo.136.1">has </span><a id="_idIndexMarker197" class="calibre3"/><span class="kobospan" id="kobo.137.1">demonstrated outstanding capabilities in domains such as math, science, and reasoning, and coding and multilinguality:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.138.1"><img alt="" role="presentation" src="../Images/B21714_03_06.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.139.1">Figure 3.6: Gemini 1.5 Pro compared to Gemini 1.0 Pro and Ultra on different benchmarks (source: </span><a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" class="calibre3"><span class="kobospan" id="kobo.140.1">https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf</span></a><span class="kobospan" id="kobo.141.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.142.1">Note that </span><a id="_idIndexMarker198" class="calibre3"/><span class="kobospan" id="kobo.143.1">Gemini 1.5 Pro is outperforming Gemini 1.0 Ultra (which is</span><a id="_idIndexMarker199" class="calibre3"/><span class="kobospan" id="kobo.144.1"> remarkably bigger) in many benchmarks across the various domains. </span><span class="kobospan" id="kobo.144.2">As of today, Gemini Pro can be tried via a web app at gemini.</span><a href="https://google.com" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.145.1">google.com</span></span></a><span class="kobospan" id="kobo.146.1"> for free, while Gemini Ultra is available via a premium subscription with a monthly fee. </span><span class="kobospan" id="kobo.146.2">On the other hand, Gemini Nano, which is tailored for mobile devices, can be executed on capable Android devices via the Google AI Edge SDK for Android. </span><span class="kobospan" id="kobo.146.3">Note that, as of April 2024, this SDK is still under early access preview and you can apply for the early access program at </span><a href="https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.147.1">https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform</span></span></a><span class="kobospan" id="kobo.148.1">. </span><span class="kobospan" id="kobo.148.2">Finally, Gemini Pro and Ultra can also be consumed </span><a id="_idIndexMarker200" class="calibre3"/><span class="kobospan" id="kobo.149.1">by developers via the REST API from Google </span><a id="_idIndexMarker201" class="calibre3"/><span class="kobospan" id="kobo.150.1">AI Studio.</span></p>
<h3 class="heading2" id="_idParaDest-44"><span class="kobospan" id="kobo.151.1">Claude 2</span></h3>
<p class="normal"><span class="kobospan" id="kobo.152.1">Claude 2, which stands</span><a id="_idIndexMarker202" class="calibre3"/><span class="kobospan" id="kobo.153.1"> for Constitutional Large-scale Alignment via User </span><a id="_idIndexMarker203" class="calibre3"/><span class="kobospan" id="kobo.154.1">Data and Expertise, is an LLM developed by Anthropic, a research company founded by former OpenAI researchers and focused on AI safety and alignment. </span><span class="kobospan" id="kobo.154.2">It was announced in July 2023.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.155.1">Claude 2 is a transformer-based LLM that has been trained on a mix of publicly available information from the internet and proprietary data, via</span><a id="_idIndexMarker204" class="calibre3"/><span class="kobospan" id="kobo.156.1"> unsupervised learning, RLHF, and </span><strong class="screentext"><span class="kobospan" id="kobo.157.1">constitutional AI</span></strong><span class="kobospan" id="kobo.158.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.159.1">CAI</span></strong><span class="kobospan" id="kobo.160.1">).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.161.1">CAI is a real peculiarity of Claude. </span><span class="kobospan" id="kobo.161.2">In fact, Anthropic paid extraordinary attention to Claude 2 alignment with safety principles. </span><span class="kobospan" id="kobo.161.3">More specifically, Anthropic developed this unique technique called CAI, which was disclosed in December 2022 in the paper </span><em class="italic"><span class="kobospan" id="kobo.162.1">Constitutional AI: Harmlessness from AI Feedback</span></em><span class="kobospan" id="kobo.163.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.164.1">CAI aims to make the model safer and more aligned with human values and intentions by preventing toxic or discriminatory output, not helping a human engage in illegal or unethical activities, and broadly creating an AI system that is helpful, honest, and harmless. </span><span class="kobospan" id="kobo.164.2">To achieve this, it uses a set of principles to guide the model’s behavior and outputs, rather than relying on human feedback or data alone. </span><span class="kobospan" id="kobo.164.3">The principles are derived from various sources, such as the UN Declaration of Human Rights, trust and safety best practices, principles proposed by other AI research labs, non-Western perspectives, and empirical research.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.165.1">CAI uses these principles in two stages of the training process:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.166.1">First, the</span><a id="_idIndexMarker205" class="calibre3"/><span class="kobospan" id="kobo.167.1"> model is trained to critique and revise its own responses using the principles and a few examples.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.168.1">Second, the model is trained via reinforcement learning, but rather than using human feedback, it uses AI-generated feedback based on the principles to choose the more harmless output.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.169.1">The following illustration shows the training process according to the CAI technique:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.170.1"><img alt="A diagram of a process flow  Description automatically generated" src="../Images/B21714_03_07.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.171.1">Figure 3.7: Claude’s training process according to the CAI technique (source: </span><a href="https://arxiv.org/abs/2212.08073" class="calibre3"><span class="kobospan" id="kobo.172.1">https://arxiv.org/abs/2212.08073</span></a><span class="kobospan" id="kobo.173.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.174.1">Another peculiarity of Claude 2 is the context length, which has a limit of 100,000 tokens. </span><span class="kobospan" id="kobo.174.2">This means that users can input longer prompts, namely pages of technical documentation or even a book, which do not need to be embedded. </span><span class="kobospan" id="kobo.174.3">Plus, the model can also generate longer output compared to other LLMs.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.175.1">Finally, Claude 2 demonstrates </span><a id="_idIndexMarker206" class="calibre3"/><span class="kobospan" id="kobo.176.1">relevant capabilities also when working with</span><a id="_idIndexMarker207" class="calibre3"/><span class="kobospan" id="kobo.177.1"> code, scoring 71.2% on the HumanEval benchmark.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.178.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.179.1">HumanEval is a benchmark for evaluating the code generation ability of LLMs. </span><span class="kobospan" id="kobo.179.2">It consists of 164 human-crafted coding problems in Python, each with a prompt, a solution, and a test suite. </span><span class="kobospan" id="kobo.179.3">The problems cover various topics, such as data structures, algorithms, logic, math, and string manipulation. </span><span class="kobospan" id="kobo.179.4">The benchmark can be used to measure the functional correctness, syntactic validity, and semantic coherence of the LLM’s outputs.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.180.1">Overall, Claude 2 is a very interesting model and competitor of GPT-4 to pay attention to. </span><span class="kobospan" id="kobo.180.2">It can be consumed via the REST API or directly via the Anthropic beta chat experience (limited for US and UK users as of August 2023).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.181.1">The following comparison table shows the main differences between the three models:</span></p>
<table class="table-container" id="table001-1">
<tbody class="calibre18">
<tr class="calibre19">
<td class="table-cell"/>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.182.1">GPT-4</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.183.1">Gemini</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.184.1">Claude 2</span></strong></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.185.1">Company or institution</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.186.1">OpenAI</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.187.1">Google</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.188.1">Anthropic</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.189.1">First release</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.190.1">March 2023</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.191.1">December 2023</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.192.1">July 2023</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.193.1">Architecture</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.194.1">Transformer-based, decoder only</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.195.1">Transformer-based</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.196.1">Transformer-based</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.197.1">Sizes and variants</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.198.1">Parameters not officially specified</span></p>
<p class="normal1"><span class="kobospan2" id="kobo.199.1">Two context-length variants:</span></p>
<p class="normal1"><span class="kobospan2" id="kobo.200.1">GPT-4 8K tokens</span></p>
<p class="normal1"><span class="kobospan2" id="kobo.201.1">GPT-4 32K tokens</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.202.1">Three sizes, from smallest to largest: Nano, Pro, and Ultra</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.203.1">Not officially specified</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.204.1">How to use</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.205.1">REST API at OpenAI developer platforms</span></p>
<p class="normal1"><span class="kobospan3" id="kobo.206.1">Using OpenAI Playground at </span><a href="https://platform.openai.com/playground" class="calibre20"><span class="url"><span class="kobospan2" id="kobo.207.1">https://platform.openai.com/playground</span></span></a></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.208.1">REST API at Google AI Studio</span></p>
<p class="normal1"><span class="kobospan3" id="kobo.209.1">Using Gemini at </span><a href="https://gemini.google.com/" class="calibre20"><span class="url"><span class="kobospan2" id="kobo.210.1">https://gemini.google.com/</span></span></a></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan3" id="kobo.211.1">REST API after compiling the form at </span><a href="https://www.anthropic.com/claude" class="calibre20"><span class="url"><span class="kobospan2" id="kobo.212.1">https://www.anthropic.com/claude</span></span></a></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref"><span class="kobospan" id="kobo.213.1">Table 3.1: Comparison table of GPT-4, PaLM 2, and Claude 2</span></p>
<p class="normal1"><span class="kobospan" id="kobo.214.1">In addition to proprietary models, there is a huge market for open-source LLMs available today. </span><span class="kobospan" id="kobo.214.2">Let’s </span><a id="_idIndexMarker208" class="calibre3"/><span class="kobospan" id="kobo.215.1">discuss </span><a id="_idIndexMarker209" class="calibre3"/><span class="kobospan" id="kobo.216.1">some of these in the next section.</span></p>
<h2 class="heading1" id="_idParaDest-45"><span class="kobospan" id="kobo.217.1">Open-source models</span></h2>
<p class="normal"><span class="kobospan" id="kobo.218.1">The advantage</span><a id="_idIndexMarker210" class="calibre3"/><span class="kobospan" id="kobo.219.1"> of an </span><a id="_idIndexMarker211" class="calibre3"/><span class="kobospan" id="kobo.220.1">open-source model is that, by definition, developers have full visibility and access to the source code. </span><span class="kobospan" id="kobo.220.2">In the context of LLMs, this implies the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.221.1">You have major control over the architecture, meaning that you can also modify it in the local version you are going to use within your project. </span><span class="kobospan" id="kobo.221.2">This also implies that they are not prone to potential updates to the source code made by models’ owners.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.222.1">There is the possibility to train your model from scratch, on top of the classical fine-tuning, which is also available for proprietary models.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.223.1">Free to use, meaning that you won’t incur any charge while using those LLMs, in contrast with the proprietary ones that have pay-per-use pricing.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.224.1">To compare open-source models, throughout this book, we will refer to the independent Hugging Face Open LLM Leaderboard (you can find it at </span><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.225.1">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)</span></span></a><span class="kobospan" id="kobo.226.1">, a project that aims to evaluate and compare the performance of LLMs on various </span><strong class="screentext"><span class="kobospan" id="kobo.227.1">natural language understanding</span></strong><span class="kobospan" id="kobo.228.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.229.1">NLU</span></strong><span class="kobospan" id="kobo.230.1">) tasks. </span><span class="kobospan" id="kobo.230.2">The project is hosted on Hugging Face Spaces, a platform for </span><a id="_idIndexMarker212" class="calibre3"/><span class="kobospan" id="kobo.231.1">creating and sharing machine-learning applications.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.232.1">The Open LLM Leaderboard uses four main evaluation benchmarks, which we covered in </span><em class="italic"><span class="kobospan" id="kobo.233.1">Chapter 1</span></em><span class="kobospan" id="kobo.234.1">, in the </span><em class="italic"><span class="kobospan" id="kobo.235.1">Model evaluation</span></em><span class="kobospan" id="kobo.236.1"> section:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.237.1">AI2 Reasoning Challenge</span></strong><span class="kobospan" id="kobo.238.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.239.1">ARC</span></strong><span class="kobospan" id="kobo.240.1">): Grade-school </span><a id="_idIndexMarker213" class="calibre3"/><span class="kobospan" id="kobo.241.1">science questions and complex NLU tasks.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.242.1">HellaSwag</span></strong><span class="kobospan" id="kobo.243.1">: Common</span><a id="_idIndexMarker214" class="calibre3"/><span class="kobospan" id="kobo.244.1"> sense reasoning.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.245.1">MMLU</span></strong><span class="kobospan" id="kobo.246.1">: Tasks in various domains, including math, computer science, and law.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.247.1">TruthfulQA</span></strong><span class="kobospan" id="kobo.248.1">: An evaluation</span><a id="_idIndexMarker215" class="calibre3"/><span class="kobospan" id="kobo.249.1"> of how truthful the model is when generating answers.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.250.1">Even though</span><a id="_idIndexMarker216" class="calibre3"/><span class="kobospan" id="kobo.251.1"> those are just a subsample of the </span><a id="_idIndexMarker217" class="calibre3"/><span class="kobospan" id="kobo.252.1">plethora of LLMs’ benchmarks, we will stick to this leaderboard as a reference evaluation framework as it being widely adopted.</span></p>
<h3 class="heading2" id="_idParaDest-46"><span class="kobospan" id="kobo.253.1">LLaMA-2</span></h3>
<p class="normal"><strong class="screentext"><span class="kobospan" id="kobo.254.1">Large Language Model Meta AI 2</span></strong><span class="kobospan" id="kobo.255.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.256.1">LLaMA-2</span></strong><span class="kobospan" id="kobo.257.1">) is a new family of models developed by Meta and </span><a id="_idIndexMarker218" class="calibre3"/><span class="kobospan" id="kobo.258.1">unveiled to the public on July 18, 2023, open source and for free (its first version was originally limited to researchers).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.259.1">It is an </span><strong class="screentext"><span class="kobospan" id="kobo.260.1">autoregressive</span></strong><span class="kobospan" id="kobo.261.1"> model </span><a id="_idIndexMarker219" class="calibre3"/><span class="kobospan" id="kobo.262.1">with an optimized, decoder-only transformer architecture.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.263.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.264.1">The concept of autoregressive in the context of transformers refers to the fact that the model predicts the next token in the sequence, conditioned on all the previous tokens. </span><span class="kobospan" id="kobo.264.2">This is done by masking the future tokens in the input so that the model can only attend to the past tokens. </span><span class="kobospan" id="kobo.264.3">For example, if the input sequence is “The sky is blue,” the model would predict “The” first, then “sky,” then “is,” and finally “blue,” using a mask to hide the tokens that come after each prediction.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.265.1">LLaMA-2 models come in three sizes: 7, 13, and 70 billion parameters. </span><span class="kobospan" id="kobo.265.2">All the versions have been trained on 2 trillion tokens and have a context length of 4,092 tokens.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.266.1">On top of that, all model sizes come with a “chat” version, called LLaMA-2-chat, which is more versatile for</span><a id="_idIndexMarker220" class="calibre3"/><span class="kobospan" id="kobo.267.1"> general-purpose conversational scenarios compared to the base model LLama-2.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.268.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.269.1">In the context of LLMs, the difference between </span><strong class="screentext"><span class="kobospan" id="kobo.270.1">base models</span></strong><span class="kobospan" id="kobo.271.1"> and “chat” or </span><strong class="screentext"><span class="kobospan" id="kobo.272.1">assistant models</span></strong><span class="kobospan" id="kobo.273.1"> is </span><a id="_idIndexMarker221" class="calibre3"/><span class="kobospan" id="kobo.274.1">primarily in their</span><a id="_idIndexMarker222" class="calibre3"/><span class="kobospan" id="kobo.275.1"> training and</span><a id="_idIndexMarker223" class="calibre3"/><span class="kobospan" id="kobo.276.1"> intended use:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.277.1">Base models: These </span><a id="_idIndexMarker224" class="calibre3"/><span class="kobospan" id="kobo.278.1">models are trained on vast amounts of text data, often sourced from the internet, and their primary function is to predict the next word in a given context, which makes them great at understanding and generating language. </span><span class="kobospan" id="kobo.278.2">However, they might not always be precise or focused on specific instructions.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.279.1">Assistant models: These</span><a id="_idIndexMarker225" class="calibre3"/><span class="kobospan" id="kobo.280.1"> models start as base LLMs but are further fine-tuned with input-output pairs that include instructions and the model’s attempts to follow those instructions. </span><span class="kobospan" id="kobo.280.2">They often employ RLHF to refine the model, making it better at being helpful, honest, and harmless. </span><span class="kobospan" id="kobo.280.3">As a result, they are less likely to generate problematic text and are more suitable for practical applications like chatbots and content generation. </span><span class="kobospan" id="kobo.280.4">For example, the assistant model GPT-3.5 Turbo (the model behind ChatGPT) is a fine-tuned version of the completion model GPT-3.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.281.1">In essence, while base models provide a broad understanding of language, assistant models are optimized to follow instructions and provide more accurate and contextually relevant responses.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.282.1">LLaMA-2-chat was developed with a fine-tuning process that consisted of two main steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><strong class="screentext"><span class="kobospan" id="kobo.283.1">Supervised fine-tuning</span></strong><span class="kobospan" id="kobo.284.1">: This </span><a id="_idIndexMarker226" class="calibre3"/><span class="kobospan" id="kobo.285.1">step involves fine-tuning the model on publicly available instruction datasets and over 1 million human annotations, to make them more helpful and safe for conversational use cases. </span><span class="kobospan" id="kobo.285.2">The fine-tuning process uses a selected list of prompts to guide the model outputs, and a loss function that encourages diversity and relevance (that’s the reason why it is “supervised”).</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.286.1">RLHF</span></strong><span class="kobospan" id="kobo.287.1">: As we saw while introducing GPT-4, RLHF is a technique that aims at using human feedback as an evaluating metric for LLMs’ generated output, and then using that feedback to further optimize the model.</span></li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.288.1">The following is an</span><a id="_idIndexMarker227" class="calibre3"/><span class="kobospan" id="kobo.289.1"> illustration of how the training process for LLaMA works:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.290.1"><img alt="" role="presentation" src="../Images/B21714_03_08.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.291.1">Figure 3.8: Two-step fine-tuning to obtain LLaMa-2 chat (source: </span><a href="https://ai.meta.com/resources/models-and-libraries/llama/" class="calibre3"><span class="kobospan" id="kobo.292.1">https://ai.meta.com/resources/models-and-libraries/llama/</span></a><span class="kobospan" id="kobo.293.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.294.1">To access the model, you need to submit a request on Meta’s website (the form is available at </span><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.295.1">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</span></span></a><span class="kobospan" id="kobo.296.1">). </span><span class="kobospan" id="kobo.296.2">Once a request is submitted, you will receive an email with the GitHub repository where you will be</span><a id="_idIndexMarker228" class="calibre3"/><span class="kobospan" id="kobo.297.1"> able to download the following assets:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.298.1">Model code</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.299.1">Model weights</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.300.1">README (User Guide)</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.301.1">Responsible Use Guide</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.302.1">License</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.303.1">Acceptable Use Policy</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.304.1">Model Card</span></li>
</ul>
<h3 class="heading2" id="_idParaDest-47"><span class="kobospan" id="kobo.305.1">Falcon LLM</span></h3>
<p class="normal"><span class="kobospan" id="kobo.306.1">Falcon LLM is </span><a id="_idIndexMarker229" class="calibre3"/><span class="kobospan" id="kobo.307.1">a representation of a new trend of LLMs, consisting of building lighter models (with fewer parameters) and focusing rather on the quality of the training dataset. </span><span class="kobospan" id="kobo.307.2">Indeed, it is a matter of fact that complex models like GPT-4 with trillions of parameters are extremely heavy, both in the training phase and inference phase. </span><span class="kobospan" id="kobo.307.3">This implies the need for high and expensive computational power (GPU and TPU-powered) as well as a long training time.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.308.1">Falcon LLM is an open-source model launched by Abu Dhabi’s </span><strong class="screentext"><span class="kobospan" id="kobo.309.1">Technology Innovation Institute</span></strong><span class="kobospan" id="kobo.310.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.311.1">TII</span></strong><span class="kobospan" id="kobo.312.1">) in May 2023. </span><span class="kobospan" id="kobo.312.2">It </span><a id="_idIndexMarker230" class="calibre3"/><span class="kobospan" id="kobo.313.1">is an autoregressive, decoder-only transformer, trained on 1 trillion tokens, and it has 40 billion parameters (even though it has also been released as a lighter version with 7 billion parameters). </span><span class="kobospan" id="kobo.313.2">Similarly to what we saw for LlaMA, Falcon LLM also comes with a fine-tuned variant, called “Instruct,” which is tailored toward following the user’s instructions.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.314.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.315.1">Instruct models are specialized for short-form instruction following. </span><span class="kobospan" id="kobo.315.2">Instruction following is a task where the model has to execute a natural language command or query, such as “write a haiku about cats” or “tell me about the weather in Paris.” </span><span class="kobospan" id="kobo.315.3">The Instruct fine-tuned models are trained on a large dataset of instructions and their corresponding outputs, such as the Stanford Alpaca dataset.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.316.1">According to the Open LLM leaderboard, since its launch, Falcon LLM has been among the first positions globally, second only to some versions of LlaMA.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.317.1">So, the question might be: how can a model with “only” 40 billion parameters perform so well? </span><span class="kobospan" id="kobo.317.2">In fact, the answer is in the quality of the dataset. </span><span class="kobospan" id="kobo.317.3">Falcon was developed using specialized tools and incorporates a unique data pipeline, which is capable of extracting valuable content from web data. </span><span class="kobospan" id="kobo.317.4">The pipeline was designed to extract high-quality content by employing extensive filtering and deduplication techniques. </span><span class="kobospan" id="kobo.317.5">The resulting dataset, called </span><em class="italic"><span class="kobospan" id="kobo.318.1">RefinedWeb</span></em><span class="kobospan" id="kobo.319.1">, has been released by TII under the Apache-2.0 license and can be found at </span><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.320.1">https://huggingface.co/datasets/tiiuae/falcon-refinedweb</span></span></a><span class="kobospan" id="kobo.321.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.322.1">By combining superior data quality with these optimizations, Falcon achieves remarkable performance </span><a id="_idIndexMarker231" class="calibre3"/><span class="kobospan" id="kobo.323.1">while utilizing around 75% and 80% of the training compute budget of GPT-3 and PaLM-62B, respectively.</span></p>
<h3 class="heading2" id="_idParaDest-48"><span class="kobospan" id="kobo.324.1">Mistral</span></h3>
<p class="normal"><span class="kobospan" id="kobo.325.1">The third and last </span><a id="_idIndexMarker232" class="calibre3"/><span class="kobospan" id="kobo.326.1">open-source model series we are going to cover is Mistral, developed by Mistral AI, a company founded in April 2023 by a team of AI scientists who previously worked at Meta Platforms and Google DeepMind. </span><span class="kobospan" id="kobo.326.2">Based in France, the company has quickly made a name for itself by raising significant funding and releasing open-source LLMs, emphasizing the importance of transparency and accessibility in AI development.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.327.1">The Mistral model, particularly the Mistral-7B-v0.1, is a decoder-only transformer with 7.3 billion parameters, designed for generative text tasks. </span><span class="kobospan" id="kobo.327.2">It’s known for its innovative architecture choices like </span><strong class="screentext"><span class="kobospan" id="kobo.328.1">grouped-query attention</span></strong><span class="kobospan" id="kobo.329.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.330.1">GQA</span></strong><span class="kobospan" id="kobo.331.1">) and </span><strong class="screentext"><span class="kobospan" id="kobo.332.1">sliding-window attention</span></strong><span class="kobospan" id="kobo.333.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.334.1">SWA</span></strong><span class="kobospan" id="kobo.335.1">), which have allowed it to outperform other models in benchmarks.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.336.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.337.1">GQA and SWA are mechanisms designed to improve the efficiency and performance of an LLM.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.338.1">GQA is a</span><a id="_idIndexMarker233" class="calibre3"/><span class="kobospan" id="kobo.339.1"> technique that allows for faster inference times compared to standard full attention mechanisms. </span><span class="kobospan" id="kobo.339.2">It does this by partitioning the attention mechanism’s query heads into groups, with each group sharing a single key head and value head.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.340.1">SWA is used to </span><a id="_idIndexMarker234" class="calibre3"/><span class="kobospan" id="kobo.341.1">handle longer text sequences efficiently. </span><span class="kobospan" id="kobo.341.2">It extends the model’s attention beyond a fixed window size, allowing each layer to reference a range of positions from the preceding layer. </span><span class="kobospan" id="kobo.341.3">This means that the hidden state at a certain position in one layer can attend to hidden states within a specific range in the previous layer, thus enabling the model to access tokens at a greater distance and manage sequences </span><a id="_idIndexMarker235" class="calibre3"/><span class="kobospan" id="kobo.342.1">of varying lengths with a reduced inference cost.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.343.1">The model also provides a variant that was fine-tuned for general-purpose capabilities. </span><span class="kobospan" id="kobo.343.2">This variant is called Mistral-7B-instruct, which outperformed all other 7 billion LLMs on the market (as of April 2024) on MT-Bench (an evaluation framework that uses an LLM as a judge).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.344.1">Like many other open-source models, Mistral can be consumed and downloaded via Hugging Face Hub.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.345.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.346.1">In February 2024, Mistral AI and Microsoft entered a multi-year partnership to accelerate AI innovation. </span><span class="kobospan" id="kobo.346.2">This collaboration will leverage Microsoft’s Azure AI supercomputing infrastructure to support the development and deployment of Mistral AI’s LLMs. </span><span class="kobospan" id="kobo.346.3">Mistral AI’s models, including their advanced model, Mistral Large, will be available to customers through Azure AI Studio and Azure Machine Learning model catalog. </span><span class="kobospan" id="kobo.346.4">The partnership aims to expand Mistral AI’s reach to global markets and foster ongoing research collaboration.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.347.1">The following </span><a id="_idIndexMarker236" class="calibre3"/><span class="kobospan" id="kobo.348.1">comparison table provides the main differences between the three models:</span></p>
<table class="table-container" id="table002">
<tbody class="calibre18">
<tr class="calibre19">
<td class="table-cell"/>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.349.1">LlaMA</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.350.1">Falcon LLM</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.351.1">Mistral</span></strong></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.352.1">Company or institution</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.353.1">Meta</span></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword1"><span class="kobospan2" id="kobo.354.1">Technology Innovation Institute</span></strong><span class="kobospan" id="kobo.355.1"> (</span><strong class="screentext"><span class="kobospan2" id="kobo.356.1">TII</span></strong><span class="kobospan4" id="kobo.357.1">)</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.358.1">Mistral AI</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.359.1">First release</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.360.1">July 2023</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.361.1">May 2023</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.362.1">September 2023</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.363.1">Architecture</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.364.1">Autoregressive transformer, decoder-only</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.365.1">Autoregressive transformer, decoder-only</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.366.1">Transformer, decoder only</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.367.1">Sizes and variants</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.368.1">Three sizes: 7B, 13B, and 70B, alongside the fine-tuned version (chat)</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.369.1">Two sizes: 7B and 40B, alongside the fine-tuned version (instruct)</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.370.1">7B size alongside the fine-tuned version (instruct)</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.371.1">Licenses</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan3" id="kobo.372.1">A custom commercial license is available at </span><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" class="calibre20"><span class="url"><span class="kobospan2" id="kobo.373.1">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</span></span></a></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.374.1">Commercial Apache 2.0 licensed</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.375.1">Commercial Apache 2.0 licensed</span></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword"><span class="kobospan2" id="kobo.376.1">How to use</span></strong></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan3" id="kobo.377.1">Submit request form at </span><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" class="calibre20"><span class="url"><span class="kobospan2" id="kobo.378.1">https://ai.meta.com/resources/models-and-libraries/llama-downloads/ and download the GitHub repo</span></span></a></p>
<p class="normal1"><span class="kobospan2" id="kobo.379.1">Also available in Hugging Face Hub</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.380.1">Download or use Hugging Face Hub Inference API/Endpoint</span></p>
</td>
<td class="table-cell">
<p class="normal1"><span class="kobospan2" id="kobo.381.1">Download or use Hugging Face Hub Inference API/Endpoint or Azure AI Studio</span></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref"><span class="kobospan" id="kobo.382.1">Table 3.2: Comparison table of LLMs</span></p>
<h1 class="heading" id="_idParaDest-49"><span class="kobospan" id="kobo.383.1">Beyond language models</span></h1>
<p class="normal"><span class="kobospan" id="kobo.384.1">So far, we have</span><a id="_idIndexMarker237" class="calibre3"/><span class="kobospan" id="kobo.385.1"> only been covering language-specific foundation models as they are the focus of this book. </span><span class="kobospan" id="kobo.385.2">Nevertheless, in the context of AI-powered applications, it is worth mentioning that there are additional foundation models that can handle data that is different from text, which can be embedded and orchestrated.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.386.1">Here, you can find some examples of </span><strong class="screentext"><span class="kobospan" id="kobo.387.1">large foundation models</span></strong><span class="kobospan" id="kobo.388.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.389.1">LFMs</span></strong><span class="kobospan" id="kobo.390.1">) on the market today:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.391.1">Whisper</span></strong><span class="kobospan" id="kobo.392.1">: It is a general-purpose </span><a id="_idIndexMarker238" class="calibre3"/><span class="kobospan" id="kobo.393.1">speech </span><a id="_idIndexMarker239" class="calibre3"/><span class="kobospan" id="kobo.394.1">recognition model developed by OpenAI that can transcribe and translate speech in multiple languages. </span><span class="kobospan" id="kobo.394.2">It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, spoken language identification, and voice activity detection.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.395.1">Midjourney</span></strong><span class="kobospan" id="kobo.396.1">: Developed </span><a id="_idIndexMarker240" class="calibre3"/><span class="kobospan" id="kobo.397.1">by the independent research lab of the same name, Midjourney is based on a sequence-to-sequence transformer model that takes text prompts and outputs a set of four images that match the prompts. </span><span class="kobospan" id="kobo.397.2">Midjourney is designed to be a tool for artists and creative professionals, who can use it for rapid prototyping of artistic concepts, inspiration, or </span><a id="_idIndexMarker241" class="calibre3"/><span class="kobospan" id="kobo.398.1">experimentation.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.399.1">DALL-E</span></strong><span class="kobospan" id="kobo.400.1">: Similar to </span><a id="_idIndexMarker242" class="calibre3"/><span class="kobospan" id="kobo.401.1">the previous one, DALL-E, developed by OpenAI, generates images from natural language descriptions, using a 12-billion parameter version</span><a id="_idIndexMarker243" class="calibre3"/><span class="kobospan" id="kobo.402.1"> of GPT-3 trained on a dataset of text-image pairs.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.403.1">The idea is that we can combine and orchestrate multiple LFMs within our applications to achieve extraordinary results. </span><span class="kobospan" id="kobo.403.2">For example, let’s say we want to write a review about an interview with a</span><a id="_idIndexMarker244" class="calibre3"/><span class="kobospan" id="kobo.404.1"> young chef and post it on Instagram. </span><span class="kobospan" id="kobo.404.2">The involved models might be the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.405.1">Whisper</span></strong><span class="kobospan" id="kobo.406.1"> will convert the interview audio into a transcript.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.407.1">An </span><strong class="screentext"><span class="kobospan" id="kobo.408.1">LLM</span></strong><span class="kobospan" id="kobo.409.1">, such as Falcon-7B-instruct, with a web plugin, will extrapolate the name of the young chef and search it on the internet to retrieve the biography.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.410.1">Another </span><strong class="screentext"><span class="kobospan" id="kobo.411.1">LLM</span></strong><span class="kobospan" id="kobo.412.1">, such as LlaMA, will process the transcript and generate a review with an Instagram post style. </span><span class="kobospan" id="kobo.412.2">We can also ask the same model to generate a prompt that will ask the following model to generate a picture based on the post content.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.413.1">Dall-E</span></strong><span class="kobospan" id="kobo.414.1"> will generate an image based on the prompt generated by the LLM.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.415.1">We will then provide our LFMs flow with an Instagram plugin so that the application is able to post the whole review, including the illustration, on our profile.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.416.1">Finally, there are emerging LFMs that are meant to be multi-modal, meaning that they can handle multiple data formats with just one architecture. </span><span class="kobospan" id="kobo.416.2">An example is GPT-4 itself.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.417.1">The following screenshot shows an example of an early OpenAI experiment with GPT-4 visuals, demonstrating its understanding of funny aspects within an image:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.418.1"><img alt="A cell phone with a cable plugged into it  Description automatically generated" src="../Images/B21714_03_09.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.419.1">Figure 3.9: Early experiments with GPT-4 visuals (source: </span><a href="https://openai.com/research/gpt-4" class="calibre3"><span class="kobospan" id="kobo.420.1">https://openai.com/research/gpt-4</span></a><span class="kobospan" id="kobo.421.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.422.1">The following</span><a id="_idIndexMarker245" class="calibre3"/><span class="kobospan" id="kobo.423.1"> screenshot shows another example of an earlier version of GPT-4, demonstrating how it could understand and explain graphs in detail:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.424.1"><img alt="A screenshot of a graph  Description automatically generated" src="../Images/B21714_03_10.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.425.1">Figure 3.10: Early experiments with GPT-4 visuals (source: </span><a href="https://openai.com/research/gpt-4" class="calibre3"><span class="kobospan" id="kobo.426.1">https://openai.com/research/gpt-4</span></a><span class="kobospan" id="kobo.427.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.428.1">The following example</span><a id="_idIndexMarker246" class="calibre3"/><span class="kobospan" id="kobo.429.1"> shows how an early version of GPT-4 could understand and solve complex mathematical problems while also providing the corresponding justification for its response:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.430.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_03_11.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.431.1">Figure 3.11: Early experiments with GPT-4 visuals (source: </span><a href="https://openai.com/research/gpt-4" class="calibre3"><span class="kobospan" id="kobo.432.1">https://openai.com/research/gpt-4</span></a><span class="kobospan" id="kobo.433.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.434.1">GPT-4 is just one example of a </span><strong class="screentext"><span class="kobospan" id="kobo.435.1">large multimodal model</span></strong><span class="kobospan" id="kobo.436.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.437.1">LMM</span></strong><span class="kobospan" id="kobo.438.1">), and it is representative of the trend that we </span><a id="_idIndexMarker247" class="calibre3"/><span class="kobospan" id="kobo.439.1">will probably witness in the next few years.</span></p>
<h1 class="heading" id="_idParaDest-50"><span class="kobospan" id="kobo.440.1">A decision framework to pick the right LLM</span></h1>
<p class="normal"><span class="kobospan" id="kobo.441.1">In previous paragraphs, we </span><a id="_idIndexMarker248" class="calibre3"/><span class="kobospan" id="kobo.442.1">covered some of the most promising LLMs available in the market today. </span><span class="kobospan" id="kobo.442.2">Now, the question is: which one should I use within my applications? </span><span class="kobospan" id="kobo.442.3">The truth is that there is not a straightforward answer to this question.</span></p>
<h2 class="heading1" id="_idParaDest-51"><span class="kobospan" id="kobo.443.1">Considerations</span></h2>
<p class="normal"><span class="kobospan" id="kobo.444.1">There are many</span><a id="_idIndexMarker249" class="calibre3"/><span class="kobospan" id="kobo.445.1"> factors to consider when choosing an LLM for your application. </span><span class="kobospan" id="kobo.445.2">Those factors also need to be declined in two scenarios: proprietary and open-source LLMs. </span><span class="kobospan" id="kobo.445.3">The following are some factors and trade-offs you might want to consider while choosing your LLMs:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.446.1">Size and performance</span></strong><span class="kobospan" id="kobo.447.1">: We saw that more complex models (that means, with a high number of parameters) tend to have better performance, especially in terms of parametric knowledge and generalization capabilities. </span><span class="kobospan" id="kobo.447.2">Nevertheless, the larger the model, the more computation and memory it requires to process the input and generate the output, which can result in higher latency and, as we will see, higher costs.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.448.1">Cost and hosting strategy</span></strong><span class="kobospan" id="kobo.449.1">: When incorporating LLMs within our applications, there are two types of costs we have to keep in mind:</span><ul class="calibre17">
<li class="bulletlist2"><strong class="screentext"><span class="kobospan" id="kobo.450.1">Cost for model consumption</span></strong><span class="kobospan" id="kobo.451.1">: This refers to the fee we pay to consume the model. </span><span class="kobospan" id="kobo.451.2">Proprietary models like GPT-4 or Claude 2 require a fee, which is typically proportional to the number of tokens processed. </span><span class="kobospan" id="kobo.451.3">On the other hand, open-source models like LlaMA or Falcon LLM are free to use.</span></li>
<li class="bulletlist3"><strong class="screentext"><span class="kobospan" id="kobo.452.1">Cost for model hosting</span></strong><span class="kobospan" id="kobo.453.1">: This refers to your hosting strategy. </span><span class="kobospan" id="kobo.453.2">Typically, proprietary models are hosted in a private or public hyperscaler, so that they can be consumed via a REST API and you don’t have to worry about the underlying infrastructure (for example, GPT-4 is hosted in a super-computer built in the Microsoft Azure cloud). </span><span class="kobospan" id="kobo.453.3">With open-source models, we typically need to provide our own infrastructure, since those models can be downloaded locally. </span><span class="kobospan" id="kobo.453.4">Of course, the larger the model, the more powerful the computational power needed.</span><div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.454.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.455.1">In the context of open-source models, another option to consume those models is that of using the Hugging Face Inference API. </span><span class="kobospan" id="kobo.455.2">The free version allows you to test and evaluate, with a limited rate, all the available LLMs on a shared infrastructure hosted on Hugging Face. </span><span class="kobospan" id="kobo.455.3">For production use cases, Hugging Face also offers Inference Endpoints, so that you can easily deploy your LLMs on a dedicated and fully managed infrastructure, with the possibility to configure parameters like region, compute power, and security level to accommodate your constraints in terms of latency, throughput, and compliance.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.456.1">Pricing for the Inference Endpoint is publicly available at </span><a href="https://huggingface.co/docs/inference-endpoints/pricing" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.457.1">https://huggingface.co/docs/inference-endpoints/pricing</span></span></a><span class="kobospan" id="kobo.458.1">.</span></p>
</div>
</li>
</ul>
</li>
</ul>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.459.1">Customization</span></strong><span class="kobospan" id="kobo.460.1">: This might be a requirement you want to evaluate before deciding which</span><a id="_idIndexMarker250" class="calibre3"/><span class="kobospan" id="kobo.461.1"> model to adopt. </span><span class="kobospan" id="kobo.461.2">In fact, not all models are equally flexible in terms of customization. </span><span class="kobospan" id="kobo.461.3">When we talk about customization, we refer to two activities:</span><ul class="calibre17">
<li class="bulletlist2"><strong class="screentext"><span class="kobospan" id="kobo.462.1">Fine-tuning</span></strong><span class="kobospan" id="kobo.463.1">: This is the process of slightly adjusting LLMs’ parameters to better fit into a domain. </span><span class="kobospan" id="kobo.463.2">All open-source models can be fine-tuned. </span><span class="kobospan" id="kobo.463.3">When it comes to proprietary models, not all LLMs can be fine-tuned: for example, OpenAI’s GPT-3.5 can be fine-tuned, while the process of fine-tuning the GPT-4-0613 is still experimental and accessible under request to OpenAI (as per December 2023).</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.464.1">Henceforth, it is important to understand whether you will need fine-tuning in your application and decide accordingly.</span></p>
<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext"><span class="kobospan" id="kobo.465.1">Training from scratch</span></strong><span class="kobospan" id="kobo.466.1">: If you really want an LLM that is super specific about your domain knowledge, you might want to retrain the model from scratch. </span><span class="kobospan" id="kobo.466.2">To train an LLM from scratch, without having to reinvent an architecture, you can download open-source LLMs and simply re-train them on custom datasets. </span><span class="kobospan" id="kobo.466.3">Of course, this implies that we have access to the source code, which is not the case when we work with proprietary LLMs.</span></li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.467.1">Domain-specific capabilities</span></strong><span class="kobospan" id="kobo.468.1">: We saw that the most popular way of evaluating LLMs’ performance is that of averaging different benchmarks across domains. </span><span class="kobospan" id="kobo.468.2">However, there are benchmarks that are tailored towards specific capabilities: if MMLU measures LLMs’ generalized culture and commonsense reasoning, TruthfulQA is more concerned with LLMs’ alignment, while HumanEval is tailored towards LLMs’ coding capabilities.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.469.1">Henceforth, if you have a tailored use case in mind, you might want to use a model that is a top performer in one specific benchmark, rather than a top performer, on average, across all benchmarks. </span><span class="kobospan" id="kobo.469.2">Namely, you might pick Claude 2 if you are looking for exceptional coding capabilities, or PaLM 2 if analytical reasoning is what you are looking for. </span><span class="kobospan" id="kobo.469.3">On the other hand, if you need a model that encompasses all of these capabilities, GPT-4 might be the right choice for you.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.470.1">Picking a domain-specific model is also a way to make some savings in terms of model complexity. </span><span class="kobospan" id="kobo.470.2">The thing is, it might be sufficient for you to use a relatively small </span><a id="_idIndexMarker251" class="calibre3"/><span class="kobospan" id="kobo.471.1">model (for example, a LlaMA-7B-instruct) if you need to use it for a specific use case, which comes with all the benefits in terms of cost and performance.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.472.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.473.1">If you are looking for LLMs that are </span><em class="italic"><span class="kobospan" id="kobo.474.1">extremely</span></em><span class="kobospan" id="kobo.475.1"> specific, there is a plethora of models that have been trained on domain-specific technical documentation. </span><span class="kobospan" id="kobo.475.2">For example, at the beginning of 2023, the </span><strong class="screentext"><span class="kobospan" id="kobo.476.1">Stanford</span></strong> <strong class="screentext"><span class="kobospan" id="kobo.477.1">Center for Research on Foundation Models</span></strong><span class="kobospan" id="kobo.478.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.479.1">CRFM)</span></strong><span class="kobospan" id="kobo.480.1"> and</span><a id="_idIndexMarker252" class="calibre3"/><span class="kobospan" id="kobo.481.1"> MosaicML announced the release of BioMedLM, a decoder-only transformer-based LLM with 2.7 billion parameters, trained on biomedical abstracts and papers.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.482.1">Another example is BloombergGPT, a 50 billion parameter LLM specialized for the financial domain developed by Bloomberg and trained on a 363 billion token dataset based on Bloomberg’s extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.483.1">To make this decision framework more practical, let’s consider the following imaginary case study</span><a id="_idIndexMarker253" class="calibre3"/><span class="kobospan" id="kobo.484.1"> about the company TechGen.</span></p>
<h2 class="heading1" id="_idParaDest-52"><span class="kobospan" id="kobo.485.1">Case study</span></h2>
<p class="normal"><span class="kobospan" id="kobo.486.1">TechGen Solutions, a </span><a id="_idIndexMarker254" class="calibre3"/><span class="kobospan" id="kobo.487.1">leading provider </span><a id="_idIndexMarker255" class="calibre3"/><span class="kobospan" id="kobo.488.1">of AI-driven analytics, face a decision between two advanced language models for their next-generation customer interaction system: GPT-4 and LLaMa-2. </span><span class="kobospan" id="kobo.488.2">They require a robust language model that can handle diverse customer queries, provide accurate technical information, and integrate with their proprietary software. </span><span class="kobospan" id="kobo.488.3">The following are their options:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.489.1">GPT-4: Developed by OpenAI, GPT-4 is known for its vast parameter count and the ability to process both text and image inputs</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.490.1">LLama 2: Created by Meta AI, LLama 2 is an open-source model praised for its accessibility and performance on a smaller dataset.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.491.1">The following are the factors that they consider when making their decision:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.492.1">Performance: TechGen evaluates the models’ performance, particularly in generating technical content and code, where GPT-4 has shown higher accuracy.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.493.1">Integration: The ease of integration with TechGen’s systems is critical, with GPT-4 potentially offering more seamless compatibility due to its widespread adoption.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.494.1">Cost: While LLama 2 is free for commercial use under certain conditions, GPT-4 comes with a cost, which TechGen must factor into their decision.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.495.1">Future-proofing: TechGen considers the long-term viability of each model, including the potential for updates and improvements.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.496.1">Based on these considerations, TechGen opts for GPT-4, swayed by its superior performance in generating complex, technical responses and its multilingual capabilities, which align with their international expansion plans. </span><span class="kobospan" id="kobo.496.2">The decision is also influenced by GPT-4’s image processing feature, which TechGen anticipates will become increasingly relevant as they incorporate more multimedia content into their customer service.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.497.1">TechGen’s choice of GPT-4 over LLama 2 is driven by the need for a high-performing, versatile language model that can scale with their growing global presence and diverse customer needs. </span><span class="kobospan" id="kobo.497.2">While LLama 2’s open-source nature and cost effectiveness are appealing, GPT-4’s advanced capabilities and future-proof features present a more compelling case for TechGen’s ambitious goals.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.498.1">Note that these decision factors are not meant to be an exhaustive guide to deciding which models to embed within applications. </span><span class="kobospan" id="kobo.498.2">Nevertheless, those are useful elements of reflection while setting up your application flow, so that you can determine your requirements</span><a id="_idIndexMarker256" class="calibre3"/><span class="kobospan" id="kobo.499.1"> and then shortlist those LLMs that are more suitable for your goals.</span></p>
<h1 class="heading" id="_idParaDest-53"><span class="kobospan" id="kobo.500.1">Summary</span></h1>
<p class="normal"><span class="kobospan" id="kobo.501.1">This chapter covered some of the most promising LLMs in the market. </span><span class="kobospan" id="kobo.501.2">It first differentiated between proprietary and open-source models, with all the related pros and cons. </span><span class="kobospan" id="kobo.501.3">It then offered a deep dive into the architecture and technical features of GPT-4, PaLM-2, Claude 2, LLaMa-2, Falcon LLM, and MPT, with the addition of a section covering some LMMs. </span><span class="kobospan" id="kobo.501.4">Finally, it provided a light framework to help developers decide which LLMs to pick while building AI-powered applications. </span><span class="kobospan" id="kobo.501.5">This is pivotal to get the greatest impact from your application, given your industry-specific scenario.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.502.1">Starting from the next chapter, we will start working hands-on with LLMs within applications.</span></p>
<h1 class="heading" id="_idParaDest-54"><span class="kobospan" id="kobo.503.1">References</span></h1>
<ul class="calibre16">
<li class="bulletlist"><span class="kobospan" id="kobo.504.1">GPT-4 Technical Report. </span><a href="https://cdn.openai.com/papers/gpt-4.pdf" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.505.1">https://cdn.openai.com/papers/gpt-4.pdf</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.506.1">Train short, test long: attention with linear biases enables input length extrapolation. </span><a href="https://arxiv.org/pdf/2108.12409.pdf" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.507.1">https://arxiv.org/pdf/2108.12409.pdf</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.508.1">Constitutional AI: Harmlessness from AI Feedback. </span><a href="https://arxiv.org/abs/2212.08073" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.509.1">https://arxiv.org/abs/2212.08073</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.510.1">Hugging Face Inference Endpoint. </span><a href="https://huggingface.co/docs/inference-endpoints/index" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.511.1">https://huggingface.co/docs/inference-endpoints/index</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.512.1">Hugging Face Inference Endpoint Pricing. </span><a href="https://huggingface.co/docs/inference-endpoints/pricing" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.513.1">https://huggingface.co/docs/inference-endpoints/pricing</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.514.1">Model Card for BioMedLM 2.7B. </span><a href="https://huggingface.co/stanford-crfm/BioMedLM" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.515.1">https://huggingface.co/stanford-crfm/BioMedLM</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.516.1">PaLM 2 Technical Report. </span><a href="https://ai.google/static/documents/palm2techreport.pdf" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.517.1">https://ai.google/static/documents/palm2techreport.pdf</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.518.1">Solving Quantitative Reasoning Problems with Language Models. </span><a href="https://arxiv.org/abs/2206.14858" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.519.1">https://arxiv.org/abs/2206.14858</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.520.1">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. </span><a href="https://arxiv.org/abs/2306.05685" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.521.1">https://arxiv.org/abs/2306.05685</span></span></a></li>
</ul>
<h1 class="heading"><span class="kobospan" id="kobo.522.1">Join our community on Discord</span></h1>
<p class="normal"><span class="kobospan" id="kobo.523.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.524.1">https://packt.link/llm</span></span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.525.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png" class="calibre4"/></span></p>
</div>
</body></html>