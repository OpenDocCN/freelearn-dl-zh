<html><head></head><body>
<div><h1 class="chapternumber">3</h1>
<h1 class="chaptertitle" id="_idParaDest-39">Choosing an LLM for Your Application</h1>
<p class="normal">In the last chapter, we saw how pivotal it is to properly orchestrate <strong class="screentext">large language models</strong> (<strong class="screentext">LLMs</strong>) and<a id="_idIndexMarker168" class="calibre3"/> their components within applications. In fact, we saw that not all LLMs are created equal. The next key decision is which LLMs to actually use. Different LLMs may have different architectures, sizes, training data, capabilities, and limitations. Choosing the right LLM for your application is not a trivial decision, as it can have a significant impact on the performance, quality, and cost of your solution.</p>
<p class="normal1">In this chapter, we will guide you through the process of choosing the right LLM for your application. We will cover the following topics:</p>
<ul class="calibre14">
<li class="bulletlist">An overview of the most promising LLMs in the market</li>
<li class="bulletlist1">The main criteria and tools to use when comparing LLMs</li>
<li class="bulletlist1">Trade-offs between size and performance</li>
</ul>
<p class="normal1">By the end of this chapter, you should have a clear understanding of how to choose the right LLM for your application and how to use it effectively and responsibly.</p>
<h1 class="heading" id="_idParaDest-40">The most promising LLMs in the market</h1>
<p class="normal">The last year has <a id="_idIndexMarker169" class="calibre3"/>witnessed an unprecedented surge in the research and development of LLMs. Several new models have been released or announced by different organizations, each with its own features and capabilities. Some of these models are the largest and most advanced ever created, surpassing the previous <strong class="screentext">state-of-the-art</strong> (<strong class="screentext">SOTA</strong>) by orders of magnitude. Others are lighter yet more <a id="_idIndexMarker170" class="calibre3"/>specialized in specific tasks.</p>
<p class="normal1">In this chapter, we will review some of the most promising LLMs in the market as of 2024. We will introduce their background, key findings, and main techniques. We will also compare their performance, strengths, and limitations on various benchmarks and tasks. We will also <a id="_idIndexMarker171" class="calibre3"/>discuss their potential applications, challenges, and implications for the future of AI and society.</p>
<h2 class="heading1" id="_idParaDest-41">Proprietary models</h2>
<p class="normal">Proprietary LLMs are<a id="_idIndexMarker172" class="calibre3"/> developed and<a id="_idIndexMarker173" class="calibre3"/> owned by private companies, and they are not disclosed with code. They are also typically subject to a fee for consumption.</p>
<p class="normal1">Proprietary models offer a series of advantages, including better support and maintenance as well as safety and alignment. They also tend to outperform open-source models in terms of generalization, because of their complexity and training datasets. On the other hand, they act as a “black box,” meaning that owners do not disclose the source code to developers.</p>
<p class="normal1">In the next sections, we will cover three of the most popular proprietary LLMs in the market, as of August 2023.</p>
<h3 class="heading2" id="_idParaDest-42">GPT-4</h3>
<p class="normal">Released in<a id="_idIndexMarker174" class="calibre3"/> March 2023, GPT-4 is, together with its newly released “cousin” GPT-4 Turbo, one<a id="_idIndexMarker175" class="calibre3"/> of the latest models <a id="_idIndexMarker176" class="calibre3"/>developed by <strong class="screentext">OpenAI</strong>, is among the top performers in the market at the time of writing this book (while OpenAI, as confirmed by its CEO Sam Altman, is already working on GPT-5).</p>
<p class="normal1">It belongs to the <a id="_idIndexMarker177" class="calibre3"/>class of <strong class="screentext">generative pretrained transformer</strong> (<strong class="screentext">GPT</strong>) models, a decoder-only transformer-based architecture introduced by OpenAI. The following diagram shows the basic architecture:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_03_01.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.1: High-level architecture of a decoder-only transformer</p>
<p class="normal1">As you can see <a id="_idIndexMarker178" class="calibre3"/>from the preceding diagram, the decoder-only<a id="_idIndexMarker179" class="calibre3"/> architecture still includes the main elements that feature in transformer architecture that we covered in <em class="italic">Chapter 1</em>, <em class="italic">Positional Embeddings, Multi-Head Attention</em>, and <em class="italic">Feed Forward </em>layers. However, in this architecture, the model solely comprises a decoder, which is trained to predict the next token in a sequence based on the preceding tokens. Unlike the encoder-decoder architecture, the decoder-only design lacks an explicit encoder for summarizing input information. Instead, the information is implicitly encoded within the hidden state of the decoder, which is updated at each step during the generation process.</p>
<p class="normal1">Now, we’ll look at some of the improvements in GPT-4 over previous versions.</p>
<p class="normal1">GPT-4, like the previous models in the GPT series, has been trained on both publicly available and OpenAI-licensed datasets (OpenAI didn’t disclose the exact composition of the training set).</p>
<p class="normal1">Additionally, to make the<a id="_idIndexMarker180" class="calibre3"/> model more aligned with the user’s intent, the training process<a id="_idIndexMarker181" class="calibre3"/> also<a id="_idIndexMarker182" class="calibre3"/> involved <strong class="screentext">reinforcement learning from human feedback</strong> (<strong class="screentext">RLHF</strong>) training.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">RLHF is a technique that aims at using human feedback as an evaluating metric for LLMs’ generated output and then using that feedback to further optimize the model. There are two main steps to achieve that goal:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">Training a reward model based on human preferences.</li>
<li class="bulletlist1">Optimizing the LLM with respect to the reward model. This step is done via reinforcement learning and it is a type of machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, and its goal is to maximize the cumulative reward over time by continuously adapting its behavior through trial and error.</li>
</ol>
<p class="normal1">With RLHF, thanks to the reward model, the LLM is able to learn from human preferences and be more aligned with users’ intents.</p>
<p class="normal1">As an example, think about ChatGPT. This model integrates various training methods, including unsupervised pretraining, supervised fine-tuning, instruction tuning, and RLHF. The RLHF component involves training the model to predict human preferences by using feedback from human trainers. These trainers review the model’s responses and provide ratings or corrections, guiding the model to generate more helpful, accurate, and aligned responses.</p>
<p class="normal1">For instance, if a language model initially produces an output that is not quite helpful or accurate, human trainers can provide feedback that indicates the preferred output. The model then uses this feedback to adjust its parameters and improve future responses. This process iteratively continues, with the model learning from a series of human judgments to better align with what is considered helpful or appropriate by human standards.</p>
</div>
<p class="normal1">GPT-4 demonstrated outstanding capabilities in commonsense reasoning and analytical skills. It has been benchmarked with SOTA systems, including the <strong class="screentext">Massive Multitask Language Understanding</strong> (<strong class="screentext">MMLU</strong>) we <a id="_idIndexMarker183" class="calibre3"/>covered in <em class="italic">Chapter 1</em>. On MMLU, GPT-4 outperformed previous models not only in English, but also in other languages.</p>
<p class="normal1">The following<a id="_idIndexMarker184" class="calibre3"/> is an <a id="_idIndexMarker185" class="calibre3"/>illustration that shows GPT-4’s performance on MMLU:</p>
<figure class="mediaobject"><img alt="A graph with green and blue bars  Description automatically generated" src="img/B21714_03_02.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.2: GPT-4 3-shot accuracy on MMLU across languages (source: <a href="https://openai.com/research/gpt-4" class="calibre3">https://openai.com/research/gpt-4</a>)</p>
<p class="normal1">In addition to MMLU, GPT-4 has been benchmarked on a variety of SOTA systems and academic exams, as you can see from the following graph:</p>
<figure class="mediaobject"><img alt="A graph of a performance  Description automatically generated" src="img/B21714_03_03.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.3: GPT performance on academic and professional exams (source: <a href="https://arxiv.org/pdf/2303.08774.pdf" class="calibre3">https://arxiv.org/pdf/2303.08774.pdf</a>)</p>
<div><p class="normal1"><strong class="screentext">Note</strong>: in<a id="_idIndexMarker186" class="calibre3"/> the<a id="_idIndexMarker187" class="calibre3"/> preceding graph, you can see two versions of GPT-4, vision and no vision (along with the GPT-3.5 for benchmarking purposes). This is because GPT-4 is a multi-modal model, meaning that it can take images as input, in addition to text. However, in this chapter, we will benchmark only its textual capabilities.</p>
</div>
<p class="normal1">Another great improvement of GPT-4 with respect to its predecessors (GPT-3.5 and GPT-3) is its noticeable reduction in the risk of hallucination.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Hallucination is a term that describes a phenomenon where LLMs generate text that is incorrect, nonsensical, or not real, but appears to be plausible or coherent. For example, an LLM may hallucinate a fact that contradicts the source or common knowledge, a name that does not exist, or a sentence that does not make sense.</p>
<p class="normal1">Hallucination can happen because LLMs are not databases or search engines that store or retrieve factual information. Rather, they are statistical models that learn from massive amounts of text data and produce outputs based on the patterns and probabilities they have learned. However, these patterns and probabilities may not reflect the truth or the reality, as the data may be incomplete, noisy, or biased. Moreover, LLMs have limited contextual understanding and memory, as they can only process a certain number of tokens at a time and abstract them into latent representations. Therefore, LLMs may generate text that is not supported by any data or logic but is the most likely or correlated from the prompt.</p>
</div>
<p class="normal1">In fact, even<a id="_idIndexMarker188" class="calibre3"/> though it is still not 100% reliable, GPT-4 made great improvements <a id="_idIndexMarker189" class="calibre3"/>with TruthfulQA benchmarks, which test the model’s ability to separate fact from incorrect statements (we covered TruthfulQA benchmarks in <em class="italic">Chapter 1</em>, in the <em class="italic">Model evaluation </em>section).</p>
<p class="normal1">Here, you can see an illustration that compares GPT-4 results in a TruthfulQA benchmark with those of GPT-3.5 (the model behind OpenAI’s ChatGPT) and Anthropic-LM (we will cover this latter model in the next sections).</p>
<figure class="mediaobject"><img alt="A graph of different colored squares  Description automatically generated" src="img/B21714_03_04.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.4: Model comparison in TruthfulQA benchmark (source: <a href="https://openai.com/research/gpt-4" class="calibre3">https://openai.com/research/gpt-4</a>)</p>
<p class="normal1">Finally, with GPT-4, OpenAI made an additional effort to make it safer and more aligned, engaging from the <a id="_idIndexMarker190" class="calibre3"/>beginning a team of over 50 experts in domains like AI <a id="_idIndexMarker191" class="calibre3"/>alignment risks, privacy, and cybersecurity, with the goal of understanding the extent of the risks of such a powerful model and how to prevent them.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Alignment is a term that describes the degree to which LLMs behave in ways that are useful and harmless for their human users. For example, an LLM may be aligned if it generates text that is accurate, relevant, coherent, and respectful. An LLM may be misaligned if it generates text that is false, misleading, harmful, or offensive.</p>
</div>
<p class="normal1">Thanks to this<a id="_idIndexMarker192" class="calibre3"/> analysis, further data have been collected and used while training GPT-4 to mitigate its potential risks, resulting in a reduced risk compared to its predecessor, GPT-3.5.</p>
<h3 class="heading2" id="_idParaDest-43">Gemini 1.5</h3>
<p class="normal">Gemini 1.5 is a SOTA generative<a id="_idIndexMarker193" class="calibre3"/> AI model developed by Google<a id="_idIndexMarker194" class="calibre3"/> and released in December 2023. Like GPT-4, Gemini is designed to be multimodal, meaning that it can process and generate content across various modalities, including text, images, audio, video, and code. It is based <a id="_idIndexMarker195" class="calibre3"/>on a <strong class="screentext">mixture-of-expert</strong> (<strong class="screentext">MoE</strong>) transformer.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">In the context of transformer architecture, MoE refers to a model that incorporates multiple specialized sub-models, known as “experts,” within its layers. Each expert is a neural network designed to handle different types of data or tasks more efficiently. The MoE model uses a gating mechanism or router to determine which expert should process a given input, allowing the model to dynamically allocate resources and specialize in processing certain types of information. This approach can lead to more efficient training and inference, as it enables the model to scale up in size and complexity without a proportional increase in computational cost.</p>
</div>
<p class="normal1">Gemini comes in various sizes, including Ultra, Pro, and Nano, to cater to different computational needs, from data centers to mobile devices. To use Gemini, developers can access it via the APIs provided for different model variants, allowing the integration of its capabilities into applications.</p>
<p class="normal1">Compared to its previous version, Gemini 1.0, the current model outperforms it in text, vision, and audio tasks, as shown in the following screenshot:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_03_05.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.5: Gemini 1.5 Pro and Ultra compared to its previous version 1.0 (source: <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" class="calibre3">https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf</a> )</p>
<p class="normal1">Similarly, it <a id="_idIndexMarker196" class="calibre3"/>has <a id="_idIndexMarker197" class="calibre3"/>demonstrated outstanding capabilities in domains such as math, science, and reasoning, and coding and multilinguality:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_03_06.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.6: Gemini 1.5 Pro compared to Gemini 1.0 Pro and Ultra on different benchmarks (source: <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" class="calibre3">https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf</a>)</p>
<p class="normal1">Note that <a id="_idIndexMarker198" class="calibre3"/>Gemini 1.5 Pro is outperforming Gemini 1.0 Ultra (which is<a id="_idIndexMarker199" class="calibre3"/> remarkably bigger) in many benchmarks across the various domains. As of today, Gemini Pro can be tried via a web app at gemini.<a href="https://google.com" class="calibre3">google.com</a> for free, while Gemini Ultra is available via a premium subscription with a monthly fee. On the other hand, Gemini Nano, which is tailored for mobile devices, can be executed on capable Android devices via the Google AI Edge SDK for Android. Note that, as of April 2024, this SDK is still under early access preview and you can apply for the early access program at <a href="https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform" class="calibre3">https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform</a>. Finally, Gemini Pro and Ultra can also be consumed <a id="_idIndexMarker200" class="calibre3"/>by developers via the REST API from Google <a id="_idIndexMarker201" class="calibre3"/>AI Studio.</p>
<h3 class="heading2" id="_idParaDest-44">Claude 2</h3>
<p class="normal">Claude 2, which stands<a id="_idIndexMarker202" class="calibre3"/> for Constitutional Large-scale Alignment via User <a id="_idIndexMarker203" class="calibre3"/>Data and Expertise, is an LLM developed by Anthropic, a research company founded by former OpenAI researchers and focused on AI safety and alignment. It was announced in July 2023.</p>
<p class="normal1">Claude 2 is a transformer-based LLM that has been trained on a mix of publicly available information from the internet and proprietary data, via<a id="_idIndexMarker204" class="calibre3"/> unsupervised learning, RLHF, and <strong class="screentext">constitutional AI</strong> (<strong class="screentext">CAI</strong>).</p>
<p class="normal1">CAI is a real peculiarity of Claude. In fact, Anthropic paid extraordinary attention to Claude 2 alignment with safety principles. More specifically, Anthropic developed this unique technique called CAI, which was disclosed in December 2022 in the paper <em class="italic">Constitutional AI: Harmlessness from AI Feedback</em>.</p>
<p class="normal1">CAI aims to make the model safer and more aligned with human values and intentions by preventing toxic or discriminatory output, not helping a human engage in illegal or unethical activities, and broadly creating an AI system that is helpful, honest, and harmless. To achieve this, it uses a set of principles to guide the model’s behavior and outputs, rather than relying on human feedback or data alone. The principles are derived from various sources, such as the UN Declaration of Human Rights, trust and safety best practices, principles proposed by other AI research labs, non-Western perspectives, and empirical research.</p>
<p class="normal1">CAI uses these principles in two stages of the training process:</p>
<ul class="calibre14">
<li class="bulletlist">First, the<a id="_idIndexMarker205" class="calibre3"/> model is trained to critique and revise its own responses using the principles and a few examples.</li>
<li class="bulletlist1">Second, the model is trained via reinforcement learning, but rather than using human feedback, it uses AI-generated feedback based on the principles to choose the more harmless output.</li>
</ul>
<p class="normal1">The following illustration shows the training process according to the CAI technique:</p>
<figure class="mediaobject"><img alt="A diagram of a process flow  Description automatically generated" src="img/B21714_03_07.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.7: Claude’s training process according to the CAI technique (source: <a href="https://arxiv.org/abs/2212.08073" class="calibre3">https://arxiv.org/abs/2212.08073</a>)</p>
<p class="normal1">Another peculiarity of Claude 2 is the context length, which has a limit of 100,000 tokens. This means that users can input longer prompts, namely pages of technical documentation or even a book, which do not need to be embedded. Plus, the model can also generate longer output compared to other LLMs.</p>
<p class="normal1">Finally, Claude 2 demonstrates <a id="_idIndexMarker206" class="calibre3"/>relevant capabilities also when working with<a id="_idIndexMarker207" class="calibre3"/> code, scoring 71.2% on the HumanEval benchmark.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">HumanEval is a benchmark for evaluating the code generation ability of LLMs. It consists of 164 human-crafted coding problems in Python, each with a prompt, a solution, and a test suite. The problems cover various topics, such as data structures, algorithms, logic, math, and string manipulation. The benchmark can be used to measure the functional correctness, syntactic validity, and semantic coherence of the LLM’s outputs.</p>
</div>
<p class="normal1">Overall, Claude 2 is a very interesting model and competitor of GPT-4 to pay attention to. It can be consumed via the REST API or directly via the Anthropic beta chat experience (limited for US and UK users as of August 2023).</p>
<p class="normal1">The following comparison table shows the main differences between the three models:</p>
<table class="table-container" id="table001-1">
<tbody class="calibre18">
<tr class="calibre19">
<td class="table-cell"/>
<td class="table-cell">
<p class="normal1"><strong class="keyword">GPT-4</strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword">Gemini</strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword">Claude 2</strong></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Company or institution</strong></p>
</td>
<td class="table-cell">
<p class="normal1">OpenAI</p>
</td>
<td class="table-cell">
<p class="normal1">Google</p>
</td>
<td class="table-cell">
<p class="normal1">Anthropic</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">First release</strong></p>
</td>
<td class="table-cell">
<p class="normal1">March 2023</p>
</td>
<td class="table-cell">
<p class="normal1">December 2023</p>
</td>
<td class="table-cell">
<p class="normal1">July 2023</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Architecture</strong></p>
</td>
<td class="table-cell">
<p class="normal1">Transformer-based, decoder only</p>
</td>
<td class="table-cell">
<p class="normal1">Transformer-based</p>
</td>
<td class="table-cell">
<p class="normal1">Transformer-based</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Sizes and variants</strong></p>
</td>
<td class="table-cell">
<p class="normal1">Parameters not officially specified</p>
<p class="normal1">Two context-length variants:</p>
<p class="normal1">GPT-4 8K tokens</p>
<p class="normal1">GPT-4 32K tokens</p>
</td>
<td class="table-cell">
<p class="normal1">Three sizes, from smallest to largest: Nano, Pro, and Ultra</p>
</td>
<td class="table-cell">
<p class="normal1">Not officially specified</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">How to use</strong></p>
</td>
<td class="table-cell">
<p class="normal1">REST API at OpenAI developer platforms</p>
<p class="normal1">Using OpenAI Playground at <a href="https://platform.openai.com/playground" class="calibre20">https://platform.openai.com/playground</a></p>
</td>
<td class="table-cell">
<p class="normal1">REST API at Google AI Studio</p>
<p class="normal1">Using Gemini at <a href="https://gemini.google.com/" class="calibre20">https://gemini.google.com/</a></p>
</td>
<td class="table-cell">
<p class="normal1">REST API after compiling the form at <a href="https://www.anthropic.com/claude" class="calibre20">https://www.anthropic.com/claude</a></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 3.1: Comparison table of GPT-4, PaLM 2, and Claude 2</p>
<p class="normal1">In addition to proprietary models, there is a huge market for open-source LLMs available today. Let’s <a id="_idIndexMarker208" class="calibre3"/>discuss <a id="_idIndexMarker209" class="calibre3"/>some of these in the next section.</p>
<h2 class="heading1" id="_idParaDest-45">Open-source models</h2>
<p class="normal">The advantage<a id="_idIndexMarker210" class="calibre3"/> of an <a id="_idIndexMarker211" class="calibre3"/>open-source model is that, by definition, developers have full visibility and access to the source code. In the context of LLMs, this implies the following:</p>
<ul class="calibre14">
<li class="bulletlist">You have major control over the architecture, meaning that you can also modify it in the local version you are going to use within your project. This also implies that they are not prone to potential updates to the source code made by models’ owners.</li>
<li class="bulletlist1">There is the possibility to train your model from scratch, on top of the classical fine-tuning, which is also available for proprietary models.</li>
<li class="bulletlist1">Free to use, meaning that you won’t incur any charge while using those LLMs, in contrast with the proprietary ones that have pay-per-use pricing.</li>
</ul>
<p class="normal1">To compare open-source models, throughout this book, we will refer to the independent Hugging Face Open LLM Leaderboard (you can find it at <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" class="calibre3">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)</a>, a project that aims to evaluate and compare the performance of LLMs on various <strong class="screentext">natural language understanding</strong> (<strong class="screentext">NLU</strong>) tasks. The project is hosted on Hugging Face Spaces, a platform for <a id="_idIndexMarker212" class="calibre3"/>creating and sharing machine-learning applications.</p>
<p class="normal1">The Open LLM Leaderboard uses four main evaluation benchmarks, which we covered in <em class="italic">Chapter 1</em>, in the <em class="italic">Model evaluation</em> section:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">AI2 Reasoning Challenge</strong> (<strong class="screentext">ARC</strong>): Grade-school <a id="_idIndexMarker213" class="calibre3"/>science questions and complex NLU tasks.</li>
<li class="bulletlist1"><strong class="screentext">HellaSwag</strong>: Common<a id="_idIndexMarker214" class="calibre3"/> sense reasoning.</li>
<li class="bulletlist1"><strong class="screentext">MMLU</strong>: Tasks in various domains, including math, computer science, and law.</li>
<li class="bulletlist1"><strong class="screentext">TruthfulQA</strong>: An evaluation<a id="_idIndexMarker215" class="calibre3"/> of how truthful the model is when generating answers.</li>
</ul>
<p class="normal1">Even though<a id="_idIndexMarker216" class="calibre3"/> those are just a subsample of the <a id="_idIndexMarker217" class="calibre3"/>plethora of LLMs’ benchmarks, we will stick to this leaderboard as a reference evaluation framework as it being widely adopted.</p>
<h3 class="heading2" id="_idParaDest-46">LLaMA-2</h3>
<p class="normal"><strong class="screentext">Large Language Model Meta AI 2</strong> (<strong class="screentext">LLaMA-2</strong>) is a new family of models developed by Meta and <a id="_idIndexMarker218" class="calibre3"/>unveiled to the public on July 18, 2023, open source and for free (its first version was originally limited to researchers).</p>
<p class="normal1">It is an <strong class="screentext">autoregressive</strong> model <a id="_idIndexMarker219" class="calibre3"/>with an optimized, decoder-only transformer architecture.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">The concept of autoregressive in the context of transformers refers to the fact that the model predicts the next token in the sequence, conditioned on all the previous tokens. This is done by masking the future tokens in the input so that the model can only attend to the past tokens. For example, if the input sequence is “The sky is blue,” the model would predict “The” first, then “sky,” then “is,” and finally “blue,” using a mask to hide the tokens that come after each prediction.</p>
</div>
<p class="normal1">LLaMA-2 models come in three sizes: 7, 13, and 70 billion parameters. All the versions have been trained on 2 trillion tokens and have a context length of 4,092 tokens.</p>
<p class="normal1">On top of that, all model sizes come with a “chat” version, called LLaMA-2-chat, which is more versatile for<a id="_idIndexMarker220" class="calibre3"/> general-purpose conversational scenarios compared to the base model LLama-2.</p>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">In the context of LLMs, the difference between <strong class="screentext">base models</strong> and “chat” or <strong class="screentext">assistant models</strong> is <a id="_idIndexMarker221" class="calibre3"/>primarily in their<a id="_idIndexMarker222" class="calibre3"/> training and<a id="_idIndexMarker223" class="calibre3"/> intended use:</p>
<ul class="calibre14">
<li class="bulletlist">Base models: These <a id="_idIndexMarker224" class="calibre3"/>models are trained on vast amounts of text data, often sourced from the internet, and their primary function is to predict the next word in a given context, which makes them great at understanding and generating language. However, they might not always be precise or focused on specific instructions.</li>
<li class="bulletlist1">Assistant models: These<a id="_idIndexMarker225" class="calibre3"/> models start as base LLMs but are further fine-tuned with input-output pairs that include instructions and the model’s attempts to follow those instructions. They often employ RLHF to refine the model, making it better at being helpful, honest, and harmless. As a result, they are less likely to generate problematic text and are more suitable for practical applications like chatbots and content generation. For example, the assistant model GPT-3.5 Turbo (the model behind ChatGPT) is a fine-tuned version of the completion model GPT-3.</li>
</ul>
<p class="normal1">In essence, while base models provide a broad understanding of language, assistant models are optimized to follow instructions and provide more accurate and contextually relevant responses.</p>
</div>
<p class="normal1">LLaMA-2-chat was developed with a fine-tuning process that consisted of two main steps:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><strong class="screentext">Supervised fine-tuning</strong>: This <a id="_idIndexMarker226" class="calibre3"/>step involves fine-tuning the model on publicly available instruction datasets and over 1 million human annotations, to make them more helpful and safe for conversational use cases. The fine-tuning process uses a selected list of prompts to guide the model outputs, and a loss function that encourages diversity and relevance (that’s the reason why it is “supervised”).</li>
<li class="bulletlist1"><strong class="screentext">RLHF</strong>: As we saw while introducing GPT-4, RLHF is a technique that aims at using human feedback as an evaluating metric for LLMs’ generated output, and then using that feedback to further optimize the model.</li>
</ol>
<p class="normal1">The following is an<a id="_idIndexMarker227" class="calibre3"/> illustration of how the training process for LLaMA works:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_03_08.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.8: Two-step fine-tuning to obtain LLaMa-2 chat (source: <a href="https://ai.meta.com/resources/models-and-libraries/llama/" class="calibre3">https://ai.meta.com/resources/models-and-libraries/llama/</a>)</p>
<p class="normal1">To access the model, you need to submit a request on Meta’s website (the form is available at <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" class="calibre3">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a>). Once a request is submitted, you will receive an email with the GitHub repository where you will be<a id="_idIndexMarker228" class="calibre3"/> able to download the following assets:</p>
<ul class="calibre14">
<li class="bulletlist">Model code</li>
<li class="bulletlist1">Model weights</li>
<li class="bulletlist1">README (User Guide)</li>
<li class="bulletlist1">Responsible Use Guide</li>
<li class="bulletlist1">License</li>
<li class="bulletlist1">Acceptable Use Policy</li>
<li class="bulletlist1">Model Card</li>
</ul>
<h3 class="heading2" id="_idParaDest-47">Falcon LLM</h3>
<p class="normal">Falcon LLM is <a id="_idIndexMarker229" class="calibre3"/>a representation of a new trend of LLMs, consisting of building lighter models (with fewer parameters) and focusing rather on the quality of the training dataset. Indeed, it is a matter of fact that complex models like GPT-4 with trillions of parameters are extremely heavy, both in the training phase and inference phase. This implies the need for high and expensive computational power (GPU and TPU-powered) as well as a long training time.</p>
<p class="normal1">Falcon LLM is an open-source model launched by Abu Dhabi’s <strong class="screentext">Technology Innovation Institute</strong> (<strong class="screentext">TII</strong>) in May 2023. It <a id="_idIndexMarker230" class="calibre3"/>is an autoregressive, decoder-only transformer, trained on 1 trillion tokens, and it has 40 billion parameters (even though it has also been released as a lighter version with 7 billion parameters). Similarly to what we saw for LlaMA, Falcon LLM also comes with a fine-tuned variant, called “Instruct,” which is tailored toward following the user’s instructions.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Instruct models are specialized for short-form instruction following. Instruction following is a task where the model has to execute a natural language command or query, such as “write a haiku about cats” or “tell me about the weather in Paris.” The Instruct fine-tuned models are trained on a large dataset of instructions and their corresponding outputs, such as the Stanford Alpaca dataset.</p>
</div>
<p class="normal1">According to the Open LLM leaderboard, since its launch, Falcon LLM has been among the first positions globally, second only to some versions of LlaMA.</p>
<p class="normal1">So, the question might be: how can a model with “only” 40 billion parameters perform so well? In fact, the answer is in the quality of the dataset. Falcon was developed using specialized tools and incorporates a unique data pipeline, which is capable of extracting valuable content from web data. The pipeline was designed to extract high-quality content by employing extensive filtering and deduplication techniques. The resulting dataset, called <em class="italic">RefinedWeb</em>, has been released by TII under the Apache-2.0 license and can be found at <a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb" class="calibre3">https://huggingface.co/datasets/tiiuae/falcon-refinedweb</a>.</p>
<p class="normal1">By combining superior data quality with these optimizations, Falcon achieves remarkable performance <a id="_idIndexMarker231" class="calibre3"/>while utilizing around 75% and 80% of the training compute budget of GPT-3 and PaLM-62B, respectively.</p>
<h3 class="heading2" id="_idParaDest-48">Mistral</h3>
<p class="normal">The third and last <a id="_idIndexMarker232" class="calibre3"/>open-source model series we are going to cover is Mistral, developed by Mistral AI, a company founded in April 2023 by a team of AI scientists who previously worked at Meta Platforms and Google DeepMind. Based in France, the company has quickly made a name for itself by raising significant funding and releasing open-source LLMs, emphasizing the importance of transparency and accessibility in AI development.</p>
<p class="normal1">The Mistral model, particularly the Mistral-7B-v0.1, is a decoder-only transformer with 7.3 billion parameters, designed for generative text tasks. It’s known for its innovative architecture choices like <strong class="screentext">grouped-query attention</strong> (<strong class="screentext">GQA</strong>) and <strong class="screentext">sliding-window attention</strong> (<strong class="screentext">SWA</strong>), which have allowed it to outperform other models in benchmarks.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">GQA and SWA are mechanisms designed to improve the efficiency and performance of an LLM.</p>
<p class="normal1">GQA is a<a id="_idIndexMarker233" class="calibre3"/> technique that allows for faster inference times compared to standard full attention mechanisms. It does this by partitioning the attention mechanism’s query heads into groups, with each group sharing a single key head and value head.</p>
<p class="normal1">SWA is used to <a id="_idIndexMarker234" class="calibre3"/>handle longer text sequences efficiently. It extends the model’s attention beyond a fixed window size, allowing each layer to reference a range of positions from the preceding layer. This means that the hidden state at a certain position in one layer can attend to hidden states within a specific range in the previous layer, thus enabling the model to access tokens at a greater distance and manage sequences <a id="_idIndexMarker235" class="calibre3"/>of varying lengths with a reduced inference cost.</p>
</div>
<p class="normal1">The model also provides a variant that was fine-tuned for general-purpose capabilities. This variant is called Mistral-7B-instruct, which outperformed all other 7 billion LLMs on the market (as of April 2024) on MT-Bench (an evaluation framework that uses an LLM as a judge).</p>
<p class="normal1">Like many other open-source models, Mistral can be consumed and downloaded via Hugging Face Hub.</p>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">In February 2024, Mistral AI and Microsoft entered a multi-year partnership to accelerate AI innovation. This collaboration will leverage Microsoft’s Azure AI supercomputing infrastructure to support the development and deployment of Mistral AI’s LLMs. Mistral AI’s models, including their advanced model, Mistral Large, will be available to customers through Azure AI Studio and Azure Machine Learning model catalog. The partnership aims to expand Mistral AI’s reach to global markets and foster ongoing research collaboration.</p>
</div>
<p class="normal1">The following <a id="_idIndexMarker236" class="calibre3"/>comparison table provides the main differences between the three models:</p>
<table class="table-container" id="table002">
<tbody class="calibre18">
<tr class="calibre19">
<td class="table-cell"/>
<td class="table-cell">
<p class="normal1"><strong class="keyword">LlaMA</strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword">Falcon LLM</strong></p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword">Mistral</strong></p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Company or institution</strong></p>
</td>
<td class="table-cell">
<p class="normal1">Meta</p>
</td>
<td class="table-cell">
<p class="normal1"><strong class="keyword1">Technology Innovation Institute</strong> (<strong class="screentext">TII</strong>)</p>
</td>
<td class="table-cell">
<p class="normal1">Mistral AI</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">First release</strong></p>
</td>
<td class="table-cell">
<p class="normal1">July 2023</p>
</td>
<td class="table-cell">
<p class="normal1">May 2023</p>
</td>
<td class="table-cell">
<p class="normal1">September 2023</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Architecture</strong></p>
</td>
<td class="table-cell">
<p class="normal1">Autoregressive transformer, decoder-only</p>
</td>
<td class="table-cell">
<p class="normal1">Autoregressive transformer, decoder-only</p>
</td>
<td class="table-cell">
<p class="normal1">Transformer, decoder only</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Sizes and variants</strong></p>
</td>
<td class="table-cell">
<p class="normal1">Three sizes: 7B, 13B, and 70B, alongside the fine-tuned version (chat)</p>
</td>
<td class="table-cell">
<p class="normal1">Two sizes: 7B and 40B, alongside the fine-tuned version (instruct)</p>
</td>
<td class="table-cell">
<p class="normal1">7B size alongside the fine-tuned version (instruct)</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">Licenses</strong></p>
</td>
<td class="table-cell">
<p class="normal1">A custom commercial license is available at <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" class="calibre20">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p>
</td>
<td class="table-cell">
<p class="normal1">Commercial Apache 2.0 licensed</p>
</td>
<td class="table-cell">
<p class="normal1">Commercial Apache 2.0 licensed</p>
</td>
</tr>
<tr class="calibre19">
<td class="table-cell">
<p class="normal1"><strong class="keyword">How to use</strong></p>
</td>
<td class="table-cell">
<p class="normal1">Submit request form at <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" class="calibre20">https://ai.meta.com/resources/models-and-libraries/llama-downloads/ and download the GitHub repo</a></p>
<p class="normal1">Also available in Hugging Face Hub</p>
</td>
<td class="table-cell">
<p class="normal1">Download or use Hugging Face Hub Inference API/Endpoint</p>
</td>
<td class="table-cell">
<p class="normal1">Download or use Hugging Face Hub Inference API/Endpoint or Azure AI Studio</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 3.2: Comparison table of LLMs</p>
<h1 class="heading" id="_idParaDest-49">Beyond language models</h1>
<p class="normal">So far, we have<a id="_idIndexMarker237" class="calibre3"/> only been covering language-specific foundation models as they are the focus of this book. Nevertheless, in the context of AI-powered applications, it is worth mentioning that there are additional foundation models that can handle data that is different from text, which can be embedded and orchestrated.</p>
<p class="normal1">Here, you can find some examples of <strong class="screentext">large foundation models</strong> (<strong class="screentext">LFMs</strong>) on the market today:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Whisper</strong>: It is a general-purpose <a id="_idIndexMarker238" class="calibre3"/>speech <a id="_idIndexMarker239" class="calibre3"/>recognition model developed by OpenAI that can transcribe and translate speech in multiple languages. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, spoken language identification, and voice activity detection.</li>
<li class="bulletlist1"><strong class="screentext">Midjourney</strong>: Developed <a id="_idIndexMarker240" class="calibre3"/>by the independent research lab of the same name, Midjourney is based on a sequence-to-sequence transformer model that takes text prompts and outputs a set of four images that match the prompts. Midjourney is designed to be a tool for artists and creative professionals, who can use it for rapid prototyping of artistic concepts, inspiration, or <a id="_idIndexMarker241" class="calibre3"/>experimentation.</li>
<li class="bulletlist1"><strong class="screentext">DALL-E</strong>: Similar to <a id="_idIndexMarker242" class="calibre3"/>the previous one, DALL-E, developed by OpenAI, generates images from natural language descriptions, using a 12-billion parameter version<a id="_idIndexMarker243" class="calibre3"/> of GPT-3 trained on a dataset of text-image pairs.</li>
</ul>
<p class="normal1">The idea is that we can combine and orchestrate multiple LFMs within our applications to achieve extraordinary results. For example, let’s say we want to write a review about an interview with a<a id="_idIndexMarker244" class="calibre3"/> young chef and post it on Instagram. The involved models might be the following:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Whisper</strong> will convert the interview audio into a transcript.</li>
<li class="bulletlist1">An <strong class="screentext">LLM</strong>, such as Falcon-7B-instruct, with a web plugin, will extrapolate the name of the young chef and search it on the internet to retrieve the biography.</li>
<li class="bulletlist1">Another <strong class="screentext">LLM</strong>, such as LlaMA, will process the transcript and generate a review with an Instagram post style. We can also ask the same model to generate a prompt that will ask the following model to generate a picture based on the post content.</li>
<li class="bulletlist1"><strong class="screentext">Dall-E</strong> will generate an image based on the prompt generated by the LLM.</li>
</ul>
<p class="normal1">We will then provide our LFMs flow with an Instagram plugin so that the application is able to post the whole review, including the illustration, on our profile.</p>
<p class="normal1">Finally, there are emerging LFMs that are meant to be multi-modal, meaning that they can handle multiple data formats with just one architecture. An example is GPT-4 itself.</p>
<p class="normal1">The following screenshot shows an example of an early OpenAI experiment with GPT-4 visuals, demonstrating its understanding of funny aspects within an image:</p>
<figure class="mediaobject"><img alt="A cell phone with a cable plugged into it  Description automatically generated" src="img/B21714_03_09.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.9: Early experiments with GPT-4 visuals (source: <a href="https://openai.com/research/gpt-4" class="calibre3">https://openai.com/research/gpt-4</a>)</p>
<p class="normal1">The following<a id="_idIndexMarker245" class="calibre3"/> screenshot shows another example of an earlier version of GPT-4, demonstrating how it could understand and explain graphs in detail:</p>
<figure class="mediaobject"><img alt="A screenshot of a graph  Description automatically generated" src="img/B21714_03_10.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.10: Early experiments with GPT-4 visuals (source: <a href="https://openai.com/research/gpt-4" class="calibre3">https://openai.com/research/gpt-4</a>)</p>
<p class="normal1">The following example<a id="_idIndexMarker246" class="calibre3"/> shows how an early version of GPT-4 could understand and solve complex mathematical problems while also providing the corresponding justification for its response:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_03_11.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 3.11: Early experiments with GPT-4 visuals (source: <a href="https://openai.com/research/gpt-4" class="calibre3">https://openai.com/research/gpt-4</a>)</p>
<p class="normal1">GPT-4 is just one example of a <strong class="screentext">large multimodal model</strong> (<strong class="screentext">LMM</strong>), and it is representative of the trend that we <a id="_idIndexMarker247" class="calibre3"/>will probably witness in the next few years.</p>
<h1 class="heading" id="_idParaDest-50">A decision framework to pick the right LLM</h1>
<p class="normal">In previous paragraphs, we <a id="_idIndexMarker248" class="calibre3"/>covered some of the most promising LLMs available in the market today. Now, the question is: which one should I use within my applications? The truth is that there is not a straightforward answer to this question.</p>
<h2 class="heading1" id="_idParaDest-51">Considerations</h2>
<p class="normal">There are many<a id="_idIndexMarker249" class="calibre3"/> factors to consider when choosing an LLM for your application. Those factors also need to be declined in two scenarios: proprietary and open-source LLMs. The following are some factors and trade-offs you might want to consider while choosing your LLMs:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Size and performance</strong>: We saw that more complex models (that means, with a high number of parameters) tend to have better performance, especially in terms of parametric knowledge and generalization capabilities. Nevertheless, the larger the model, the more computation and memory it requires to process the input and generate the output, which can result in higher latency and, as we will see, higher costs.</li>
<li class="bulletlist1"><strong class="screentext">Cost and hosting strategy</strong>: When incorporating LLMs within our applications, there are two types of costs we have to keep in mind:<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext">Cost for model consumption</strong>: This refers to the fee we pay to consume the model. Proprietary models like GPT-4 or Claude 2 require a fee, which is typically proportional to the number of tokens processed. On the other hand, open-source models like LlaMA or Falcon LLM are free to use.</li>
<li class="bulletlist3"><strong class="screentext">Cost for model hosting</strong>: This refers to your hosting strategy. Typically, proprietary models are hosted in a private or public hyperscaler, so that they can be consumed via a REST API and you don’t have to worry about the underlying infrastructure (for example, GPT-4 is hosted in a super-computer built in the Microsoft Azure cloud). With open-source models, we typically need to provide our own infrastructure, since those models can be downloaded locally. Of course, the larger the model, the more powerful the computational power needed.<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">In the context of open-source models, another option to consume those models is that of using the Hugging Face Inference API. The free version allows you to test and evaluate, with a limited rate, all the available LLMs on a shared infrastructure hosted on Hugging Face. For production use cases, Hugging Face also offers Inference Endpoints, so that you can easily deploy your LLMs on a dedicated and fully managed infrastructure, with the possibility to configure parameters like region, compute power, and security level to accommodate your constraints in terms of latency, throughput, and compliance.</p>
<p class="normal1">Pricing for the Inference Endpoint is publicly available at <a href="https://huggingface.co/docs/inference-endpoints/pricing" class="calibre3">https://huggingface.co/docs/inference-endpoints/pricing</a>.</p>
</div>
</li>
</ul>
</li>
</ul>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Customization</strong>: This might be a requirement you want to evaluate before deciding which<a id="_idIndexMarker250" class="calibre3"/> model to adopt. In fact, not all models are equally flexible in terms of customization. When we talk about customization, we refer to two activities:<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext">Fine-tuning</strong>: This is the process of slightly adjusting LLMs’ parameters to better fit into a domain. All open-source models can be fine-tuned. When it comes to proprietary models, not all LLMs can be fine-tuned: for example, OpenAI’s GPT-3.5 can be fine-tuned, while the process of fine-tuning the GPT-4-0613 is still experimental and accessible under request to OpenAI (as per December 2023).</li>
</ul>
<p class="normal1">Henceforth, it is important to understand whether you will need fine-tuning in your application and decide accordingly.</p>
<ul class="calibre17">
<li class="bulletlist2"><strong class="screentext">Training from scratch</strong>: If you really want an LLM that is super specific about your domain knowledge, you might want to retrain the model from scratch. To train an LLM from scratch, without having to reinvent an architecture, you can download open-source LLMs and simply re-train them on custom datasets. Of course, this implies that we have access to the source code, which is not the case when we work with proprietary LLMs.</li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext">Domain-specific capabilities</strong>: We saw that the most popular way of evaluating LLMs’ performance is that of averaging different benchmarks across domains. However, there are benchmarks that are tailored towards specific capabilities: if MMLU measures LLMs’ generalized culture and commonsense reasoning, TruthfulQA is more concerned with LLMs’ alignment, while HumanEval is tailored towards LLMs’ coding capabilities.</li>
</ul>
<p class="normal-one">Henceforth, if you have a tailored use case in mind, you might want to use a model that is a top performer in one specific benchmark, rather than a top performer, on average, across all benchmarks. Namely, you might pick Claude 2 if you are looking for exceptional coding capabilities, or PaLM 2 if analytical reasoning is what you are looking for. On the other hand, if you need a model that encompasses all of these capabilities, GPT-4 might be the right choice for you.</p>
<p class="normal-one">Picking a domain-specific model is also a way to make some savings in terms of model complexity. The thing is, it might be sufficient for you to use a relatively small <a id="_idIndexMarker251" class="calibre3"/>model (for example, a LlaMA-7B-instruct) if you need to use it for a specific use case, which comes with all the benefits in terms of cost and performance.</p>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">If you are looking for LLMs that are <em class="italic">extremely</em> specific, there is a plethora of models that have been trained on domain-specific technical documentation. For example, at the beginning of 2023, the <strong class="screentext">Stanford</strong> <strong class="screentext">Center for Research on Foundation Models</strong> (<strong class="screentext">CRFM)</strong> and<a id="_idIndexMarker252" class="calibre3"/> MosaicML announced the release of BioMedLM, a decoder-only transformer-based LLM with 2.7 billion parameters, trained on biomedical abstracts and papers.</p>
<p class="normal1">Another example is BloombergGPT, a 50 billion parameter LLM specialized for the financial domain developed by Bloomberg and trained on a 363 billion token dataset based on Bloomberg’s extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets.</p>
</div>
<p class="normal1">To make this decision framework more practical, let’s consider the following imaginary case study<a id="_idIndexMarker253" class="calibre3"/> about the company TechGen.</p>
<h2 class="heading1" id="_idParaDest-52">Case study</h2>
<p class="normal">TechGen Solutions, a <a id="_idIndexMarker254" class="calibre3"/>leading provider <a id="_idIndexMarker255" class="calibre3"/>of AI-driven analytics, face a decision between two advanced language models for their next-generation customer interaction system: GPT-4 and LLaMa-2. They require a robust language model that can handle diverse customer queries, provide accurate technical information, and integrate with their proprietary software. The following are their options:</p>
<ul class="calibre14">
<li class="bulletlist">GPT-4: Developed by OpenAI, GPT-4 is known for its vast parameter count and the ability to process both text and image inputs</li>
<li class="bulletlist1">LLama 2: Created by Meta AI, LLama 2 is an open-source model praised for its accessibility and performance on a smaller dataset.</li>
</ul>
<p class="normal1">The following are the factors that they consider when making their decision:</p>
<ul class="calibre14">
<li class="bulletlist">Performance: TechGen evaluates the models’ performance, particularly in generating technical content and code, where GPT-4 has shown higher accuracy.</li>
<li class="bulletlist1">Integration: The ease of integration with TechGen’s systems is critical, with GPT-4 potentially offering more seamless compatibility due to its widespread adoption.</li>
<li class="bulletlist1">Cost: While LLama 2 is free for commercial use under certain conditions, GPT-4 comes with a cost, which TechGen must factor into their decision.</li>
<li class="bulletlist1">Future-proofing: TechGen considers the long-term viability of each model, including the potential for updates and improvements.</li>
</ul>
<p class="normal1">Based on these considerations, TechGen opts for GPT-4, swayed by its superior performance in generating complex, technical responses and its multilingual capabilities, which align with their international expansion plans. The decision is also influenced by GPT-4’s image processing feature, which TechGen anticipates will become increasingly relevant as they incorporate more multimedia content into their customer service.</p>
<p class="normal1">TechGen’s choice of GPT-4 over LLama 2 is driven by the need for a high-performing, versatile language model that can scale with their growing global presence and diverse customer needs. While LLama 2’s open-source nature and cost effectiveness are appealing, GPT-4’s advanced capabilities and future-proof features present a more compelling case for TechGen’s ambitious goals.</p>
<p class="normal1">Note that these decision factors are not meant to be an exhaustive guide to deciding which models to embed within applications. Nevertheless, those are useful elements of reflection while setting up your application flow, so that you can determine your requirements<a id="_idIndexMarker256" class="calibre3"/> and then shortlist those LLMs that are more suitable for your goals.</p>
<h1 class="heading" id="_idParaDest-53">Summary</h1>
<p class="normal">This chapter covered some of the most promising LLMs in the market. It first differentiated between proprietary and open-source models, with all the related pros and cons. It then offered a deep dive into the architecture and technical features of GPT-4, PaLM-2, Claude 2, LLaMa-2, Falcon LLM, and MPT, with the addition of a section covering some LMMs. Finally, it provided a light framework to help developers decide which LLMs to pick while building AI-powered applications. This is pivotal to get the greatest impact from your application, given your industry-specific scenario.</p>
<p class="normal1">Starting from the next chapter, we will start working hands-on with LLMs within applications.</p>
<h1 class="heading" id="_idParaDest-54">References</h1>
<ul class="calibre16">
<li class="bulletlist">GPT-4 Technical Report. <a href="https://cdn.openai.com/papers/gpt-4.pdf" class="calibre3">https://cdn.openai.com/papers/gpt-4.pdf</a></li>
<li class="bulletlist1">Train short, test long: attention with linear biases enables input length extrapolation. <a href="https://arxiv.org/pdf/2108.12409.pdf" class="calibre3">https://arxiv.org/pdf/2108.12409.pdf</a></li>
<li class="bulletlist1">Constitutional AI: Harmlessness from AI Feedback. <a href="https://arxiv.org/abs/2212.08073" class="calibre3">https://arxiv.org/abs/2212.08073</a></li>
<li class="bulletlist1">Hugging Face Inference Endpoint. <a href="https://huggingface.co/docs/inference-endpoints/index" class="calibre3">https://huggingface.co/docs/inference-endpoints/index</a></li>
<li class="bulletlist1">Hugging Face Inference Endpoint Pricing. <a href="https://huggingface.co/docs/inference-endpoints/pricing" class="calibre3">https://huggingface.co/docs/inference-endpoints/pricing</a></li>
<li class="bulletlist1">Model Card for BioMedLM 2.7B. <a href="https://huggingface.co/stanford-crfm/BioMedLM" class="calibre3">https://huggingface.co/stanford-crfm/BioMedLM</a></li>
<li class="bulletlist1">PaLM 2 Technical Report. <a href="https://ai.google/static/documents/palm2techreport.pdf" class="calibre3">https://ai.google/static/documents/palm2techreport.pdf</a></li>
<li class="bulletlist1">Solving Quantitative Reasoning Problems with Language Models. <a href="https://arxiv.org/abs/2206.14858" class="calibre3">https://arxiv.org/abs/2206.14858</a></li>
<li class="bulletlist1">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. <a href="https://arxiv.org/abs/2306.05685" class="calibre3">https://arxiv.org/abs/2306.05685</a></li>
</ul>
<h1 class="heading">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3">https://packt.link/llm</a></p>
<p class="normal1"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png" class="calibre4"/></p>
</div>
</body></html>