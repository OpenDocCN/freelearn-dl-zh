<html><head></head><body>
  <div id="_idContainer139" class="Basic-Text-Frame">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-168" class="chapterTitle">Fine-Tuning with Preference Alignment</h1>
    <p class="normal"><strong class="keyWord">Supervised Fine-Tuning</strong> (<strong class="keyWord">SFT</strong>) has been crucial in adapting LLMs to perform specific tasks. However, SFT struggles to capture the nuances of human preferences and the <a id="_idIndexMarker579"/>long tail of potential interactions that a model might encounter. This limitation has led to the development of more advanced techniques for aligning AI systems with human preferences, grouped under the umbrella term <em class="italic">preference alignment</em>.</p>
    <p class="normal">Preference alignment addresses the shortcomings of SFT by incorporating direct human or AI feedback into the training process. This method allows a more nuanced understanding of human preferences, especially in complex scenarios where simple supervised learning falls short. While numerous techniques exist for preference <a id="_idIndexMarker580"/>alignment, this chapter will primarily focus on <strong class="keyWord">Direct Preference Optimization</strong> (<strong class="keyWord">DPO</strong>) for simplicity and efficiency.</p>
    <p class="normal">In this chapter, we will talk about the type of data that is required by preference alignment algorithms like DPO. We will build our own dataset to modify the writing style of our model, making it less artificial and more authentic. We will introduce the DPO algorithm and implement it to align the model trained in <em class="italic">Chapter 5</em>.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Understanding preference datasets</li>
      <li class="bulletList">How to create our own preference dataset</li>
      <li class="bulletList"><strong class="keyWord">Direct preference optimization</strong> (<strong class="keyWord">DPO</strong>)</li>
      <li class="bulletList">Implementing DPO in practice to align our model</li>
    </ul>
    <p class="normal">By the end of this chapter, you will be able to create your own preference datasets and align models with diverse techniques.</p>
    <div class="note">
      <p class="normal">All the code examples from this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/LLM-Engineering"><span class="url">https://github.com/PacktPublishing/LLM-Engineering</span></a>.</p>
    </div>
    <h1 id="_idParaDest-169" class="heading-1">Understanding preference datasets</h1>
    <p class="normal">The principles for creating high-quality preference datasets are the same as those <a id="_idIndexMarker581"/>discussed in <em class="italic">Chapter 5</em> for instruction datasets. We want to maximize the accuracy, diversity, and complexity of our samples. To achieve this, we follow the same stages, as outlined in <em class="italic">Figure 6.1</em>: data curation, deduplication, decontamination, quality evaluation, exploration, generation, and augmentation.</p>
    <figure class="mediaobject"><img src="../Images/B31105_05_01.png" alt="A diagram of a data flow  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.1 – Overview of the post-training data pipeline covered in this chapter</p>
    <p class="normal">To avoid repetition, this section will focus on the main differences between instruction and preference datasets. We will introduce the structure of preference samples and the ideal size for preference datasets. Then, we will focus on the two stages that differ most from creating instruction datasets: data generation and evaluation.</p>
    <h2 id="_idParaDest-170" class="heading-2">Preference data</h2>
    <p class="normal">Preference datasets lack the standardization of instruction datasets due to varying data requirements across different training algorithms. Preference data comprises a <a id="_idIndexMarker582"/>collection of responses to a given instruction, ranked by humans or language models. This chapter focuses on DPO, so we will examine the specific data format required by this algorithm.</p>
    <p class="normal">As illustrated in <em class="italic">Table 6.1</em>, the structure of DPO datasets is straightforward: each instruction is paired with one preferred answer and one rejected answer. The objective is to train the model to generate the preferred response rather than the rejected one.</p>
    <table id="table001-2" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell" colspan="2">
            <p class="normal"><strong class="keyWord">Instruction</strong></p>
            <p class="normal">Tell me a joke about octopuses.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Chosen answer</strong></p>
            <p class="normal">Why don’t octopuses play cards in casinos? Because they can’t count past eight.</p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Rejected answer</strong></p>
            <p class="normal">How many tickles does it take to make an octopus laugh? Ten tickles.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 6.1 – Example of sample from the mlabonne/orpo-dpo-mix-40k dataset</p>
    <p class="normal">In preference datasets, the rejected response is as important as the chosen one. Without the rejected response, the dataset would be a simple instruction set. Rejected responses represent the behavior we aim to eliminate from the model. This provides a lot of flexibility and allows us to use preference datasets in many contexts. Here is a list of examples where preference datasets are more beneficial to use compared to using SFT alone:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Chatbots</strong>: In <a id="_idIndexMarker583"/>conversational AI, the quality of responses <a id="_idIndexMarker584"/>often depends on subjective factors like naturalness, engagement, and contextual appropriateness. A preference dataset allows the model to learn these nuanced aspects by comparing better and worse responses. Simple SFT might not capture the subtleties of what makes one response preferable over another in a given context.</li>
      <li class="bulletList"><strong class="keyWord">Content moderation</strong>: Determining whether content is appropriate or violates <a id="_idIndexMarker585"/>guidelines often involves nuanced judgments. Preference datasets can help the model <a id="_idIndexMarker586"/>learn to distinguish between borderline cases by comparing examples of content that is and isn’t acceptable. This is more effective than binary classification through SFT, as it helps the model understand the reasoning behind moderation decisions.</li>
      <li class="bulletList"><strong class="keyWord">Summarization</strong>: The quality of a summary often depends on factors like <a id="_idIndexMarker587"/>conciseness, relevance, and coherence. By using preference datasets, models can learn to generate <a id="_idIndexMarker588"/>summaries that humans find more useful and informative. Simple SFT might result in summaries that are technically correct but less preferable to human readers.</li>
      <li class="bulletList"><strong class="keyWord">Code generation</strong>: In coding tasks, there are often multiple correct solutions, but <a id="_idIndexMarker589"/>some are more <a id="_idIndexMarker590"/>efficient or readable, or follow better practices than others. Preference datasets can help the model learn these qualitative aspects of code quality, which might not be captured by simple correctness-based SFT.</li>
      <li class="bulletList"><strong class="keyWord">Creative writing</strong>: For tasks like story generation or poetry writing, the quality of <a id="_idIndexMarker591"/>the output is highly subjective and multifaceted. Preference datasets can capture human judgments about style, creativity, and emotional impact better than instruction datasets, which might focus more on technical correctness or adherence to prompts.</li>
      <li class="bulletList"><strong class="keyWord">Translation</strong>: While <a id="_idIndexMarker592"/>traditional metrics like BLEU scores can measure translation accuracy, they don’t always capture the fluency or naturalness of the translation. Preference datasets can help models learn to produce translations that native <a id="_idIndexMarker593"/>speakers prefer, even when multiple translations are technically correct.</li>
    </ul>
    <p class="normal">In all these scenarios, preference datasets enable a more refined training approach. They capture subjective quality assessments and human preferences that extend beyond simple correctness or adherence to instructions. This method can produce models that generate output that is not only technically accurate but also better <a id="_idIndexMarker594"/>aligned with human judgment and preferences in complex, open-ended tasks.</p>
    <p class="normal">Unlike instruction datasets, there are no standardized storage formats like Alpaca or ShareGPT. Most preference datasets follow a structure similar to that shown in <em class="italic">Table 6.1</em>, with columns for an instruction, a preferred answer, and a rejected answer. Multi-turn conversations are uncommon in preference alignment. At the time of writing, major fine-tuning libraries do not support multi-turn conversations and typically extract only the first or last message in a conversation.</p>
    <h3 id="_idParaDest-171" class="heading-3">Data quantity</h3>
    <p class="normal">DPO datasets typically require fewer samples than instruction datasets to significantly impact <a id="_idIndexMarker595"/>model behavior. As with instruction datasets, the required sample count depends on model size and task complexity. Larger models are more sample-efficient and thus require less data, while complex tasks demand more examples to capture the desired behavior. Once again, data quality is crucial, and a large number of preference pairs is generally beneficial.</p>
    <p class="normal">General-purpose alignment is used by LLM providers to improve the overall performance of the fine-tuned models. This requires preference datasets with millions of samples. Major players in the AI industry, including Nvidia and Meta, are converging on similar post-training pipelines, involving multiple rounds of preference alignment, and extensive use of synthetic data. This consensus suggests that these methods are proving to be the most effective for pushing the boundaries of language model capabilities.</p>
    <p class="normal">On a smaller scale, the open-source community uses datasets ranging from 10,000 to 100,000 samples to enhance model performance. This approach has proven effective not only in improving benchmark scores but also in healing networks <a id="_idIndexMarker596"/>after merging, pruning, and other modifications. Generally, DPO is less destructive than SFT and has a milder impact on the final model.</p>
    <p class="normal">On the other hand, tasks like the ones previously described require fewer preference pairs. Task-specific alignment focuses on improving model performance for a particular function, such as modifying the writing style, refusing certain instructions, and so on. These alignments can often be achieved with smaller datasets, ranging from 100 to 10,000 preference pairs, depending on the task’s complexity.</p>
    <p class="normal">An example of an application that requires few samples is instructing the model to state that it wasn’t trained by OpenAI, Meta, or another LLM provider. This can be achieved using a preference dataset, where the rejected answers are those claiming alternative origins, and the chosen answers are responses where the model correctly states that it was trained by you. A relatively small dataset of 200 to 500 pairs can be enough for this task.</p>
    <h2 id="_idParaDest-172" class="heading-2">Data generation and evaluation</h2>
    <p class="normal">When creating <a id="_idIndexMarker597"/>preference datasets, data generation <a id="_idIndexMarker598"/>and evaluation are closely <a id="_idIndexMarker599"/>linked. We first <a id="_idIndexMarker600"/>create answers and then rate them to make the final dataset. In the following, we introduce both steps as one process instead of two separate ones.</p>
    <h3 id="_idParaDest-173" class="heading-3">Generating preferences</h3>
    <p class="normal">Before making new preference data, it’s good to look at relevant open-source datasets. There are fewer of these compared to instruction datasets, but you can find high-quality <a id="_idIndexMarker601"/>preference datasets on the Hugging Face Hub. These can be used for specific tasks or to add to your own dataset. Well-known preference datasets include the Anthropic HH-RLHF dataset, which has human preferences for helpful and harmless AI responses, and the OpenAI Summarize from Human Feedback dataset, which focuses on article summaries.</p>
    <p class="normal">DPO datasets can be created using various methods, each with its own trade-offs between quality, cost, and scalability. These methods can be tailored to specific applications and require varying degrees of human feedback. We divide them into four main categories:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Human-generated, human-evaluated datasets</strong>: This method involves hiring <a id="_idIndexMarker602"/>people to both create responses to prompts and evaluate the quality of these responses. While this approach can capture nuanced <a id="_idIndexMarker603"/>human preferences and is ideal for complex tasks, it’s extremely resource-intensive and difficult to scale. As a result, it’s primarily used by large AI companies with substantial resources.</li>
      <li class="bulletList"><strong class="keyWord">Human-generated, LLM-evaluated datasets</strong>: This method can be useful <a id="_idIndexMarker604"/>if you have a lot of existing human-generated content. However, it’s <a id="_idIndexMarker605"/>rarely used in practice due to inefficiency, as it still requires significant human input for response generation while potentially missing nuanced preferences during the LLM evaluation stage.</li>
      <li class="bulletList"><strong class="keyWord">LLM-generated, human-evaluated datasets</strong>: This <a id="_idIndexMarker606"/>method offers a good balance between quality and efficiency. LLMs generate multiple <a id="_idIndexMarker607"/>responses to prompts, and humans rank these responses. This approach is often preferred because humans are generally better at judging answers than writing them from scratch. It allows the rapid generation of diverse responses while still capturing human preferences effectively. However, it may not provide creative or unexpected responses that humans might generate.</li>
      <li class="bulletList"><strong class="keyWord">LLM-generated, LLM-evaluated datasets</strong>: Fully synthetic datasets, where <a id="_idIndexMarker608"/>both generation and evaluation are done by LLMs, are becoming increasingly common due to their scalability and cost-effectiveness. This <a id="_idIndexMarker609"/>method can produce massive datasets quickly and improves as LLM capabilities advance. However, it requires careful prompt engineering to ensure quality and diversity, and may perpetuate biases or limitations of the generating LLM.</li>
    </ul>
    <p class="normal">In practice, human-generated datasets are expensive, difficult to scale, and not necessarily <a id="_idIndexMarker610"/>of the highest quality. On the other hand, human evaluation is quite valuable but can be difficult to scale, which is why large datasets benefit from LLM evaluation. In addition to these high-level considerations, the way you obtain your data and how you plan to use it also need to be considered. For example, applications with many users can embed a feedback mechanism to provide preferences. This can be as simple as a <code class="inlineCode">like</code> and <code class="inlineCode">dislike</code> score, or something more in-depth with text.</p>
    <p class="normal">Note that evaluation is not always required and preferences can emerge naturally from the generation process. For instance, it is possible to use a high-quality model to generate preferred outputs and a lower-quality or intentionally flawed model to produce less preferred alternatives. This creates a clear distinction in the preference dataset, allowing more effective training of AI systems to recognize and emulate high-quality outputs. The <code class="inlineCode">Intel/orca_dpo_pairs</code> dataset available on the Hugging Face Hub was created with this process.</p>
    <p class="normal">Another approach is to compare model-generated outputs with human-written responses, which can provide insights into how well the model aligns with actual human <a id="_idIndexMarker611"/>preferences and highlight areas where the model may be lacking. This can be used to copy a particular style and give a more authentic tone to the model.</p>
    <h3 id="_idParaDest-174" class="heading-3">Tips for data generation</h3>
    <p class="normal">The data generation is consistent between instruction and preference datasets. Prompts should <a id="_idIndexMarker612"/>be designed to encourage diversity and complexity in the model’s responses. By crafting prompts that explicitly request different approaches or styles, we can ensure a wide range of outputs that capture the varied nature of human preferences. </p>
    <p class="normal">For instance, when generating summaries, one might request variations such as concise summaries, detailed summaries, and summaries focusing on key points. This approach not only produces a diverse dataset but also helps in understanding how different styles and approaches align with human preferences.</p>
    <p class="normal">Introducing variability in the outputs is another crucial aspect of generating synthetic preference datasets. This can be achieved by manipulating the temperature settings or employing other sampling methods in the LLM. Higher temperature settings tend to produce more creative and diverse responses, while lower settings result in more focused and deterministic outputs. This creates a trade-off between diversity and coherence, which depends on the kind of data we want to generate. For example, generating code requires low creativity, thus low temperature, while writing articles can be high temperature.</p>
    <p class="normal">Using multiple LLMs to generate samples can be better than using just one model. Some LLMs are better at specific tasks, and this approach also adds more variety. This approach is used by popular open-source datasets like <code class="inlineCode">argilla/Capybara-Preferences</code>, combining GPT-4 with open-weight models. The evaluation process then selects the chosen and the rejected answers.</p>
    <h3 id="_idParaDest-175" class="heading-3">Evaluating preferences</h3>
    <p class="normal">Data evaluation can be performed by human raters or automated with LLMs. <strong class="keyWord">LLM evaluation</strong> involves <a id="_idIndexMarker613"/>developing detailed criteria, creating a <a id="_idIndexMarker614"/>prompt that clearly communicates these guidelines to the LLM, and using the model to select preferred and rejected responses. While more scalable than human rating and allowing the consistent application of criteria, this quality of LLM evaluation depends directly on the model’s performance and the provided guidelines. It may miss subtle human preferences or cultural nuances. However, as LLMs continue to improve, their ability to make nuanced judgments improves as well, potentially leading to higher-quality datasets over time.</p>
    <p class="normal">Implementing LLM evaluation for preference datasets can be done through absolute scoring or pairwise ranking. In absolute scoring, the LLM assigns a numerical score or categorical rating to each response based on predefined criteria. This method is straightforward but may suffer from inconsistency across different prompts or evaluation sessions. Pairwise ranking, on the other hand, involves presenting the LLM with two responses and asking it to choose the better one or rank them. This approach more closely mimics the format of human evaluation and can lead to more consistent results.</p>
    <p class="normal">For absolute scoring, you would create a prompt that outlines the evaluation criteria and asks the LLM to rate the response on a specific scale (e.g., 1-5 or poor/fair/good/excellent). The prompt might look like this: “Rate the following response on a scale of 1-5 based on relevance, coherence, and helpfulness: [<code class="inlineCode">INSERT RESPONSE</code>].” For pairwise ranking, the prompt could be: “Compare the following two responses. Which one is better in terms of relevance, coherence, and helpfulness? Response A: [<code class="inlineCode">INSERT RESPONSE A</code>] Response B: [<code class="inlineCode">INSERT RESPONSE B</code>].”</p>
    <p class="normal">The comparative nature of preference datasets makes pairwise ranking an ideal approach for evaluation. This method is generally more accurate and more closely correlated to human judgment than absolute scoring. Pairwise ranking mimics the natural way humans compare options, making it easier for both human raters and LLMs to provide consistent and meaningful evaluations.</p>
    <p class="normal">We can further improve the accuracy of pairwise ranking by providing a ground-truth answer and using chain-of-thought reasoning. This approach encourages the evaluating LLM to consider multiple aspects of the responses and articulate its decision-making process, leading to more thorough and justified evaluations. When <a id="_idIndexMarker615"/>no ground-truth answer is available, we can prompt the LLM to create a grading note, which is a description of the expected answer. This technique works particularly well in scenarios where the LLM doesn’t have extensive knowledge about a given topic, as it forces the model to establish clear criteria for evaluation before assessing the responses.</p>
    <p class="normal">Here’s a concrete implementation of an LLM-as-a-judge prompt to perform pairwise ranking:</p>
    <table id="table002-1" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Instruction</strong></p>
            <p class="normal">You are an answer judge. Your goal is to compare answer A and answer B. I want to know which answer does a better job of answering the instruction in terms of relevance, accuracy, completeness, clarity, structure, and conciseness.</p>
            <p class="normal">Instruction: {instruction}</p>
            <p class="normal">Answer A: {answer_a}</p>
            <p class="normal">Answer B: {answer_b}</p>
            <p class="normal">Explain your reasoning step by step and output the letter of the best answer using the following structure:</p>
            <p class="normal">Reasoning: (compare the two answers)</p>
            <p class="normal">Best answer: (A or B)</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref"><em class="italic">Table 6.2</em> – Example of LLM-as-a-judge prompt for pairwise ranking with one instruction and two answers</p>
    <p class="normal">However, it’s important to note that LLM-based evaluation can be subject to several types of bias:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Position bias</strong>: In <a id="_idIndexMarker616"/>relative scoring, LLM judges <a id="_idIndexMarker617"/>tend to favor the first answer presented. This bias can skew results and lead to inaccurate preferences.</li>
      <li class="bulletList"><strong class="keyWord">Length bias</strong>: Similar <a id="_idIndexMarker618"/>to humans, LLM judges often <a id="_idIndexMarker619"/>show a preference for longer answers, potentially overlooking the quality of shorter, more concise responses.</li>
      <li class="bulletList"><strong class="keyWord">Family bias</strong>: LLM judges <a id="_idIndexMarker620"/>may favor responses <a id="_idIndexMarker621"/>that are generated by themselves or models from the same family, potentially due to similarities in language patterns or knowledge bases.</li>
    </ul>
    <p class="normal">To mitigate these biases and enhance the quality of preference datasets, several solutions can be implemented. One key approach is to randomize the order of answer A and answer B in each comparison, which can counteract position bias by ensuring <a id="_idIndexMarker622"/>that the order of presentation doesn’t consistently influence the evaluation. Another valuable strategy involves providing few-shot examples that demonstrate a balanced distribution of scores. These examples serve to calibrate the judge LLM’s internal scoring mechanism and can effectively address both length and family bias by illustrating that shorter answers or those from different model families can also be of high quality. Additionally, employing multiple models as a jury, rather than relying on a single LLM judge, can significantly improve the robustness of the evaluation process. This multi-model approach helps to balance out individual biases that may be present in any single model, leading to a more comprehensive and accurate assessment of the responses.</p>
    <p class="normal">In the next section, we will create our own preference dataset. We will rely on the data generation process to naturally create chosen (human-generated) and rejected (LLM-generated) answers.</p>
    <h1 id="_idParaDest-176" class="heading-1">Creating our own preference dataset</h1>
    <p class="normal">Our model can currently write paragraphs about topics related to machine learning, but it doesn’t have the same writing style as the original authors. This is a typical use <a id="_idIndexMarker623"/>case for preference alignment, where we want to change the “voice” of the model to closely imitate the source data. It’s important to note that, experimentally, DPO tends to make models more verbose and pushes them to use very formal language. Therefore, the training will need to use DPO surgically to avoid this pitfall and instead adopt the less formal style of these blog articles.</p>
    <p class="normal">In this section, we will create a preference dataset where the chosen answers are extracts from the text, while rejected answers are generated by the model. To implement it, we will modify the code created in <em class="italic">Chapter 5</em>, which was designed to generate instruction datasets.</p>
    <p class="normal">As seen in the previous section, preference and instruction datasets rely on the same principles. Instead of pairs of instructions and answers, we need triples (instruction, answer 1, answer 2). What’s interesting in this setting is that we have ground-truth answers in the text chunks, which means we don’t need complex evaluation processes like LLM judges. To make sure that these extracts are high-quality, we will implement two additional quality filters, based on length and punctuation. <em class="italic">Figure 6.2</em> summarizes the end-to-end process:</p>
    <figure class="mediaobject"><img src="../Images/B31105_06_02.png" alt="A black background with white lines  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.2 – Synthetic data generation pipeline from raw text to preference dataset</p>
    <p class="normal">We are now ready to implement the preference data generation pipeline:</p>
    <ol>
      <li class="numberedList" value="1">We start by importing the necessary libraries.
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> concurrent.futures
<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> re
<span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, <span class="hljs-type">Tuple</span>
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
</code></pre>
      </li>
      <li class="numberedList">Instead <a id="_idIndexMarker624"/>of the <code class="inlineCode">InstructionAnswerSet</code> class, we now have a <code class="inlineCode">PreferenceSet</code> class. This class is designed to handle triples of instructions, generated answers (rejected), and extracted answers (chosen).
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">PreferenceSet</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, triples: </span><span class="hljs-type">List</span><span class="hljs-params">[</span><span class="hljs-type">Tuple</span><span class="hljs-params">[</span><span class="hljs-built_in">str</span><span class="hljs-params">, </span><span class="hljs-built_in">str</span><span class="hljs-params">, </span><span class="hljs-built_in">str</span><span class="hljs-params">]]</span>):
        self.triples = triples
<span class="hljs-meta">    @classmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title">from_json</span>(<span class="hljs-params">cls, json_str: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-string">'PreferenceSet'</span>:
        data = json.loads(json_str)
        triples = [(triple[<span class="hljs-string">'instruction'</span>], triple[<span class="hljs-string">'generated_answer'</span>], triple[<span class="hljs-string">'extracted_answer'</span>])
                   <span class="hljs-keyword">for</span> triple <span class="hljs-keyword">in</span> data[<span class="hljs-string">'preference_triples'</span>]]
        <span class="hljs-keyword">return</span> cls(triples)
    <span class="hljs-keyword">def</span> <span class="hljs-title">__iter__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">iter</span>(self.triples)
</code></pre>
      </li>
      <li class="numberedList">The <code class="inlineCode">load_articles_from_json</code>, <code class="inlineCode">clean_text</code>, and <code class="inlineCode">extract_substrings</code> functions remain unchanged from the original code. Let’s start with <code class="inlineCode">load_articles_from_json</code>, which takes our JSON file (<code class="inlineCode">cleaned_documents.json</code>) containing the articles as input and returns a Hugging Face dataset with the text and metadata (ID, platform, author ID, author full name, link).
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">load_articles_from_json</span>(<span class="hljs-params">file_path: </span><span class="hljs-built_in">str</span>) -&gt; Dataset:
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> file:
        data = json.load(file)
    <span class="hljs-keyword">return</span> Dataset.from_dict(
        {
            <span class="hljs-string">"id"</span>: [item[<span class="hljs-string">"id"</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[<span class="hljs-string">"artifact_data"</span>]],
            <span class="hljs-string">"content"</span>: [item[<span class="hljs-string">"content"</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[<span class="hljs-string">"artifact_data"</span>]],
            <span class="hljs-string">"platform"</span>: [item[<span class="hljs-string">"platform"</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[<span class="hljs-string">"artifact_data"</span>]],
            <span class="hljs-string">"author_id"</span>: [item[<span class="hljs-string">"author_id"</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[<span class="hljs-string">"artifact_data"</span>]],
            <span class="hljs-string">"author_full_name"</span>: [item[<span class="hljs-string">"author_full_name"</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[<span class="hljs-string">"artifact_data"</span>]],
            <span class="hljs-string">"link"</span>: [item[<span class="hljs-string">"link"</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> data[<span class="hljs-string">"artifact_data"</span>]],
        }
    )
</code></pre>
      </li>
      <li class="numberedList">The <code class="inlineCode">clean_text</code> function removes non-alphanumeric characters except for <a id="_idIndexMarker625"/>apostrophes, periods, commas, exclamation marks, and question marks. It also replaces multiple whitespaces with a single space to ensure proper formatting.
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">clean_text</span>(<span class="hljs-params">text: </span><span class="hljs-built_in">str</span>) -&gt; <span class="hljs-built_in">str</span>:    text = re.sub(<span class="hljs-string">r"[^\w\s.,!?']"</span>, <span class="hljs-string">" "</span>, text)    text = re.sub(<span class="hljs-string">r"\s+"</span>, <span class="hljs-string">" "</span>, text)
    <span class="hljs-keyword">return</span> text.strip()
</code></pre>
      </li>
      <li class="numberedList">The <code class="inlineCode">extract_substrings</code> function splits articles into chunks with a length between 1,000 and 2,000 characters. To make sure that the splitting doesn’t break sentences, which could modify their meanings, we use a regex to only split after the end of a sentence.
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">extract_substrings</span>(<span class="hljs-params">dataset: Dataset, min_length: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">1000</span><span class="hljs-params">, max_length: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">2000</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:
    extracts = []
    sentence_pattern = <span class="hljs-string">r"(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?|\!)\s"</span>
    <span class="hljs-keyword">for</span> article <span class="hljs-keyword">in</span> dataset[<span class="hljs-string">"content"</span>]:
        cleaned_article = clean_text(article)
        sentences = re.split(sentence_pattern, cleaned_article)
        current_chunk = <span class="hljs-string">""</span>
        <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences:
            sentence = sentence.strip()
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> sentence:
                <span class="hljs-keyword">continue</span>
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_chunk) + <span class="hljs-built_in">len</span>(sentence) &lt;= max_length:
                current_chunk += sentence + <span class="hljs-string">" "</span>
            <span class="hljs-keyword">else</span>:
                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_chunk) &gt;= min_length:
                    extracts.append(current_chunk.strip())
                current_chunk = sentence + <span class="hljs-string">" "</span>
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(current_chunk) &gt;= min_length:
            extracts.append(current_chunk.strip())
    <span class="hljs-keyword">return</span> extracts
</code></pre>
      </li>
      <li class="numberedList">The <code class="inlineCode">generate_preference_triples</code> function replaces the original <code class="inlineCode">generate_instruction_answer_pairs</code> function. The prompt is adapted from the <a id="_idIndexMarker626"/>instruction version and is designed to generate triples instead of pairs. It also provides general guidance about the type of instructions we’re interested in, how to extract answers from articles, and how to style them:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_preference_triples</span>(<span class="hljs-params">extract: </span><span class="hljs-built_in">str</span><span class="hljs-params">, client: OpenAI</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]]:
    prompt = <span class="hljs-string">f"""Based on the following extract, generate five instruction-answer triples. Each triple should consist of:</span>
<span class="hljs-string">1. An instruction asking about a specific topic in the context.</span>
<span class="hljs-string">2. A generated answer that attempts to answer the instruction based on the context.</span>
<span class="hljs-string">3. An extracted answer that is a relevant excerpt directly from the given context.</span>
<span class="hljs-string">Instructions must be self-contained and general, without explicitly mentioning a context, system, course, or extract.</span>
<span class="hljs-string">Important:</span>
<span class="hljs-string">- Ensure that the extracted answer is a verbatim copy from the context, including all punctuation and apostrophes.</span>
<span class="hljs-string">- Do not add any ellipsis (...) or [...]  to indicate skipped text in the extracted answer.</span>
<span class="hljs-string">- If the relevant text is not continuous, use two separate sentences from the context instead of skipping text.</span>
<span class="hljs-string">Provide your response in JSON format with the following structure:</span>
<span class="hljs-string">{{</span>
<span class="hljs-string">    "preference_triples": [</span>
<span class="hljs-string">        {{</span>
<span class="hljs-string">            "instruction": "...",</span>
<span class="hljs-string">            "generated_answer": "...",</span>
<span class="hljs-string">            "extracted_answer": "..."</span>
<span class="hljs-string">        }},</span>
<span class="hljs-string">        ...</span>
<span class="hljs-string">    ]</span>
<span class="hljs-string">}}</span>
<span class="hljs-string">    Extract:</span>
<span class="hljs-string">    </span><span class="hljs-subst">{extract}</span>
<span class="hljs-string">"""</span>
</code></pre>
      </li>
      <li class="numberedList">In the <a id="_idIndexMarker627"/>same function, we use GPT-4o-mini to generate our answers using JSON mode. We specify in the system prompt that we want triples instead of pairs. The JSON answers are directly parsed by our <code class="inlineCode">PreferenceSet</code> class to return the expected list of tuples.
        <pre class="programlisting code-one"><code class="hljs-code">    completion = client.chat.completions.create(
        model=<span class="hljs-string">"gpt-4o-mini"</span>,
        messages=[
            {
                <span class="hljs-string">"role"</span>: <span class="hljs-string">"system"</span>,
                <span class="hljs-string">"content"</span>: <span class="hljs-string">"You are a helpful assistant who generates instruction-answer triples based on the given context. Each triple should include an instruction, a generated answer, and an extracted answer from the context. Provide your response in JSON format."</span>,
            },
            {<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: prompt},
        ],
        response_format={<span class="hljs-string">"type"</span>: <span class="hljs-string">"json_object"</span>},
        max_tokens=<span class="hljs-number">2000</span>,
        temperature=<span class="hljs-number">0.7</span>,
    )
    result = PreferenceSet.from_json(completion.choices[<span class="hljs-number">0</span>].message.content)
    <span class="hljs-keyword">return</span> result.triples
</code></pre>
      </li>
      <li class="numberedList">Two new <a id="_idIndexMarker628"/>filtering functions are introduced for the preference data pipeline: <code class="inlineCode">filter_short_answers</code> and <code class="inlineCode">filter_answer_format</code>. These functions filter out short answers and ensure that answers start with an uppercase letter and end with proper punctuation. We use them as heuristics to filter out samples with poor quality.
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">filter_short_answers</span>(<span class="hljs-params">dataset: Dataset, min_length: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">100</span>) -&gt; Dataset:
    <span class="hljs-keyword">def</span> <span class="hljs-title">is_long_enough</span>(<span class="hljs-params">example</span>):
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(example[<span class="hljs-string">'chosen'</span>]) &gt;= min_length
    <span class="hljs-keyword">return</span> dataset.<span class="hljs-built_in">filter</span>(is_long_enough)
<span class="hljs-keyword">def</span> <span class="hljs-title">filter_answer_format</span>(<span class="hljs-params">dataset: Dataset</span>) -&gt; Dataset:
    <span class="hljs-keyword">def</span> <span class="hljs-title">is_valid_format</span>(<span class="hljs-params">example</span>):
        chosen = example[<span class="hljs-string">'chosen'</span>]
        <span class="hljs-keyword">return</span> (<span class="hljs-built_in">len</span>(chosen) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span>
                chosen[<span class="hljs-number">0</span>].isupper() <span class="hljs-keyword">and</span>
                chosen[-<span class="hljs-number">1</span>] <span class="hljs-keyword">in</span> (<span class="hljs-string">'.'</span>, <span class="hljs-string">'!'</span>, <span class="hljs-string">'?'</span>))
    <span class="hljs-keyword">return</span> dataset.<span class="hljs-built_in">filter</span>(is_valid_format)
</code></pre>
      </li>
      <li class="numberedList">The <code class="inlineCode">create_preference_dataset</code> function replaces the original <code class="inlineCode">create_instruction_dataset</code> function. This function now works with triples instead <a id="_idIndexMarker629"/>of pairs and uses different column names in the resulting dataset.
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">create_preference_dataset</span>(<span class="hljs-params">dataset: Dataset, client: OpenAI, num_workers: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">4</span>) -&gt; Dataset:
    extracts = extract_substrings(dataset)
    preference_triples = []
    <span class="hljs-keyword">with</span> concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) <span class="hljs-keyword">as</span> executor:
        futures = [
            executor.submit(generate_preference_triples, extract, client)
            <span class="hljs-keyword">for</span> extract <span class="hljs-keyword">in</span> extracts
        ]
        <span class="hljs-keyword">for</span> future <span class="hljs-keyword">in</span> tqdm(concurrent.futures.as_completed(futures), total=<span class="hljs-built_in">len</span>(futures)):
            preference_triples.extend(future.result())
    instructions, generated_answers, extracted_answers = <span class="hljs-built_in">zip</span>(*preference_triples)
    <span class="hljs-keyword">return</span> Dataset.from_dict(
        {
            <span class="hljs-string">"prompt"</span>: <span class="hljs-built_in">list</span>(instructions),
            <span class="hljs-string">"rejected"</span>: <span class="hljs-built_in">list</span>(generated_answers),
            <span class="hljs-string">"chosen"</span>: <span class="hljs-built_in">list</span>(extracted_answers)
        }
    )
</code></pre>
      </li>
      <li class="numberedList">The main function is updated to include the new filtering steps and to use the preference dataset creation function:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>(<span class="hljs-params">dataset_id: </span><span class="hljs-built_in">str</span>) -&gt; Dataset:
    client = OpenAI()
    <span class="hljs-comment"># 1. Load the raw data</span>
    raw_dataset = load_articles_from_json(<span class="hljs-string">"cleaned_documents.json"</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Raw dataset:"</span>)
    <span class="hljs-built_in">print</span>(raw_dataset.to_pandas())
    <span class="hljs-comment"># 2. Create preference dataset</span>
    dataset = create_preference_dataset(raw_dataset, client)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Preference dataset:"</span>)
    <span class="hljs-built_in">print</span>(dataset.to_pandas())
    <span class="hljs-comment"># 3. Filter out samples with short answers</span>
    dataset = filter_short_answers(dataset)
    <span class="hljs-comment"># 4. Filter answers based on format</span>
    dataset = filter_answer_format(dataset)
    <span class="hljs-comment"># 5. Export</span>
    dataset.push_to_hub(dataset_id)
    <span class="hljs-keyword">return</span> dataset
</code></pre>
      </li>
    </ol>
    <p class="normal">The <code class="inlineCode">create_preference_dataset()</code> function generated 2,970 samples. This dataset is then heavily <a id="_idIndexMarker630"/>filtered to only retain 1,467 samples by removing answers that are too short or not properly formatted (for example, answers that start with an uppercase letter or end with a period, exclamation mark, or question mark).</p>
    <p class="normal">The final <a id="_idIndexMarker631"/>dataset is available on the Hugging Face Hub at the following address: <a href="https://huggingface.co/datasets/mlabonne/llmtwin-dpo"><span class="url">https://huggingface.co/datasets/mlabonne/llmtwin-dpo</span></a>. You can see in <em class="italic">Figure 6.3</em> an example that captures a subtle nuance in terms of writing style. Both answers are correct, but the <strong class="screenText">chosen</strong> (extracted) answer sounds slightly more casual.</p>
    <figure class="mediaobject"><img src="../Images/B31105_06_03.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.3 – Screenshot of the mlabonne/llmtwin-dpo preference dataset on the Hugging Face Hub</p>
    <p class="normal">To produce this dataset, we iterated many times over the prompt to generate the data. This required some manual evaluation and experiments until we reached satisfying results. The quality of the prompt is fundamental in this process, which is why it is recommended to follow a similar process to generate your own preference datasets.</p>
    <p class="normal">In the <a id="_idIndexMarker632"/>next section, we will introduce concepts related to <strong class="keyWord">Reinforcement Learning from Human Feedback</strong> (<strong class="keyWord">RLHF</strong>) and DPO. This will cover new parameters and ideas that are implemented in the final section of this chapter.</p>
    <h1 id="_idParaDest-177" class="heading-1">Preference alignment</h1>
    <p class="normal">Preference <a id="_idIndexMarker633"/>alignment regroups techniques to fine-tune models on preference data. In this section, we provide an overview of this field and <a id="_idIndexMarker634"/>then focus on the technique we will implement: <strong class="keyWord">Direct Preference Optimization</strong> (<strong class="keyWord">DPO</strong>).</p>
    <h2 id="_idParaDest-178" class="heading-2">Reinforcement Learning from Human Feedback</h2>
    <p class="normal"><strong class="keyWord">Reinforcement Learning from Human Feedback</strong> (<strong class="keyWord">RLHF</strong>) combines <strong class="keyWord">reinforcement learning</strong> (<strong class="keyWord">RL</strong>) with <a id="_idIndexMarker635"/>human <a id="_idIndexMarker636"/>input to align models with human preferences and values. RLHF emerged as a response to challenges in traditional RL methods, particularly the difficulty of specifying reward functions for complex tasks and the potential for misalignment between engineered rewards and intended objectives.</p>
    <p class="normal">The origins <a id="_idIndexMarker637"/>of RLHF can be traced back to the field of <strong class="keyWord">preference-based reinforcement learning</strong> (<strong class="keyWord">PbRL</strong>), which was independently introduced by Akrour et al. and Cheng et al. in 2011. PbRL aimed to infer objectives from qualitative feedback, such as pairwise preferences between behaviors, rather than relying on quantitative reward signals. This approach addressed some of the limitations of conventional RL, where defining appropriate reward functions can be challenging and prone to reward hacking or unintended behaviors.</p>
    <p class="normal">The term <a id="_idIndexMarker638"/>RLHF was coined later, around 2021-2022, as the approach gained prominence in the context of training LLMs. However, the core ideas had been developing for years prior. A seminal paper by Christiano et al. in 2017 demonstrated the effectiveness of learning reward models from human preferences and using them to train RL agents. This work showed that RLHF could match or exceed the performance of agents trained on hand-engineered rewards, but with significantly less human effort.</p>
    <p class="normal">At its core, RLHF works by iteratively improving both a reward model and a policy:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Reward model learning</strong>: Instead of using a pre-defined reward function, RLHF learns <a id="_idIndexMarker639"/>a reward model from human <a id="_idIndexMarker640"/>feedback. This is typically done by presenting humans with different answers and asking them to indicate which one they prefer. These preferences are used to train a reward model, often using a Bradley-Terry model or similar approaches that map preferences to underlying utility functions.</li>
      <li class="bulletList"><strong class="keyWord">Policy optimization</strong>: With the learned reward model, standard RL algorithms <a id="_idIndexMarker641"/>can be used to optimize a policy. This policy <a id="_idIndexMarker642"/>generates new behaviors that aim to maximize the predicted rewards from the learned model.</li>
      <li class="bulletList"><strong class="keyWord">Iterative improvement</strong>: As the policy improves, it generates new behaviors <a id="_idIndexMarker643"/>that can be evaluated by humans, leading <a id="_idIndexMarker644"/>to refinements in the reward model. This cycle continues, ideally resulting in a policy that aligns well with human preferences.</li>
    </ul>
    <p class="normal">A key innovation in RLHF is its approach to handling the high cost of human feedback. Rather than requiring constant human oversight, RLHF allows for asynchronous <a id="_idIndexMarker645"/>and sparse feedback.</p>
    <p class="normal">The learned reward model serves as a proxy for human preferences, enabling the RL algorithm to train continuously without direct human input for every action.</p>
    <p class="normal">As an example, <em class="italic">Figure 6.4</em> shows a high-level view of the <strong class="keyWord">Proximal Policy Optimization</strong> (<strong class="keyWord">PPO</strong>) algorithm, which is one of the most popular RLHF algorithms. Here, the reward model is used to score the text that is generated by the trained model. This reward <a id="_idIndexMarker646"/>is regularized by an additional <strong class="keyWord">Kullback–Leibler</strong> (<strong class="keyWord">KL</strong>) divergence factor, ensuring that the distribution of tokens stays similar to the model before training (frozen model).</p>
    <figure class="mediaobject"><img src="../Images/B31105_06_04.png" alt="A diagram of a data flow  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.4 – High-level view of the PPO algorithm for preference alignment</p>
    <p class="normal">While <a id="_idIndexMarker647"/>RLHF has proven effective for aligning AI systems with human preferences, it faces challenges due to its iterative nature and reliance on a separate reward model, which can be computationally expensive and potentially unstable. Despite theoretical superiority, RLHF algorithms have also experimentally underperformed compared to simpler approaches. One such approach that has gained significant attention is DPO.</p>
    <h2 id="_idParaDest-179" class="heading-2">Direct Preference Optimization</h2>
    <p class="normal">Introduced by Rafailov et al. in their 2023 paper <em class="italic">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</em>, DPO offers a streamlined alternative to traditional RLHF methods.</p>
    <p class="normal">DPO’s <a id="_idIndexMarker648"/>core innovation lies in its reformulation of the preference learning problem. Unlike RLHF, which typically involves training a separate reward model and then using reinforcement learning algorithms like PPO to fine-tune the language model, DPO takes a more direct approach. </p>
    <p class="normal">It derives a closed-form expression for the optimal policy under the standard RLHF objective of maximizing expected reward subject to a KL-divergence constraint with a reference policy. This mathematical insight allows DPO to express the preference learning problem directly in terms of the policy, eliminating the need for a separate reward model or complex reinforcement learning algorithms.</p>
    <p class="normal">In practical terms, DPO can be implemented as a simple binary cross-entropy loss function that operates directly on the language model’s output probabilities. This loss function encourages the model to assign higher probability to preferred responses and lower probability to non-preferred responses, while maintaining closeness to a reference (frozen) model. The importance of the reference model is directly controlled via a beta parameter between 0 and 1. The reference model is ignored when beta is equal to 0, which means that the trained model can be very different from the SFT one. In practice, a value of 0.1 is the most popular one, but this can be tweaked, as we’ll see in the next section.</p>
    <p class="normal">The simplicity of this approach allows optimization using standard gradient descent techniques, without the need for sampling from the model during training or implementing complex RL algorithms. <em class="italic">Figure 6.5</em> shows a high-level view of the DPO algorithm, greatly simplifying the training process compared to <em class="italic">Figure 6.4</em>.</p>
    <figure class="mediaobject"><img src="../Images/B31105_06_05.png" alt="A diagram of a data flow  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 6.5 – High-level view of the DPO algorithm for preference alignment</p>
    <p class="normal">DPO has several advantages over traditional RLHF methods. As previously mentioned, it significantly simplifies the preference learning pipeline, reducing the engineering <a id="_idIndexMarker649"/>complexity associated with RLHF methods. By eliminating the need for a separate reward model and RL algorithms, DPO is more computationally efficient than traditional RLHF approaches. Particularly when trained with adapters (LoRA, QLoRA), the frozen and trained models don’t have to be separated. Indeed, since we’re only training adapters, the trained model is not modified. This allows us to only load one model instead of two, which saves additional VRAM.</p>
    <p class="normal">Despite its simplicity, DPO often matches the performance of more complex RLHF methods. It also tends to be more stable during training and less sensitive to hyperparameters. The simplified approach makes DPO easier to implement and scale, particularly for small teams without extensive RL knowledge.</p>
    <p class="normal">While RLHF allows iterative improvement through multiple training rounds and can dynamically adapt to new preferences, DPO offers a more straightforward path to achieving similar results. The choice between DPO and PPO-based RLHF often comes down to a trade-off between ease of implementation and potential peak performance. For large-scale training runs with millions of preference samples, PPO-inspired methods still have a higher performance ceiling. However, for most applications, DPO provides the majority of the performance benefits at a lower computational and engineering cost.</p>
    <p class="normal">Both RLHF <a id="_idIndexMarker650"/>and DPO benefit significantly from the integration of synthetic data. As LLMs become more capable, they can generate data that surpasses human-created content in quality and diversity. This enables a virtuous cycle where better models produce better training data, which in turn leads to further model improvements. The iterative nature of both approaches allows multiple rounds of model refinement, each focusing on different aspects of model performance and gradually enhancing capabilities across various domains.</p>
    <p class="normal">Despite its advantages, DPO is not without drawbacks. Like RLHF, DPO still requires paired preference data, which can be expensive and time-consuming to collect. DPO lacks some of the theoretical guarantees associated with reinforcement learning approaches. There may be scenarios where the added flexibility of RLHF is beneficial, particularly for complex tasks or environments.</p>
    <p class="normal">Nonetheless, DPO is ideal in most cases, including our twin LLM example. In the next section, we will implement it using Unsloth.</p>
    <h1 id="_idParaDest-180" class="heading-1">Implementing DPO</h1>
    <p class="normal">In this <a id="_idIndexMarker651"/>section, we will DPO <a id="_idIndexMarker652"/>fine-tune the <strong class="keyWord">TwinLlama-3.1-8B</strong> model we created in <em class="italic">Chapter 5</em>. For ease of use and to maximize performance, we will again use the Unsloth library for our DPO implementation. Depending on the available VRAM, you can choose between LoRA (higher quality, speed, and VRAM usage) and QLoRA (lower quality, speed, and VRAM usage). This technique, along with other preference alignment algorithms, is also available in TRL and Axolotl.</p>
    <p class="normal">This example can be seen as an advanced application of DPO. Indeed, our objective of imitating a writing style conflicts with the natural tendency of DPO to encourage formal language. This is partly due to the fact that chosen answers are often more <a id="_idIndexMarker653"/>formal than rejected ones. In practice, this will force us to do light fine-tuning, with a low learning rate and number of epochs. To find the best hyperparameters, we trained over 20 models and compared their outputs on a set of questions, including “Write a paragraph to introduce supervised fine-tuning.” This allowed us to select the model and parameters that worked best for this task.</p>
    <p class="normal">The dependencies are the same as those in <em class="italic">Chapter 5</em> with SFT and can be found in the book’s GitHub repository (<a href="https://github.com/PacktPublishing/LLM-Engineering"><span class="url">https://github.com/PacktPublishing/LLM-Engineering</span></a>) or in Unsloth’s repo (<a href="https://github.com/unslothai/unsloth"><span class="url">https://github.com/unslothai/unsloth</span></a>):</p>
    <ol>
      <li class="numberedList" value="1">First, we want to access a gated model and (optionally) upload our fine-tuned <a id="_idIndexMarker654"/>model to Hugging Face (<a href="https://huggingface.co/"><span class="url">https://huggingface.co/</span></a>). This requires us to log in to an account. If you don’t have an account, you can create one and store your API key (<strong class="screenText">Settings | Access Tokens | Create new token</strong>) in the <code class="inlineCode">.env</code> file:
        <pre class="programlisting code-one"><code class="hljs-code">HF_TOKEN = YOUR_API_KEY
</code></pre>
      </li>
      <li class="numberedList">Make sure that your Comet ML API key is also in the <code class="inlineCode">.env</code> file. Otherwise, the code will crash and raise an error when training starts.
        <pre class="programlisting code-one"><code class="hljs-code">COMET_API_KEY = YOUR_API_KEY
</code></pre>
      </li>
      <li class="numberedList">Before we import all the necessary packages, we want to apply a patch for the <code class="inlineCode">DPOTrainer</code> class from TRL. This fixes the DPO logs in notebook environments.
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> PatchDPOTrainer
PatchDPOTrainer()
</code></pre>
      </li>
      <li class="numberedList">We can now import the other libraries. The main difference between DPO and SFT is the import of <code class="inlineCode">DPOConfig</code> and <code class="inlineCode">DPOTrainer</code> from TRL, which are specific to DPO training.
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, TextStreamer
<span class="hljs-keyword">from</span> unsloth <span class="hljs-keyword">import</span> FastLanguageModel, is_bfloat16_supportedfrom trl <span class="hljs-keyword">import</span> DPOConfig, DPOTrainer
</code></pre>
      </li>
      <li class="numberedList">This <a id="_idIndexMarker655"/>step loads our fine-tuned model from <em class="italic">Chapter 5</em>. We use the same configuration with a <code class="inlineCode">max_seq_length</code> of 2048. You can activate QLoRA by setting <code class="inlineCode">load_in_4bit</code> to <code class="inlineCode">True</code>. In the following, we will perform LoRA DPO fine-tuning for increased speed and quality.
        <pre class="programlisting code-one"><code class="hljs-code">max_seq_length = <span class="hljs-number">2048</span>
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=<span class="hljs-string">"mlabonne/TwinLlama-3.1-8B"</span>,
    max_seq_length=max_seq_length,
    load_in_4bit=<span class="hljs-literal">False</span>,
)
</code></pre>
      </li>
      <li class="numberedList">Let’s now prepare the model for PEFT with the LoRA configuration. We increase the rank (<code class="inlineCode">r</code>) and <code class="inlineCode">lora_alpha</code> from <code class="inlineCode">32</code> (as it was in <em class="italic">Chapter 5</em>) to <code class="inlineCode">64</code>. This will allow more expressive fine-tuning. We keep a dropout of <code class="inlineCode">0</code> for speed and we target every linear module as per usual.
        <pre class="programlisting code-one"><code class="hljs-code">model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    lora_alpha=32,
    lora_dropout=0,
    target_modules=["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"],
)
</code></pre>
      </li>
      <li class="numberedList">We load the <code class="inlineCode">llmtwin-dpo</code> dataset (training split), which contains our prompts, chosen, and rejected answers.
        <pre class="programlisting code-one"><code class="hljs-code">dataset = load_dataset(<span class="hljs-string">"mlabonne/llmtwin-dpo"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
      </li>
      <li class="numberedList">The <a id="_idIndexMarker656"/>data preparation is significantly different from the SFT example in <em class="chapterRef">Chapter 5</em>. Here, we have triples with a prompt, a chosen answer, and a rejected answer. In the <code class="inlineCode">format_samples</code> function, we apply the Alpaca chat template to each individual message. Note that the instruction is the only one that requires the chat format: chosen and rejected answers <a id="_idIndexMarker657"/>only need to be concatenated with the <strong class="keyWord">end of sentence</strong> (<strong class="keyWord">EOS</strong>) token. Finally, we create a train/test split with a 95%/5% ratio.
        <pre class="programlisting code-one"><code class="hljs-code">alpaca_template = <span class="hljs-string">"""Below is an instruction that describes a task. Write a response that appropriately completes the request.</span>
<span class="hljs-string">### Instruction:</span>
<span class="hljs-string">{}</span>
<span class="hljs-string">### Response:</span>
<span class="hljs-string">"""</span>
EOS_TOKEN = tokenizer.eos_token
<span class="hljs-keyword">def</span> <span class="hljs-title">format_samples</span>(<span class="hljs-params">example</span>):
    example[<span class="hljs-string">"prompt"</span>] = alpaca_template.<span class="hljs-built_in">format</span>(example[<span class="hljs-string">"prompt"</span>])
    example[<span class="hljs-string">"chosen"</span>] = example[<span class="hljs-string">'chosen'</span>] + EOS_TOKEN
    example[<span class="hljs-string">"rejected"</span>] = example[<span class="hljs-string">'rejected'</span>] + EOS_TOKEN
    <span class="hljs-keyword">return</span> {<span class="hljs-string">"prompt"</span>: example[<span class="hljs-string">"prompt"</span>], <span class="hljs-string">"chosen"</span>: example[<span class="hljs-string">"chosen"</span>], <span class="hljs-string">"rejected"</span>: example[<span class="hljs-string">"rejected"</span>]}
dataset = dataset.<span class="hljs-built_in">map</span>(format_samples)
dataset = dataset.train_test_split(test_size=<span class="hljs-number">0.05</span>)
</code></pre>
      </li>
      <li class="numberedList">The model and data are now ready, so we can start fine-tuning. Compared to SFT, there are a few new parameters, like <code class="inlineCode">ref_model</code> and <code class="inlineCode">beta</code>. Since we’re using LoRA (or QLoRA), we don’t directly train the model <a id="_idIndexMarker658"/>but instead the adapters. This means we can use the original model (without adapters) as a reference, saving a lot of VRAM. The <code class="inlineCode">beta</code> parameter controls the importance of the reference model. A standard value of 0.1 works well in most scenarios, but we decided to increase it to 0.5 based on our experiments. This is due to the fact that the trained model used formal language with lower values. Having it closer to the reference model helps to fix this issue.</li>
    </ol>
    <p class="normal-one">The learning rate is also lower (from 3e-4 for SFT to 2e-6 here). We train for 1 epoch instead of 3, and the <code class="inlineCode">max_seq_length</code> parameter is now broken down into two new parameters: <code class="inlineCode">max_prompt_length</code> (prompt only) and <code class="inlineCode">max_length</code> (prompt and answer). Note that we also replaced the <code class="inlineCode">TrainingArguments</code> class with <code class="inlineCode">DPOConfig</code>.</p>
    <pre class="programlisting code-one"><code class="hljs-code">trainer = DPOTrainer(
    model=model,
    ref_model=<span class="hljs-literal">None</span>,
    tokenizer=tokenizer,
    beta=<span class="hljs-number">0.5</span>,
    train_dataset=dataset[<span class="hljs-string">"train"</span>],
    eval_dataset=dataset[<span class="hljs-string">"test"</span>],
    max_length=max_seq_length//<span class="hljs-number">2</span>,
    max_prompt_length=max_seq_length//<span class="hljs-number">2</span>,
    args=DPOConfig(
        learning_rate=<span class="hljs-number">2e-6</span>,
        lr_scheduler_type=<span class="hljs-string">"linear"</span>,
        per_device_train_batch_size=<span class="hljs-number">2</span>,
        per_device_eval_batch_size=<span class="hljs-number">2</span>,
        gradient_accumulation_steps=<span class="hljs-number">8</span>,
        num_train_epochs=<span class="hljs-number">1</span>,
        fp16=<span class="hljs-keyword">not</span> is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        optim=<span class="hljs-string">"adamw_8bit"</span>,
        weight_decay=<span class="hljs-number">0.01</span>,
        warmup_steps=<span class="hljs-number">10</span>,
        output_dir=<span class="hljs-string">"output"</span>,
        eval_strategy=<span class="hljs-string">"steps"</span>,
        eval_steps=<span class="hljs-number">0.2</span>,
        logging_steps=<span class="hljs-number">1</span>,
        report_to=<span class="hljs-string">"</span><span class="hljs-string">comet_ml"</span>,
        seed=<span class="hljs-number">0</span>,
    ),
)
trainer.train()
</code></pre>
    <ol>
      <li class="numberedList" value="10">Once the model is trained, we can run it for a quick sanity check. This step <a id="_idIndexMarker659"/>is similar to the SFT example. It prepares the model for inference and generates a response to a prompt.
        <pre class="programlisting code-one"><code class="hljs-code">FastLanguageModel.for_inference(model)
message = alpaca_template.<span class="hljs-built_in">format</span>(<span class="hljs-string">"Write a paragraph to introduce supervised fine-tuning."</span>, <span class="hljs-string">""</span>)
inputs = tokenizer([message], return_tensors=<span class="hljs-string">"pt"</span>).to(<span class="hljs-string">"cuda"</span>)
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=<span class="hljs-number">256</span>, use_cache=<span class="hljs-literal">True</span>)
</code></pre>
      </li>
      <li class="numberedList">The trained DPO model returns the following response:
        <pre class="programlisting code-one"><code class="hljs-code">Supervised fine-tuning is a method used to enhance the performance of pre-trained language models by utilizing labeled data. This technique involves taking a pre-trained model and refining it on a specific task, such as content creation or customer service. By providing the model with relevant data and guidance, it can learn to generate outputs that align more closely with the desired outcomes. This approach allows for the creation of more specialized models that can tackle complex tasks with greater accuracy and efficiency.
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We can compare it with the answer provided by the SFT model:</p>
    <pre class="programlisting code-one"><code class="hljs-code">Supervised fine-tuning is a method used to enhance a language model by providing it with a curated dataset of instructions and their corresponding answers. This process is designed to align the model's responses with human expectations, thereby improving its accuracy and relevance. The goal is to ensure that the model can respond effectively to a wide range of queries, making it a valuable tool for applications such as chatbots and virtual assistants.
</code></pre>
    <p class="normal-one">The <a id="_idIndexMarker660"/>DPO model provides an answer that is both more accurate and closer to the desired writing style. It correctly identifies pre-training language models as source models for SFT. It also mentions domain or task-specific finetunes instead of alignment with “human expectations,” which is closer to the preference alignment stage. The answer is also less formal and something we would use in a blog post.</p>
    <ol>
      <li class="numberedList" value="12">Finally, the last step consists of saving the trained model locally and pushing it to the Hugging Face Hub.
        <pre class="programlisting code-one"><code class="hljs-code">model.save_pretrained_merged(<span class="hljs-string">"model"</span>, tokenizer, save_method=<span class="hljs-string">"merged_16bit"</span>)
</code></pre>
      </li>
    </ol>
    <p class="normal">Congratulations! We have trained and exported our DPO model. It is now available on the Hugging Face Hub at <a href="https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO"><span class="url">https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO</span></a>. Compared to SFT, DPO has a few additional metrics that need to be tracked during training. <em class="italic">Figure 6.6</em> shows the Comet ML dashboard with the main metrics. You <a id="_idIndexMarker661"/>can publicly access it using the following URL: <a href="https://www.comet.com/mlabonne/llm-twin-training/"><span class="url">https://www.comet.com/mlabonne/llm-twin-training/</span></a></p>
    <figure class="mediaobject"><img src="../Images/B31105_06_06.png" alt=""/><img src="../Images/B31105_06_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.6 – Experiment tracking in Comet ML with DPO metrics</p>
    <p class="normal">Let’s review these metrics:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Training loss</strong>: We still want the loss to continuously decrease on average. Note that it can rapidly fall to zero, meaning that the model is no longer learning anything. This behavior doesn’t necessarily lead to overfitting or bad models but needs to be monitored closely.</li>
      <li class="bulletList"><strong class="keyWord">Validation loss</strong>: The same thing can be said about the validation loss. We expect a small gap compared to the training loss.</li>
      <li class="bulletList"><strong class="keyWord">Gradient norm</strong>: We expect small gradient norms with few spikes.</li>
      <li class="bulletList"><strong class="keyWord">Rewards</strong>: We have two different rewards: chosen and rejected. They correspond to the mean difference between the log probabilities output by the trained and reference models. Over time, we expect the <a id="_idIndexMarker662"/>model to choose the chosen answers and reject the rejected answers, which means that the gap between them should increase. This difference is directly tracked by the <code class="inlineCode">margins</code> metric, defined as the difference between chosen and rejected rewards. A well-trained model’s margin will quickly increase and then plateau.</li>
      <li class="bulletList"><strong class="keyWord">Accuracies</strong>: This metric represents the percentage of times the model correctly identifies the chosen answers. We want this accuracy to gradually increase during training, but it doesn’t need to reach 100%. An accuracy of 100%, especially if it’s achieved quickly, indicates that the preference dataset might be too easy for the model. While the LLM can still learn from such a dataset, it might be beneficial to add more challenging examples.</li>
    </ul>
    <p class="normal">In general, DPO is slightly harder to monitor and debug than SFT because it’s a more complex process, involving a reference model. However, it’s also significantly easier to use than PPO and other RLHF algorithms. As long as you have a high-quality preference dataset and a strong fine-tuned model, you can experiment with different ranks, beta parameters, learning rates, and number of epochs to see which experiment best captures your preferences.</p>
    <p class="normal">While this is not the purpose of this chapter, it is possible to automate the evaluation of models designed to imitate a writing style. A possible solution consists of comparing the distribution of words in the text generated by different models (SFT and DPO) with our ground-truth dataset. In this example, we expect the SFT model <a id="_idIndexMarker663"/>to output a lot of words that are overrepresented in GPT-4o-mini (like “delve into”). The distribution output by our DPO model should be a lot closer to the chosen answers.</p>
    <h1 id="_idParaDest-181" class="heading-1">Summary</h1>
    <p class="normal">This chapter explored preference alignment techniques for improving LLMs. It introduced the concept of preference datasets, explaining their structure and importance in capturing nuanced human preferences. We implemented our own custom preference data generation pipeline by comparing original and AI-generated text from real articles. This pipeline can be reused and customized based on your use case.</p>
    <p class="normal">We also provided an overview of the evolution of RLHF, leading to the introduction of DPO as a simpler and more efficient alternative. Finally, we implemented DPO using the Unsloth library to fine-tune our TwinLlama-3.1-8B model from <em class="chapterRef">Chapter 5</em>. Our step-by-step tutorial gave practical instructions for training the model, as well as highlighting key differences from SFT. The final model is available on the Hugging Face Hub.</p>
    <p class="normal">In the next chapter, we will explore the crucial topic of LLM evaluation, addressing the challenges and current approaches in assessing LLM performance. We’ll cover the creation of domain-specific evaluation sets, examine why evaluation remains a persistent problem in the field, and introduce the concept of using larger models to evaluate smaller ones (LLM-as-a-judge). The chapter will conclude with a comprehensive evaluation pipeline, providing a structured framework for consistent and effective LLM evaluation.</p>
    <h1 id="_idParaDest-182" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">Rafael Rafailov et al.. “<em class="italic">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</em>.” arXiv preprint arXiv:2305.18290, May 2023.</li>
      <li class="bulletList">Timo Kaufmann et al.. “<em class="italic">A Survey of Reinforcement Learning from Human Feedback</em>.” arXiv preprint arXiv:2312.14925, December 2023.</li>
      <li class="bulletList">Anthropic. “<em class="italic">GitHub - anthropics/hh-rlhf: Human preference data for “Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</em>”.” github.com, 2022, <a href="https://github.com/anthropics/hh-rlhf"><span class="url">https://github.com/anthropics/hh-rlhf</span></a>.</li>
      <li class="bulletList">Nisan Stiennon et al.. “<em class="italic">Learning to summarize from human feedback</em>.” arXiv preprint arXiv:2009.01325, September 2020.</li>
      <li class="bulletList">Intel(R) Neural Compressor. “Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2.” medium.com, March 26, 2024, <a href="https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3"><span class="url">https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3</span></a>.</li>
      <li class="bulletList">Argilla. “<em class="italic">GitHub - argilla-io/distilabel</em>.” <a href="https://github.com"><span class="url">github.com</span></a>, August 23, 2024, <a href="https://github.com/argilla-io/distilabel"><span class="url">https://github.com/argilla-io/distilabel</span></a>.</li>
      <li class="bulletList">Databricks. “<em class="italic">Enhancing LLM-as-a-Judge with Grading Notes</em>.” databricks.com, July 22, 2024, <a href="https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes"><span class="url">https://www.databricks.com/blog/enhancing-llm-as-a-judge-with-grading-notes</span></a>.</li>
      <li class="bulletList">Akrour, Riad &amp; Schoenauer, Marc &amp; Sebag, Michèle. (2011). Preference-Based Policy Learning. 12-27. 10.1007/978-3-642-23780-5_11.</li>
      <li class="bulletList">Cheng, Weiwei &amp; Fürnkranz, Johannes &amp; Hüllermeier, Eyke &amp; Park, Sang-Hyeun. (2011). <em class="italic">Preference-Based Policy Iteration: Leveraging Preference Learning for Reinforcement Learning</em>. 312-327. 10.1007/978-3-642-23780-5_30.</li>
      <li class="bulletList">Paul Christiano et al.. “<em class="italic">Deep reinforcement learning from human preferences</em>.” arXiv preprint arXiv:1706.03741, June 2017.</li>
      <li class="bulletList">Long Ouyang et al.. “<em class="italic">Training language models to follow instructions with human feedback</em>.” arXiv preprint arXiv:2203.02155, March 2022.</li>
      <li class="bulletList">John Schulman et al.. “<em class="italic">Proximal Policy Optimization Algorithms</em>.” arXiv preprint arXiv:1707.06347, July 2017.</li>
      <li class="bulletList">unslothai. “<em class="italic">GitHub - unslothai/unsloth: Finetune Llama 3.1, Mistral</em>, Phi &amp; Gemma LLMs 2-5x faster with 80% less memory.” github.com, August 21, 2024, <a href="https://github.com/unslothai/unsloth"><span class="url">https://github.com/unslothai/unsloth</span></a>.</li>
    </ul>
    <h1 id="_idParaDest-183" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/llmeng"><span class="url">https://packt.link/llmeng</span></a></p>
    <p class="normal"><span class="url"><img src="../Images/QR_Code79969828252392890.png" alt=""/></span></p>
  </div>
</body></html>