["```py\n vectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=OpenAIEmbeddings())\n```", "```py\n retriever = vectorstore.as_retriever()\n```", "```py\n question = \"What are the advantages of using RAG?\" question_embedding=embedding_function.embed_query(question)first_5_numbers = question_embedding[:5]\nprint(f\"User question embedding (first 5 dimensions):\n    {first_5_numbers}\")\n```", "```py\n User question embedding (first 5 dim): [\n-0.006319054113595048, -0.0023517232115089787, 0.015498643243434815, -0.02267445873596028, 0.017820641897159206]\n```", "```py\n embedding_size = len(question_embedding)\nprint(f\"Embedding size: {embedding_size}\")\n```", "```py\n Embedding size: 1536\n```", "```py\n rag_chain_with_source = RunnableParallel(\n    {\"context\": <st c=\"14325\">retriever</st>,\n     \"question\":RunnablePassthrough()}\n).assign(answer=rag_chain_from_docs)\n```", "```py\n loader = WebBaseLoader(\n    web_paths=(\"https://kbourne.github.io/chapter1.html\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\",\n                    \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n```", "```py\n text_splitter = SemanticChunker(embedding_function)\nsplits = text_splitter.split_documents(docs)\n```", "```py\n There are also generative models that generate images from text prompts, while others generate video from text prompts. There are other models that generate text descriptions from images. We will talk about these other types of models in Chapter 16, Going Beyond the LLM. But for most of the book, I felt it would keep things simple and let you focus on the core principles of RAG if we focus on the type of model that most RAG pipelines use, the LLM. But I did want to make sure it was clear, that while the book focuses primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images and videos. Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta LLaMA models, Google's PaLM and Gemini models, and Anthropic's Claude models. Foundation model\\nA foundation model is the base model for most LLMs. In the case of ChatGPT, the foundation model is based on the GPT (Generative Pre-trained Transformer) architecture, and it was fine-tuned for Chat. The specific model used for ChatGPT is not publicly disclosed. The base GPT model cannot talk with you in chatbot-style like ChatGPT does. It had to get further trained to gain that skill.\n```", "```py\n %pip install gensim --user\n%pip install transformers\n%pip install torch\n```", "```py\n from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ntfidf_documents = [split.page_content for split in splits]\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(\n    tfidf_documents)\nvocab = tfidf_vectorizer.get_feature_names_out()\ntf_values = tfidf_matrix.toarray()\nidf_values = tfidf_vectorizer.idf_\nword_stats = list(zip(vocab, tf_values.sum(axis=0),\n    idf_values))\nword_stats.sort(key=lambda x: x[2], reverse=True)\nprint(\"Word\\t\\tTF\\t\\tIDF\")\nprint(\"----\\t\\t--\\t\\t---\")\nfor word, tf, idf in word_stats[:10]:\n         print(f\"{word:<12}\\t{tf:.2f}\\t\\t{idf:.2f}\")\n```", "```py\n Word                     TF    IDF\n000                      0.16  2.95\n1024                     0.04  2.95\n123                      0.02  2.95\n13                       0.04  2.95\n15                       0.01  2.95\n16                       0.07  2.95\n192                      0.06  2.95\n1m                       0.08  2.95\n200                      0.08  2.95\n2024                     0.01  2.95\n```", "```py\n tfidf_user_query = [\"What are the advantages of RAG?\"]\nnew_tfidf_matrix = tfidf_vectorizer.transform(\n    tfidf_user_query)\ntfidf_similarity_scores = cosine_similarity(\n    new_tfidf_matrix, tfidf_matrix)\ntfidf_top_doc_index = tfidf_similarity_scores.argmax()\nprint(\"TF-IDF Top Document:\\n\",\n    tfidf_documents[tfidf_top_doc_index])\n```", "```py\n TF-IDF Top Document:\nCan you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? You do not have to imagine it, that is what RAG does…[TRUNCATED FOR BREVITY]\n```", "```py\n Retrieved Document:\nCan you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? You do not have to imagine it, that is what RAG does…[TRUNCATED FOR BREVITY]\n```", "```py\n from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sklearn.metrics.pairwise import cosine_similarity\ndoc2vec_documents = [\n    split.page_content for split in splits]\ndoc2vec_tokenized_documents = [\n    doc.lower().split() for doc in doc2vec_documents]\ndoc2vec_tagged_documents = [TaggedDocument(words=doc,\n    tags=[str(i)]) for i, doc in enumerate(\n    doc2vec_tokenized_documents)]\ndoc2vec_model = Doc2Vec(doc2vec_tagged_documents,\n    vector_size=100, window=5, min_count=1, workers=4)\ndoc2vec_model.save(\"doc2vec_model.bin\")\n```", "```py\n loaded_doc2vec_model = Doc2Vec.load(\"doc2vec_model.bin\")\ndoc2vec_document_vectors = [loaded_doc2vec_model.dv[\n    str(i)] for i in range(len(doc2vec_documents))]\ndoc2vec_user_query = [\"What are the advantages of RAG?\"]\ndoc2vec_tokenized_user_query = [content.lower().split() for content in doc2vec_user_query]\ndoc2vec_user_query_vector = loaded_doc2vec_model.infer_vector(\n    doc2vec_tokenized_user_query[0])\ndoc2vec_similarity_scores = cosine_similarity([\n    doc2vec_user_query_vector], doc2vec_document_vectors)\ndoc2vec_top_doc_index = doc2vec_similarity_scores.argmax()\nprint(\"\\nDoc2Vec Top Document:\\n\",\n    doc2vec_documents[doc2vec_top_doc_index])\n```", "```py\n Doc2Vec Top Document:\nOnce you have introduced the new knowledge, it will always have it! It is also how the model was originally created, by training with data, right? That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall…[TRUNCATED FOR BREVITY]\n```", "```py\n doc2vec_model = Doc2Vec(doc2vec_tagged_documents,\n    vector_size=100, window=5, min_count=1, workers=4)\n```", "```py\n doc2vec_model = Doc2Vec(doc2vec_tagged_documents,\n    vector_size=1536, window=5, min_count=1, workers=4)\n```", "```py\n Doc2Vec Top Document:\nCan you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? You do not have to imagine it, that is what RAG does…[TRUNCATED FOR BREVITY]\n```", "```py\n from transformers import BertTokenizer, BertModel\nimport torch\nfrom sklearn.metrics.pairwise import cosine_similarity\nbert_documents = [split.page_content for split in splits]\nbert_tokenizer = BertTokenizer.from_pretrained(\n    'bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\nbert_vector_size = bert_model.config.hidden_size\nprint(f\"Vector size of BERT (base-uncased) embeddings:\n    {bert_vector_size}\\n\")\nbert_tokenized_documents = [bert_tokenizer(doc,\n    return_tensors='pt', max_length=512, truncation=True)\n    for doc in bert_documents]\nbert_document_embeddings = []\nwith torch.no_grad():\n    for doc in bert_tokenized_documents:\n        bert_outputs = bert_model(**doc)\n        bert_doc_embedding =\n            bert_outputs.last_hidden_state[0, 0, :].numpy()\n        bert_document_embeddings.append(bert_doc_embedding)\nbert_user_query = [\"What are the advantages of RAG?\"]\nbert_tokenized_user_query = bert_tokenizer(\n    bert_user_query[0], return_tensors='pt',\n    max_length=512, truncation=True)\nbert_user_query_embedding = []\nwith torch.no_grad():\n    bert_outputs = bert_model(\n        **bert_tokenized_user_query)\n         bert_user_query_embedding =\n             bert_outputs.last_hidden_state[\n                 0, 0, :].numpy()\nbert_similarity_scores = cosine_similarity([\n    bert_user_query_embedding], bert_document_embeddings)\nbert_top_doc_index = bert_similarity_scores.argmax()\nprint(\"BERT Top Document:\\n\", bert_documents[\n    bert_top_doc_index])\n```", "```py\n Vector size of BERT (base-uncased) embeddings: 768\nBERT Top Document:\nOr if you are developing in a legal field, you may want it to sound more like a lawyer. Vector Store or Vector Database?\n```", "```py\n tokenizer = BertTokenizer.from_pretrained(\n    'bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\n```"]