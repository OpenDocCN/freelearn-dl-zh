- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Future of Generative AI – Trends and Emerging Use Cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have reached the final chapter of this book on building generative AI solutions
    in the cloud. In this chapter, we would like you to get a sense of the future
    and where things are going by delving into the transformative possibilities and
    emerging trends that will shape the landscape of generative AI technologies such
    as ChatGPT. This chapter is not just a summary of what we’ve learned but a forward-looking
    exploration into the evolving world of cloud-based AI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by talking about the evolution of multimodal interactions. Here,
    we explore how integrating various communication methods through text, images,
    audio, and video is revolutionizing user interaction with AI. This is vital for
    those seeking to innovate in AI user interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts with *Emerging trends and industry-specific generative AI
    apps*, drawing inspiration from industry leaders. This segment reveals the versatile
    applications of generative AI across different sectors.
  prefs: []
  type: TYPE_NORMAL
- en: Next, in the *Integrating generative AI with intelligent edge devices* section,
    we’ll discuss the fusion of ChatGPT and generative AI with smart technologies.
    This part is crucial for integrating AI into hardware and intelligent systems,
    particularly with the **Internet of** **Things** (**IoT**).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, *From quantum computing to AGI – charting ChatGPT’s future trajectory*
    offers a speculative glimpse into how emerging technologies could dramatically
    evolve ChatGPT’s capabilities, inching closer to **artificial general** **intelligence**
    (**AGI**).
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be equipped with a comprehensive understanding
    of the current trends and potential future directions of generative AI, along
    with the knowledge and inspiration to innovate and implement cutting-edge AI solutions
    in the cloud. This chapter provides a vision of the future of AI, empowering you
    to lead in the AI revolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The era of multimodal interactions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Industry-specific generative AI apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rise of SLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emerging trends and 2024-25 predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating ChatGPT with intelligent edge devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From quantum computing to AGI – charting ChatGPT’s future trajectory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Comic depiction of the future of generative AI](img/B21443_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Comic depiction of the future of generative AI
  prefs: []
  type: TYPE_NORMAL
- en: The era of multimodal interactions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multimodal interaction in **large language models** (**LLMs**) refers to the
    ability of these models to understand “input prompts” and generate content as
    “output completions” in multiple modalities, typically combining text with other
    forms of data, such as images, audio, or even video. It’s the capacity to process
    and generate information using different sensory channels.
  prefs: []
  type: TYPE_NORMAL
- en: We already know that LLMs such as GPT-4 perform well with text input and outputs.
    Renowned LLMs such as GPT-4 have already demonstrated exceptional proficiency
    with textual inputs and outputs. The recent surge in advanced image generation
    models, including DALL-E 3 and Midjourney, further illustrates this progress.
    The next significant leap in generative AI applications is anticipated to incorporate
    groundbreaking capabilities, extending to text-to-video and image-to-video conversions,
    thus broadening the horizons of AI’s creative and functional potential.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the benefits and use cases of multimodal LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost-effective production**: Producing videos traditionally can be expensive
    and time-consuming. LMMs with text-to-video technology can offer a more cost-effective
    alternative, particularly for small businesses or individuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced understanding and interaction**: By incorporating multiple modalities,
    these models better understand and interpret the context and nuances of real-world
    scenarios. This leads to more accurate and contextually relevant responses, particularly
    in complex interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Richer content generation/creative storytelling**: Multimodal LLMs can create
    more comprehensive and detailed content. For instance, they can generate descriptive
    narratives for images or videos, or even create visual content from textual descriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved accessibility**: They can be instrumental in making technology more
    accessible. For example, converting text into speech or vice versa can help individuals
    with visual or auditory impairments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better data analysis**: Multimodal LLMs can analyze data from various sources
    simultaneously, offering more nuanced insights. This is particularly useful in
    fields such as market research, media analysis, and scientific research, where
    data comes in various formats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced learning and training tools**: In educational contexts, these models
    can provide a more interactive and engaging learning experience by incorporating
    various media types, making learning more dynamic and effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Innovative applications in creative industries**: In creative fields such
    as art, music, and film, multimodal LLMs can assist in the creative process by
    offering new ways to generate and modify content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced customer experience**: In customer service, they can interact in
    a more human-like manner, understanding queries better and providing more relevant
    information, sometimes even using visual aids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language and cultural adaptation**: This technology can include features
    such as subtitles or dubbing in different languages, making content accessible
    to a wider, multilingual audience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalization**: They can tailor experiences and content to individual
    users by understanding and integrating cues from various data types, leading to
    more personalized interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support for content creators**: For bloggers, educators, or marketers, this
    technology provides a simple way to diversify content formats, enhancing their
    digital presence and engagement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 Turbo Vision and beyond – a closer look at this LMM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GPT-4 Turbo with Vision** (**GPT-4V**), released by OpenAI in late 2023,
    is a new version of the LLM that supports 128,000 tokens of context (~300 pages
    of text as input prompts), is cheaper, has updated knowledge and image capabilities,
    provides text-to-speech offerings, and has a copyright shield. It can also understand
    images as inputs and generate captions and descriptions, all while providing intricate
    analyses of them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-4V is an improvement over GPT-V4 in terms of its broader general knowledge
    and advanced reasoning capabilities. The following figure from the research paper
    *The Dawn of the LMMs: Preliminary Explorations with GPT-4V(ision)* demonstrates
    the remarkable reasoning capabilities of GPT-4V with different prompting techniques
    (*The Dawn of LMMs: Preliminary Explorations with* *GPT-4*, [https://export.arxiv.org/pdf/2309.17421](https://export.arxiv.org/pdf/2309.17421)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Demonstration of GPT-4V following text instructions](img/B21443_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Demonstration of GPT-4V following text instructions
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Demonstration of GPT-4V with visual referring prompting](img/B21443_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Demonstration of GPT-4V with visual referring prompting
  prefs: []
  type: TYPE_NORMAL
- en: 'It also possesses multilingual multimodal understanding so that it can understand
    text in different languages in images and answer your questions in English or
    a language of your choice, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – GPT-4V’s capabilities regarding multilingual scene text recognition](img/B21443_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – GPT-4V’s capabilities regarding multilingual scene text recognition
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – GPT-4V’s capabilities regarding multimodal multicultural understanding](img/B21443_10_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – GPT-4V’s capabilities regarding multimodal multicultural understanding
  prefs: []
  type: TYPE_NORMAL
- en: Video prompts for video understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A novel feature not present in earlier GPT models is the capability to comprehend
    videos. With video prompting, you can prompt the LLM with not only text but also
    video. GPT-4V can analyze brief video clips and produce comprehensive descriptions.
    Though GPT-4V doesn’t directly process video inputs, the Azure Open AI Chat playground,
    enhanced with GPT-4V and Azure Vision services, allows for interactive questioning
    of video content. This system operates by identifying key frames from the video
    that are relevant to your query. It then examines these frames in detail to generate
    a response. This integration bridges the gap between video content and AI-driven
    insights. For example, you can upload a short video of a boy playing football
    on Azure Open AI Chat playground and simultaneously state, “Give me a summary
    of the video and what sport is being played in the video.”
  prefs: []
  type: TYPE_NORMAL
- en: The frames are examined by GPT-4V seamlessly due to its varying capabilities,
    such as temporal ordering, temporal anticipation, and temporal localization and
    reasoning. Let’s dig into these concepts in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal ordering** means being able to put things in the right order based
    on time. For GPT-4V, this skill is really important. It’s like if you mixed up
    a bunch of photos from an event, say making sushi, and then asked the AI to put
    them back in the right order. GPT-4V can look at these shuffled pictures and figure
    out the correct sequence, showing how the sushi was made step by step. There are
    two types of temporal ordering: long-term and short-term. Long-term is like the
    sushi example, where the AI organizes a series of events over a longer period.
    Short-term is more about quick actions, such as opening or closing a door. GPT-4V
    can understand these actions and put them in the right order too. These tests
    are a way to check if GPT-4V understands how things happen over time, both for
    long processes and quick actions. It’s like testing if the AI can make sense of
    a story or an event just by looking at pictures, even if they’re all mixed up
    at first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Long-term temporal ordering: “GPT-4V” is shown a series of
    mixed-up images that show the process of making sushi. Despite the images being
    out of order, GPT-4V successfully recognizes the event and arranges the images
    in the proper chronological sequence (2309.17421 (arxiv.org))](img/B21443_10_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6 – Long-term temporal ordering: “GPT-4V” is shown a series of mixed-up
    images that show the process of making sushi. Despite the images being out of
    order, GPT-4V successfully recognizes the event and arranges the images in the
    proper chronological sequence (2309.17421 (arxiv.org))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Short-term temporal ordering: when presented with a specific
    action, such as opening or closing a door, GPT-4V proves its ability to understand
    the content of the images and accurately arrange them in the right sequence that
    matches the given action](img/B21443_10_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7 – Short-term temporal ordering: when presented with a specific
    action, such as opening or closing a door, GPT-4V proves its ability to understand
    the content of the images and accurately arrange them in the right sequence that
    matches the given action'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal anticipation** is where GPT-4V predicts future events from the beginning
    frames of an action. It does this for both short-term and long-term events. For
    example, with a soccer penalty kick, GPT-4V can guess the next moves of the kicker
    and goalkeeper by understanding the game’s rules. Similarly, in sushi making,
    it predicts the next steps in the process by recognizing the current stage and
    the overall procedure. This ability lets GPT-4V understand and predict actions
    that happen over different lengths of time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Long-term temporal anticipation: GPT-4V can predict the next
    moves based on the initial frames](img/B21443_10_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8 – Long-term temporal anticipation: GPT-4V can predict the next
    moves based on the initial frames'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal localization and reasoning** refer to GPT-4V’s skill in pinpointing
    specific moments in time and making logical connections. An example is its ability
    to identify the exact moment a soccer player hits the ball. Moreover, GPT-4V can
    understand cause and effect relationships, such as figuring out whether a goalkeeper
    will successfully stop the ball. This involves not just seeing where the goalkeeper
    and ball are, but also understanding how they interact and predicting what will
    happen next. This shows a high level of complex reasoning in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Temporal localization and reasoning: GPT-4V exhibits its skill
    in temporal localization by precisely pinpointing the moment the player hits the
    ball. Additionally, it showcases its understanding of cause and effect by assessing
    if the ball was stopped and analyzing the interaction between the goalkeeper and
    the ball](img/B21443_10_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9 – Temporal localization and reasoning: GPT-4V exhibits its skill
    in temporal localization by precisely pinpointing the moment the player hits the
    ball. Additionally, it showcases its understanding of cause and effect by assessing
    if the ball was stopped and analyzing the interaction between the goalkeeper and
    the ball'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4V limitations (as of Jan 2024)
  prefs: []
  type: TYPE_NORMAL
- en: 'Although GPT-4V is very intelligent compared to its predecessors, we must be
    aware of its limitations when leveraging it in applications. These limitations
    are mentioned on the OpenAI website ([https://platform.openai.com/docs/guides/vision](https://platform.openai.com/docs/guides/vision)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Medical diagnostics**: It’s not equipped to interpret specialized medical
    imagery such as CT scans and is not a source for medical guidance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-Latin scripts**: Performance may falter with image texts in non-Latin
    scripts such as Japanese or Korean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text size**: Amplifying text size can enhance readability, but important
    parts of the image should not be excluded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orientation**: Misinterpretation is possible with rotated or upside-down
    text and images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex visuals**: The model might struggle with graphs or texts where there
    are variations in color or line styles (solid, dashed, dotted, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial analysis**: The model has limitations in tasks that require precise
    spatial understanding, such as identifying chessboard positions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: In certain contexts, it might generate incorrect image descriptions
    or captions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unusual image formats**: Challenges arise with panoramic and fisheye photographs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata and image resizing**: Original filenames and metadata are not processed,
    and images undergo resizing which alters their original dimensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object counting**: The model may only provide approximate counts of items
    in an image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CAPTCHAs**: Due to safety measures, CAPTCHA submissions are blocked'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving past GPT-4V’s limitations, we expect future models, such as GPT-5, to
    offer better features for interaction and smarter reasoning, leading to more creative
    and useful applications. Anticipated improvements include a deeper understanding
    of language and context, advanced multimodal capabilities for interacting with
    various types of content, and enhanced reasoning for complex problem-solving.
    Furthermore, GPT-5 is likely to offer more precise customization options, demonstrate
    a significant reduction in biases for more ethical responses, and possess an expanded
    knowledge base that remains current with the latest information, ensuring more
    accurate and relevant outputs across a wide array of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Video generation models – a far-fetched dream?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first wave of generative AI marked remarkable advancements in text-to-text
    and text-to-image models, bringing photorealistic images to the forefront. Models
    such as DALL-E have continually enhanced their capabilities, producing increasingly
    lifelike images. The next leap forward, anticipated in the near future, lies in
    video generation models that include text-to-video, image-to-video, and audio-to-video,
    a progression hinted at in 2023\. The text-to-video conversion process faces significant
    challenges, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Computational demands for ensuring spatial and temporal frame consistency. Hence,
    training such models becomes unaffordable for most researchers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lack of quality in multi-modal datasets for training the models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complexity of effectively describing videos for the models to learn. This
    often requires a series of detailed prompts or narratives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although there have been some limitations with these models, we have seen some
    continual progress in video generation techniques such as GANs, Variational Auto
    Encoders, Transformers, and Stable Diffusion. Some popular video generation models
    have been released by organizations such as Runway ML, Stable Video Diffusion
    by Stability AI, Moonshot by Salesforce, and Google’s VideoPoet.
  prefs: []
  type: TYPE_NORMAL
- en: SORA, from OpenAI, is the most recent one with complex scene generation and
    advanced language comprehension capabilities. We provided more details on this
    model in [*Chapter 1*](B21443_01.xhtml#_idTextAnchor015).
  prefs: []
  type: TYPE_NORMAL
- en: Video generation models possess profound capabilities, with the potential to
    influence society, especially as they evolve and mature. This influence becomes
    particularly critical during election seasons, where the information landscape
    can shape public opinion and democratic outcomes significantly. However, this
    power also carries the risk of severe consequences if not implemented responsibly.
    Consequently, it’s imperative to establish robust ethical guidelines and safeguards,
    especially during sensitive periods such as elections, to ensure that these technologies
    are used in a manner that is beneficial and does not undermine the integrity of
    democratic processes.
  prefs: []
  type: TYPE_NORMAL
- en: Can AI smell?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have learned that AI can hear, see, and speak. But can AI smell too? Recent
    research in the field of AI has shown significant progress in AI’s ability to
    “smell.” Various studies have explored how AI can analyze and interpret odors,
    a task that’s traditionally been challenging due to the complexity and subjective
    nature of olfaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AI model outperforms humans in describing odors**: A study demonstrated that
    an AI model was more accurate than human panelists in predicting the smell of
    different molecules. The model was particularly effective at identifying pairs
    of structurally dissimilar molecules that had similar smells, as well as characterizing
    a variety of odor properties, such as odor strength, for a large number of potential
    scent molecules. [https://techxplore.com/news/2023-08-closer-digitizing-odors-human-panelists.html](https://techxplore.com/news/2023-08-closer-digitizing-odors-human-panelists.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI in detecting illnesses through breath analysis**: Laboratories have been
    using machines such as **gas-chromatography mass-spectrometers** (**GC-MSs**)
    to detect substances in the air, including volatile organic compounds present
    in human breath. These compounds can indicate various illnesses, including cancers.
    AI, particularly deep learning networks, is being adapted to analyze these compounds
    more efficiently, significantly speeding up the process of identifying specific
    patterns in breath samples that indicate certain diseases. [https://www.smithsonianmag.com/innovation/artificial-intelligence-may-be-able-to-smell-illnesses-in-human-breath-180969286/](https://www.smithsonianmag.com/innovation/artificial-intelligence-may-be-able-to-smell-illnesses-in-human-breath-180969286/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artificial networks learning to smell like the brain**: Research at MIT has
    involved building an artificial smell network inspired by the fruit fly’s olfactory
    system. This network, comprising an input layer, a compression layer, and an expansion
    layer, mirrors the structure of the fruit fly’s olfactory system. The network
    was able to organize itself and process odor information in a manner strikingly
    similar to the fruit fly brain, demonstrating AI’s potential to mimic biological
    olfactory systems. [https://news.mit.edu/2021/artificial-networks-learn-smell-like-the-brain-1018](https://news.mit.edu/2021/artificial-networks-learn-smell-like-the-brain-1018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI “nose” predicts smells from molecular structures**: AI technology has
    been developed to predict the smell of chemicals based on their molecular structures.
    This advancement is significant as it opens up the possibility of designing new
    synthetic scents and provides insights into how the human brain interprets smell.
    [https://phys.org/news/2023-09-ai-nose-molecular.html](https://phys.org/news/2023-09-ai-nose-molecular.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training AI to understand and map odors**: Researchers have trained a neural
    network with thousands of compounds and corresponding smell labels from perfumery
    databases. The AI was able to create a “principal odor map” that visually shows
    the relationships between different smells. When tested, the AI’s predictions
    of how a new molecule would smell were found to be more accurate than those of
    human panelists. [https://www.popsci.com/science/teach-ai-how-to-smell/](https://www.popsci.com/science/teach-ai-how-to-smell/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section primarily focused on multimodal capabilities and how they will
    enhance our communication with AI as these capabilities mature. In the next section,
    we will discuss how these multimodal capabilities can foster creativity and innovation
    within industry-specific, generative AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Industry-specific generative AI apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can anticipate a sustained surge in sector-specific generative AI applications,
    heralding a wave of remarkable advancements and innovations across industries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AI in art, music, and cinema**: Generative AI is revolutionizing the realms
    of music, art, movies, and literature by fostering innovative creation, personalized
    experiences, and broader accessibility. In music, the maturity of audio generation
    models is transforming composition, production, and performance, offering tailored
    listening experiences and enabling new forms of interactive and virtual performances.
    In art, AI is a collaborator in generating unique visual works through image generation
    models. In literature, AI aids in writing, editing, and exploring new narrative
    forms, while also making literary works more accessible through advanced translation
    and localization. This integration of AI into creative domains is not just reshaping
    existing paradigms but is also unlocking unprecedented avenues for creative expression
    and cultural exchange.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI in finance**: Generative AI is set to revolutionize the finance sector
    by enabling highly personalized services, automating trading and investment strategies,
    enhancing risk management, and improving fraud detection. Its advanced analytics
    will streamline regulatory compliance and revolutionize customer service through
    intelligent chatbots. An example is BloombergGPT, a 50 billion parameter LLM built
    ground-up just for finance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI in education**: Generative AI, particularly through the rise of multimodal
    LLMs, is substantially enhancing the education landscape by creating highly customized
    and interactive learning experiences. These advanced AI models are adept at generating
    dynamic educational content, providing personalized tutoring, and adapting to
    individual learning styles and needs. For instance, platforms such as Khan Academy
    are at the forefront of this transformative wave, as evidenced by their Khanmigo
    App, which leverages generative AI to offer tailored educational experiences.
    This integration of multimodal LLMs and their advanced reasoning capabilities
    in education is not only automating administrative tasks and optimizing curriculum
    development but is also pioneering a more engaging, inclusive, and student-focused
    approach to learning, promising a future where education is deeply personalized,
    interactive, and accessible to all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI in scientific research and innovation**: Generative AI will continue to
    revolutionize scientific research and innovation by accelerating drug discovery,
    enhancing genomic analysis, and improving the precision of experiments across
    various disciplines. Its powerful data analysis and pattern recognition capabilities
    are unlocking new insights in complex fields such as astrophysics and climate
    science, while predictive modeling aids in designing sustainable systems. By automating
    routine tasks and fostering interdisciplinary collaboration, Gen AI is significantly
    enhancing efficiency and creativity in scientific endeavors, heralding a new era
    of accelerated discovery and advanced innovation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI in communication/translation**: Advancements in audio generation will
    facilitate real-time, accurate translation and enable seamless communication across
    different languages and cultures. This will also give rise to AI avatars that
    will be able to understand and talk to you in different languages and will be
    an integral part of consumer applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI in gaming**: Generative AI will be able to create more dynamic, immersive
    environments and enhance **non-player character** (**NPC**) behavior, leading
    to more engaging and unpredictable gameplay. It personalizes experiences by adapting
    to individual player actions and preferences and introduces advanced technologies
    such as voice and facial recognition for more intuitive interactions. Additionally,
    AI will continue to streamline game development, enforcing fair play through cheating
    detection, and making gaming more accessible and globally connected through assistive
    features and real-time translation. These advancements will not only elevate the
    player experience but also transform how games are designed and developed, signaling
    a new era in the gaming world where each interaction is more interactive, inclusive,
    and personalized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI in healthcare and medical research**: Generative AI will continue to revolutionize
    healthcare by personalizing medicine, enhancing diagnostic accuracy, and accelerating
    drug discovery, leading to more effective and targeted treatments. It leverages
    predictive analytics for proactive healthcare management and assists in precise,
    robot-assisted surgeries. AI-powered medical copilots, virtual health assistants,
    and wearable devices provide continuous patient monitoring and support, while
    also democratizing access to healthcare services. Furthermore, AI enhances medical
    training by simulating realistic clinical scenarios, preparing professionals for
    various situations. These advancements signify a transformative shift in healthcare
    toward a future where treatments are not only more personalized and precise but
    also more accessible and preventive, fundamentally improving patient outcomes
    and healthcare efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BioGPT
  prefs: []
  type: TYPE_NORMAL
- en: BioGPT, a tailored language model, is meticulously pre-trained on biomedical
    literature, equipping it with a profound comprehension of medical and biological
    concepts and terminology. Its purpose is to support a variety of biomedical NLP
    tasks, including answering medical queries and summarizing research articles,
    by offering precise, contextually relevant insights. The field is poised for further
    innovation, with specialized LLMs such as BioGPT simplifying the intricacies of
    medical research.
  prefs: []
  type: TYPE_NORMAL
- en: '**AI in consumer applications**: Generative AI will continue to revolutionize
    consumer applications by offering highly personalized and intuitive experiences
    across various domains. It will power personalized shopping recommendations, smart
    home automation, and customized entertainment content, enhancing user engagement
    and convenience. AI-driven chatbots improve customer service, while interactive
    gaming and personalized health and fitness apps cater to individual preferences
    and lifestyles. Moreover, AI facilitates seamless language translation and enables
    businesses to analyze consumer data for targeted marketing and product development.
    This transformative technology will continue to reimagine consumer interactions,
    making them more engaging, efficient, and tailored to individual needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we delved into a select few of the countless industries on
    the brink of transformation due to the emergence of generative AI. Although this
    only scratches the surface of potential applications, the influence of generative
    AI is unmistakably substantial and holds the promise of ushering in an era of
    significant evolution and innovation across various sectors. Nonetheless, it’s
    crucial to acknowledge and address the apprehensions surrounding job displacement
    attributed to AI advancements. The writers’ strike of 2023 serves as a notable
    example, highlighting the growing concern among professionals about AI potentially
    encroaching on their roles. (*TV’s war with a robot is already here*: [https://tinyurl.com/yvdw5h3y](https://tinyurl.com/yvdw5h3y)).
    It’s imperative for society to engage in thoughtful discourse on these ethical
    dilemmas and to establish robust frameworks that strike a harmonious balance between
    fostering innovation and mitigating the impact on employment.'
  prefs: []
  type: TYPE_NORMAL
- en: The rise of small language models (SLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the popularity of LLMs, we have seen a rise in SLMs. Researchers began
    exploring SLMs as a response to the challenges posed by their larger counterparts.
    While large models offer impressive performance, they also bring substantial demands
    in terms of computational resources, energy consumption, and data requirements.
    These factors limit accessibility and practicality, especially for individuals
    and organizations with constrained resources.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of SLMs is fundamentally similar to that of LLMs, with both
    based on the transformer architecture (for example, Llama). The differences mainly
    lie in the scale and some specific optimizations tailored to their respective
    use cases. Language models in the range of millions and the order of 10 billion
    parameters or less are considered to be SLMs. They are streamlined versions of
    language models that are designed to deliver a balance between performance and
    efficiency. Unlike their larger counterparts, SLMs require significantly less
    computational power and data to train and run, making them more accessible, lower
    cost to build, and environmentally friendly.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of SLMs include Tiny Llama (1.1 B parameters), Llama 2 (7 B parameters),
    Orca-2 (7B, 13B parameters) and Phi-2 (2.7B parameters), Mistral (7B parameters),
    and Falcon-7B, and each offers a unique trade-off between size, speed, and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Phi-2, an open source model developed by Microsoft, trained in textbook quality
    data, sets a new standard in performance efficiency, outshining models tenfold
    its size across a range of popular benchmarks. This model showcases greater proficiency
    in areas such as commonsense reasoning, language understanding, mathematical problem-solving,
    and coding!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the benefits of SLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficiency**: SLMs, with their fewer parameters, offer notable computational
    advantages over larger models such as GPT-3\. They provide quicker inference speeds,
    demand less memory and storage, and use smaller datasets for training compared
    to larger models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tunable**: SLMs can be easily tailored to specific domains and specialized
    uses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy access**: Since they are often open source, they democratize access
    to advanced NLP capabilities, allowing a broader range of users and developers
    to incorporate sophisticated language understanding into their applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment on the edge**: Additionally, the reduced resource requirements
    of SLMs make them ideal for deployment in edge computing scenarios – offline mode
    and on devices with limited processing capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, their lower energy consumption contributes to a more sustainable AI
    ecosystem, addressing some of the environmental concerns associated with larger
    models.
  prefs: []
  type: TYPE_NORMAL
- en: While SLMs are gaining traction, some are not yet fully developed for production
    use. However, we expect continued enhancements in their efficiency and readiness
    for deployment. Furthermore, SLMs are set to become a core component in edge devices
    such as smartphones and other cutting-edge gadgets. This trend presents an exciting
    segue into the next section, where we’ll delve into the opportunities this technology
    brings to edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating generative AI with intelligent edge devices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we progress into 2024, the fusion of generative AI with intelligent edge
    devices is poised to revolutionize the technology landscape. Examples of edge
    devices include smartphones, tablets, autonomous vehicles, medical devices, wearable
    devices, and IoT devices such as smart thermostats, cameras, and more. SLMs are
    becoming a pivotal component of edge computing, offering a new dimension of smart,
    localized processing. This is because we face challenges with LLMs when they’re
    integrated on edge devices. LLMs need to be optimized before deploying edge devices
    for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limited resources**: Edge devices typically have constrained computational
    resources, including CPU, GPU, memory, and storage. Large models require substantial
    resources for both storage (>500 GB) and computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Energy efficiency**: Running large models can consume significant power,
    which is critical for battery-operated devices. Optimizations aim to reduce the
    energy consumption of these models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency**: For real-time applications, it’s crucial to have low latency.
    Large models can lead to slower inference times, so optimizing the model can help
    meet the latency requirements of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bandwidth**: Deploying large models or updating them over the network can
    consume significant bandwidth, which might be limited or costly in some edge environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: Computational resources on edge devices are not only limited but
    also potentially more expensive. Optimizing models can reduce the overall cost
    of deployment and operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different techniques to achieve this kind of efficiency in LLMs. One
    method, known as “knowledge distillation” or “domain reduction,” trains a smaller
    model to emulate a larger one using less data. Another method, “quantization,”
    shrinks the model size and boosts performance by decreasing the precision of its
    weights and activations, while still maintaining accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: A device named Rabbit R1, which was announced at CES this year, a 2.88-inch
    touchscreen is an early example of the integration of generative AI on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: More important emerging trends and 2024–2025 predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following trends and predictions are derived from our comprehensive research
    and experience, as well as insights shared by leading industry experts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LLMs optimized for structured data**: LLMs excel in comprehending and generating
    natural language text, benefiting from extensive training on diverse textual sources,
    such as books and web pages. Yet, their proficiency in interpreting structured,
    tabular data remains less developed. Nevertheless, this domain is witnessing burgeoning
    research, with promising advancements anticipated in 2024 and beyond. A notable
    initiative in this trajectory is Table-GPT by Microsoft, which signifies a concerted
    effort to enhance LLMs’ capabilities in processing tabular data by specifically
    fine-tuning them on such datasets ([https://arxiv.org/abs/2310.09263](https://arxiv.org/abs/2310.09263)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maturity of LLMOps**: In 2023, the focus was predominantly on developing
    and transitioning **Proof of Concepts** (**PoCs**) into production environments.
    As we progress, the emphasis will shift toward refining and streamlining **large
    language model operations** (**LLMOps**) by leveraging automation and enhancing
    efficiency. This next phase is poised to attract increased investment from organizations,
    signaling a commitment to optimize and scale the operational aspects of these
    advanced AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building products with Agentive AI**: In [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117),
    we delved into frameworks for autonomous agents, such as Autogen, and explored
    groundbreaking research and applications in this arena. These innovative developments
    showcase AI systems autonomously interacting and executing tasks. As we move from
    2024 and the years that follow, we anticipate a surge in products that integrate
    agentive actions, marking a significant evolution in how AI enhances user productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increasing context window**: We can expect ongoing progress in the realm
    of context window capabilities. Google recently unveiled the Gemini 1.5 model,
    which boasts an impressive context window of 1 million tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More AI-generated influencers**: The popularity of virtual AI avatars is
    growing, as seen with figures such as Lil Miquela on Instagram, who has millions
    of followers and partnerships with big brands such as Chanel, Prada, and Calvin
    Klein, despite being a digital creation. We will continue to see more AI influencers
    gain popularity in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time AI**: Real-time AI matters a lot for user experience. As compute
    prices start to go down, we will see evolving LLM architectures that produce faster
    responses. An example we saw in 2023 was Krea AI’s real-time image transfer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The rise of open source models**: We anticipate a growing trend in the adoption
    of open source models. However, industry leaders maintain that closed source models
    will likely maintain their edge in performance. This perspective is rooted in
    the challenges associated with managing open source models, particularly the potential
    for increased maintenance demands and security or privacy vulnerabilities that
    may arise from untimely community-driven updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better embedding models**: We will continue to witness advancements in embedding
    models that incorporate multimodality with higher dimensions, meaning they will
    also be capable of embedding images to enhance image search functionalities. The
    increasing number of dimensions signifies data representation in a richer format,
    capturing more intricate nuances within the data and yielding improved retrieval
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rising deepfake threats**: The proliferation of deepfake technology poses
    a substantial threat to the integrity of upcoming elections as it enables the
    creation of convincingly altered media. It’s crucial for individuals to critically
    assess and verify information sources, especially during such pivotal times, to
    ensure that what they perceive as true is not a product of sophisticated manipulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute continues to be precious**: Nvidia’s expansion in 2023 was remarkable,
    primarily fueled by the soaring demand for its chips among major cloud computing
    giants such as Microsoft, Amazon, and Google. Looking ahead, it’s anticipated
    that these conglomerates will shift toward manufacturing their chips internally.
    This strategic pivot aims to diminish dependency on third-party suppliers and
    enhance their capability to meet the burgeoning demand for AI applications among
    their clientele. We have already started seeing this trend.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulations**: As highlighted in [*Chapter 9*](B21443_09.xhtml#_idTextAnchor184),
    the passage of executive orders in the US, EU, India, and other nations marks
    a significant turn toward stricter regulation in the AI sector. We can expect
    more defined and stringent regulatory frameworks to emerge, shaping the future
    of AI development and deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Digital Copilots**: Microsoft has been at the forefront of the copilot revolution.
    Copilots are digital assistants, a conversational interface that has become an
    integral part of every product in the Microsoft Stack. A prime example is GitHub
    Copilot, which has not only enhanced developers’ coding efficiency but also reshaped
    the coding paradigm by providing code autocompletion, troubleshooting, and generation
    capabilities, thereby amplifying developer productivity exponentially. The horizon
    looks even more promising as these digital assistants are poised to become fundamental
    components of an expanding array of SaaS offerings across various industries.
    This evolution will be characterized by the integration of multimodal capabilities
    and the emergence of autonomous agents capable of executing tasks, interfacing
    with both internal databases and external applications, and harnessing internet
    data to deliver unparalleled efficiency and innovation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advancements in brain-machine interfaces (BMIs)**: BMIs such as Neuralink
    will get a boost. They utilize AI to decode and interpret complex neural signals,
    enabling the translation of brain activity into actionable commands for computers
    or prosthetic devices. This technology promises enhanced mobility and communication
    for individuals with physical disabilities, offering a seamless integration between
    human intention and machine action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robotic AI/robotic process automation (RPA)**: We will continue to witness
    advancements in robotic systems through the integration of LLMs that further enhance
    their reasoning capabilities. Tesla unveiled its humanoid robot, Optimus, in 2022\.
    Since then, remarkable improvements have been observed in the robot. It is now
    capable of picking up objects and folding shirts. Similarly, Amazon is experimenting
    with robots in its warehouses to move items, a development that is quite impressive.
    This demonstrates the physical ingenuity of modern robots and their potential
    to assist humans in repetitive, tedious, and mundane tasks. While robotics and
    AI have been deeply intertwined, we’ll see compelling advancements through the
    continued integration of RPA technology and generative AI:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Image of Tesla’s Humanoid Robot Optimus. Source: Tesla](img/B21443_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10 – Image of Tesla’s Humanoid Robot Optimus. Source: Tesla'
  prefs: []
  type: TYPE_NORMAL
- en: From quantum computing to AGI – charting ChatGPT’s future trajectory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AGI has emerged as a prevalent buzzword in the wake of significant advancements
    in generative AI. The growing curiosity and anticipation surrounding the timeline
    to achieve AGI underscores its importance. To truly understand AGI, it’s important
    to get to the heart of what it is, recognize why it matters so much, and consider
    how cutting-edge technologies such as quantum computing could speed up our progress
    toward achieving AGI.
  prefs: []
  type: TYPE_NORMAL
- en: What is AGI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although there is no single definition of what AGI is, we synthesized information
    from credible sources to form a definition. AGI is generally understood as a form
    of AI that can understand, learn, and apply knowledge in a way that is not specifically
    tied to certain tasks, environments, or domains. It is characterized by its versatility
    and flexibility, similar to the cognitive capabilities of a human being. OpenAI,
    as a leading AI research organization, has been at the forefront of developing
    advanced AI systems. Although OpenAI has not provided a singular, definitive definition
    of AGI, they describe it as highly autonomous systems that outperform humans at
    most economically valuable work. This description implies a level of general intelligence
    that allows these systems to perform a wide range of tasks, adapt to new environments,
    and continually improve themselves through self-feedback and learning.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum computing and AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AGI could potentially be significantly enhanced by quantum computing, a technology
    that operates on the principles of quantum mechanics. Quantum computers, with
    their ability to perform complex calculations at unprecedented speeds, offer a
    promising solution to the immense computational demands of AGI. They could drastically
    reduce the time needed for data processing and pattern recognition, key components
    of machine learning and AI. Additionally, quantum computing could enable AGI systems
    to analyze vast datasets more efficiently, optimize algorithms to a degree unimaginable,
    and solve optimization and simulation problems that are intractable for classical
    computers. This synergy might not only accelerate the development of AGI but also
    expand its capabilities, leading to more sophisticated and adaptable AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of AGI on society
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AGI could have a profound impact on society because it embodies the potential
    to perform a wide range of cognitive tasks at human or superhuman levels, promising
    breakthroughs in virtually every domain – from medicine to economics to science
    – by solving complex problems, driving innovation, and reshaping our understanding
    of intelligence itself. Unlike narrow AI, which excels in specific tasks, AGI’s
    comprehensive and adaptable nature could lead to unprecedented advancements in
    technology and productivity, and our ability to address the most challenging and
    intricate issues facing humanity. However, alongside its vast potential, AGI also
    poses profound ethical, societal, and existential questions, necessitating careful
    consideration and governance to ensure its benefits are harnessed responsibly
    and equitably. OpenAI’s mission statement emphasizes its commitment to ensuring
    that AGI, when it’s developed, benefits all humanity. They focus on creating safe
    and beneficial AI systems, acknowledging the profound impact that AGI could have
    on society.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored our predictions for the future of generative AI.
    We comprehensively covered what we think will happen next, starting with advancements
    in multimodal LLMs, industry-specific specialized models, and AI regulations,
    and discussed the emergence of more efficient, SLMs that promise to significantly
    enhance intelligent edge devices. We will see a rise in open source models, which
    will democratize AI innovation, enabling widespread access to cutting-edge technology
    and fostering a global community of collaborators to accelerate progress and creativity.
    We also discussed predictions from leading industry figures and charted the path
    toward AGI and quantum computing.
  prefs: []
  type: TYPE_NORMAL
- en: As we turn the final page of our journey together, this book reaches its conclusion,
    culminating in a chapter that has navigated the pivotal advancements and anticipated
    directions in the realm of generative AI. Our exploration embarked from the shores
    of an introductory overview, where generative AI’s harmony with cloud technologies
    was unveiled. We ventured deeper, dissecting strategies to refine the relevance
    of GPT outputs through prompt engineering, fine-tuning, and innovative **retrieval-augmented
    generation** (**RAG**). Our voyage also charted the territories of building generative
    AI applications with sturdy frameworks such as Semantic Kernel, Langchain, and
    Autogen, delving into the complexities of scaling and securing applications, and
    championing the crucial ethos of responsible AI development.
  prefs: []
  type: TYPE_NORMAL
- en: This book has been more than a guide; it has been a shared expedition, offering
    you the compass and tools to navigate the vast ocean of AI possibilities. As we
    bid farewell, remember that the end of this book is not the conclusion but a new
    beginning. Armed with knowledge, may you embark on your own adventures, crafting
    sophisticated, end-to-end AI applications. The prospects of AI are indeed thrilling;
    as AI technology advances, it promises to augment human productivity, thereby
    liberating time for more meaningful endeavors. Thank you for joining us on this
    remarkable journey. Together, we stand on the brink of a bright future with potential,
    ready to explore, innovate, and transform the world with generative AI. Farewell,
    and may your path be ever illuminated by the light of curiosity and the joy of
    discovery.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Phi-2, the surprising power of small language models: [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?msclkid=12a004f4700c6f8608db16e471a46efa](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?msclkid=12a004f4700c6f8608db16e471a46efa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text-to-Video: The Task, Challenges and the Current State: [https://huggingface.co/blog/text-to-video](https://huggingface.co/blog/text-to-video)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Dawn of LMMs:Preliminary Explorations with GPT-4V(ision): [https://export.arxiv.org/pdf/2309.17421](https://export.arxiv.org/pdf/2309.17421GPT-4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Video Retrieval: GPT-4 Turbo with Vision Integrates with Azure to Redefine
    Video Understanding (microsoft.com): https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/video-retrieval-gpt-4-turbo-with-vision-integrates-with-azure-to/ba-p/3982753'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/video-retrieval-gpt-4-turbo-with-vision-integrates-with-azure-to/ba-p/3982753](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/video-retrieval-gpt-4-turbo-with-vision-integrates-with-azure-to/ba-p/3982753)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moonshot Video Generation Model: [https://arxiv.org/abs/2401.01827](https://arxiv.org/abs/2401.01827)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SLM [https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?msclkid=12a004f4700c6f8608db16e471a46efa](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/?msclkid=12a004f4700c6f8608db16e471a46efa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Orca 2: Teaching Small Language Models How to Reason [https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/](https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TinyLlama: An Open-Source Small Language Model [https://arxiv.org/pdf/2401.02385.pdf](https://arxiv.org/pdf/2401.02385.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabbit R1 Technology https://www.rabbit.tech/research
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How To Run Large AI Models On An Edge Device [https://www.forbes.com/sites/karlfreund/2023/07/10/how-to-run-large-ai-models-on-an-edge-device/?sh=634476263d67](https://www.forbes.com/sites/karlfreund/2023/07/10/how-to-run-large-ai-models-on-an-edge-device/?sh=634476263d67)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Table-GPT: Table-tuned GPT for Diverse Table Tasks [https://arxiv.org/abs/2310.09263](https://arxiv.org/abs/2310.09263)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
