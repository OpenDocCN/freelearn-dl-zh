- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Useful Frameworks, Libraries, and APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you might expect, **Python** is the most popular programming language for
    building intelligent AI applications. This is due to its flexibility and ease
    of use, as well as for its vast number of **AI and machine learning** (**ML**)
    libraries. Python has a specialized library for nearly all the necessary tasks
    required to build a **generative AI** (**GenAI**) application.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B22495_01.xhtml#_idTextAnchor009), *Getting Started with Generative
    AI*, you read about the GenAI stack and the evolution of AI. Like the AI landscape,
    the Python library and framework space also went through an evolution phase. Earlier,
    libraries such as pandas, NumPy, and polars were used for data cleanup and transformation
    work, while PyTorch, TensorFlow, and scikit-learn were used for training ML models.
    Now, with the rise of the GenAI stack, LLMs, and vector databases, a new type
    of AI framework has emerged.
  prefs: []
  type: TYPE_NORMAL
- en: These new libraries and frameworks are designed to simplify the creation of
    new applications powered by LLMs. Since building GenAI applications requires the
    seamless integration of data from many sources and the use of diverse AI models,
    these AI frameworks provide built-in functionalities to facilitate acquiring,
    migrating, and transforming data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter delves into the world of AI/ML frameworks, exploring their importance
    and highlighting why Python has emerged as the go-to language for AI/ML development.
    By the end of this chapter, you’ll be able to understand the most popular frameworks
    and libraries, as well as how they help you—the developer—build your GenAI application.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: AI/ML frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publicly available APIs and other tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform the steps shown in this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Latest major version of Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A free tier Atlas cluster running MongoDB version 6.0.11, 7.0.2, or later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your current IP address added to your Atlas project access list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An environment set up to run Python code in an interactive environment, such
    as Jupyter Notebook or Colab. This chapter uses Jupyter Notebook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python for AI/ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python has established itself as the go-to programming language in various fields,
    but most notably in AI, ML, and building applications powered by **large language
    models** (**LLMs**). Python offers simplicity, readability, and a robust ecosystem
    of libraries, making it an ideal choice for all kinds of users, whether they are
    developers, researchers, or even students just getting started with programming.
    Python has also emerged as the language of choice for building new LLM-powered
    applications, underscoring Python’s usefulness, popularity, and versatility.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will learn some of the reasons that make Python a great
    choice for building modern AI-powered applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity and readability**: Python’s syntax is designed to be intuitive
    and clear, which is one of its core strengths. Python can represent complex algorithms
    and tasks in a few lines of code that are easily readable and understandable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rich ecosystem of libraries and frameworks**: Python offers an extensive
    range of libraries and frameworks specifically designed for AI/ML use cases. Libraries
    such as TensorFlow, PyTorch, and scikit-learn have traditionally been popular
    for ML tasks. Hugging Face’s Transformers library has also become an indispensable
    part of the developer workflow for building modern LLM-powered applications. It
    provides pre-trained models and straightforward APIs to fine-tune models for specific
    tasks. These libraries not only accelerate development time but also provide cutting-edge
    solutions to developers across the world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strong community and support**: Python is one of the most popular programming
    languages in the world, and hence has a huge community. According to the Stack
    Overflow survey 2023 ([https://survey.stackoverflow.co/2023/](https://survey.stackoverflow.co/2023/)),
    it’s the second most popular programming language after JavaScript (excluding
    HTML/CSS). This strong and large community provides a wealth of resources, including
    tutorials, discussion forum engagements, and open source projects, which offer
    a helpful support system for someone working on building modern applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with other technologies**: Python’s ability to integrate seamlessly
    with other technologies and programming languages makes it a great choice for
    AI/ML tasks and building LLM-powered applications. For example, Python can easily
    interface with programming languages such as C/C++ for performance-critical tasks.
    It also interfaces well with languages such as Java and C#. This flexibility of
    Python is helpful for deploying LLM-powered applications in diverse environments,
    ensuring that Python can be part of large heterogeneous systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rapid prototyping and experimentation**: Building a sophisticated AI/ML-powered
    application requires many iterations of tests, experiments, and fine-tuning. Python
    allows developers to quickly build prototypes in a few lines of code. Easy testing
    and debugging also help to prototype a quick solution. Python’s interactive environments,
    such as Jupyter Notebook, provide an excellent platform for this purpose. With
    Python, developers building LLM-powered applications can quickly test hypotheses,
    visualize data, and debug code in an interactive manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python combines speed, simplicity, specialized libraries and frameworks, and
    strong community support with easy integration with other languages and technologies,
    all of which make it an excellent choice for building modern LLM-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: AI/ML frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AI/ML frameworks** are essential tools that streamline the development and
    deployment of ML models, providing pre-built algorithms, optimized performance,
    and scalable solutions. They enable developers to focus on refining their models
    and GenAI applications rather than getting bogged down by low-level implementations.
    Using frameworks ensures efficiency, adaptability, and the ability to harness
    cutting-edge AI advancements. Developers should be interested in these frameworks
    as they also reduce development time and enhance the potential for breakthroughs
    in GenAI.'
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB has integrations with many AI/ML frameworks that may be familiar to
    developers, such as LangChain, LlamaIndex, Haystack, Microsoft Semantic Kernel,
    DocArray, and Flowise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will learn about **LangChain**, one of the most popular
    GenAI frameworks. Although it is very popular, it is certainly not the only popular
    framework. If you are interested in other frameworks, you can check out the documentation
    linked in the *Appendix: Further Reading* chapter at the end of this book or see
    the latest list of supported AI/ML frameworks for Python at [https://www.mongodb.com/docs/languages/python/](https://www.mongodb.com/docs/languages/python/).'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain is a framework for developing applications powered by LLMs. LangChain
    simplifies every stage of the LLM application lifecycle. It enables building applications
    that connect external sources of data and computation to LLMs. The basic LLM chain
    relies solely on the information provided in the prompt template to generate a
    response, and the concept of a *LangChain* allows you to extend these chains for
    advanced processing.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn how to use LangChain to perform semantic search
    on your data and build a **retrieval-augmented generation (RAG)** implementation.
    Before you begin, make sure you have all the necessary tools installed and set
    up on your computer, as listed in the *Technical requirements* section of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps to set up your environment for LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by installing the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following code to import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After you have imported the necessary packages, make sure the environment variables
    are set properly. You have two important secrets to store as environment variables:
    your **OpenAI API key** and **MongoDB Atlas** **connection string**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command to store your OpenAI API key as an environment variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to store your MongoDB Atlas connection string as
    an environment variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You are now ready to connect to the MongoDB Atlas cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you’ll instantiate the `MongoClient` and pass your connection string
    to establish communications with your MongoDB Atlas database. Run the following
    code to establish the connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you’ll specify the name of the database and the collection you want to
    create. In this example, you’ll create a database named `langchain_db` and a collection
    called `test`. You’ll also define the name of the vector search index to create
    and use with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With these steps, you’ve set up the basics of connectivity. Now that you have
    the bare bones of your database, you’ll want to define what your application does.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, you will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch a publicly accessible PDF document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split it into smaller chunks of information for easy consumption by your GenAI
    application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the data into the MongoDB database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This functionality is not something you have to build from scratch. Instead,
    you’ll use the free, open source library integration provided by LangChain called
    `PyPDFLoader`, which you imported in *Step 2* earlier in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching and splitting public PDF documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using `PyPDFLoader` to fetch publicly available PDFs is quite simple. In the
    following code, you will fetch a publicly accessible PDF document and split it
    into smaller chunks that you can later upload into your MongoDB database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You will then receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With this code, you first instantiated `PyPDFLoader` and then passed it the
    URL to the publicly accessible PDF file: [https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4HkJP).
    Next, you loaded the fetched PDF file into the `data` variable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, you split the PDF file’s text into smaller chunks. For this example,
    you set the chunk size to 200 characters and allowed an overlap of 20 characters
    between chunks. The overlap maintains the context between chunks. Note that this
    number is not arbitrary, and there are many opinions about what your chunking
    strategy should be. Some of those resources are discussed in the *Appendix: Further
    Reading* chapter of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: You stored the split chunks in the `docs` variable and printed the first chunk
    of the split document. This indicates that your output request via the `print`
    command was successful, and you can easily confirm whether the information is
    correct for this entry.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the vector store
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After you have split your documents into chunks, you will instantiate the vector
    store with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, you created a vector store named `vector_store` using
    the `MongoDBAtlasVectorSearch.from_documents` method and specified various parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`documents = docs`: The name of the document that you want to store in your
    vector database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding = OpenAIEmbeddings(disallowed_special=())`: The class that generates
    vector embeddings for the documents using OpenAI’s embedding model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collection = atlas_collection`: The Atlas collection where documents will
    be stored'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index_name = vector_search_index`: The name of the index to use for querying
    the vector store'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You’ll also need to create your **Atlas Vector Search index** in the MongoDB
    database. For explicit instructions on how this is done, see [*Chapter 8*](B22495_08.xhtml#_idTextAnchor180),
    *Implementing Vector Search in AI Applications*. This must be completed before
    you can successfully run the previous code. When you are creating a Vector Search
    index, use the following index definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This index defines two fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text-embedding-ada-002` model. It has 1,536 dimensions and uses cosine similarity
    to measure similarity. You may also want to consider other newer models from OpenAI,
    `text-embedding-3-small` and `text-embedding-3-large`, which are optimized for
    different use cases and therefore have a different number of dimensions. See [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
    for more details as well as current options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Page field**: A filter type field used for pre-filtering data based on the
    page number in the PDF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, you can run your code successfully, fetch a publicly available PDF, chunk
    it into smaller portions of data, and store them in a MongoDB Atlas database.
    With these steps accomplished, you can conduct additional tasks, such as running
    queries to perform semantic search on your data. You can learn about basic semantic
    search in [*Chapter 8*](B22495_08.xhtml#_idTextAnchor180), *Implementing Vector
    Search in AI Applications*, and [*Chapter 10*](B22495_10.xhtml#_idTextAnchor214),
    *Refining the Semantic Data Model to* *Improve Accuracy*.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on this topic, you can also consult the official documentation
    from LangChain, available at [https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/#pre-filtering-with-similarity-search](https://python.langchain.com/v0.2/docs/integrations/vectorstores/mongodb_atlas/#pre-filtering-with-similarity-search).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s cover some specific LangChain functionalities that you will find
    most useful when building GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain semantic search with score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain provides some particularly helpful methods to perform semantic search
    on your data and return a **score**. This score refers to the measure of relevance
    between the query and the matching documents based on their semantic content.
    You can use this score when you want to return more than one result to your users
    and also limit the number of results. For example, this score can prove useful
    in returning the top three most relevant pieces of content about a topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method that you will use here is `similarity_search_with_score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You pass the query to the `similarity_search_with_score` function and specify
    the `k` parameter as `3` to limit the number of documents to return to 3\. Then,
    you can print the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the output, three documents are returned that have the highest
    relevance score. Each returned document also has a relevance score attached to
    it that ranges between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic search with pre-filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MongoDB allows you to pre-filter your data using a match expression to narrow
    down the search space before performing a more computationally intensive vector
    search. This offers several benefits to developers, such as increased performance,
    improved accuracy, and enhanced query relevancy. When pre-filtering, remember
    to index any metadata fields by which you want to filter during index creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a code snippet that shows how you can perform semantic search with
    pre-filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this code example, you have the same query string for which you performed
    a plain semantic search earlier. The `k` value is set to `3` so that it only returns
    the top three matching documents. You have also provided a `pre_filter` query,
    which is basically an MQL expression that uses the `$eq` operator to specify that
    MongoDB should only return content and chunked information that is on page `17`
    of the original PDF document.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a basic RAG solution with LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain’s functionalities are not only limited to performing semantic search
    queries on your data stored in vector databases. It also allows you to build powerful
    GenAI applications. With the following code snippet, you will learn an easy way
    to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up a MongoDB Atlas Vector Search retriever for similarity-based search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return the 10 most relevant documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Utilize a custom RAG prompt with an LLM to answer questions based on the retrieved
    documents:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code instantiates Atlas Vector Search as a `k` as `3` to search
    for only the three most relevant documents.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, notice the line that says `custom_rag_prompt = PromptTemplate.from_template(template)`.
    It refers to prompt templates, which are detailed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain prompt templates and chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`context` as an input variable and the original query for the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up a **chain**, a key feature of LangChain that specifies three main
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retriever**: You will use MongoDB Atlas Vector Search to find relevant documents
    that provide context for the language model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt template**: This is the template you created earlier to format the
    query and the contextual information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLM**: You will use the OpenAI chat model to generate responses based on
    the provided context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will use this chain to process a sample input query about MongoDB Atlas
    Security recommendations, format the query, retrieve the results of the query,
    and then return a response to the user along with the documents used as context.
    Due to LLM variability, you will likely never receive the exact same response
    twice, but here is an example showing the potential output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This output both answers the user’s inquiry and provides the source information,
    increasing not only user trust but also the ability of the user to follow up and
    get more details as they require.
  prefs: []
  type: TYPE_NORMAL
- en: This brief overview of the LangChain framework has tried to convince you of
    this framework’s utility and potential and give you a preview of its capabilities
    to save you valuable time when crafting your GenAI application.
  prefs: []
  type: TYPE_NORMAL
- en: Key Python libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to AI/ML frameworks, there are also many Python libraries that will
    make the experience of building your GenAI application easier. Whether you require
    assistance with data cleansing, formatting, or transformation, there are likely
    half a dozen potential Python libraries to solve every problem. The following
    subsections list some favorites and explain how they can assist you during your
    GenAI journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this book, you can broadly divide these libraries into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**General-purpose scientific libraries** such as pandas, NumPy, and scikit-learn'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MongoDB-specific libraries** such as PyMongoArrow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning frameworks** such as PyTorch and TensorFlow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of this section covers one relevant and popular library from each of
    these categories
  prefs: []
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pandas library is a powerful and flexible open source data manipulation
    and analysis library for Python. It provides data structures such as DataFrames
    and Series, which are designed to handle structured data intuitively and efficiently.
    When working with tabular data stored in spreadsheets or databases, pandas is
    a great tool for data analysis. With pandas, you can perform a wide range of operations,
    including cleaning, transforming, and aggregating data.
  prefs: []
  type: TYPE_NORMAL
- en: Among many other noticeably out-of-the-box functionalities, pandas also offers
    great support for time series and has an extensive set of tools for working with
    dates, times, and time-indexed data. In addition to providing a wide range of
    methods to work with numerical data, pandas gives great support for working with
    text-based data.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a short example of how to work with the pandas library. In the following
    example, you will create a pandas DataFrame from a Python dictionary. Then, you
    will print the entire DataFrame. Next, you will select a specific column, which
    is `Age`, and print it. Then, you will filter data by row label or by the specific
    position of a row.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next line shows how you can filter data using Boolean masking in pandas.
    Here, you will print out the DataFrame format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Your output should be in the format of a pandas DataFrame, similar to *Figure
    7**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: DataFrame output from pandas'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then manipulate this data in various ways, each time outputting the
    results as you see fit, but always formatted as a pandas DataFrame. To print only
    the ages of the users, you would use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll get the output shown in *Figure 7**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: DataFrame output of ages'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also filter the output. Here, you will filter data to show only those
    people who are older than 25, and then present the results as a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will filter the data and then output the results in DataFrame format,
    as in *Figure 7**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Filtered DataFrame output'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also perform calculations with the pandas library in a straightforward
    way. To calculate the average age, for instance, you would use code such as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And your output would look like *Figure 7**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Calculated field output'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, data manipulation in pandas is fairly easy, and the outputs
    are immediately readable and well-formatted for further analysis. The intuitive
    syntax and powerful functions of pandas make it an essential tool for Python developers,
    enabling them to handle large datasets with ease and precision. For those building
    GenAI applications, pandas streamlines the data preprocessing steps, ensuring
    that data is clean, structured, and ready for model training. Additionally, its
    robust integration with other Python libraries enhances its utility, making complex
    data analysis and visualization straightforward and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: PyMongoArrow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**PyMongoArrow** is a Python library built on top of the official MongoDB Python
    driver, **PyMongo**, which allows you to move data out of the MongoDB database
    into some of the most popular Python libraries, such as pandas, NumPy, PyArrow,
    and polars, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyMongoArrow simplifies loading data from MongoDB into other supported data
    formats. The example covered below demonstrates how you can work with MongoDB,
    PyMongoArrow, and libraries such as pandas and NumPy. You may find this useful
    in the context of GenAI applications in the following situations:'
  prefs: []
  type: TYPE_NORMAL
- en: When you require data in a specific format for summarization and analysis (CSV,
    DataFrame, NumPy array, Parquet file, etc.) from MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need to merge data of various types for calculations or transformations
    that are then used for GenAI analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As an example, if you have inbound financial data from *Application A*, inbound
    sales data from *Application B*, PDF files from *Team 1*, and `.txt` files from
    *Team 2*, and you’d like your GenAI application to summarize annual data from
    all these different places, you will likely get more accurate results if all types
    of data are in the same format. This will require some upfront programmatic effort,
    and PyMongoArrow simplifies transforming MongoDB JSON into other data types as
    well as ingesting those other data types and converting them into JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this example with PyMongoArrow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by installing and importing the latest version of PyMongoArrow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, make sure you have your Atlas cluster connection string handy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, you will extend the PyMongo driver via the `pymongoarrow.monkey` module.
    This allows you to add the PyMongoArrow functionality directly to MongoDB collections
    in Atlas. By calling `patch_all()` from `pymongoarrow.monkey`, new collection
    instances will include PyMongoArrow APIs, such as `pymongoarrow.api.find_pandas_all()`.
    This is useful because you can now easily export your data from MongoDB to various
    formats such as pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add some test data to your collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'PyMongoArrow uses a `schema` object and mapping field names to type-specifiers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: MongoDB’s key feature is its ability to represent nested data using embedded
    documents, along with its support for lists and nested lists. PyMongoArrow fully
    supports these features out of the box, providing first-class functionality for
    handling embedded documents, lists, and nested lists seamlessly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s perform some `find` operations on the data. The following code demonstrates
    querying a MongoDB collection called `data` for documents where the `amount` field
    is greater than `0`, using PyMongoArrow to convert the results into different
    data formats. A predefined schema is used for the conversion, but it’s optional.
    If you omit the schema, PyMongoArrow tries to automatically apply a schema based
    on the data contained in the first batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first line of code converts the query results into a pandas DataFrame. The
    second line of code converts the query results set into an arrow table. The third
    line converts the query results set into a polars DataFrame, and finally, the
    fourth line converts the query result set into a NumPy array.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You are not limited to performing `find` operations to convert the query result
    set into other supported data formats. PyMongoArrow also allows you to use MongoDB’s
    powerful aggregation pipeline to perform complex queries on your data to filter
    out the needed data before exporting it to other data formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following code performs an aggregation query on the data collection
    in a MongoDB database, grouping all documents and calculating the total sum of
    the `amount` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The result of this code is converted into a pandas DataFrame that would consist
    of a total sum.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have learned a little bit about pandas and NumPy, it’s important
    you also have some knowledge of another popular Python ML library, PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch, developed by Meta’s AI Research lab, is an open source deep learning
    framework known for its flexibility and ease of use. It is widely appreciated
    for its dynamic computation graph, which allows intuitive coding and immediate
    execution of code. This feature is particularly useful for researchers and developers
    who need to experiment and iterate quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of building a GenAI application, PyTorch serves as a powerful
    tool for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model training and development**: PyTorch is utilized for developing and
    training the core generative models, such as **generative pre-trained transformer**
    (**GPT**) variants, which form the backbone of the GenAI application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility and real-time experimentation**: The dynamic computation graph
    in PyTorch allows on-the-fly modifications and real-time experimentation, which
    are crucial for fine-tuning generative models to produce high-quality output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers who are adapting pre-trained language models to their specific requirements
    or developing their own custom model for unique tasks may be interested in using
    this library, along with some of the APIs discussed in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: AI/ML APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When developing GenAI applications, developers have access to a variety of
    APIs that can significantly enhance the capabilities and efficiency of their projects.
    As these APIs are widely used, they offer performance, stability, and consistency
    across thousands of projects, ensuring that developers don’t need to reinvent
    the wheel. Here are just some of the functionalities that these APIs offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text generation and processing**: APIs such as **OpenAI**, **Hugging Face**,
    and **Google Gemini API** enable developers to generate coherent and contextually
    appropriate text, which is crucial for applications such as content creation,
    dialogue systems, and virtual assistants.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translation capabilities**: The **Google Cloud Translation API**, **Azure
    AI Translator**, and **Amazon Translate API** provide robust translation capabilities,
    making GenAI applications multilingual and globally accessible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech synthesis and recognition**: Services such as **Google Text-to-Speech**,
    **Amazon Polly**, and **IBM Watson Text-to-Speech** convert generated text into
    natural-sounding speech, enhancing user interaction and accessibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image and video processing**: APIs from **Clarifai** and **DeepAI** allow
    GenAI applications to create, modify, and analyze visual content, enabling tasks
    such as image generation from text and object recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These APIs provide a range of capabilities that, when combined, can significantly
    accelerate the development and enhance the functionality of GenAI applications.
    Next, you’re going to dig deeper into two of these APIs, the OpenAI API and the
    Hugging Face Transformers APIs.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you may recall from [*Chapter 3*](B22495_03.xhtml#_idTextAnchor041), *Large
    Language Models*, OpenAI provides a foundational model trained on a broad spectrum
    of data. It offers this model via an API, which allows you to harness the power
    of advanced ML models without needing to manage the underlying infrastructure.
    The computational and financial costs of retraining or hosting a custom LLM for
    an organization or domain-specific information are very high, so most developers
    will utilize someone else’s LLM to provide GenAI capabilities to their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Although each API has its own strengths and weaknesses, the OpenAI API is currently
    the most widely used. It provides a simple interface for developers to create
    an intelligence layer in their applications. It is powered by OpenAI’s state-of-the-art
    models and cutting-edge **natural language processing** (**NLP**) capabilities,
    enabling applications to perform tasks such as text generation, summarization,
    translation, and conversation. The API is designed to be flexible and scalable,
    making it suitable for a wide range of use cases, from chatbots to content creation
    tools. It is also well documented, with a large community, and there are many
    tutorials available for seemingly every use case.
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI API is already somewhat of an industry standard, and many GenAI tools
    and technologies have support and seamless integrations with it. If you’d like
    to avoid a lot of unnecessary effort and costs, your best bet is to work with
    the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get started with the OpenAI API in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you’ll need to install `openai` from the terminal or command
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Include your API key from OpenAI in the environment variable file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Send your first API test request to the OpenAI API using the Python library.
    To do this, create a file named `openai-test.py` using the terminal or an IDE.
    Then, inside the file, copy and paste one of the following examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the code by entering `python openai-test.py` into the terminal or command
    line. This should output a creative poem about recursion. Every result is different
    because the GPT will use creativity to invent something new each time. This is
    what it created in this attempt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is surprisingly good. You should try it for yourself to see what
    new creative poem will be crafted.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: GPT excels at answering questions, but only on the topics it recalls from its
    training data. In most cases, you’ll want GPT to answer questions about your business
    or products or answer commonly asked questions from your users. In such cases,
    you’ll want to add the ability to search a library of your own documents for relevant
    text, and then have GPT use that text as part of its reference information for
    responses. This is referred to as RAG, which you can read more about in [*Chapter
    8*](B22495_08.xhtml#_idTextAnchor180), *Implementing Vector Search in* *AI Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Hugging Face** is a prominent AI community and ML platform. Its ecosystem
    is the **Hugging Face Hub**, a platform designed to facilitate collaboration and
    innovation in the AI community. The Hub, located at [https://huggingface.co/docs/hub/en/index](https://huggingface.co/docs/hub/en/index),
    boasts a vast repository of over 120,000 models, 20,000 datasets, and 50,000 demonstrations
    as of writing, making it one of the largest collections of ML resources available.
    It has the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extensive model repositories**: The Hub includes pre-trained models for a
    variety of tasks, such as text classification, translation, summarization, and
    question answering, providing a wide range of options for developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Datasets**: It provides access to a diverse array of datasets that are crucial
    for training and evaluating ML models. Datasets cover multiple domains and languages,
    supporting the development of robust and versatile AI applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Community and collaboration**: The platform supports collaboration by allowing
    users to share models, datasets, and code. Developers can contribute to the community
    by uploading their own models and datasets, fostering a collaborative environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration and deployment options**: The Hugging Face Hub integrates seamlessly
    with popular ML frameworks, such as PyTorch and TensorFlow. The Hub also provides
    deployment solutions, enabling developers to deploy their models in production
    environments easily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI application developers can use the **Hugging Face Transformers APIs**
    to get access to thousands of pre-trained ML models on specific datasets for specific
    tasks. With transformer models, you can use pre-trained models for inference or
    fine-tune them with your own data using PyTorch and TensorFlow libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate what is possible for your GenAI application, let’s see how to
    use a pre-trained transformer model for inference in order to perform two tasks:
    basic sentiment analysis and text generation. Both could be useful for your GenAI
    projects if you, for instance, want to sort customer feedback or score it based
    on sentiment and generate a response.'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ll use the `transformers` library to utilize shared models, then explore
    the `pipeline()` function, the core component of the `transformers` library. This
    function seamlessly integrates the model with necessary pre-processing and post-processing
    steps, enabling direct text input and generating intelligible responses:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, ensure you have the necessary packages installed. Note that at least
    one of TensorFlow or PyTorch should be installed. Here, let’s use TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, import the `pipeline()` function. You’ll also create an instance of the
    `pipeline()` function and specify the task you want to use it for, that is, sentiment
    analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You’ll receive the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B22495_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Hugging Face Transformers sentiment analysis output'
  prefs: []
  type: TYPE_NORMAL
- en: The model performs the analysis and outputs a label and a score. The `label`
    indicates the sentiment type as positive or negative, and the `score` indicates
    the degree of confidence in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass multiple input texts as an array for sentiment classification
    to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll receive the following as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Multiple input texts for sentiment classification in Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the model outputs an array of objects. Each output object corresponds
    to the individual text inputs.
  prefs: []
  type: TYPE_NORMAL
- en: You might be holding your breath, expecting things to become more complicated—but
    they won’t. You conducted your first sentiment analysis in Hugging Face with a
    pre-trained model with just those few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to sentiment analysis, you can also perform many other NLP tasks
    with Transformers libraries, such as text generation. Here, you will provide a
    prompt, and the model will auto-complete it by generating the remaining text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll get the following output for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Text generation using the Hugging Face Transformers'
  prefs: []
  type: TYPE_NORMAL
- en: Since you did not provide a model name to the pipeline instance, it decided
    to use the default, which in this case is GPT-2\. You may or may not get the same
    results as the ones here because text generation involves some randomness. Again,
    however, you can see how easy this task was.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, specify a model name to be used in the `pipeline` function at the time
    of text generation. With the following code, you provide some more custom details,
    such as the number of different sequences to be generated and the maximum length
    of the output texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With these additional parameters provided, you’ll now receive the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Hugging Face text generation output with parameters'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code outputs two different pairs of text, each having fewer than
    25 words.
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, Hugging Face offers many more tools and functionalities
    that developers can use to build their GenAI applications. With its comprehensive
    library support and active community, Hugging Face continues to be a pivotal resource
    for advancing NLP and ML projects. Additionally, its seamless integration with
    various AI/ML frameworks ensures that developers can efficiently deploy and scale
    their GenAI models with minimal effort and maximum productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you looked at the evolution of AI/ML frameworks in the Python
    space as LLM-powered applications have gained prominence. You also learned why
    Python remains a top choice for building modern LLM-powered applications. You
    reviewed the most popular Python frameworks, libraries, and APIs that can assist
    you in the different stages of GenAI application development.
  prefs: []
  type: TYPE_NORMAL
- en: The GenAI space is evolving so rapidly that by the time this book is published,
    there will probably be more libraries available, more APIs in use, and the framework’s
    capabilities will have expanded. You owe it to yourself to do your own due diligence
    about which framework is best suited for your business needs, but also make sure
    to choose one that is appropriately supported. As with any rapidly evolving technology,
    some of the tools and technologies that are in existence today will be gone tomorrow.
    This chapter has tried, therefore, to only include those that have the community,
    enablement, and feature set to ensure their longevity.
  prefs: []
  type: TYPE_NORMAL
- en: Undoubtedly there is still plenty of innovation to be done, and new tools to
    be created, even in the short term—the tools discussed in this chapter are barely
    the tip of the iceberg. So, take a deep breath and begin your own discovery. You
    will inevitably realize that there are tools you need, and that you have too many
    choices on how to fulfill those needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will explore how to leverage the vector search feature
    of MongoDB Atlas to create intelligent applications. You will learn about RAG
    architecture systems and gain a deeper understanding of various complex RAG architecture
    patterns with MongoDB Atlas.
  prefs: []
  type: TYPE_NORMAL
