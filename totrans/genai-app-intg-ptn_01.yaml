- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Generative AI Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides an overview of key concepts, techniques, and integration
    patterns related to generative AI that will empower you to harness these capabilities
    in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: We will provide an overview of generative AI architectures, such as transformers
    and diffusion models, which are the basis for these generative models to produce
    text, images, audio, and more. You’ll get a brief introduction to specialized
    training techniques, like pre-training and prompt engineering, that upgrade basic
    language models into creative powerhouses.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the relentless pace of innovation in this space is critical due
    to new models and ethical considerations emerging constantly. We’ll introduce
    strategies for experimenting rapidly while ensuring responsible, transparent development.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also introduces common integration patterns for connecting generative
    AI into practical workflows. Whether crafting chatbots that leverage models in
    real time or performing batch enrichment of data, we will introduce prototyping
    blueprints to jumpstart building AI-powered systems.
  prefs: []
  type: TYPE_NORMAL
- en: By the end, you will have a one-thousand-foot view of which generative AI models
    are available, why experimentation is important, and how these integration patterns
    can help create value for your organization leveraging generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following main topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive AI vs generative AI use case ideation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A change in the paradigm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General generative AI concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to generative AI integration patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From AI predictions to generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The intent of this section is to provide a brief overview of artificial intelligence,
    highlighting our initial experiences with it. In the early 2000s, AI started to
    become more tangible for consumers. For example, in 2001, Google introduced the
    “Did you mean?” feature ([https://blog.google/intl/en-mena/product-updates/explore-get-answers/25-biggest-moments-in-search-from-helpful-images-to-ai/](https://blog.google/intl/en-mena/product-updates/explore-get-answers/25-biggest-moments-in-search-from-helpful-images-to-ai/)),
    which suggests spelling corrections. This was one of Google’s first applications
    of machine learning and one of the early AI features that the general public got
    to experience on a large scale.
  prefs: []
  type: TYPE_NORMAL
- en: Over the following years, AI systems became more sophisticated, especially in
    areas like computer vision, speech-to-text conversion, and text-to-speech synthesis.
    Working in the telecom industry helped me witness the innovation driven by speech-to-text
    in particular. Integrating speech-to-text capabilities into **interactive voice
    response** (**IVR**) systems led to better user experiences by allowing people
    to speak their requests rather than punch numbers into a keypad. For example,
    you could be calling a bank where you would be welcomed by a message asking you
    to say “balance” to check your balance, “open account” in order to open an account,
    etc. Nowadays we are seeing more and more implementations of AI, simplifying more
    complex and time-consuming tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The exponential increase in available computing power, paired with the massive
    datasets needed to train machine learning models, unleashed new AI capabilities.
    In the 2010s, AI started matching and even surpassing human performance on certain
    tightly defined tasks like image classification.
  prefs: []
  type: TYPE_NORMAL
- en: The advent of generative AI has reignited interest and innovation in the AI
    field, introducing new approaches for exploring use cases and system integration.
    Models like Gemini, PaLM, Claude, DALL-E, OpenAI GPT, and Stable Diffusion showcase
    the ability of AI systems to generate synthetic text, images, and other media.
    The outputs exhibit creativity and imagination that capture the public’s attention.
    However, the powerful capabilities of generative models also highlight new challenges
    around system design and responsible deployment. There is a need to rethink integration
    patterns and architecture to support safe, robust, and cost-effective implementations.
    Specifically, issues around security, bias, toxicity, and misinformation must
    be addressed through techniques like dataset filtering, human-in-the-loop systems,
    enhanced monitoring, and immediate remediation.
  prefs: []
  type: TYPE_NORMAL
- en: As generative AI continues maturing, best practices and governance frameworks
    must evolve in tandem. Industry leaders have formed partnerships like the Content
    Authenticity Initiative to develop technical standards and policy guidance around
    the responsible development of the next iteration of AI. This technology’s incredible
    potential, from accelerating drug discovery to envisioning new products, can only
    be realized through a commitment to transparency, ethics, and human rights. Constructive
    collaboration that balances innovation with caution is imperative.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI marks an inflection point for the field. The ripples from this
    groundswell of creative possibility are just beginning to reach organizations
    and communities. Maintaining an open, evidence-driven dialogue around not just
    capabilities but also challenges lays a foundation for AI deployment that empowers
    people, unlocks new utility, and earns widespread trust.
  prefs: []
  type: TYPE_NORMAL
- en: We are witnessing an unprecedented democratization of generative AI capabilities
    through publicly accessible APIs from established companies like Google, Meta,
    and Amazon, and startups such as Anthropic, Mistral AI, Stability AI, and OpenAI.
    The table below summarizes several leading models that provide versatile foundations
    for natural language and image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Just a few years ago, developing with generative AI required specialized expertise
    in deep learning and access to vast computational resources. Now, models like
    Gemini, Claude, GPT-4, DALL-E, and Stable Diffusion can be accessed via simple
    API calls at near-zero cost. The bar for experimentation has never been lower.
  prefs: []
  type: TYPE_NORMAL
- en: This commoditization has sparked an explosion of new applications leveraging
    these pre-trained models – from creative tools for content generation to process
    automation solutions infused with AI. Expect integrations with generative foundations
    across all industries in the coming months and years.
  prefs: []
  type: TYPE_NORMAL
- en: Models are becoming more knowledgeable, with broader capabilities and reasoning
    that will reduce hallucinations and increase accuracy across model responses.
    Multimodality is also gaining traction, with models able to ingest and generate
    content across text, images, audio, video, and 3D scenes. In terms of scalability,
    model size and context windows continue expanding exponentially; for example,
    Google’s Gemini 1.5 now supports a context window of 1 million tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the outlook points to a future where generative AI will become deeply
    integrated into most technologies. These models introduce new efficiencies and
    automation potential and inspire creativity across nearly every industry imaginable.
  prefs: []
  type: TYPE_NORMAL
- en: The table below highlights some of the most popular LLMs and their providers.
    The purpose of the table is to highlight the vast number of options available
    on the market at the time of writing this book. We expect this table to quickly
    become outdated by the time of publication and highly encourage readers to dive
    deep into the model providers’ websites to stay up to date with any new launches.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Provider** | **Landing Page** |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini | Google | [https://deepmind.google/technologies/gemini](https://deepmind.google/technologies/gemini)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Claude | Anthropic | [https://claude.ai/](https://claude.ai/) |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | OpenAI | [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion | Stability AI | [https://stability.ai/](https://stability.ai/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Mistral | Mistral AI | [https://mistral.ai/](https://mistral.ai/) |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA | Meta | [https://llama.meta.com/](https://llama.meta.com/) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.1: Overview of popular LLMs and their providers'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive AI vs generative AI use case ideation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predictive AI refers to systems that analyze data to identify patterns and make
    forecasts or classifications about future events. In contrast, generative AI models
    create new synthetic content like images, text, or code based on the patterns
    gleaned from their training data. For example, with predictive AI, you can confidently
    identify if an image contains a cat or not, whereas with generative AI you can
    create an image of a cat from a text prompt, modify an existing image to include
    a cat where there was none, or generate a creative text blurb about a cat.
  prefs: []
  type: TYPE_NORMAL
- en: Product innovation focused on AI involves various phases of the product development
    lifecycle. With the emergence of generative AI, the paradigm has shifted away
    from initially needing to compile training data to train traditional ML models
    and toward leveraging flexible pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Foundational models like Google’s PaLM 2 and Gemini, OpenAI’s GPT and DALL-E,
    and Stable Diffusion provide broad foundations enabling rapid prototype development.
    Their versatile capabilities lower the barrier for experimenting with novel AI
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Where previously data curation and model training from scratch could take months
    before assessing viability, now proof-of-concept generation is possible within
    days without the need to fine-tune a foundation model.
  prefs: []
  type: TYPE_NORMAL
- en: This generative approach facilitates more iterative concept validation. After
    quickly building an initial prototype powered by the baseline model, developers
    can then collect niche training data and perform knowledge transfer via techniques
    like distillation to customize later versions; we will deep dive into the concept
    of distillation later in the book. The model’s primary foundation contains already
    encoded patterns useful for kickstarting and for iterations of new models.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the predictive modeling approach requires upfront data gathering
    and training before any application testing. This more linear progression limits
    early-stage flexibility. However, predictive systems can efficiently learn specialized
    correlations and achieve a high level of confidence inference metrics once substantial
    data exists.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging versatile generative foundations supports rapid prototyping and use
    case exploration. But, later, custom predictive modeling boosts performance on
    narrow tasks with sufficient data. Blending these AI approaches capitalizes on
    their complementary strengths throughout the model deployment lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the basic use – prompt engineering – of a foundational model, several
    auxiliary, more complex techniques can enhance its capabilities. Examples include
    **Chain-of-Thought** (**CoT**) and **ReAct**, which empower the model to not only
    reason about a situation but also define and evaluate a course of action.
  prefs: []
  type: TYPE_NORMAL
- en: 'ReAct, presented in the paper *ReAct: Synergizing Reasoning and Acting in Language
    Models* ([https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)),
    addresses the current disconnect between LLMs’ language understanding and their
    ability to make decisions. While LLMs excel at tasks like comprehension and question
    answering, their reasoning and action-taking skills (for example, generating action
    plans or adapting to unforeseen situations) are often treated separately.'
  prefs: []
  type: TYPE_NORMAL
- en: ReAct bridges this gap by prompting LLMs to generate both “reasoning traces,”
    detailing the model’s thought process, and task-specific actions in an interleaved
    manner. This tight coupling allows the model to leverage reasoning for planning,
    execution monitoring, and error handling, while simultaneously using actions to
    gather additional information from external sources like knowledge bases or environments.
    This integrated approach demonstrably improves LLM performance in both language
    and decision-making tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in question-answering and fact-verification tasks, ReAct combats
    common issues like hallucination and error propagation by utilizing a simple Wikipedia
    API. This interaction allows the model to generate more transparent and trustworthy
    solutions compared to methods lacking reasoning or action components. LLM hallucinations
    are defined as content generated that seems plausible yet factually unsupported.
    There are various papers that aim to address this phenomenon. For example, *A
    survey of Hallucination in Large Language Models – Principles, Taxonomy, Challenges,
    and Open Questions* deep dives into an approach to not only identify but also
    mitigate hallucinations. Another good example of a mitigation technique is covered
    in the paper *Chain-of-Verification Reduces Hallucination in Large Language Models*
    ([https://arxiv.org/pdf/2309.11495.pdf](https://arxiv.org/pdf/2309.11495.pdf)).
    At the time of writing this book, hallucinations are a very rapidly changing field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both CoT and ReAct rely on prompting: feeding the LLM with carefully crafted
    instructions that guide its thought process. CoT, as presented in the paper *Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models* ([https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)),
    focuses on building a chain of reasoning steps, mimicking human thinking. Imagine
    prompting the model with: “I want to bake a cake. First, I need flour. Where can
    I find some?” The model responds with a potential source, like your pantry. This
    back-and-forth continues, building a logical chain of actions and decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: ReAct takes things a step further, integrating action into the reasoning loop.
    Think of it as a dynamic dance between thought and action. The LLM not only reasons
    about the situation but also interacts with the world, fetching information or
    taking concrete steps, and then updates its reasoning based on the results. It’s
    like the model simultaneously planning a trip and checking maps to adjust the
    route if it hits a roadblock.
  prefs: []
  type: TYPE_NORMAL
- en: This powerful synergy between reasoning and action unlocks a new realm of possibilities
    for LLMs. CoT and ReAct tackle challenges like error propagation (jumping to the
    wrong conclusions based on faulty assumptions) by allowing the model to trace
    its logic and correct course. They also improve transparency, making the LLM’s
    thought process clear and understandable.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, **large language models** (**LLMs**) are like brilliant linguists,
    adept at understanding and generating text. But when it comes to real-world tasks
    demanding reasoning and action, they often stumble. Here’s where techniques like
    CoT and ReAct enter the scene, transforming LLMs into reasoning powerhouses.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine an LLM helping diagnose a complex disease. CoT could guide it through
    a logical chain of symptoms and examinations, while ReAct could prompt it to consult
    medical databases or run simulations. This not only leads to more accurate diagnoses
    but also enables doctors to understand the LLM’s reasoning, fostering trust and
    collaboration.
  prefs: []
  type: TYPE_NORMAL
- en: These futuristic applications are what drive us to keep building and investing
    in this technology, which is very exciting. Before we dive deep into the patterns
    that are needed to leverage generative AI technology to generate business value,
    let’s take a step back and look at some initial concepts.
  prefs: []
  type: TYPE_NORMAL
- en: A change in the paradigm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It feels like eons ago in tech years, but let’s rewind just a couple of years,
    back when if you were embarking on solving an AI problem, you couldn’t default
    to utilizing a pre-trained model through the web or a managed endpoint. The process
    was meticulous – you’d have to first clearly define the specific use case, identify
    what data you had available and could collect to train a custom model, select
    the appropriate algorithm and model architecture, train the model using specialized
    hardware and software, and validate if the outputs would actually help solve the
    task at hand. If all went well, you would have a model that would take a predefined
    input and also provide a predefined output.
  prefs: []
  type: TYPE_NORMAL
- en: The paradigm profoundly shifted with the advent of LLMs and large multimodal
    models. Suddenly, you could access a pre-trained model with billions of parameters
    and start experimenting right off the bat with these versatile foundational models
    where the inputs and outputs are dynamic in nature. After tinkering around, you’d
    then evaluate if any fine-tuning is necessary to adapt the model to your needs,
    rather than pre-training an entire model from scratch. And spoiler alert – in
    most cases, chances are you won’t even need to fine-tune a foundational model.
  prefs: []
  type: TYPE_NORMAL
- en: Another key shift relates to the early belief that one model would outperform
    all others and solve all tasks. However, the model itself is just the engine;
    you still need an entire ecosystem packaged together to provide a complete solution.
    Foundational models have certainly demonstrated some incredible capabilities beyond
    initial expectations. But we also observe that certain models are better suited
    for certain tasks. And running the same prompt through other models can produce
    very different outputs depending on the underlying model’s training datasets and
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: So, the new experimental path often focuses first on prompt engineering, response
    evaluation, and then fine-tuning the foundational model if gaps exist. This contrasts
    sharply with the previous flow of data prep, training, and experimentation before
    you could get your hands dirty. The bar to start creating with AI has never been
    lower.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explore the difference between the development
    lifecycle of predictive AI and generative AI use cases. In each section, we have
    provided a high-level visual representation of a simplified development lifecycle
    and an explanation of the thought process behind each approach.
  prefs: []
  type: TYPE_NORMAL
- en: Predictive AI use case development – simplified lifecycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/B22175_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Predictive AI use case development simplified lifecycle'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into the process of developing a predictive AI model first. Everything
    starts with a good use case, and ROI (return on investment) is top of mind when
    evaluating AI use cases. Think about pain points in your business or industry
    that could be solved by predicting an outcome. It is very important to always
    keep an eye on feasibility – whether you can procure the data you need, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve landed on a compelling value-driven use case, next up is picking
    algorithms. You’ve got endless options here – decision trees, neural nets, regressions,
    random forests, and on and on. It is very important not to be swayed by the bias
    for the latest and greatest and to focus on the core requirements of your data
    and use case to narrow the options down. You can always switch it up or add additional
    experiments as you iterate through your testing.
  prefs: []
  type: TYPE_NORMAL
- en: With a plan in place, now it is time to get your hands dirty with the data.
    Identifying sources, cleaning things up, and carrying out feature engineering
    is an art and, more often than not, the key to improving your model’s results.
    There is no shortcut for rigor here, unfortunately! Garbage in, garbage out, as
    they say. But once you’ve wrangled datasets you can rely on, then comes the fun
    part.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to work with your model. Define your evaluation process upfront, split
    data wisely, and start training various configurations. Don’t forget to monitor
    and tune based on validation performance. Then, once you’ve got your golden model,
    implement robust serving infrastructure so it scales without a hitch.
  prefs: []
  type: TYPE_NORMAL
- en: But wait, not so fast! Testing doesn’t end when models are in production. Collect
    performance data continuously, monitor for concept drifts, and retrain when needed.
    A solid predictive model requires ongoing feedback mechanisms, as shown via the
    arrow connecting **Model Enhancement** to **Testing** in *Figure 1.1*. There is
    no such thing as set and forget in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI use case development – simplified lifecycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](img/B22175_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Generative AI use case development simplified lifecycle'
  prefs: []
  type: TYPE_NORMAL
- en: The process of generative AI use case development is similar but not the same
    as in predictive AI; there are some common steps, but the order of tasks is different.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is the ideation of potential use cases. This selection needs
    to be balanced with business needs as satisfying them is our main objective.
  prefs: []
  type: TYPE_NORMAL
- en: With a clear problem definition in place, extensive analysis of published model
    benchmarks often informs the selection of a robust foundational model best suited
    for the task. In this step, it is worth asking ourselves the question is this
    use case better suited for a predictive model?
  prefs: []
  type: TYPE_NORMAL
- en: As foundational models provide capabilities out of the box, initial testing
    comes as a step early in the process. A structured testing methodology helps reveal
    innate strengths, weaknesses, and quirks of a specific model. Both quantitative
    metrics and qualitative human evaluations fuel iterative improvement throughout
    the full development lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to move to the art of prompt engineering. Prompting is the
    mechanism used to interact with LLMs. Techniques like chain-of-thought prompting,
    skeleton prompts, and retrieval augmentation build guardrails enabling more consistent,
    logical outputs.
  prefs: []
  type: TYPE_NORMAL
- en: If gaps remain after prompt optimization, model enhancement via fine-tuning
    and distillation offers a precision tool to adapt models closer to the target
    task.
  prefs: []
  type: TYPE_NORMAL
- en: In rare cases, pretraining a fully custom model from scratch is warranted when
    no existing model can viably serve the use case. However, it is important to keep
    in mind that due to the massive data requirements posed by model retraining, this
    task won’t be suitable for most use cases and teams; retraining a foundational
    model requires an extensive amount of data and processing power that makes the
    process unpractical from a financial and technical perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Above all, the interplay between evaluation and model improvement underscores
    the deeply empirical nature of advancing generative AI responsibly. Testing often
    reveals that better solutions come from creativity in problem framing rather than
    pure technological advances alone.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Predictive and generative AI development lifecycle side-by-side
    comparison'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the preceding figure, the development lifecycle is an iterative
    process that enables us to realize value from a given use case and technology
    type. Across the rest of this chapter and this book, we are going to focus on
    generative AI general concepts, some that are going to be familiar if you are
    experienced in predictive AI and others that are specific to this new field in
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: General generative AI concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When integrating generative AI into practical applications, it is important
    to have an understanding of concepts such as model architecture and training.
    In this section, we cover an overview of prominent concepts, including transformers,
    diffusion models, pre-training, and prompt engineering, that enable systems to
    generate impressively accurate text, images, audio, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these core concepts will equip you to make informed decisions
    when selecting foundations for your use cases. However, putting models into production
    requires further architectural considerations. We will be highlighting these decision
    points in the rest of the chapters in the book and in practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI model architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI models are based on specialized neural network architectures optimized
    for generative tasks. The two more widely known models are transformers and diffusion
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer models** are not a new concept. They were first introduced by
    Google in a 2017 paper called *Attention Is All You Need* ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
    The paper explains the **Transformer neural network architecture**, which is entirely
    based on attention mechanisms using the encoder and decoder concepts. This architecture
    enables models to identify relationships across an input text. By having these
    relationships, the model predicts the next token, leveraging its previous prediction
    as an input, creating this recursive loop to generate new content.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diffusion models** have drawn considerable interest as generative models
    due to their foundation in the physical processes of non-equilibrium thermodynamics.
    In physics, diffusion refers to the motion of particles from areas of high concentration
    to low concentration over time. Diffusion models try to mimic this concept in
    their training process. These models are trained through two phases: the **forward
    diffusion** process adds “noise” to the original training data, followed by a
    **reverse conditioning** process, which then learns how to remove noise in the
    reverse diffusion process. By learning this process, these models can produce
    samples by starting from pure noise and letting the reverse diffusion model clear
    away unnecessary “noise” and preserving the desired “generated” content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other types of deep learning architectures, such as **Generative Adversarial
    Networks** (**GANs**), allow you to generate synthetic data based on existing
    data. GANs are useful because they leverage two models: one to generate a synthetic
    output and another one that tries to predict if this output is real or fake.'
  prefs: []
  type: TYPE_NORMAL
- en: Through this iterative process, we can generate data that is indistinguishable
    from the real data but different enough to be used to enhance our training datasets.
    Another example of data generation architectures is **Variational Autoencoders**
    (**VAEs**), which use an encoder-decoder approach to generate new data samples
    resembling their training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques available to optimize foundational models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several techniques used to develop and optimize foundational models
    that have driven significant gains in AI capabilities, some of which are more
    complex than others from a technical and monetary perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-training** refers to fully training a model on a large dataset. It allows
    models to learn very broad representations from billions of data points, which
    help the model adapt to other closely related tasks. Popular methods include contrastive
    self-supervised pre-training on unlabeled data and pre-training on vast supervised
    data like the internet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning** adapts a pre-trained model’s already learned feature representations
    to perform a specific task. This only tunes some higher-level model layers rather
    than training from scratch. On the other hand, adapter tuning equips models with
    small, lightweight adapters that can rapidly tune to new tasks without interfering
    with existing capabilities. These pluggable adapters give a parameter-efficient
    way of accumulating knowledge across multiple tasks by learning task-specific
    behaviors while reusing the bulk of model weights. They help mitigate forgetting
    previous tasks and simplify personalization. For example, models may first be
    pre-trained on billions of text webpages to acquire general linguistic knowledge,
    before being fine-tuned on more domain-specific datasets for question answering,
    classification, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distillation** uses a “teacher” model to train a smaller “student” model
    to reproduce the performance of the larger pre-trained model at a lower cost and
    latency. Quantizing and compressing large models into efficient forms for deployments
    also helps optimize performance and cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combination of comprehensive pre-training followed by specialized fine-tuning,
    adapter tuning, and portable distillation has enabled unprecedented versatility
    of deep learning across domains. Each approach smartly reuses and transfers available
    knowledge, enabling the customization and scaling of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques to augment your foundational model responses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to architecture and training advances, progress in generative AI
    has been fueled by innovations in how these models are augmented by external data
    at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt engineering** tunes the text prompts provided to models to steer their
    generation quality, capabilities, and properties. Well-designed prompts guide
    the model to produce the desired output format, reduce ambiguity, and provide
    helpful contextual constraints. This allows simpler model architectures to solve
    complex problems by encoding human knowledge into the prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval augmented generation**, also known as **RAG**, enhances text generation
    through efficient retrieval of relevant knowledge from external stores. Models
    receive contextual pieces of information as “context” to be considered as additional
    input before generating its output. **Grounding** LLMs (large language models)
    refers to providing model-specific factual knowledge rather than just model parameters,
    enabling more accurate, knowledgeable, and specific language generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Together, these approaches augment basic predictive language models to become
    far more versatile, robust, and scalable. They reduce brittleness via tight integration
    of human knowledge and grounded learning rather than just statistical patterns.
    RAG handles the breadth and real-time retrieval of information, prompts provide
    depth and rules to the desired outputs, and grounding binds them to reality. We
    would highly encourage readers to get familiar with this topic, as it is an industry
    best practice to perform RAG and to ground your model to prevent it from hallucinating.
    A good start is the following paper: *Retrieval-Augmented Generation for Large
    Language Models: A Survey* ([https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997)).'
  prefs: []
  type: TYPE_NORMAL
- en: Constant evolution across the generative AI space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The generative AI space is characterized by relentless innovation and rapid
    advancement across model architectures, applications, and ethical considerations.
    As soon as one method or architecture shows promising results, hundreds of competing
    and complementary approaches emerge to push capabilities even further. Transformers
    gave way to BERT, which was outpaced by GPT-3, soon rivaled by image synthesizers
    like DALL-E, and now GPT-4 and Gemini are competing for the top spot. All of which
    happened in the past few years.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, we are seeing new modalities like audio, video, and 3D scene generation
    gaining vast popularity and usability. On the business front, new services are
    launched monthly, targeting media and entertainment, finance, healthcare, art,
    code, music, and more. However, considerations around ethics, access control,
    and legalities are key in order to maintain public trust.
  prefs: []
  type: TYPE_NORMAL
- en: One breakthrough enables several more, and each unlocks added potential. This
    self-fueling cycle arises from the very nature of AI – its ability to recursively
    assist innovation. The only certainty is that the field will look very different
    within months, not years. Maintaining awareness, responsiveness, and responsibility
    is critical amid this constant evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing generative AI integration patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now assume you already have a promising use case in mind. As I’m sure
    you would agree, clearly defining the use case is critical before proceeding further.
    You’ve already identified which foundational model provides acceptable performance
    for your needs. So now you’re starting to consider how GenAI fits into the application
    development process.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, there are two main workflows for integrating applications with
    GenAI. One is **real time**, where you’ll typically interact with an end user
    or AI agent, providing responses as prompts come in. The second is **batch processing**,
    where requirements are bundled up and processed in groups (batches).
  prefs: []
  type: TYPE_NORMAL
- en: A prime example of a real-time workflow would be a chatbot. Here, prompts from
    the user are processed and then sent to the model and the responses are returned
    immediately, as you need to consume the outputs without delay. On the other hand,
    consider a data enrichment use case for batch processing. You could collect multiple
    data points over time for later consumption after being enriched by the model
    in batches.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will explore these integration patterns through practical examples.
    This will help you to obtain hands-on experience with GenAI-driven applications
    and allow you to integrate these patterns in your own use cases.
  prefs: []
  type: TYPE_NORMAL
- en: By “integration pattern,” we refer to a standardized architectural approach
    for incorporating a technology into your application or system. In this context,
    integration patterns provide proven methods for connecting generative AI models
    to real-world software.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few key reasons why we need integration patterns when working with
    generative AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time savings**: Following established patterns allows developers to avoid
    reinventing the wheel for common integration challenges. This accelerates time
    to value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improving quality**: Leveraging best practices encoded in integration patterns
    leads to more robust, production-grade integrations. Things like scalability,
    security, and reliability are top of mind.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reducing risk**: Well-defined integration patterns enable developers to mitigate
    risks around performance, costs, and other pitfalls that can emerge when integrating
    new technologies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, integration patterns deliver templates and guardrails, so developers
    don’t have to start integration efforts from scratch. By relying on proven blueprints,
    readers can integrate generative AI more efficiently while avoiding common mistakes.
    This speeds up development cycles significantly and sets integrations up for long-term
    success.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered an overview of key concepts, techniques, and integration
    patterns related to generative AI. You now have a high-level background on prominent
    generative AI model architectures like transformers and diffusion models, as well
    as various methods for developing and enhancing these models, covering pre-training,
    fine-tuning, adapter tuning, distillation, prompt engineering, retrieval augmented
    generation, and grounding.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed how rapid innovation in generative AI leads to constant evolution,
    with new models and capabilities emerging at a fast pace. It emphasizes the need
    to keep pace with progress while ensuring ethical, responsible development.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we introduced common integration patterns for connecting generative
    AI to real-world applications, considering real-time use cases like chatbots as
    well as batch processing for data enrichment. Real examples were provided to demonstrate
    workflows for integrating generative models into production systems.
  prefs: []
  type: TYPE_NORMAL
- en: Innovation in AI has a very fast pace, demanding constant awareness, swift experimentation,
    and a responsible approach to harnessing the latest advances. This is particularly
    evident in the field of generative AI, where we’re witnessing a paradigm shift
    in AI-powered applications that allows for faster experimentation and development.
  prefs: []
  type: TYPE_NORMAL
- en: A wide array of techniques has emerged to enhance models’ capabilities and efficiency.
    These include pre-training, adapter tuning, distillation, and prompt engineering,
    each offering unique advantages in different scenarios. When it comes to integrating
    these AI models into practical applications, key patterns have emerged for both
    real-time workflows, such as chatbots, and batch processing tasks like data enrichment.
  prefs: []
  type: TYPE_NORMAL
- en: The art of crafting well-designed prompts has become crucial in constraining
    and steering model outputs effectively. Additionally, techniques like retrieval
    augmentation and grounding have proven invaluable in improving the accuracy of
    AI-generated content. The potential in blending predictive and generative approaches
    is a very interesting space. This combination leverages the strengths of both
    methodologies, allowing for custom modeling where sufficient data exists while
    utilizing generative foundations to enable rapid prototyping and innovation.
  prefs: []
  type: TYPE_NORMAL
- en: These core concepts empower informed decision-making when architecting generative
    AI systems. The integration patterns offer blueprints for connecting models to
    practical applications across diverse domains.
  prefs: []
  type: TYPE_NORMAL
- en: Harnessing the power of LLMs begins with identifying the right use cases where
    they can drive value for your business. In the next chapter, we will present a
    framework and examples for categorizing LLM use cases based on projected business
    value.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore identifying use cases that can be solved
    with Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genpat](https://packt.link/genpat)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code134841911667913109.png)'
  prefs: []
  type: TYPE_IMG
