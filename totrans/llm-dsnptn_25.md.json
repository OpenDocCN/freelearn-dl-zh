["```py\ndef task_decomposition_prompt(task, available_tools):\n    prompt = f\"\"\"Given the following complex task:\n{task}\nAnd the following available tools:\n{' '.join(f'- {tool}' for tool in available_tools)}\nPlease break down the task into smaller, logical steps. For each step, indicate if a specific tool should be used. If no tool is needed, explain the reasoning required.\nYour task decomposition:\nStep 1:\nStep 2:\nStep 3:\n...\nEnsure that the steps are in a logical order and cover all aspects of the task.\n\"\"\"\n    return prompt\n# Example usage\ntask = \"Analyze the sentiment of tweets about a new product launch and create a summary report with visualizations.\"\navailable_tools = [\"Twitter API\", \"Sentiment Analysis Model\",\n    \"Data Visualization Library\"]\nprompt = task_decomposition_prompt(task, available_tools)\nprint(prompt)\n```", "```py\n    import requests\n    from textblob import TextBlob\n    import matplotlib.pyplot as plt\n    class ToolKit:\n        def __init__(self):\n            self.tools = {\n                \"Twitter API\": self.fetch_tweets,\n                \"Sentiment Analysis\": self.analyze_sentiment,\n                \"Data Visualization\": self.create_visualization\n            }\n    ```", "```py\n        def fetch_tweets(self, query, count=100):\n            return [f\"Tweet about {query}\" for _ in range(count)]\n        def analyze_sentiment(self, texts):\n            sentiments = [TextBlob(text).sentiment.polarity\n                for text in texts]\n            return sentiments\n        def create_visualization(self, data, title):\n            plt.figure(figsize=(10, 6))\n            plt.hist(data, bins=20)\n            plt.title(title)\n            plt.xlabel(\"Sentiment\")\n            plt.ylabel(\"Frequency\")\n            plt.savefig(\"sentiment_visualization.png\")\n            return \"sentiment_visualization.png\"\n    ```", "```py\n        def use_tool(self, tool_name, *args, kwargs):\n            if tool_name in self.tools:\n                return self.tools[tool_name](*args, kwargs)\n            else:\n                return f\"Error: Tool '{tool_name}' not found.\"\n    ```", "```py\n    toolkit = ToolKit()\n    tweets = toolkit.use_tool(\n        \"Twitter API\", \"new product launch\", count=50\n    )\n    sentiments = toolkit.use_tool(\"Sentiment Analysis\", tweets)\n    visualization = toolkit.use_tool(\n        \"Data Visualization\", sentiments,\n        \"Sentiment Analysis of Product Launch Tweets\"\n    )\n    print(f\"Generated visualization: {visualization}\")\n    ```", "```py\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    def auto_tool_use(model, tokenizer, task, toolkit):\n        # Generate task decomposition\n        decomposition_prompt = task_decomposition_prompt(\n            task, toolkit.tools.keys()\n        )\n        inputs = tokenizer(decomposition_prompt,\n            return_tensors=\"pt\")\n        outputs = model.generate(\n            inputs, max_length=1000, num_return_sequences=1\n        )\n        decomposition = tokenizer.decode(outputs[0],\n            skip_special_tokens=True)\n        # Parse decomposition and execute tools\n        steps = parse_steps(decomposition)\n        results = []\n        for step in steps:\n            if step['tool']:\n                result = toolkit.use_tool(step['tool'],\n                    *step['args'])\n            else:\n                result = f\"Reasoning: {step['reasoning']}\"\n            results.append(result)\n    ```", "```py\n        report_prompt = f\"Task: {task}\\n\\nSteps and Results:\\n\"\n        for i, (step, result) in enumerate(zip(steps, results), 1):\n            report_prompt += (\n                f\"Step {i}: {step['description']}\\n\"\n                f\"Result: {result}\\n\\n\"\n            )\n        report_prompt += \"Please provide a comprehensive report summarizing the results and insights.\"\n        inputs = tokenizer(report_prompt, return_tensors=\"pt\")\n        outputs = model.generate(\n            inputs, max_length=1500, num_return_sequences=1\n        )\n        report = tokenizer.decode(outputs[0],\n            skip_special_tokens=True)\n        return report\n    ```", "```py\n    def parse_steps(decomposition):\n        steps = []\n        for line in decomposition.split('\\n'):\n            if line.startswith(\"Step\"):\n                tool = \"Twitter API\" if \"Twitter\" in line else \\\n                       \"Sentiment Analysis\" if \"sentiment\" in line else \\\n                       \"Data Visualization\" if \"visualization\" in line else None\n                steps.append({\n                    'description': line,\n                    'tool': tool,\n                    'args': [],\n                    'reasoning': line if not tool else \"\"\n                })\n        return steps\n    ```", "```py\n    model_name = \"llama3.3\"  # Replace with your preferred model\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    task = \"Analyze the sentiment of tweets about a new product launch and create a summary report with visualizations.\"\n    toolkit = ToolKit()\n    report = auto_tool_use(model, tokenizer, task, toolkit)\n    print(report)\n    ```", "```py\ndef market_analysis(model, tokenizer, toolkit, product_name):\n    task = f\"\"\"Conduct a comprehensive market analysis for the product: {product_name}.\n    Include competitor analysis, sentiment analysis of customer reviews, and market trends visualization.\"\"\"\n    analysis_report = auto_tool_use(model, tokenizer, task, toolkit)\n    return analysis_report\n# Example usage\nproduct_name = \"SmartHome AI Assistant\"\nmarket_report = market_analysis(model, tokenizer, toolkit,\n        product_name)\nprint(market_report)\n```", "```py\ndef evaluate_multistep_tooluse(\n    task, generated_report, ground_truth, criteria\n):\n    scores = {}\n    for criterion in criteria:\n        scores[criterion] = evaluate_criterion(generated_report,\n            ground_truth, criterion)\n    # Evaluate tool use effectiveness\n    tool_use_score = evaluate_tool_use(task, generated_report)\n    scores['Tool Use Effectiveness'] = tool_use_score\n    return scores\ndef evaluate_criterion(generated_report, ground_truth, criterion):\n    # Implement criterion-specific evaluation logic\n    # This is a placeholder implementation\n    return 0.0  # Return a score between 0 and 1\ndef evaluate_tool_use(task, generated_report):\n    # Implement logic to evaluate how effectively tools were used\n    # This could involve checking for specific tool outputs or insights\n    # This is a placeholder implementation\n    return 0.0  # Return a score between 0 and 1\n# Example usage\ncriteria = ['Accuracy', 'Comprehensiveness', 'Insight Quality',\n    'Logical Flow']\nground_truth = \"Ideal market analysis report content...\"  # This would be a benchmark report\nevaluation_scores = evaluate_multistep_tooluse(task, market_report,\n    ground_truth, criteria)\nprint(\"Evaluation Scores:\", evaluation_scores)\n```", "```py\ndef self_correcting_tooluse(\n    model, tokenizer, task, toolkit, max_attempts=3\n):\n    for attempt in range(max_attempts):\n        report = auto_tool_use(model, tokenizer, task, toolkit)\n        # Prompt the model to evaluate its own work\n        evaluation_prompt = f\"\"\"Task: {task}\nGenerated Report:\n{report}\nPlease evaluate the quality and completeness of this report. Identify any errors, omissions, or areas for improvement. If necessary, suggest specific steps to enhance the analysis.\nYour evaluation:\n\"\"\"\n        inputs = tokenizer(evaluation_prompt, return_tensors=\"pt\")\n        outputs = model.generate(\n            inputs, max_length=1000, num_return_sequences=1\n        )\n        evaluation = tokenizer.decode(outputs[0],\n            skip_special_tokens=True)\n        if \"satisfactory\" in evaluation.lower() and \"no major issues\" in evaluation.lower():\n            break\n        # If issues were identified, use the evaluation to improve the next attempt\n        task += f\"\\n\\nPrevious attempt evaluation: {evaluation}\\nPlease address these issues in your next attempt.\"\n    return report\n# Example usage\nfinal_report = self_correcting_tooluse(model, tokenizer, task,\n    toolkit)\nprint(final_report)\n```"]