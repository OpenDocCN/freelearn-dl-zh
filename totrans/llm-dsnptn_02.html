<html><head></head><body><div><div><div><h1 id="_idParaDest-27" class="chapter-number"><a id="_idTextAnchor035"/>2</h1>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor036"/>Data Cleaning for LLM Training</h1>
			<p>In this chapter, we’ll dive into the <strong class="bold">data cleaning</strong> pattern for LLM training.</p>
			<p>Clean, high-quality data is the foundation of robust and reliable language models. We’ll explore common data quality issues, preprocessing techniques, and strategies for handling diverse data types. <em class="italic">Figure 2</em><em class="italic">.1</em> depicts a data cleaning pipeline specifically designed for processing raw text data before it’s used to train language models.</p>
			<div><div><img src="img/B31249_02_01.jpg" alt="Figure 2.1 – Data cleaning pipeline" width="307" height="843"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Data cleaning pipeline</p>
			<p>The process begins with an initial data quality check to assess the raw data’s suitability. Following this, text preprocessing and deduplication steps are applied to refine and streamline the dataset. If the data fails to meet the required standards at any point, it is rerouted through an automated cleaning pipeline for additional processing. Successful completion of this stage leads to data validation to ensure the dataset’s integrity and compliance with training standards. If the data passes validation, it is marked as clean and ready for use in language model training, ensuring high-quality input for effective model development.</p>
			<p>By the end of this chapter, you’ll be equipped with practical tools and techniques to clean your data for LLM training.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Understanding the importance of clean data</li>
				<li>Common data quality issues in language datasets</li>
				<li>Text preprocessing techniques for LLMs</li>
				<li>Handling multilingual and code-mixed data</li>
				<li>Deduplication strategies for large text corpora</li>
				<li>Automated data cleaning pipelines</li>
				<li>Data validation and quality assurance</li>
			</ul>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor037"/>Understanding the importance of clean data</h1>
			<p>The quality of data used in training LLMs directly<a id="_idIndexMarker041"/> impacts their performance and reliability. When we train LLMs on noisy or inconsistent data, we risk introducing bias, errors, and inconsistency into the model’s learned representations and outputs.</p>
			<p>To illustrate the impact of data quality on LLM performance, we can use a simple Python script to compare the perplexity scores of models trained on clean and noisy data.</p>
			<ol>
				<li>First, install the necessary packages and import them:<pre class="source-code">
<strong class="bold">pip install torch</strong>
<strong class="bold">pip install transformers</strong>
<strong class="bold">import torch</strong>
<code>torch</code>) is a powerful deep learning framework that provides dynamic computational graphs, GPU acceleration, and extensive neural network building blocks, making it popular for machine learning research and development. The <code>transformers</code> package, developed by Hugging Face, complements PyTorch by providing a comprehensive library of pre-trained transformer models (such as BER, GPT, and T5) and tools for natural language processing tasks. Together, these packages offer a robust ecosystem in which <code>torch</code> provides the foundational deep learning <a id="_idIndexMarker042"/>operations, tensor computations, and automatic differentiation capabilities, while <code>transformers</code> provides high-level abstractions for working with state-of-the-art language models, including functions for tokenization, model fine-tuning, and inference.</p></li>				<li>Then, define the initial part of the function:<pre class="source-code">
def calculate_perplexity(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
    outputs = model(inputs, labels=inputs["input_ids"])
    return torch.exp(outputs.loss).item()
model = GPT4LMHeadModel.from_pretrained("GPT4")
tokenizer = GPT4Tokenizer.from_pretrained("GPT4")</pre><p class="list-inset">The <code>calculate_perplexity</code> function tokenizes the input text into PyTorch tensors using the provided tokenizer. It then passes the tokenized input to the model with <code>input_ids</code> also used as labels, allowing the model to compute a loss representing prediction error. This loss is exponentiated to derive a scalar perplexity score and returned as a Python float.</p><p class="list-inset">The second part of the code initializes a language model and tokenizer using <code>GPT4LMHeadModel.from_pretrained("GPT4")</code> and <code>GPT4Tokenizer.from_pretrained("GPT4")</code>, which load the model and tokenizer weights from a pre-trained source identified as <code>"GPT4"</code>.</p></li>			</ol>
			<p class="callout-heading">Perplexity</p>
			<p class="callout"><strong class="bold">Perplexity</strong> is a measure used to evaluate<a id="_idIndexMarker043"/> language models. It quantifies how well a probability model predicts a sample.</p>
			<p class="callout">Lower perplexity indicates that the model is more confident in its predictions and considers the text more likely or “normal”. Higher perplexity suggests that the model finds the text more surprising or unusual.</p>
			<ol>
				<li value="3">Here are example<a id="_idIndexMarker044"/> texts:<pre class="source-code">
clean_text = "The quick brown fox jumps over the lazy dog."
noisy_text = "Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g."
clean_text</strong> and <code>noisy_text</code>. <code>clean_text</code> holds a standard English sentence, while <code>noisy_text</code> contains the same sentence with deliberate character substitutions, making it “noisy” or corrupted. The <code>clean_text</code> and <code>noisy_text</code> examples are used to evaluate a language model’s perplexity, where <code>clean_text</code> provides a baseline for ideal text prediction and <code>noisy_text</code> assesses the model’s robustness to real-world data corruption; by comparing the perplexity scores, we determine how well the model handles noisy input and its suitability for applications where text data is not always perfectly formatted.</pre></li>				<li>Finally, calculate perplexity and print the results:<pre class="source-code">
clean_perplexity = calculate_perplexity(model, tokenizer,
    clean_text)
noisy_perplexity = calculate_perplexity(model, tokenizer,
    noisy_text)
print(f"Clean text perplexity: {clean_perplexity:.2f}")
print(f"Noisy text perplexity: {noisy_perplexity:.2f}")</pre></li>			</ol>
			<p>This script demonstrates how <a id="_idIndexMarker045"/>even small amounts of noise in the input data can significantly impact the model’s perplexity.</p>
			<p>The perplexity score is calculated as the exponential of the cross-entropy loss. In this code, it’s computed using <code>torch.exp(outputs.loss).item()</code>.</p>
			<p>Here are our possible outcomes:</p>
			<ul>
				<li><code>The quick brown fox jumps over the lazy dog</code> is a common, grammatically <a id="_idIndexMarker046"/>correct English sentence. The clean text perplexity might be something like <code>10.25</code>.</li>
				<li><code>Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g</code> contains <a id="_idIndexMarker047"/>numbers and symbols in place of letters, making it less common and more difficult for the model to predict. The noisy text perplexity might be something like <code>52.87</code>.</li>
			</ul>
			<p>The exact numbers will depend on the specific model and tokenizer used, but the noisy text should consistently have a higher perplexity score than the clean text.</p>
			<p>This difference in scores demonstrates the model’s ability to distinguish between standard, easily predictable text and unusual, harder-to-predict text. It’s a useful metric for tasks such as detecting machine-generated or tampered text, as such text often has higher perplexity scores compared to human-written text.</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor038"/>Common data quality issues in language datasets</h1>
			<p>Language datasets often contain various<a id="_idIndexMarker048"/> quality issues that can negatively impact LLM training:</p>
			<ul>
				<li>Spelling and grammatical errors can introduce noise and inconsistencies in the learned representations.</li>
				<li>Inconsistent formatting can lead to unnecessary complexity in the model’s learned patterns.</li>
				<li>Redundant data can cause models to overfit to specific patterns or bias present in the duplicates.</li>
				<li>Irrelevant or low-quality content can dilute the useful information in the dataset.</li>
				<li>Incomplete or truncated sentences can lead to models learning incomplete language structures.</li>
				<li>Code-switching and mixed languages can confuse models trained for specific languages.</li>
				<li><strong class="bold">Personally identifiable information</strong> (<strong class="bold">PII</strong>) raises privacy concerns and can lead to the memorization of<a id="_idIndexMarker049"/> sensitive data.</li>
			</ul>
			<p>To detect these issues, we can use various Python libraries and techniques. Here’s an example using spaCy for basic text quality checks:</p>
			<ol>
				<li>Provide the imports and an overall function definition:<pre class="source-code">
import spacy
from collections import Counter
# Load spaCy model
nlp = spacy.load("en_core_web_sm")
def analyze_text_quality(text):
    doc = nlp(text)</pre></li>				<li>Check for spelling errors (using spaCy’s built-in spell checker):<pre class="source-code">
    misspelled = [
        token.text for token in doc if token._.is_misspelled
    ]</pre></li>				<li>Check for grammatical<a id="_idIndexMarker050"/> issues (a simplistic approach using parts of speech (pos) tags):<pre class="source-code">
pos_counts = Counter(token.pos_ for token in doc)
grammar_score = pos_counts['NOUN'] + pos_counts['VERB'] 
    + pos_counts['ADJ'] + pos_counts['ADV']</pre><p class="list-inset"><strong class="bold">Parts of speech</strong> (<strong class="bold">POS</strong>) tags are labels assigned to each word in a sentence to indicate its grammatical role. These <a id="_idIndexMarker051"/>tags help systems to understand the syntactic structure of sentences and are used in tasks such as parsing, machine translation, sentiment analysis, and information extraction. Each tag corresponds to a POS such as a noun, verb, or adjective, often with finer-grained distinctions to capture tense, number, or function.</p></li>				<li>Check for sentence completeness:<pre class="source-code">
incomplete_sentences = [
    sent.text for sent in doc.sents if len(sent) &lt; 3
]
    return {
        "misspelled_words": misspelled,
        "grammar_score": grammar_score,
        "incomplete_sentences": incomplete_sentences
    }</pre></li>				<li>Here’s an example usage of the code:<pre class="source-code">
text = "This iz a smple txt with sum issues. Incomplet"
quality_report = analyze_text_quality(text)
print(quality_report)</pre></li>			</ol>
			<p>The script provided in <em class="italic">steps 1 to 5</em> illustrates<a id="_idIndexMarker052"/> a basic framework for identifying some common text quality issues. We will address other quality issues in the following sections.</p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor039"/>Text preprocessing techniques for LLMs</h1>
			<p>Effective text preprocessing is <a id="_idIndexMarker053"/>crucial for preparing data for LLM training. We employ various techniques, including lowercasing, punctuation handling, whitespace normalization, special character handling, <strong class="bold">tokenization</strong>, number normalization, and contraction expansion. Tokenization is the<a id="_idIndexMarker054"/> process of breaking text into smaller units for further analysis or processing. Tokens are the smallest meaningful units of text in natural language processing. They can be words, but they could also include punctuation, numbers, or other elements depending on the tokenization strategy.</p>
			<p>In addition, <strong class="bold">subword tokenization</strong> is an advanced text <a id="_idIndexMarker055"/>processing technique that breaks words into smaller meaningful units (subwords), enabling more efficient handling of rare words, compound words, and morphological variations in natural language processing tasks. Unlike traditional word-level tokenization, subword tokenization can identify common prefixes, suffixes, and root words, allowing models to understand and process previously unseen words by recognizing their familiar components.</p>
			<p>As an example, consider the word “unbelievably”. Traditional word-level tokenization would treat this as a single token. If the model has never seen this word before, it may struggle to interpret it correctly. In contrast, subword tokenization would break it down into smaller components such as “un”, “believ”, and “ably”. These subwords are more likely to appear across different contexts—“un-” in “unlikely”, “believ” in “believe”, “ably” in “capably”—allowing the model to derive meaning even if it encounters “unbelievably” for the first time. This decomposition enhances generalization, reduces vocabulary size, and improves the model’s ability to handle rare or morphologically complex words.</p>
			<p>Popular subword tokenization <a id="_idIndexMarker056"/>algorithms include <strong class="bold">byte pair encoding</strong> (<strong class="bold">BPE</strong>), WordPiece, and SentencePiece, which learn to identify frequently occurring character sequences in a training corpus and create a vocabulary of subword tokens. This approach is particularly valuable<a id="_idIndexMarker057"/> for handling morphologically rich languages, reducing vocabulary size while maintaining semantic meaning, and has become fundamental in modern language models such as Gemini, Claude, GPT, and other transformer-based architectures.</p>
			<p>These methods help clean and standardize the text data, reducing noise and improving the model’s ability to generalize. Here’s a Python script demonstrating these preprocessing techniques:</p>
			<ol>
				<li>First, import the necessary Python packages:<pre class="source-code">
import unicodedata
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')</pre></li>				<li>Then, define the overall preprocessing function:<pre class="source-code">
def preprocess_text(text):
    # Lowercase the text
    text = text.lower()
    # Normalize unicode characters
    text = unicodedata.normalize(
        'NFKD', text
    ).encode(
        'ascii', 'ignore'
    ).decode('utf-8')
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Normalize whitespace
    text = ' '.join(text.split())
    # Tokenize :
   tokens = word_tokenize(text)</pre></li>				<li>Remove stopwords (stopwords are common words such as “the”, “is”, and “at”) that are often removed in <a id="_idIndexMarker058"/>text processing as they carry little semantic meaning):<pre class="source-code">
    stop_words = set(stopwords.words('english'))
    tokens = [
        token for token in tokens if token not in stop_words
    ]
    # Join tokens back into text
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text</pre></li>				<li>Here’s an example usage of the code:<pre class="source-code">
raw_text = "This is an EXAMPLE of text preprocessing... It's quite useful!"
cleaned_text = preprocess_text(raw_text)
print(f"Original: {raw_text}")
print(f"Preprocessed: {cleaned_text}")</pre></li>			</ol>
			<p>This script demonstrates basic text<a id="_idIndexMarker059"/> preprocessing techniques. For LLM training, we might need to adapt these techniques based on the specific requirements of the model and the dataset.</p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor040"/>Handling multilingual and code-mixed data</h1>
			<p>LLMs often encounter multilingual and code-mixed data, which is text that blends two or more languages within a single sentence or conversation. This presents a challenge as LLMs must interpret linguistic nuances, grammar, and semantic connections across multiple languages. To handle code-mixed data, LLMs need to learn language switching, vocabulary and syntax variations, and maintain coherent responses, which demands strong language modeling and multilingual training data.</p>
			<p>We need to implement strategies to handle these scenarios effectively. The following steps are needed because they create cleaner, more consistent training data that helps LLMs better understand and process text across different languages and mixed-language scenarios, ultimately improving their performance in real-world applications where language mixing is common.</p>
			<p>For multilingual data, certain tasks<a id="_idIndexMarker060"/> are crucial:</p>
			<ul>
				<li><strong class="bold">Language identification</strong>: Detects the primary language of each text sample</li>
				<li><strong class="bold">Script normalization</strong>: Converts text to a consistent script (e.g., transliteration)</li>
				<li><strong class="bold">Language-specific preprocessing</strong>: Applies language-specific tokenization and normalization</li>
			</ul>
			<p>Meanwhile, you should carry out the following <a id="_idIndexMarker061"/>steps for code-mixed data:</p>
			<ul>
				<li><strong class="bold">Token-level language identification</strong>: Identifies the language of individual tokens</li>
				<li><strong class="bold">Consistency enforcement</strong>: Ensures consistent handling of code-switching patterns</li>
			</ul>
			<p>Here’s a Python script demonstrating language detection and script normalization.</p>
			<ol>
				<li>Let’s provide the imports and the overall function definition:<pre class="source-code">
from langdetect import detect
from unidecode import unidecode
from nltk import word_tokenize
import nltk
# Download required NLTK data
nltk.download('punkt')
def handle_multilingual_text(text):
    # Detect language
    try:
        lang = detect(text)
    except:
        lang = 'unknown'
    # Transliterate non-ASCII characters
    transliterated_text = unidecode(text)</pre></li>				<li>Tokenize (using NLTK for simplicity, but consider language-specific tokenizers):<pre class="source-code">
    tokens = word_tokenize(transliterated_text)
    return {
        'original': text,
        'language': lang,
        'transliterated': transliterated_text,
        'tokens': tokens
    }</pre></li>				<li>Here’s the example <a id="_idIndexMarker062"/>usage:<pre class="source-code">
texts = [
    "This is English text.",
    "Dies ist deutscher Text.",
    "これは日本語のテキストです。",
    "This is mixed language text avec un peu de français."
]
for text in texts:
    result = handle_multilingual_text(text)
    print(f"Original: {result['original']}")
    print(f"Detected Language: {result['language']}")
    print(f"Transliterated: {result['transliterated']}")
    print(f"Tokens: {result['tokens']}\n")</pre><p class="list-inset">This code iterates through a list of multilingual text strings, including English, German, Japanese, and a code-mixed example, and for each string, it calls a <code>handle_multilingual_text</code> function (presumably defined elsewhere) to process the text, returning a dictionary <a id="_idIndexMarker063"/>containing the original text, detected language, transliterated text (if applicable), and tokenized words, which are then printed to the console.</p></li>			</ol>
			<p>Putting the preceding three code blocks together, we provide a basic framework for handling multilingual text. For more advanced scenarios, we would use specialized libraries such as Polyglot for language-specific processing and code-mixing analysis when multiple languages are used in the same conversation (<a href="https://dl.acm.org/doi/10.1145/3544548.3581445">https://dl.acm.org/doi/10.1145/3544548.3581445</a>).</p>
			<p>For example, Polyglot includes built-in language detection, named entity recognition, sentiment analysis, and transliteration capabilities across multiple languages, all while maintaining a relatively lightweight footprint compared to larger multilingual frameworks. The library is particularly valuable for projects dealing with international text data, as it provides consistent APIs across languages and comes with pre-trained models, making it an efficient choice for multilingual text analysis tasks without the complexity of managing multiple language-specific tools.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor041"/>Deduplication strategies for large text corpora</h1>
			<p>Deduplication is a critical step in preparing large text corpora for LLM training. Duplicate content can lead to biased models and wasted computational resources. We employ various strategies to identify and remove <a id="_idIndexMarker064"/>duplicates efficiently:</p>
			<ul>
				<li><strong class="bold">Exact match deduplication</strong>: Remove identical text samples.</li>
				<li><strong class="bold">Near-duplicate detection</strong>: Identify and remove highly similar text samples.</li>
				<li><strong class="bold">Shingling</strong>: Create small overlapping sequences of words for comparison.</li>
				<li><strong class="bold">Locality sensitive hashing</strong>: Efficiently find similar items in large datasets.</li>
			</ul>
			<p>The following sections show examples of each strategy.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor042"/>Exact match deduplication</h2>
			<p><strong class="bold">Scenario</strong>: You have a list of customer <a id="_idIndexMarker065"/>addresses:</p>
			<ul>
				<li><strong class="bold">Data</strong>:<ul><li>“123 Main St, Anytown, CA 91234”</li><li>“456 Oak Ave, Somecity, NY 56789”</li><li>“123 Main St, Anytown, CA 91234”</li></ul></li>
				<li><strong class="bold">Result</strong>: The third entry, “123 Main St, Anytown, CA 91234”, is removed because it is an exact duplicate of the first entry.</li>
				<li><strong class="bold">Remaining data</strong>:<ul><li>“123 Main St, Anytown, CA 91234”</li><li>“456 Oak Ave, Somecity, NY 56789”</li></ul></li>
			</ul>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor043"/>Near-duplicate detection</h2>
			<p><strong class="bold">Scenario</strong>: You have a collection of news articles:</p>
			<ul>
				<li><strong class="bold">Data</strong>:<ul><li>Article 1: “The company<a id="_idIndexMarker066"/> reported a significant increase in quarterly profits.”</li><li>Article 2: “Quarterly profits saw a large increase, the company reports.”</li></ul></li>
				<li><strong class="bold">Result</strong>: A near-duplicate detection algorithm determines that these articles are highly similar in content, even though the wording is slightly different. One of the articles is removed, based on a similarity threshold.</li>
				<li><strong class="bold">Remaining data</strong>: “The company reported a significant increase in quarterly profits.”</li>
			</ul>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor044"/>Shingling</h2>
			<p><strong class="bold">Scenario</strong>: You want to compare the<a id="_idIndexMarker067"/> similarity of text documents:</p>
			<ul>
				<li><strong class="bold">Data</strong>:<ul><li>Document 1: “The quick brown fox jumps over the lazy dog.”</li><li>k=3 word shingle.</li></ul></li>
				<li><strong class="bold">Result</strong>: The shingles generated are as follows:<ul><li>“The quick brown”</li><li>“quick brown fox”</li><li>“brown fox jumps”</li><li>“fox jumps over”</li><li>“jumps over the”</li><li>“over the lazy”</li><li>“the lazy dog”</li></ul><p class="list-inset">The document is then represented by the set of those shingles. Then another document could be turned into shingles, and the sets of shingles can be compared.</p></li>
			</ul>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor045"/>Locality Sensitive Hashing (LSH)</h2>
			<p><strong class="bold">Scenario</strong>: You have a very large database <a id="_idIndexMarker068"/>of online product <a id="_idIndexMarker069"/>descriptions:</p>
			<ul>
				<li><strong class="bold">Process</strong>:<ul><li>LSH is used to hash the product descriptions.</li><li>Similar product descriptions are more likely to be hashed into the same “buckets.”</li><li>Only the descriptions within the same buckets are then compared in detail to find near duplicates.</li></ul></li>
				<li><strong class="bold">Result</strong>: Instead of comparing every product description to every other description, LSH narrows down<a id="_idIndexMarker070"/> the comparisons to only those descriptions within the same <a id="_idIndexMarker071"/>buckets, greatly increasing the efficiency of finding near duplicates.</li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">Deduplicating is computationally <a id="_idIndexMarker072"/>very expensive, so techniques such<a id="_idIndexMarker073"/> as minhashing or parallel processing can be used to scale the deduplicating with the increase in <a id="_idIndexMarker074"/>corpus data.</p>
			<p class="callout">Minhashing efficiently approximates the similarity between documents using smaller, more manageable representations, reducing the computational load. Parallel processing further distributes the deduplication task across multiple processors or machines, allowing for simultaneous comparisons and significantly speeding up the overall process, thus enabling effective deduplication of massive corpora.</p>
			<p>Here’s a Python script demonstrating basic deduplication techniques:</p>
			<ol>
				<li>First, define the overall function:<pre class="source-code">
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
def deduplicate_corpus(corpus, similarity_threshold=0.9):
    # Create TF-IDF vectorizer
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    # Compute pairwise similarities
    similarity_matrix = cosine_similarity(tfidf_matrix)
TfidfVectorizer</strong> to convert a text corpus into a numerical <code>cosine_similarity</code> to calculate the pairwise similarity between all documents in the <a id="_idIndexMarker076"/>corpus, providing a matrix of similarity scores that can be used to identify near-duplicate texts based on a specified threshold.</pre></li>				<li>Then, find duplicates:<pre class="source-code">
    duplicates = set()
    for i in range(len(corpus)):
        for j in range(i + 1, len(corpus)):
            if similarity_matrix[i, j] &gt; similarity_threshold:
                duplicates.add(j)</pre></li>				<li>Create a deduplicated corpus:<pre class="source-code">
    deduplicated_corpus = [
        doc for i, doc in enumerate(corpus) 
        if i not in duplicates
    ]
    return deduplicated_corpus</pre></li>				<li>Here’s an example:<pre class="source-code">
corpus = [
    "The quick brown fox jumps over the lazy dog.",
    "A fast auburn fox leaps above the sleepy canine.",
    "The quick brown fox jumps over the lazy dog.",
    "An entirely different sentence about cats.",
]
deduplicated = deduplicate_corpus(corpus)
print(f"Original corpus size: {len(corpus)}")
print(f"Deduplicated corpus size: {len(deduplicated)}")
print("Deduplicated corpus:")
for doc in deduplicated:
    print(f"- {doc}")</pre></li>			</ol>
			<p>This script demonstrates a basic near-duplicate detection approach using TF-IDF and <strong class="bold">cosine similarity</strong>. TF-IDF is a numerical <a id="_idIndexMarker077"/>statistic used to reflect the importance of words in documents within a collection. It combines how often a word appears in a document (TF) with how unique it is <a id="_idIndexMarker078"/>across all documents (IDF). TF-IDF converts text into numerical vectors, enabling mathematical comparisons between documents, which is crucial for the similarity calculations used in the deduplication process. For large-scale deduplication, we would use more efficient algorithms and distributed computing techniques.</p>
			<p>Here, the similarity threshold of <code>0.9</code> used in the deduplication function code determines how similar documents must be to be considered duplicates, with 90% similarity required by default. This value can be adjusted based on specific use cases—a higher threshold (e.g., <code>0.95</code> or <code>1</code>, which is maximum) is stricter and reduces false positives, while a lower threshold (e.g., <code>0</code> which is minimum or <code>0.8</code>) is more lenient and catches more potential duplicates.</p>
			<p>Next, let’s discuss automated data cleaning pipelines.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor046"/>Automated data cleaning pipelines</h1>
			<p>To handle the massive datasets required for LLM training, we need to implement automated data cleaning pipelines. These pipelines should be scalable, efficient, and capable of handling various data quality issues.</p>
			<p>The key components of an automated data cleaning pipeline are as follows:</p>
			<ul>
				<li><strong class="bold">Data ingestion</strong>: Efficiently load and <a id="_idIndexMarker079"/>parse large text corpora.</li>
				<li><strong class="bold">Quality assessment</strong>: Automatically detect and flag data quality issues.</li>
				<li><strong class="bold">Preprocessing</strong>: Apply text cleaning and normalization techniques.</li>
				<li><strong class="bold">Deduplication</strong>: Remove exact and near-duplicate content.</li>
				<li><strong class="bold">Filtering</strong>: Remove low-quality or irrelevant samples based on predefined criteria.</li>
				<li><strong class="bold">Validation</strong>: Ensure the cleaned data meets quality standards.</li>
				<li><strong class="bold">Output</strong>: Save the cleaned data in an appropriate format for LLM training.</li>
			</ul>
			<p>Here’s a Python script outlining a basic automated data cleaning pipeline:</p>
			<ol>
				<li>We will start by defining the overall class structure:<pre class="source-code">
import pandas as pd
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
# Download required NLTK data
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
class DataCleaningPipeline:
    def __init__(
        self, similarity_threshold=0.9, min_length=10,
        max_length=1000
    ):
        self.similarity_threshold = similarity_threshold
        self.min_length = min_length
        self.max_length = max_length
        self.vectorizer = TfidfVectorizer(stop_words='english')
DataCleaningPipeline</strong> class that encapsulates text preprocessing, length filtering, and near-duplicate removal functionalities. It initializes<a id="_idIndexMarker080"/> with configurable parameters such as similarity threshold and text length constraints, leverages NLTK for stop word removal, and employs scikit-learn’s <code>TfidfVectorizer</code> and <code>cosine_similarity</code> to identify and eliminate similar text entries from a pandas DataFrame.</pre></li>				<li>Then, we will define a preprocess function:<pre class="source-code">
    def preprocess(self, text):
        # Basic preprocessing
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        tokens = [
            word for word in text.split()
            if word not in stop_words
        ]
        return ' '.join(tokens)
    def filter_by_length(self, df):
        return df[
            (df['text'].str.len() &gt;= self.min_length) &amp;
            (df['text'].str.len() &lt;= self.max_length)
        ]
 methods within a class for text processing.</pre><ul><li><code>preprocess</code>: This method takes a text string as input, converts it to lowercase, removes punctuation, splits it into words, filters out common stop words, and then joins the remaining words into a string, effectively cleaning and normalizing the text.</li><li><code>filter_by_length</code>: This method takes a pandas DataFrame containing a <code>text</code> column and filters the DataFrame to include only rows where the length of the <code>text</code> column falls within a specified minimum and maximum length, allowing the selection of text samples within a desired character range.</li></ul></li>				<li>We then define the <a id="_idIndexMarker082"/>deduplication function:<pre class="source-code">
def deduplicate(self, df):
    tfidf_matrix = self.vectorizer.fit_transform(df['text'])
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    duplicates = set()
    for i in range(len(df)):
        for j in range(i + 1, len(df)):
            if similarity_matrix[i, j] &gt; \
                    self.similarity_threshold:
                duplicates.add(j)
    
    return df.drop(df.index[list(duplicates)])</pre><p class="list-inset">This <code>deduplicate</code> method takes a pandas DataFrame as input and removes near-duplicate text entries based on their similarity. It first transforms the <code>text</code> column of the DataFrame into a TF-IDF matrix using a vectorizer, representing each text sample as a numerical vector. Then, it calculates the cosine similarity between all pairs of text samples using the TF-IDF matrix, resulting in a similarity matrix. The code iterates through the similarity matrix, and if the similarity between two text samples <a id="_idIndexMarker083"/>exceeds a defined <code>similarity_threshold</code>, the index of the second sample is added to a set of duplicates. Finally, it removes the rows corresponding to the identified duplicate indices from the DataFrame and returns the deduplicated DataFrame.</p></li>				<li>Putting all the functions together, we can now define a <code>clean</code> function:<pre class="source-code">
    def clean(self, input_file, output_file):
        # Read data
        df = pd.read_csv(input_file)
        # Preprocess
        df['text'] = df['text'].apply(self.preprocess)
        # Filter by length
        df = self.filter_by_length(df)
        # Deduplicate
        df = self.deduplicate(df)
        # Save cleaned data
        df.to_csv(output_file, index=False)
        print(f"Cleaned data saved to {output_file}")</pre><p class="list-inset">This <code>clean</code> method orchestrates a <a id="_idIndexMarker084"/>series of data cleaning steps on a CSV file. It begins by reading the input CSV file into a pandas DataFrame. Then, the <code>preprocess</code> method is applied to each text entry in the <code>text</code> column, normalizing and cleaning the text. Subsequently, it filters the DataFrame using the <code>filter_by_length</code> method to retain only text entries within a specified length range. After length filtering, near-duplicate entries are removed using the <code>deduplicate</code> method. Finally, it saves the cleaned DataFrame to a new CSV file specified by <code>output_file</code>, excluding the index, and prints a confirmation message indicating the output file’s location. Essentially, this method performs a complete text cleaning pipeline, encompassing preprocessing, length filtering, and deduplication.</p></li>				<li>The following is an example usage:<pre class="source-code">
pipeline = DataCleaningPipeline()
pipeline.clean('input_data.csv', 'cleaned_data.csv')</pre></li>			</ol>
			<p>Overall, this script provides a basic <a id="_idIndexMarker085"/>framework for an automated data cleaning pipeline. In practice, we would extend this pipeline with more sophisticated cleaning techniques, error handling, and parallel processing capabilities to handle large-scale datasets efficiently.</p>
			<p>The values <code>10</code> and <code>1000</code> in the code represent the minimum and maximum allowed lengths for text documents in the data cleaning pipeline:</p>
			<ul>
				<li><code>min_length=10</code>: This sets the minimum number of characters a document must have to be included in the cleaned dataset. It helps to filter out very short texts that might not contain meaningful information, such as single words or brief phrases.</li>
				<li><code>max_length=1000</code>: This establishes the maximum number of characters allowed for a document. It excludes extremely long texts that might be atypical or potentially problematic for processing, such as entire books or very large documents that could skew the analysis.</li>
			</ul>
			<p>These length constraints help ensure that the cleaned dataset contains documents of a reasonable and consistent size range, which can improve the quality and efficiency of subsequent text analysis or machine learning tasks. You can adjust the length based on your use cases.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor047"/>Data validation and quality assurance</h1>
			<p>After cleaning the data, you need<a id="_idIndexMarker086"/> to validate the results and ensure that the cleaned dataset <a id="_idIndexMarker087"/>meets the required quality standards for LLM training. We implement various validation checks and quality assurance measures to verify the effectiveness of our cleaning process.</p>
			<p>Key aspects include performing statistical analyses, sampling and manual reviews, automated tests, consistency verifications, and performance impact assessments.</p>
			<p>Here’s a Python script demonstrating basic data validation techniques:</p>
			<ol>
				<li>First, define the basic function:<pre class="source-code">
def validate_cleaned_data(file_path, sample_size=100):
    df = pd.read_csv(file_path)
    # Basic statistics
    print(f"Total samples: {len(df)}")    
    print(
        f"Average text length: "
        f"{df['text'].str.len().mean():.2f}"
    )
    
    print(f"Unique samples: {df['text'].nunique()}")</pre></li>				<li>Then, check for empty or very short texts:<pre class="source-code">
short_texts = df[df['text'].str.len() &lt; 10]
print(
    f"Texts shorter than 10 characters: "
    f"{len(short_texts)}"
)</pre></li>				<li>Sample for a manual review:<pre class="source-code">
    sample = df.sample(n=min(sample_size, len(df)))
    print("\nSample for manual review:")
    print(sample['text'].head())
    # Check for common issues
  common_issues = {
        'special_chars': df['text'].str.contains(
            r'[^a-zA-Z0-9\s]'
        ),
        'numbers': df['text'].str.contains(r'\d'),
        'all_caps': df['text'].str.isupper()
    }
    for issue, mask in common_issues.items():
        print(f"Samples with {issue}: {mask.sum()}")</pre></li>				<li>Evaluate<a id="_idIndexMarker088"/> the impact <a id="_idIndexMarker089"/>on the model’s perplexity:<pre class="source-code">
    model = GPT4LMHeadModel.from_pretrained('GPT4')
    tokenizer = GPT4Tokenizer.from_pretrained('GPT4')
    def calculate_perplexity(text):
        inputs = tokenizer(
            text, return_tensors='pt', truncation=True, 
                max_length=1024
        )
        with torch.no_grad():
            outputs = model(inputs, labels=inputs['input_ids'])
        return torch.exp(outputs.loss).item()
    sample_perplexities = sample['text'].apply(
        calculate_perplexity)
    print(
        f"\nAverage perplexity on sample: "
        f"{sample_perplexities.mean():.2f}"
    )</pre></li>				<li>Let’s see an example:<pre class="source-code">
validate_cleaned_data('cleaned_data.csv')</pre></li>			</ol>
			<p>The script defines a function called <code>validate_cleaned_data</code> that’s designed to perform a basic quality assessment on a text dataset stored in a CSV file (presumably after some initial cleaning steps). It loads the<a id="_idIndexMarker090"/> data, calculates some basic statistics, checks for specific potential issues in <a id="_idIndexMarker091"/>the text content, provides a sample for manual inspection, and uses a pre-trained language model (hypothetically GPT-4) to evaluate the naturalness or quality of a sample of the text via perplexity.</p>
			<p>The following issues are being checked for:</p>
			<ul>
				<li>Dataset size and basic properties:<ul><li><code>len(df)</code>: Checks the total number of samples (rows) in the CSV.</li><li><code>df['text'].str.len().mean()</code>: Calculates the average length of the text entries, which is useful to see if texts are generally long or short.</li><li><code>df['text'].nunique()</code>: Counts the number of unique text entries. A low number compared to the total number of samples might indicate many duplicates.</li></ul></li></ul></li>
				<li><code>df[df['text'].str.len() &lt; 10]</code>: Filters the DataFrame to find rows where the length<a id="_idIndexMarker092"/> of the string in the <code>text</code> column is less<a id="_idIndexMarker093"/> than 10 characters</li><li><code>len(short_texts)</code>: Counts how many such short texts were found</li></ul></li></ul></li>
				<li><code>df['text'].str.contains(r'[^a-zA-Z0-9\s]')</code>: Uses a regular expression (<code>r'[^a-zA-Z0-9\s]'</code>) with pandas’ <code>.str.contains()</code> method. The regex pattern [<code>^...</code>] matches any character <em class="italic">not</em> in the specified set (a-z, A-Z, 0-9, whitespace \s).</li><li><code>mask.sum()</code>: Sums the resulting Boolean series (true=<code>1</code>, false=0) to count how many texts contain at least one such special character.</li></ul></li></ul></li>
				<li><code>df['text'].str.contains(r'\d')</code>: Uses the regular expression <code>\d</code> (which matches any digit) with <code>.str.contains()</code></li><li><code>mask.sum()</code>: Counts how many texts contain at least one digit</li></ul></li></ul></li>
				<li><code>df['text'].str.isupper()</code>: Uses the pandas <code>.str.isupper()</code> string <a id="_idIndexMarker094"/>method, which returns <code>True</code> if all cased characters in the string are <a id="_idIndexMarker095"/>uppercase and there is at least one alphabetic character (i.e., a letter) that is uppercase and not just symbols or digits. If the string is all non-alphabetic (like numbers or punctuation), it will return <code>False</code>—even though those characters aren’t lowercase either.</li><li><code>mask.sum()</code>: Counts how many texts are entirely in uppercase</li></ul></li></ul></li>
				<li><code>df.sample(...)</code>). Perplexity calculations can be computationally expensive, so they are often done on a representative sample rather than the whole dataset.</li><li><code>GPT4LMHeadModel</code>) and its corresponding tokenizer (<code>GPT4Tokenizer</code>) are loaded. (Note: <code>'GPT4'</code> here is illustrative; you’d use actual model identifiers such as <code>'gpt2'</code> or <code>'bert-base-uncased'</code> from libraries such as Hugging Face Transformers).</li><li><code>calculate_perplexity</code> function tokenizes the text, feeds it to the model, obtains the loss (a measure of how surprised the model was by the text), and calculates perplexity using <code>torch.exp(outputs.loss)</code>.</li><li><code>sample_perplexities.mean()</code>) to get a single score representing the sample’s<a id="_idIndexMarker097"/> average quality according to the model.</li></ul></li></ul></li>
				<li><code>sample = df.sample(...)</code>: Takes a random sample of the data</li><li><code>print(sample['text'].head())</code>: Prints the first few text entries from that random sample, making it easy for a user running the script to quickly eyeball some examples</li></ul></li></ul></li>
			</ul>
			<p>To ensure comprehensive quality assurance, you can do the following:</p>
			<ul>
				<li>Implement more sophisticated <a id="_idIndexMarker098"/>automated tests tailored to your specific data characteristics and cleaning rules.</li>
				<li>Develop a systematic process for manual review, including guidelines for human annotators to assess data quality consistently.</li>
				<li>Use a known synthetic dataset with known issues to benchmark and assess the performance of the pipeline.</li>
				<li>Compare the cleaned dataset against the original dataset to verify that no unintended data loss or alteration occurred during the cleaning process.</li>
				<li>Conduct regular audits of your data cleaning pipeline to identify any emerging issues or bias introduced during cleaning.</li>
				<li>Maintain detailed logs of the<a id="_idIndexMarker099"/> cleaning process, including any decisions made and their rationale, to <a id="_idIndexMarker100"/>ensure reproducibility and facilitate future improvements.</li>
			</ul>
			<p>By implementing these measures, you can ensure that your cleaned dataset is of high quality and suitable for training robust LLMs.</p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, we explored the critical process of data cleaning for LLM training. We discussed the importance of clean data in developing robust and reliable language models and covered common data quality issues specific to language datasets. We provided techniques to address these issues, including text preprocessing, handling multilingual and code-mixed data, and deduplication strategies for large text corpora.</p>
			<p>We also delved into the implementation of automated data cleaning pipelines, which are essential for handling the massive datasets used in LLM training. Finally, we discussed data validation and quality assurance measures to ensure the effectiveness of the cleaning process.</p>
			<p>In the next chapter, we will focus on the data augmentation pattern for LLMs.</p>
		</div>
	</div></div></body></html>