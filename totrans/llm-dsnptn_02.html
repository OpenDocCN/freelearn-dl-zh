<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer013">
			<h1 id="_idParaDest-27" class="chapter-number"><a id="_idTextAnchor035"/>2</h1>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor036"/>Data Cleaning for LLM Training</h1>
			<p>In this chapter, we’ll dive into the <strong class="bold">data cleaning</strong> pattern for <span class="No-Break">LLM training.</span></p>
			<p>Clean, high-quality data is the foundation of robust and reliable language models. We’ll explore common data quality issues, preprocessing techniques, and strategies for handling diverse data types. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> depicts a data cleaning pipeline specifically designed for processing raw text data before it’s used to train <span class="No-Break">language models.</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B31249_02_01.jpg" alt="Figure 2.1 – Data cleaning pipeline" width="307" height="843"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Data cleaning pipeline</p>
			<p>The process begins with an initial data quality check to assess the raw data’s suitability. Following this, text preprocessing and deduplication steps are applied to refine and streamline the dataset. If the data fails to meet the required standards at any point, it is rerouted through an automated cleaning pipeline for additional processing. Successful completion of this stage leads to data validation to ensure the dataset’s integrity and compliance with training standards. If the data passes validation, it is marked as clean and ready for use in language model training, ensuring high-quality input for effective <span class="No-Break">model development.</span></p>
			<p>By the end of this chapter, you’ll be equipped with practical tools and techniques to clean your data for <span class="No-Break">LLM training.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Understanding the importance of <span class="No-Break">clean data</span></li>
				<li>Common data quality issues in <span class="No-Break">language datasets</span></li>
				<li>Text preprocessing techniques <span class="No-Break">for LLMs</span></li>
				<li>Handling multilingual and <span class="No-Break">code-mixed data</span></li>
				<li>Deduplication strategies for large <span class="No-Break">text corpora</span></li>
				<li>Automated data <span class="No-Break">cleaning pipelines</span></li>
				<li>Data validation and <span class="No-Break">quality assurance</span></li>
			</ul>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor037"/>Understanding the importance of clean data</h1>
			<p>The quality of data used in training LLMs directly<a id="_idIndexMarker041"/> impacts their performance and reliability. When we train LLMs on noisy or inconsistent data, we risk introducing bias, errors, and inconsistency into the model’s learned representations <span class="No-Break">and outputs.</span></p>
			<p>To illustrate the impact of data quality on LLM performance, we can use a simple Python script to compare the perplexity scores of models trained on clean and <span class="No-Break">noisy data.</span></p>
			<ol>
				<li>First, install the necessary packages and <span class="No-Break">import them:</span><pre class="source-code">
<strong class="bold">pip install torch</strong>
<strong class="bold">pip install transformers</strong>
<strong class="bold">import torch</strong>
<strong class="bold">from transformers import GPT2LMHeadModel, GPT2Tokenizer</strong></pre><p class="list-inset">PyTorch (<strong class="source-inline">torch</strong>) is a powerful deep learning framework that provides dynamic computational graphs, GPU acceleration, and extensive neural network building blocks, making it popular for machine learning research and development. The <strong class="source-inline">transformers</strong> package, developed by Hugging Face, complements PyTorch by providing a comprehensive library of pre-trained transformer models (such as BER, GPT, and T5) and tools for natural language processing tasks. Together, these packages offer a robust ecosystem in which <strong class="source-inline">torch</strong> provides the foundational deep learning <a id="_idIndexMarker042"/>operations, tensor computations, and automatic differentiation capabilities, while <strong class="source-inline">transformers</strong> provides high-level abstractions for working with state-of-the-art language models, including functions for tokenization, model fine-tuning, <span class="No-Break">and inference.</span></p></li>				<li>Then, define the initial part of <span class="No-Break">the function:</span><pre class="source-code">
def calculate_perplexity(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
    outputs = model(inputs, labels=inputs["input_ids"])
    return torch.exp(outputs.loss).item()
model = GPT4LMHeadModel.from_pretrained("GPT4")
tokenizer = GPT4Tokenizer.from_pretrained("GPT4")</pre><p class="list-inset">The <strong class="source-inline">calculate_perplexity</strong> function tokenizes the input text into PyTorch tensors using the provided tokenizer. It then passes the tokenized input to the model with <strong class="source-inline">input_ids</strong> also used as labels, allowing the model to compute a loss representing prediction error. This loss is exponentiated to derive a scalar perplexity score and returned as a <span class="No-Break">Python float.</span></p><p class="list-inset">The second part of the code initializes a language model and tokenizer using <strong class="source-inline">GPT4LMHeadModel.from_pretrained("GPT4")</strong> and <strong class="source-inline">GPT4Tokenizer.from_pretrained("GPT4")</strong>, which load the model and tokenizer weights from a pre-trained source identified <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">"GPT4"</strong></span><span class="No-Break">.</span></p></li>			</ol>
			<p class="callout-heading">Perplexity</p>
			<p class="callout"><strong class="bold">Perplexity</strong> is a measure used to evaluate<a id="_idIndexMarker043"/> language models. It quantifies how well a probability model predicts <span class="No-Break">a sample.</span></p>
			<p class="callout">Lower perplexity indicates that the model is more confident in its predictions and considers the text more likely or “normal”. Higher perplexity suggests that the model finds the text more surprising <span class="No-Break">or unusual.</span></p>
			<ol>
				<li value="3">Here are <span class="No-Break">example</span><span class="No-Break"><a id="_idIndexMarker044"/></span><span class="No-Break"> texts:</span><pre class="source-code">
clean_text = "The quick brown fox jumps over the lazy dog."
noisy_text = "Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g."</pre><p class="list-inset">The code snippet defines two string variables: <strong class="source-inline">clean_text</strong> and <strong class="source-inline">noisy_text</strong>. <strong class="source-inline">clean_text</strong> holds a standard English sentence, while <strong class="source-inline">noisy_text</strong> contains the same sentence with deliberate character substitutions, making it “noisy” or corrupted. The <strong class="source-inline">clean_text</strong> and <strong class="source-inline">noisy_text</strong> examples are used to evaluate a language model’s perplexity, where <strong class="source-inline">clean_text</strong> provides a baseline for ideal text prediction and <strong class="source-inline">noisy_text</strong> assesses the model’s robustness to real-world data corruption; by comparing the perplexity scores, we determine how well the model handles noisy input and its suitability for applications where text data is not always <span class="No-Break">perfectly formatted.</span></p></li>				<li>Finally, calculate perplexity and print <span class="No-Break">the results:</span><pre class="source-code">
clean_perplexity = calculate_perplexity(model, tokenizer,
    clean_text)
noisy_perplexity = calculate_perplexity(model, tokenizer,
    noisy_text)
print(f"Clean text perplexity: {clean_perplexity:.2f}")
print(f"Noisy text perplexity: {noisy_perplexity:.2f}")</pre></li>			</ol>
			<p>This script demonstrates how <a id="_idIndexMarker045"/>even small amounts of noise in the input data can significantly impact the <span class="No-Break">model’s perplexity.</span></p>
			<p>The perplexity score is calculated as the exponential of the cross-entropy loss. In this code, it’s computed <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">torch.exp(outputs.loss).item()</strong></span><span class="No-Break">.</span></p>
			<p>Here are our <span class="No-Break">possible outcomes:</span></p>
			<ul>
				<li><strong class="bold">Clean text perplexity</strong>: The clean text <strong class="source-inline">The quick brown fox jumps over the lazy dog</strong> is a common, grammatically <a id="_idIndexMarker046"/>correct English sentence. The clean text perplexity might be something <span class="No-Break">like </span><span class="No-Break"><strong class="source-inline">10.25</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Noisy text perplexity</strong>: The noisy text <strong class="source-inline">Th3 qu1ck br0wn f0x jumps 0ver th3 l@zy d0g</strong> contains <a id="_idIndexMarker047"/>numbers and symbols in place of letters, making it less common and more difficult for the model to predict. The noisy text perplexity might be something <span class="No-Break">like </span><span class="No-Break"><strong class="source-inline">52.87</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>The exact numbers will depend on the specific model and tokenizer used, but the noisy text should consistently have a higher perplexity score than the <span class="No-Break">clean text.</span></p>
			<p>This difference in scores demonstrates the model’s ability to distinguish between standard, easily predictable text and unusual, harder-to-predict text. It’s a useful metric for tasks such as detecting machine-generated or tampered text, as such text often has higher perplexity scores compared to <span class="No-Break">human-written text.</span></p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor038"/>Common data quality issues in language datasets</h1>
			<p>Language datasets often contain various<a id="_idIndexMarker048"/> quality issues that can negatively impact <span class="No-Break">LLM training:</span></p>
			<ul>
				<li>Spelling and grammatical errors can introduce noise and inconsistencies in the <span class="No-Break">learned representations.</span></li>
				<li>Inconsistent formatting can lead to unnecessary complexity in the model’s <span class="No-Break">learned patterns.</span></li>
				<li>Redundant data can cause models to overfit to specific patterns or bias present in <span class="No-Break">the duplicates.</span></li>
				<li>Irrelevant or low-quality content can dilute the useful information in <span class="No-Break">the dataset.</span></li>
				<li>Incomplete or truncated sentences can lead to models learning incomplete <span class="No-Break">language structures.</span></li>
				<li>Code-switching and mixed languages can confuse models trained for <span class="No-Break">specific languages.</span></li>
				<li><strong class="bold">Personally identifiable information</strong> (<strong class="bold">PII</strong>) raises privacy concerns and can lead to the memorization of<a id="_idIndexMarker049"/> <span class="No-Break">sensitive data.</span></li>
			</ul>
			<p>To detect these issues, we can use various Python libraries and techniques. Here’s an example using spaCy for basic text <span class="No-Break">quality checks:</span></p>
			<ol>
				<li>Provide the imports and an overall <span class="No-Break">function definition:</span><pre class="source-code">
import spacy
from collections import Counter
# Load spaCy model
nlp = spacy.load("en_core_web_sm")
def analyze_text_quality(text):
    doc = nlp(text)</pre></li>				<li>Check for spelling errors (using spaCy’s built-in <span class="No-Break">spell checker):</span><pre class="source-code">
    misspelled = [
        token.text for token in doc if token._.is_misspelled
    ]</pre></li>				<li>Check for grammatical<a id="_idIndexMarker050"/> issues (a simplistic approach using parts of speech (<span class="No-Break">pos) tags):</span><pre class="source-code">
pos_counts = Counter(token.pos_ for token in doc)
grammar_score = pos_counts['NOUN'] + pos_counts['VERB'] 
    + pos_counts['ADJ'] + pos_counts['ADV']</pre><p class="list-inset"><strong class="bold">Parts of speech</strong> (<strong class="bold">POS</strong>) tags are labels assigned to each word in a sentence to indicate its grammatical role. These <a id="_idIndexMarker051"/>tags help systems to understand the syntactic structure of sentences and are used in tasks such as parsing, machine translation, sentiment analysis, and information extraction. Each tag corresponds to a POS such as a noun, verb, or adjective, often with finer-grained distinctions to capture tense, number, <span class="No-Break">or function.</span></p></li>				<li>Check for <span class="No-Break">sentence completeness:</span><pre class="source-code">
incomplete_sentences = [
    sent.text for sent in doc.sents if len(sent) &lt; 3
]
    return {
        "misspelled_words": misspelled,
        "grammar_score": grammar_score,
        "incomplete_sentences": incomplete_sentences
    }</pre></li>				<li>Here’s an example usage of <span class="No-Break">the code:</span><pre class="source-code">
text = "This iz a smple txt with sum issues. Incomplet"
quality_report = analyze_text_quality(text)
print(quality_report)</pre></li>			</ol>
			<p>The script provided in <em class="italic">steps 1 to 5</em> illustrates<a id="_idIndexMarker052"/> a basic framework for identifying some common text quality issues. We will address other quality issues in the <span class="No-Break">following sections.</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor039"/>Text preprocessing techniques for LLMs</h1>
			<p>Effective text preprocessing is <a id="_idIndexMarker053"/>crucial for preparing data for LLM training. We employ various techniques, including lowercasing, punctuation handling, whitespace normalization, special character handling, <strong class="bold">tokenization</strong>, number normalization, and contraction expansion. Tokenization is the<a id="_idIndexMarker054"/> process of breaking text into smaller units for further analysis or processing. Tokens are the smallest meaningful units of text in natural language processing. They can be words, but they could also include punctuation, numbers, or other elements depending on the <span class="No-Break">tokenization strategy.</span></p>
			<p>In addition, <strong class="bold">subword tokenization</strong> is an advanced text <a id="_idIndexMarker055"/>processing technique that breaks words into smaller meaningful units (subwords), enabling more efficient handling of rare words, compound words, and morphological variations in natural language processing tasks. Unlike traditional word-level tokenization, subword tokenization can identify common prefixes, suffixes, and root words, allowing models to understand and process previously unseen words by recognizing their <span class="No-Break">familiar components.</span></p>
			<p>As an example, consider the word “unbelievably”. Traditional word-level tokenization would treat this as a single token. If the model has never seen this word before, it may struggle to interpret it correctly. In contrast, subword tokenization would break it down into smaller components such as “un”, “believ”, and “ably”. These subwords are more likely to appear across different contexts—“un-” in “unlikely”, “believ” in “believe”, “ably” in “capably”—allowing the model to derive meaning even if it encounters “unbelievably” for the first time. This decomposition enhances generalization, reduces vocabulary size, and improves the model’s ability to handle rare or morphologically <span class="No-Break">complex words.</span></p>
			<p>Popular subword tokenization <a id="_idIndexMarker056"/>algorithms include <strong class="bold">byte pair encoding</strong> (<strong class="bold">BPE</strong>), WordPiece, and SentencePiece, which learn to identify frequently occurring character sequences in a training corpus and create a vocabulary of subword tokens. This approach is particularly valuable<a id="_idIndexMarker057"/> for handling morphologically rich languages, reducing vocabulary size while maintaining semantic meaning, and has become fundamental in modern language models such as Gemini, Claude, GPT, and other <span class="No-Break">transformer-based architectures.</span></p>
			<p>These methods help clean and standardize the text data, reducing noise and improving the model’s ability to generalize. Here’s a Python script demonstrating these <span class="No-Break">preprocessing techniques:</span></p>
			<ol>
				<li>First, import the necessary <span class="No-Break">Python packages:</span><pre class="source-code">
import unicodedata
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')</pre></li>				<li>Then, define the overall <span class="No-Break">preprocessing function:</span><pre class="source-code">
def preprocess_text(text):
    # Lowercase the text
    text = text.lower()
    # Normalize unicode characters
    text = unicodedata.normalize(
        'NFKD', text
    ).encode(
        'ascii', 'ignore'
    ).decode('utf-8')
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Normalize whitespace
    text = ' '.join(text.split())
    # Tokenize :
   tokens = word_tokenize(text)</pre></li>				<li>Remove stopwords (stopwords are common words such as “the”, “is”, and “at”) that are often removed in <a id="_idIndexMarker058"/>text processing as they carry little <span class="No-Break">semantic meaning):</span><pre class="source-code">
    stop_words = set(stopwords.words('english'))
    tokens = [
        token for token in tokens if token not in stop_words
    ]
    # Join tokens back into text
    preprocessed_text = ' '.join(tokens)
    return preprocessed_text</pre></li>				<li>Here’s an example usage of <span class="No-Break">the code:</span><pre class="source-code">
raw_text = "This is an EXAMPLE of text preprocessing... It's quite useful!"
cleaned_text = preprocess_text(raw_text)
print(f"Original: {raw_text}")
print(f"Preprocessed: {cleaned_text}")</pre></li>			</ol>
			<p>This script demonstrates basic text<a id="_idIndexMarker059"/> preprocessing techniques. For LLM training, we might need to adapt these techniques based on the specific requirements of the model and <span class="No-Break">the dataset.</span></p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor040"/>Handling multilingual and code-mixed data</h1>
			<p>LLMs often encounter multilingual and code-mixed data, which is text that blends two or more languages within a single sentence or conversation. This presents a challenge as LLMs must interpret linguistic nuances, grammar, and semantic connections across multiple languages. To handle code-mixed data, LLMs need to learn language switching, vocabulary and syntax variations, and maintain coherent responses, which demands strong language modeling and multilingual <span class="No-Break">training data.</span></p>
			<p>We need to implement strategies to handle these scenarios effectively. The following steps are needed because they create cleaner, more consistent training data that helps LLMs better understand and process text across different languages and mixed-language scenarios, ultimately improving their performance in real-world applications where language mixing <span class="No-Break">is common.</span></p>
			<p>For multilingual data, certain tasks<a id="_idIndexMarker060"/> <span class="No-Break">are crucial:</span></p>
			<ul>
				<li><strong class="bold">Language identification</strong>: Detects the primary language of each <span class="No-Break">text sample</span></li>
				<li><strong class="bold">Script normalization</strong>: Converts text to a consistent script (<span class="No-Break">e.g., transliteration)</span></li>
				<li><strong class="bold">Language-specific preprocessing</strong>: Applies language-specific tokenization <span class="No-Break">and normalization</span></li>
			</ul>
			<p>Meanwhile, you should carry out the following <a id="_idIndexMarker061"/>steps for <span class="No-Break">code-mixed data:</span></p>
			<ul>
				<li><strong class="bold">Token-level language identification</strong>: Identifies the language of <span class="No-Break">individual tokens</span></li>
				<li><strong class="bold">Consistency enforcement</strong>: Ensures consistent handling of <span class="No-Break">code-switching patterns</span></li>
			</ul>
			<p>Here’s a Python script demonstrating language detection and <span class="No-Break">script normalization.</span></p>
			<ol>
				<li>Let’s provide the imports and the overall <span class="No-Break">function definition:</span><pre class="source-code">
from langdetect import detect
from unidecode import unidecode
from nltk import word_tokenize
import nltk
# Download required NLTK data
nltk.download('punkt')
def handle_multilingual_text(text):
    # Detect language
    try:
        lang = detect(text)
    except:
        lang = 'unknown'
    # Transliterate non-ASCII characters
    transliterated_text = unidecode(text)</pre></li>				<li>Tokenize (using NLTK for simplicity, but consider <span class="No-Break">language-specific tokenizers):</span><pre class="source-code">
    tokens = word_tokenize(transliterated_text)
    return {
        'original': text,
        'language': lang,
        'transliterated': transliterated_text,
        'tokens': tokens
    }</pre></li>				<li>Here’s the <span class="No-Break">example </span><span class="No-Break"><a id="_idIndexMarker062"/></span><span class="No-Break">usage:</span><pre class="source-code">
texts = [
    "This is English text.",
    "Dies ist deutscher Text.",
    "これは日本語のテキストです。",
    "This is mixed language text avec un peu de français."
]
for text in texts:
    result = handle_multilingual_text(text)
    print(f"Original: {result['original']}")
    print(f"Detected Language: {result['language']}")
    print(f"Transliterated: {result['transliterated']}")
    print(f"Tokens: {result['tokens']}\n")</pre><p class="list-inset">This code iterates through a list of multilingual text strings, including English, German, Japanese, and a code-mixed example, and for each string, it calls a <strong class="source-inline">handle_multilingual_text</strong> function (presumably defined elsewhere) to process the text, returning a dictionary <a id="_idIndexMarker063"/>containing the original text, detected language, transliterated text (if applicable), and tokenized words, which are then printed to <span class="No-Break">the console.</span></p></li>			</ol>
			<p>Putting the preceding three code blocks together, we provide a basic framework for handling multilingual text. For more advanced scenarios, we would use specialized libraries such as Polyglot for language-specific processing and code-mixing analysis when multiple languages are used in the same <span class="No-Break">conversation (</span><a href="https://dl.acm.org/doi/10.1145/3544548.3581445"><span class="No-Break">https://dl.acm.org/doi/10.1145/3544548.3581445</span></a><span class="No-Break">).</span></p>
			<p>For example, Polyglot includes built-in language detection, named entity recognition, sentiment analysis, and transliteration capabilities across multiple languages, all while maintaining a relatively lightweight footprint compared to larger multilingual frameworks. The library is particularly valuable for projects dealing with international text data, as it provides consistent APIs across languages and comes with pre-trained models, making it an efficient choice for multilingual text analysis tasks without the complexity of managing multiple <span class="No-Break">language-specific tools.</span></p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor041"/>Deduplication strategies for large text corpora</h1>
			<p>Deduplication is a critical step in preparing large text corpora for LLM training. Duplicate content can lead to biased models and wasted computational resources. We employ various strategies to identify and remove <a id="_idIndexMarker064"/><span class="No-Break">duplicates efficiently:</span></p>
			<ul>
				<li><strong class="bold">Exact match deduplication</strong>: Remove identical <span class="No-Break">text samples.</span></li>
				<li><strong class="bold">Near-duplicate detection</strong>: Identify and remove highly similar <span class="No-Break">text samples.</span></li>
				<li><strong class="bold">Shingling</strong>: Create small overlapping sequences of words <span class="No-Break">for comparison.</span></li>
				<li><strong class="bold">Locality sensitive hashing</strong>: Efficiently find similar items in <span class="No-Break">large datasets.</span></li>
			</ul>
			<p>The following sections show examples of <span class="No-Break">each strategy.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor042"/>Exact match deduplication</h2>
			<p><strong class="bold">Scenario</strong>: You have a list of <span class="No-Break">customer </span><span class="No-Break"><a id="_idIndexMarker065"/></span><span class="No-Break">addresses:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Data</strong></span><span class="No-Break">:</span><ul><li>“123 Main St, Anytown, <span class="No-Break">CA 91234”</span></li><li>“456 Oak Ave, Somecity, <span class="No-Break">NY 56789”</span></li><li>“123 Main St, Anytown, <span class="No-Break">CA 91234”</span></li></ul></li>
				<li><strong class="bold">Result</strong>: The third entry, “123 Main St, Anytown, CA 91234”, is removed because it is an exact duplicate of the <span class="No-Break">first entry.</span></li>
				<li><span class="No-Break"><strong class="bold">Remaining data</strong></span><span class="No-Break">:</span><ul><li>“123 Main St, Anytown, <span class="No-Break">CA 91234”</span></li><li>“456 Oak Ave, Somecity, <span class="No-Break">NY 56789”</span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor043"/>Near-duplicate detection</h2>
			<p><strong class="bold">Scenario</strong>: You have a collection of <span class="No-Break">news articles:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Data</strong></span><span class="No-Break">:</span><ul><li>Article 1: “The company<a id="_idIndexMarker066"/> reported a significant increase in <span class="No-Break">quarterly profits.”</span></li><li>Article 2: “Quarterly profits saw a large increase, the <span class="No-Break">company reports.”</span></li></ul></li>
				<li><strong class="bold">Result</strong>: A near-duplicate detection algorithm determines that these articles are highly similar in content, even though the wording is slightly different. One of the articles is removed, based on a <span class="No-Break">similarity threshold.</span></li>
				<li><strong class="bold">Remaining data</strong>: “The company reported a significant increase in <span class="No-Break">quarterly profits.”</span></li>
			</ul>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor044"/>Shingling</h2>
			<p><strong class="bold">Scenario</strong>: You want to compare the<a id="_idIndexMarker067"/> similarity of <span class="No-Break">text documents:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Data</strong></span><span class="No-Break">:</span><ul><li>Document 1: “The quick brown fox jumps over the <span class="No-Break">lazy dog.”</span></li><li>k=3 <span class="No-Break">word shingle.</span></li></ul></li>
				<li><strong class="bold">Result</strong>: The shingles generated are <span class="No-Break">as follows:</span><ul><li>“The <span class="No-Break">quick brown”</span></li><li>“quick <span class="No-Break">brown fox”</span></li><li>“brown <span class="No-Break">fox jumps”</span></li><li>“fox <span class="No-Break">jumps over”</span></li><li>“jumps <span class="No-Break">over the”</span></li><li>“over <span class="No-Break">the lazy”</span></li><li>“the <span class="No-Break">lazy dog”</span></li></ul><p class="list-inset">The document is then represented by the set of those shingles. Then another document could be turned into shingles, and the sets of shingles can <span class="No-Break">be compared.</span></p></li>
			</ul>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor045"/>Locality Sensitive Hashing (LSH)</h2>
			<p><strong class="bold">Scenario</strong>: You have a very large database <a id="_idIndexMarker068"/>of online <span class="No-Break">product </span><span class="No-Break"><a id="_idIndexMarker069"/></span><span class="No-Break">descriptions:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Process</strong></span><span class="No-Break">:</span><ul><li>LSH is used to hash the <span class="No-Break">product descriptions.</span></li><li>Similar product descriptions are more likely to be hashed into the <span class="No-Break">same “buckets.”</span></li><li>Only the descriptions within the same buckets are then compared in detail to find <span class="No-Break">near duplicates.</span></li></ul></li>
				<li><strong class="bold">Result</strong>: Instead of comparing every product description to every other description, LSH narrows down<a id="_idIndexMarker070"/> the comparisons to only those descriptions within the same <a id="_idIndexMarker071"/>buckets, greatly increasing the efficiency of finding <span class="No-Break">near duplicates.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">Deduplicating is computationally <a id="_idIndexMarker072"/>very expensive, so techniques such<a id="_idIndexMarker073"/> as minhashing or parallel processing can be used to scale the deduplicating with the increase in <a id="_idIndexMarker074"/><span class="No-Break">corpus data.</span></p>
			<p class="callout">Minhashing efficiently approximates the similarity between documents using smaller, more manageable representations, reducing the computational load. Parallel processing further distributes the deduplication task across multiple processors or machines, allowing for simultaneous comparisons and significantly speeding up the overall process, thus enabling effective deduplication of <span class="No-Break">massive corpora.</span></p>
			<p>Here’s a Python script demonstrating basic <span class="No-Break">deduplication techniques:</span></p>
			<ol>
				<li>First, define the <span class="No-Break">overall function:</span><pre class="source-code">
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
def deduplicate_corpus(corpus, similarity_threshold=0.9):
    # Create TF-IDF vectorizer
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(corpus)
    # Compute pairwise similarities
    similarity_matrix = cosine_similarity(tfidf_matrix)</pre><p class="list-inset">This code snippet utilizes scikit-learn’s <strong class="source-inline">TfidfVectorizer</strong> to convert a text corpus into a numerical <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>) matrix representing the importance<a id="_idIndexMarker075"/> of words in each document and then employs <strong class="source-inline">cosine_similarity</strong> to calculate the pairwise similarity between all documents in the <a id="_idIndexMarker076"/>corpus, providing a matrix of similarity scores that can be used to identify near-duplicate texts based on a <span class="No-Break">specified threshold.</span></p></li>				<li>Then, <span class="No-Break">find duplicates:</span><pre class="source-code">
    duplicates = set()
    for i in range(len(corpus)):
        for j in range(i + 1, len(corpus)):
            if similarity_matrix[i, j] &gt; similarity_threshold:
                duplicates.add(j)</pre></li>				<li>Create a <span class="No-Break">deduplicated corpus:</span><pre class="source-code">
    deduplicated_corpus = [
        doc for i, doc in enumerate(corpus) 
        if i not in duplicates
    ]
    return deduplicated_corpus</pre></li>				<li>Here’s <span class="No-Break">an example:</span><pre class="source-code">
corpus = [
    "The quick brown fox jumps over the lazy dog.",
    "A fast auburn fox leaps above the sleepy canine.",
    "The quick brown fox jumps over the lazy dog.",
    "An entirely different sentence about cats.",
]
deduplicated = deduplicate_corpus(corpus)
print(f"Original corpus size: {len(corpus)}")
print(f"Deduplicated corpus size: {len(deduplicated)}")
print("Deduplicated corpus:")
for doc in deduplicated:
    print(f"- {doc}")</pre></li>			</ol>
			<p>This script demonstrates a basic near-duplicate detection approach using TF-IDF and <strong class="bold">cosine similarity</strong>. TF-IDF is a numerical <a id="_idIndexMarker077"/>statistic used to reflect the importance of words in documents within a collection. It combines how often a word appears in a document (TF) with how unique it is <a id="_idIndexMarker078"/>across all documents (IDF). TF-IDF converts text into numerical vectors, enabling mathematical comparisons between documents, which is crucial for the similarity calculations used in the deduplication process. For large-scale deduplication, we would use more efficient algorithms and distributed <span class="No-Break">computing techniques.</span></p>
			<p>Here, the similarity threshold of <strong class="source-inline">0.9</strong> used in the deduplication function code determines how similar documents must be to be considered duplicates, with 90% similarity required by default. This value can be adjusted based on specific use cases—a higher threshold (e.g., <strong class="source-inline">0.95</strong> or <strong class="source-inline">1</strong>, which is maximum) is stricter and reduces false positives, while a lower threshold (e.g., <strong class="source-inline">0</strong> which is minimum or <strong class="source-inline">0.8</strong>) is more lenient and catches more <span class="No-Break">potential duplicates.</span></p>
			<p>Next, let’s discuss automated data <span class="No-Break">cleaning pipelines.</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor046"/>Automated data cleaning pipelines</h1>
			<p>To handle the massive datasets required for LLM training, we need to implement automated data cleaning pipelines. These pipelines should be scalable, efficient, and capable of handling various data <span class="No-Break">quality issues.</span></p>
			<p>The key components of an automated data cleaning pipeline are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Data ingestion</strong>: Efficiently load and <a id="_idIndexMarker079"/>parse large <span class="No-Break">text corpora.</span></li>
				<li><strong class="bold">Quality assessment</strong>: Automatically detect and flag data <span class="No-Break">quality issues.</span></li>
				<li><strong class="bold">Preprocessing</strong>: Apply text cleaning and <span class="No-Break">normalization techniques.</span></li>
				<li><strong class="bold">Deduplication</strong>: Remove exact and <span class="No-Break">near-duplicate content.</span></li>
				<li><strong class="bold">Filtering</strong>: Remove low-quality or irrelevant samples based on <span class="No-Break">predefined criteria.</span></li>
				<li><strong class="bold">Validation</strong>: Ensure the cleaned data meets <span class="No-Break">quality standards.</span></li>
				<li><strong class="bold">Output</strong>: Save the cleaned data in an appropriate format for <span class="No-Break">LLM training.</span></li>
			</ul>
			<p>Here’s a Python script outlining a basic automated data <span class="No-Break">cleaning pipeline:</span></p>
			<ol>
				<li>We will start by defining the overall <span class="No-Break">class structure:</span><pre class="source-code">
import pandas as pd
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import nltk
# Download required NLTK data
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
class DataCleaningPipeline:
    def __init__(
        self, similarity_threshold=0.9, min_length=10,
        max_length=1000
    ):
        self.similarity_threshold = similarity_threshold
        self.min_length = min_length
        self.max_length = max_length
        self.vectorizer = TfidfVectorizer(stop_words='english')</pre><p class="list-inset">This code snippet defines a <strong class="source-inline">DataCleaningPipeline</strong> class that encapsulates text preprocessing, length filtering, and near-duplicate removal functionalities. It initializes<a id="_idIndexMarker080"/> with configurable parameters such as similarity threshold and text length constraints, leverages NLTK for stop word removal, and employs scikit-learn’s <strong class="source-inline">TfidfVectorizer</strong> and <strong class="source-inline">cosine_similarity</strong> to identify and eliminate similar text entries from a <span class="No-Break">pandas DataFrame.</span></p></li>				<li>Then, we will define a <span class="No-Break">preprocess function:</span><pre class="source-code">
    def preprocess(self, text):
        # Basic preprocessing
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        tokens = [
            word for word in text.split()
            if word not in stop_words
        ]
        return ' '.join(tokens)
    def filter_by_length(self, df):
        return df[
            (df['text'].str.len() &gt;= self.min_length) &amp;
            (df['text'].str.len() &lt;= self.max_length)
        ]</pre><p class="list-inset">This code snippet defines two<a id="_idIndexMarker081"/> methods within a class for <span class="No-Break">text processing.</span></p><ul><li><strong class="source-inline">preprocess</strong>: This method takes a text string as input, converts it to lowercase, removes punctuation, splits it into words, filters out common stop words, and then joins the remaining words into a string, effectively cleaning and normalizing <span class="No-Break">the text.</span></li><li><strong class="source-inline">filter_by_length</strong>: This method takes a pandas DataFrame containing a <strong class="source-inline">text</strong> column and filters the DataFrame to include only rows where the length of the <strong class="source-inline">text</strong> column falls within a specified minimum and maximum length, allowing the selection of text samples within a desired <span class="No-Break">character range.</span></li></ul></li>				<li>We then define the <a id="_idIndexMarker082"/><span class="No-Break">deduplication function:</span><pre class="source-code">
def deduplicate(self, df):
    tfidf_matrix = self.vectorizer.fit_transform(df['text'])
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    duplicates = set()
    for i in range(len(df)):
        for j in range(i + 1, len(df)):
            if similarity_matrix[i, j] &gt; \
                    self.similarity_threshold:
                duplicates.add(j)
    
    return df.drop(df.index[list(duplicates)])</pre><p class="list-inset">This <strong class="source-inline">deduplicate</strong> method takes a pandas DataFrame as input and removes near-duplicate text entries based on their similarity. It first transforms the <strong class="source-inline">text</strong> column of the DataFrame into a TF-IDF matrix using a vectorizer, representing each text sample as a numerical vector. Then, it calculates the cosine similarity between all pairs of text samples using the TF-IDF matrix, resulting in a similarity matrix. The code iterates through the similarity matrix, and if the similarity between two text samples <a id="_idIndexMarker083"/>exceeds a defined <strong class="source-inline">similarity_threshold</strong>, the index of the second sample is added to a set of duplicates. Finally, it removes the rows corresponding to the identified duplicate indices from the DataFrame and returns the <span class="No-Break">deduplicated DataFrame.</span></p></li>				<li>Putting all the functions together, we can now define a <span class="No-Break"><strong class="source-inline">clean</strong></span><span class="No-Break"> function:</span><pre class="source-code">
    def clean(self, input_file, output_file):
        # Read data
        df = pd.read_csv(input_file)
        # Preprocess
        df['text'] = df['text'].apply(self.preprocess)
        # Filter by length
        df = self.filter_by_length(df)
        # Deduplicate
        df = self.deduplicate(df)
        # Save cleaned data
        df.to_csv(output_file, index=False)
        print(f"Cleaned data saved to {output_file}")</pre><p class="list-inset">This <strong class="source-inline">clean</strong> method orchestrates a <a id="_idIndexMarker084"/>series of data cleaning steps on a CSV file. It begins by reading the input CSV file into a pandas DataFrame. Then, the <strong class="source-inline">preprocess</strong> method is applied to each text entry in the <strong class="source-inline">text</strong> column, normalizing and cleaning the text. Subsequently, it filters the DataFrame using the <strong class="source-inline">filter_by_length</strong> method to retain only text entries within a specified length range. After length filtering, near-duplicate entries are removed using the <strong class="source-inline">deduplicate</strong> method. Finally, it saves the cleaned DataFrame to a new CSV file specified by <strong class="source-inline">output_file</strong>, excluding the index, and prints a confirmation message indicating the output file’s location. Essentially, this method performs a complete text cleaning pipeline, encompassing preprocessing, length filtering, <span class="No-Break">and deduplication.</span></p></li>				<li>The following is an <span class="No-Break">example usage:</span><pre class="source-code">
pipeline = DataCleaningPipeline()
pipeline.clean('input_data.csv', 'cleaned_data.csv')</pre></li>			</ol>
			<p>Overall, this script provides a basic <a id="_idIndexMarker085"/>framework for an automated data cleaning pipeline. In practice, we would extend this pipeline with more sophisticated cleaning techniques, error handling, and parallel processing capabilities to handle large-scale <span class="No-Break">datasets efficiently.</span></p>
			<p>The values <strong class="source-inline">10</strong> and <strong class="source-inline">1000</strong> in the code represent the minimum and maximum allowed lengths for text documents in the data <span class="No-Break">cleaning pipeline:</span></p>
			<ul>
				<li><strong class="source-inline">min_length=10</strong>: This sets the minimum number of characters a document must have to be included in the cleaned dataset. It helps to filter out very short texts that might not contain meaningful information, such as single words or <span class="No-Break">brief phrases.</span></li>
				<li><strong class="source-inline">max_length=1000</strong>: This establishes the maximum number of characters allowed for a document. It excludes extremely long texts that might be atypical or potentially problematic for processing, such as entire books or very large documents that could skew <span class="No-Break">the analysis.</span></li>
			</ul>
			<p>These length constraints help ensure that the cleaned dataset contains documents of a reasonable and consistent size range, which can improve the quality and efficiency of subsequent text analysis or machine learning tasks. You can adjust the length based on your <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor047"/>Data validation and quality assurance</h1>
			<p>After cleaning the data, you need<a id="_idIndexMarker086"/> to validate the results and ensure that the cleaned dataset <a id="_idIndexMarker087"/>meets the required quality standards for LLM training. We implement various validation checks and quality assurance measures to verify the effectiveness of our <span class="No-Break">cleaning process.</span></p>
			<p>Key aspects include performing statistical analyses, sampling and manual reviews, automated tests, consistency verifications, and performance <span class="No-Break">impact assessments.</span></p>
			<p>Here’s a Python script demonstrating basic data <span class="No-Break">validation techniques:</span></p>
			<ol>
				<li>First, define the <span class="No-Break">basic function:</span><pre class="source-code">
def validate_cleaned_data(file_path, sample_size=100):
    df = pd.read_csv(file_path)
    # Basic statistics
    print(f"Total samples: {len(df)}")    
    print(
        f"Average text length: "
        f"{df['text'].str.len().mean():.2f}"
    )
    
    print(f"Unique samples: {df['text'].nunique()}")</pre></li>				<li>Then, check for empty or very <span class="No-Break">short texts:</span><pre class="source-code">
short_texts = df[df['text'].str.len() &lt; 10]
print(
    f"Texts shorter than 10 characters: "
    f"{len(short_texts)}"
)</pre></li>				<li>Sample for a <span class="No-Break">manual review:</span><pre class="source-code">
    sample = df.sample(n=min(sample_size, len(df)))
    print("\nSample for manual review:")
    print(sample['text'].head())
    # Check for common issues
  common_issues = {
        'special_chars': df['text'].str.contains(
            r'[^a-zA-Z0-9\s]'
        ),
        'numbers': df['text'].str.contains(r'\d'),
        'all_caps': df['text'].str.isupper()
    }
    for issue, mask in common_issues.items():
        print(f"Samples with {issue}: {mask.sum()}")</pre></li>				<li>Evaluate<a id="_idIndexMarker088"/> the impact <a id="_idIndexMarker089"/>on the <span class="No-Break">model’s perplexity:</span><pre class="source-code">
    model = GPT4LMHeadModel.from_pretrained('GPT4')
    tokenizer = GPT4Tokenizer.from_pretrained('GPT4')
    def calculate_perplexity(text):
        inputs = tokenizer(
            text, return_tensors='pt', truncation=True, 
                max_length=1024
        )
        with torch.no_grad():
            outputs = model(inputs, labels=inputs['input_ids'])
        return torch.exp(outputs.loss).item()
    sample_perplexities = sample['text'].apply(
        calculate_perplexity)
    print(
        f"\nAverage perplexity on sample: "
        f"{sample_perplexities.mean():.2f}"
    )</pre></li>				<li>Let’s see <span class="No-Break">an example:</span><pre class="source-code">
validate_cleaned_data('cleaned_data.csv')</pre></li>			</ol>
			<p>The script defines a function called <strong class="source-inline">validate_cleaned_data</strong> that’s designed to perform a basic quality assessment on a text dataset stored in a CSV file (presumably after some initial cleaning steps). It loads the<a id="_idIndexMarker090"/> data, calculates some basic statistics, checks for specific potential issues in <a id="_idIndexMarker091"/>the text content, provides a sample for manual inspection, and uses a pre-trained language model (hypothetically GPT-4) to evaluate the naturalness or quality of a sample of the text <span class="No-Break">via perplexity.</span></p>
			<p>The following issues are being <span class="No-Break">checked for:</span></p>
			<ul>
				<li>Dataset size and <span class="No-Break">basic properties:</span><ul><li><strong class="bold">Issue</strong>: Understanding the overall scale and basic characteristics of <span class="No-Break">the dataset</span></li><li><strong class="bold">How the issue </strong><span class="No-Break"><strong class="bold">is checked</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">len(df)</strong>: Checks the total number of samples (rows) in <span class="No-Break">the CSV.</span></li><li><strong class="source-inline">df['text'].str.len().mean()</strong>: Calculates the average length of the text entries, which is useful to see if texts are generally long <span class="No-Break">or short.</span></li><li><strong class="source-inline">df['text'].nunique()</strong>: Counts the number of unique text entries. A low number compared to the total number of samples might indicate <span class="No-Break">many duplicates.</span></li></ul></li></ul></li>
				<li><strong class="bold">Very </strong><span class="No-Break"><strong class="bold">short texts</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Issue</strong>: Identifying text entries that might be too short to be meaningful or could represent errors/placeholders (e.g., empty strings mistakenly kept, such <span class="No-Break">as </span><span class="No-Break"><em class="italic">N/A</em></span><span class="No-Break">)</span></li><li><strong class="bold">How the issue </strong><span class="No-Break"><strong class="bold">is checked</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">df[df['text'].str.len() &lt; 10]</strong>: Filters the DataFrame to find rows where the length<a id="_idIndexMarker092"/> of the string in the <strong class="source-inline">text</strong> column is less<a id="_idIndexMarker093"/> than <span class="No-Break">10 characters</span></li><li><strong class="source-inline">len(short_texts)</strong>: Counts how many such short texts <span class="No-Break">were found</span></li></ul></li></ul></li>
				<li><strong class="bold">Presence of special characters (</strong><span class="No-Break"><strong class="bold">non-alphanumeric, non-whitespace)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Issue</strong>: Detecting text that contains characters other than letters, numbers, and standard whitespace. This could indicate leftover HTML tags, uncleaned punctuation, encoding issues, or <span class="No-Break">other artifacts.</span></li><li><strong class="bold">How the issue </strong><span class="No-Break"><strong class="bold">is checked</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">df['text'].str.contains(r'[^a-zA-Z0-9\s]')</strong>: Uses a regular expression (<strong class="source-inline">r'[^a-zA-Z0-9\s]'</strong>) with pandas’ <strong class="source-inline">.str.contains()</strong> method. The regex pattern [<strong class="source-inline">^...</strong>] matches any character <em class="italic">not</em> in the specified set (a-z, A-Z, 0-9, <span class="No-Break">whitespace \s).</span></li><li><strong class="source-inline">mask.sum()</strong>: Sums the resulting Boolean series (true=<strong class="source-inline">1</strong>, false=0) to count how many texts contain at least one such <span class="No-Break">special character.</span></li></ul></li></ul></li>
				<li><strong class="bold">Presence </strong><span class="No-Break"><strong class="bold">of numbers</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Issue</strong>: Identifying text entries that contain digits. Depending on the downstream task, numbers might be undesirable or require <span class="No-Break">special handling.</span></li><li><strong class="bold">How the issue </strong><span class="No-Break"><strong class="bold">is checked</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">df['text'].str.contains(r'\d')</strong>: Uses the regular expression <strong class="source-inline">\d</strong> (which matches any digit) <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">.str.contains()</strong></span></li><li><strong class="source-inline">mask.sum()</strong>: Counts how many texts contain at least <span class="No-Break">one digit</span></li></ul></li></ul></li>
				<li><strong class="bold">All </strong><span class="No-Break"><strong class="bold">caps text</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Issue</strong>: Finding text written entirely in uppercase. This can sometimes indicate shouting, headings, or specific types of formatting that might affect model performance <span class="No-Break">or analysis.</span></li><li><strong class="bold">How the issue </strong><span class="No-Break"><strong class="bold">is checked</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">df['text'].str.isupper()</strong>: Uses the pandas <strong class="source-inline">.str.isupper()</strong> string <a id="_idIndexMarker094"/>method, which returns <strong class="source-inline">True</strong> if all cased characters in the string are <a id="_idIndexMarker095"/>uppercase and there is at least one alphabetic character (i.e., a letter) that is uppercase and not just symbols or digits. If the string is all non-alphabetic (like numbers or punctuation), it will return <strong class="source-inline">False</strong>—even though those characters aren’t <span class="No-Break">lowercase either.</span></li><li><strong class="source-inline">mask.sum()</strong>: Counts how many texts are entirely <span class="No-Break">in uppercase</span></li></ul></li></ul></li>
				<li><strong class="bold">General text quality/naturalness (</strong><span class="No-Break"><strong class="bold">via perplexity)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Issue</strong>: Assessing how typical or well-formed the text looks from the perspective of an LLM. Very high perplexity can indicate unnatural language, repetitive sequences, code snippets mixed with text, or data that is very different from the model’s <span class="No-Break">training data.</span></li><li><strong class="bold">How the issue </strong><span class="No-Break"><strong class="bold">is checked</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Sampling</strong>: First, a random sample is taken from the dataset (<strong class="source-inline">df.sample(...)</strong>). Perplexity calculations can be computationally expensive, so they are often done on a representative sample rather than the <span class="No-Break">whole dataset.</span></li><li><strong class="bold">Model loading</strong>: A pre-trained language model (<strong class="source-inline">GPT4LMHeadModel</strong>) and its corresponding tokenizer (<strong class="source-inline">GPT4Tokenizer</strong>) are loaded. (Note: <strong class="source-inline">'GPT4'</strong> here is illustrative; you’d use actual model identifiers such as <strong class="source-inline">'gpt2'</strong> or <strong class="source-inline">'bert-base-uncased'</strong> from libraries such as Hugging <span class="No-Break">Face Transformers).</span></li><li><strong class="bold">Perplexity calculation</strong>: For each text in the sample, the <strong class="source-inline">calculate_perplexity</strong> function tokenizes the text, feeds it to the model, obtains the loss (a measure of how surprised the model was by the text), and calculates perplexity <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">torch.exp(outputs.loss)</strong></span><span class="No-Break">.</span></li><li><strong class="bold">Averaging</strong>: The perplexities of all texts<a id="_idIndexMarker096"/> in the sample are averaged (<strong class="source-inline">sample_perplexities.mean()</strong>) to get a single score representing the sample’s<a id="_idIndexMarker097"/> average quality according to <span class="No-Break">the model.</span></li></ul></li></ul></li>
				<li><strong class="bold">Manual </strong><span class="No-Break"><strong class="bold">review facilitation</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Issue</strong>: Catching unexpected or subtle issues not covered by automated checks. Human intuition <span class="No-Break">is valuable.</span></li><li><strong class="bold">How the issue </strong><span class="No-Break"><strong class="bold">is checked</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">sample = df.sample(...)</strong>: Takes a random sample of <span class="No-Break">the data</span></li><li><strong class="source-inline">print(sample['text'].head())</strong>: Prints the first few text entries from that random sample, making it easy for a user running the script to quickly eyeball <span class="No-Break">some examples</span></li></ul></li></ul></li>
			</ul>
			<p>To ensure comprehensive quality assurance, you can do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Implement more sophisticated <a id="_idIndexMarker098"/>automated tests tailored to your specific data characteristics and <span class="No-Break">cleaning rules.</span></li>
				<li>Develop a systematic process for manual review, including guidelines for human annotators to assess data <span class="No-Break">quality consistently.</span></li>
				<li>Use a known synthetic dataset with known issues to benchmark and assess the performance of <span class="No-Break">the pipeline.</span></li>
				<li>Compare the cleaned dataset against the original dataset to verify that no unintended data loss or alteration occurred during the <span class="No-Break">cleaning process.</span></li>
				<li>Conduct regular audits of your data cleaning pipeline to identify any emerging issues or bias introduced <span class="No-Break">during cleaning.</span></li>
				<li>Maintain detailed logs of the<a id="_idIndexMarker099"/> cleaning process, including any decisions made and their rationale, to <a id="_idIndexMarker100"/>ensure reproducibility and facilitate <span class="No-Break">future improvements.</span></li>
			</ul>
			<p>By implementing these measures, you can ensure that your cleaned dataset is of high quality and suitable for training <span class="No-Break">robust LLMs.</span></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, we explored the critical process of data cleaning for LLM training. We discussed the importance of clean data in developing robust and reliable language models and covered common data quality issues specific to language datasets. We provided techniques to address these issues, including text preprocessing, handling multilingual and code-mixed data, and deduplication strategies for large <span class="No-Break">text corpora.</span></p>
			<p>We also delved into the implementation of automated data cleaning pipelines, which are essential for handling the massive datasets used in LLM training. Finally, we discussed data validation and quality assurance measures to ensure the effectiveness of the <span class="No-Break">cleaning process.</span></p>
			<p>In the next chapter, we will focus on the data augmentation pattern <span class="No-Break">for LLMs.</span></p>
		</div>
	</div></div></body></html>