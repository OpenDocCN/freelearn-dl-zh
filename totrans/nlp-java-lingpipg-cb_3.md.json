["```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.OverfittingClassifier\n\n    ```", "```py\n    Training\n    Type a string to be classified. Empty string to quit.\n    When all else fails #Disney\n    Category is: e\n    ```", "```py\n    Type a string to be classified. Empty string to quit.\n    When all else fails #Disne\n    Category is: n\n    ```", "```py\nString dataPath = args.length > 0 ? args[0] : \"data/disney_e_n.csv\";\nList<String[]> annotatedData = Util.readAnnotatedCsvRemoveHeader(new File(dataPath));\n\nOverfittingClassifier classifier = new OverfittingClassifier();\nSystem.out.println(\"Training\");\nfor (String[] row: annotatedData) {\n  String truth = row[Util.ANNOTATION_OFFSET];\n  String text = row[Util.TEXT_OFFSET];\n  classifier.handle(text,new Classification(truth));\n}\nUtil.consoleInputBestCategory(classifier);\n```", "```py\npublic class OverfittingClassifier implements BaseClassifier<CharSequence> {\n\n  Map<String,Classification> mMap \n         = new HashMap<String,Classification>();  \n\n   public void handle(String text, Classification classification) {mMap.put(text, classification);\n  }\n```", "```py\n@Override\npublic Classification classify(CharSequence text) {\n  if (mMap.containsKey(text)) {\n    return mMap.get(text);\n  }\n  return new Classification(\"n\");\n}\n```", "```py\n    int maxTokenNGram = 2;\n    TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n    String[] categories = Util.getCategories(annotatedData);\n    ```", "```py\n    DynamicLMClassifier<TokenizedLM> classifier = DynamicLMClassifier.createTokenized(categories,tokenizerFactory,maxTokenNGram);\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier\n    ```", "```py\np(tokens,cat) = p(tokens|cat) * p(cat)\n```", "```py\n    p(cat|tokens) = p(tokens,cat) / p(tokens)\n                   = p(tokens|cat) * p(cat) / p(tokens)\n    ```", "```py\n    p(tokens|cat) = p(tokens[0]|cat) * p(tokens[1]|cat) * . . . * p(tokens[n]|cat)\n    ```", "```py\n    p(tokens) = p(tokens|cat1) * p(cat1) + p(tokens|cat2) * p(cat2) + . . . + p(tokens|catN) * p(catN)\n    ```", "```py\n    p(cat) = frequency(cat) / (frequency(cat1) + frequency(cat2) + . . . + frequency(catN))\n    ```", "```py\n    p(token|cat) = frequency(token,cat)/(frequency(token1,cat) + frequency(token2,cat) + . . . + frequency(tokenN,cat)\n    ```", "```py\n    p(cat) = frequency(cat) + alpha / [(frequency(cat1) + alpha) + (frequency(cat2)+alpha) + . . . + (frequency(catN) + alpha)]\n    ```", "```py\n    p(token|cat) = (frequency(token,cat)+beta) / [(frequency(token1,cat)+beta) + frequency(token2,cat)+beta) + . . . + (frequency(tokenN,cat) + beta)]\n    ```", "```py\n    hot : super steamy today\n    hot : boiling out\n    hot : steamy out\n\n    cold : freezing out\n    cold : icy\n    ```", "```py\n    p(hot|super) = p(super|hot) * p(hot)/ p(super)\n\n    p(super|hot) = (freq(super,hot) + beta) / [(freq(super|hot)+beta) + (freq(steamy|hot) + beta) + . . . + (freq(freezing|hot)+beta)\n    ```", "```py\n    freq(super|hot) + beta = 1 + 1 = 2\n    freq(steamy|hot) + beta = 2 + 1 = 3\n    freq(today|hot) + beta = 1 + 1 = 2\n    freq(boiling|hot) + beta = 1 + 1 = 2\n    freq(out|hot) + beta = 1 + 1 = 2\n    freq(freezing|hot) + beta = 0 + 1 = 1\n    freq(icy|hot) + beta = 0 + 1 = 1\n    ```", "```py\n    2+3+2+2+2+1+1 = 13\n    ```", "```py\n    p(hot) = (freq(hot) + alpha) / \n                        ((freq(hot) + alpha) + freq(cold)+alpha)) \n    ```", "```py\n    p(hot) = (3 + 1) / (3 + 1) + (2 +1) = 4/7\n    Similarly p(cold) = (2 + 1) / (3 + 1) + (2 +1) = 3/7\n    Please note that p(hot) = 1 – p(cold)\n\n    p(super) = p(super|hot) * p(hot) + p(super|cold) + p(cold)\n    ```", "```py\n    p(super|cold) = (freq(super,cold) + beta) / [(freq(super|cold)+beta) + (freq(steamy|cold) + beta) + . . . + (freq(freezing|cold)+beta)\n\n    freq(super|cold) + beta = 0 + 1 = 1\n    freq(steamy|cold) + beta = 0 + 1 = 1\n    freq(today|cold) + beta = 0 + 1 = 1\n    freq(boiling|cold) + beta = 0 + 1 = 1\n    freq(out|cold) + beta = 1 + 1 = 2\n    freq(freezing|cold) + beta = 1 + 1 = 2\n    freq(icy|cold) + beta = 1 + 1 = 2\n\n    p(super|cold) = freq(super|cold)+beta/sum of all terms above\n\n                  = 0 + 1 / (1+1+1+1+2+2+2) = 1/10\n    ```", "```py\n    P(super) = p(super|hot) * p(hot) + p(super|cold) * p(cold)\n             = 2/13 * 4/7 + 1/10 * 3/7\n    ```", "```py\n    p(hot|super) = p(super|hot) * p(hot) / p(super)\n                 = (2/13 * 4/7) / (2/13 * 4/7 + 1/10 * 3/7)\n\n                 = 0.6722\n    p(cold|super) = p(super|cold) * p(cold) /p(super)\n                 = (1/10 * 3/7) / (2/13 * 4/7 + 1/10 * 3/7)\n                 = 0.3277\n\n    Obviously, p(hot|super) = 1 – p(cold|super)\n    ```", "```py\n    p(hot|super super) = p(super super|hot) * p(hot) / p(super super)\n                 = (2/13 * 2/13 * 4/7) / (2/13 * 2/13 * 4/7 + 1/10 * 1/10 * 3/7)\n                 = 0.7593\n    p(cold|super super) = p(super super|cold) * p(cold) /p(super super)\n                 = (1/10 * 1/10 * 3/7) / (2/13 * 2/13 * 4/7 + 1/10 * 1/10 * 3/7)\n                 = 0.2406\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TrainAndRunNaiveBayesClassifier\n\n    ```", "```py\n    Type a string to be classified\n    super\n    h 0.67   \n    c 0.33   \n    ```", "```py\n    Type a string to be classified\n    hello\n    h 0.57   \n    c 0.43\n    ```", "```py\n    Type a string to be classified\n    super super\n\n    ```", "```py\n    h 0.76   \n    c 0.24    \n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter3.SimpleFeatureExtractor\n\n    ```", "```py\n    Type a string to see its features\n    My first feature extraction!\n    ```", "```py\n    !=1\n    My=1\n    extraction=1\n    feature=1\n    first=1\n    ```", "```py\n    Type a string to see its features\n    My my my what a nice feature extractor.\n    my=2\n    .=1\n    My=1\n    a=1\n    extractor=1\n    feature=1\n    nice=1\n    what=1\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n  TokenizerFactory tokFact \n    = IndoEuropeanTokenizerFactory.INSTANCE;\n  FeatureExtractor<CharSequence> tokenFeatureExtractor \n    = new TokenFeatureExtractor(tokFact);\n```", "```py\nBufferedReader reader \n  = new BufferedReader(new   InputStreamReader(System.in));\nwhile (true) {\n  System.out.println(\"\\nType a string to see its features\");\n  String text = reader.readLine();\n  Map<String, ? extends Number > features \n    = tokenFeatureExtractor.features(text);\n  System.out.println(features);\n}\n```", "```py\nFEATURE    e          n\nI :   0.37    0.0\n! :   0.30    0.0\nDisney :   0.15    0.0\n\" :   0.08    0.0\nto :   0.07    0.0\nanymore : 0.06    0.0\nisn :   0.06    0.0\n' :   0.06    0.0\nt :   0.04    0.0\nfor :   0.03    0.0\nque :   -0.01    0.0\nmoi :   -0.01    0.0\n_ :   -0.02    0.0\n, :   -0.08    0.0\npra :   -0.09    0.0\n? :   -0.09    0.0\n```", "```py\npublic static void main(String[] args) throws IOException {\n  String trainingFile = args.length > 0 ? args[0] \n           : \"data/disney_e_n.csv\";\n  List<String[]> training \n    = Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));\n\n  int numFolds = 0;\n  XValidatingObjectCorpus<Classified<CharSequence>> corpus \n    = Util.loadXValCorpus(training,numFolds);\n\n  TokenizerFactory tokenizerFactory \n    = IndoEuropeanTokenizerFactory.INSTANCE;\n```", "```py\nFeatureExtractor<CharSequence> featureExtractor\n  = new TokenFeatureExtractor(tokenizerFactory);\n```", "```py\nint minFeatureCount = 1;\n```", "```py\nboolean addInterceptFeature = true;\nboolean noninformativeIntercept = true;\n```", "```py\ndouble priorVariance = 2;\nRegressionPrior prior \n  = RegressionPrior.laplace(priorVariance,\n          noninformativeIntercept);\n```", "```py\nAnnealingSchedule annealingSchedule\n  = AnnealingSchedule.exponential(0.00025,0.999);\ndouble minImprovement = 0.000000001;\nint minEpochs = 100;\nint maxEpochs = 2000;\n```", "```py\nPrintWriter progressWriter = new PrintWriter(System.out,true);\nprogressWriter.println(\"Reading data.\");\nReporter reporter = Reporters.writer(progressWriter);\nreporter.setLevel(LogLevel.INFO);  \n```", "```py\n    LogisticRegressionClassifier<CharSequence> classifier\n        = LogisticRegressionClassifier.\n            <CharSequence>train(corpus,\n            featureExtractor,\n            minFeatureCount,\n            addInterceptFeature,\n            prior,\n            annealingSchedule,\n            minImprovement,\n            minEpochs,\n            maxEpochs,\n            reporter);\n    ```", "```py\n    AbstractExternalizable.compileTo(classifier,\n      new File(\"models/myModel.LogisticRegression\"));\n    ```", "```py\n    Util.consoleInputPrintClassification(classifier);\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TrainAndRunLogReg\n\n    ```", "```py\n    Reading data.\n    :00 Feature Extractor class=class com.aliasi.tokenizer.TokenFeatureExtractor\n    :00 min feature count=1\n    :00 Extracting Training Data\n    :00 Cold start\n    :00 Regression callback handler=null\n    :00 Logistic Regression Estimation\n    :00 Monitoring convergence=true\n    :00 Number of dimensions=233\n    :00 Number of Outcomes=2\n    :00 Number of Parameters=233\n    :00 Number of Training Instances=21\n    :00 Prior=LaplaceRegressionPrior(Variance=2.0, noninformativeIntercept=true)\n    :00 Annealing Schedule=Exponential(initialLearningRate=2.5E-4, base=0.999)\n    :00 Minimum Epochs=100\n    :00 Maximum Epochs=2000\n    :00 Minimum Improvement Per Period=1.0E-9\n    :00 Has Informative Prior=true\n    :00 epoch=    0 lr=0.000250000 ll=   -20.9648 lp= -232.0139 llp=  -252.9787 llp*=  -252.9787\n    :00 epoch=    1 lr=0.000249750 ll=   -20.9406 lp= -232.0195 llp=  -252.9602 llp*=  -252.9602\n    ```", "```py\n    :00 epoch= 1998 lr=0.000033868 ll=   -15.4568 lp=  -233.8125 llp=  -249.2693 llp*=  -249.2693\n    :00 epoch= 1999 lr=0.000033834 ll=   -15.4565 lp=  -233.8127 llp=  -249.2692 llp*=  -249.2692\n    ```", "```py\n    Type a string to be classified. Empty string to quit.\n    I luv Disney\n    Rank  Category  Score  P(Category|Input)\n    0=e 0.626898085027528 0.626898085027528\n    1=n 0.373101914972472 0.373101914972472\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TuneLogRegParams\n\n    ```", "```py\n    Reading data.\n    RUNNING thread Fold 5 (1 of 10)\n    RUNNING thread Fold 9 (2 of 10)\n    RUNNING thread Fold 3 (3 of 10)\n    RUNNING thread Fold 4 (4 of 10)\n    RUNNING thread Fold 0 (5 of 10)\n    RUNNING thread Fold 2 (6 of 10)\n    RUNNING thread Fold 8 (7 of 10)\n    RUNNING thread Fold 6 (8 of 10)\n    RUNNING thread Fold 7 (9 of 10)\n    RUNNING thread Fold 1 (10 of 10)\n    reference\\response\n              \\e,n,\n             e 11,0,\n             n 6,4,\n    ```", "```py\nint numThreads = 2;\nint numFolds = 10;\nUtil.xvalLogRegMultiThread(corpus,\n        featureExtractor,\n        minFeatureCount,\n        addInterceptFeature,\n        prior,\n        annealingSchedule,\n        minImprovement,\n        minEpochs,\n        maxEpochs,\n        reporter,\n        numFolds,\n        numThreads,\n        categories);\n```", "```py\npublic static <E> ConditionalClassifierEvaluator<E> xvalLogRegMultiThread(\n    final XValidatingObjectCorpus<Classified<E>> corpus,\n    final FeatureExtractor<E> featureExtractor,\n    final int minFeatureCount, \n    final boolean addInterceptFeature,\n    final RegressionPrior prior, \n    final AnnealingSchedule annealingSchedule,\n    final double minImprovement, \n    final int minEpochs, final int maxEpochs,\n    final Reporter reporter, \n    final int numFolds, \n    final int numThreads, \n    final String[] categories) {\n```", "```py\n    corpus.setNumFolds(numFolds);\n    corpus.permuteCorpus(new Random(11211));\n    final boolean storeInputs = true;\n    final ConditionalClassifierEvaluator<E> crossFoldEvaluator\n      = new ConditionalClassifierEvaluator<E>(null, categories, storeInputs);\n    ```", "```py\n    List<Thread> threads = new ArrayList<Thread>();\n    for (int i = 0; i < numFolds; ++i) {\n      final XValidatingObjectCorpus<Classified<E>> fold \n        = corpus.itemView();\n      fold.setFold(i);\n    ```", "```py\n    Runnable runnable \n      = new Runnable() {\n        @Override\n        public void run() {\n        try {\n          LogisticRegressionClassifier<E> classifier\n            = LogisticRegressionClassifier.<E>train(fold,\n                    featureExtractor,\n                    minFeatureCount,\n                    addInterceptFeature,\n                    prior,\n                    annealingSchedule,\n                    minImprovement,\n                    minEpochs,\n                    maxEpochs,\n                    reporter);\n    ```", "```py\n    ConditionalClassifierEvaluator<E> withinFoldEvaluator \n      = new ConditionalClassifierEvaluator<E>(classifier, categories, storeInputs);\n    fold.visitTest(withinFoldEvaluator);\n    ```", "```py\n    addToEvaluator(withinFoldEvaluator,crossFoldEvaluator);\n    ```", "```py\n    public synchronized static <E> void addToEvaluator(BaseClassifierEvaluator<E> foldEval, ScoredClassifierEvaluator<E> crossFoldEval) {\n      for (String category : foldEval.categories()) {\n       for (Classified<E> classified : foldEval.truePositives(category)) {\n        crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());\n       }\n       for (Classified<E> classified : foldEval.falseNegatives(category)) {\n        crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());\n       }\n      }\n     }\n    ```", "```py\n        catch (Exception e) {\n          e.printStackTrace();\n        }\n      }\n    };\n    threads.add(new Thread(runnable,\"Fold \" + i));\n    ```", "```py\n\n      runThreads(threads,numThreads); \n      printConfusionMatrix(crossFoldEvaluator.confusionMatrix());\n    }\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TuneLogRegParams\n\n    ```", "```py\n    reference\\response\n              \\e,n,\n             e 11,0,\n             n 6,4,\n    ```", "```py\n    False Positives for e\n    ES INSUPERABLE DISNEY !! QUIERO VOLVER:( : n\n    @greenath_ t'as de la chance d'aller a Disney putain : n \n    jamais été moi. : n\n    @HedyHAMIDI au quartier pas a Disney moi: n\n    …\n    ```", "```py\n    Feature coefficients for category e\n    I : 0.36688604\n    ! : 0.29588525\n    Disney : 0.14954419\n    \" : 0.07897427\n    to : 0.07378086\n    …\n    Got feature count: 113\n    ```", "```py\n    Type a string to be classified\n    I luv disney\n    Rank  Category  Score  P(Category|Input)\n    0=e 0.5907060507161321 0.5907060507161321\n    1=n 0.40929394928386786 0.40929394928386786\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n    …\n  TokenizerFactory tokenizerFactory \n     = IndoEuropeanTokenizerFactory.INSTANCE;\n  FeatureExtractor<CharSequence> featureExtractor\n     = new TokenFeatureExtractor(tokenizerFactory);\n  int minFeatureCount = 1;\n  boolean addInterceptFeature = false;\n```", "```py\n  boolean noninformativeIntercept = true;\n  double priorVariance = 2 ;\n  RegressionPrior prior \n    = RegressionPrior.laplace(priorVariance,\n            noninformativeIntercept);\n```", "```py\n  AnnealingSchedule annealingSchedule\n    = AnnealingSchedule.exponential(0.00025,0.999);\n  double minImprovement = 0.000000001;\n  int minEpochs = 10;\n  int maxEpochs = 20;\n```", "```py\nUtil.xvalLogRegMultiThread(corpus,…);\n```", "```py\ncorpus.setNumFolds(0);\nLogisticRegressionClassifier<CharSequence> classifier\n  = LogisticRegressionClassifier.<CharSequence>train(corpus,…\n```", "```py\nint featureCount = 0;\nfor (String category : categories) {\n  ObjectToDoubleMap<String> featureCoeff \n    = classifier.featureValues(category);\n  System.out.println(\"Feature coefficients for category \" \n        + category);\n  for (String feature : featureCoeff.keysOrderedByValueList()) {\n    System.out.print(feature);\n    System.out.printf(\" :%.8f\\n\",featureCoeff.getValue(feature));\n    ++featureCount;\n  }\n}\nSystem.out.println(\"Got feature count: \" + featureCount);\n```", "```py\nUtil.consoleInputPrintClassification(classifier);    \n```", "```py\nType a string to be classified. Empty string to quit.\nThe rain in Spain\nRank  Category  Score  P(Category|Input)\n0=e 0.5 0.5\n1=n 0.5 0.5\n```", "```py\nint min = 2;\nint max = 4;\nTokenizerFactory tokenizerFactory \n  = new NGramTokenizerFactory(min,max);\n```", "```py\nType a string to be classified. Empty string to quit.\nThe rain in Spain\nRank  Category  Score  P(Category|Input)\n0=e 0.5113903651380305 0.5113903651380305\n1=n 0.4886096348619695 0.4886096348619695\n```", "```py\nFalse Positives for e\n@greenath_ t'as de la chance d'aller a Disney putain j'y ai jamais été moi. : n\n@HedyHAMIDI au quartier pas a Disney moi : n\nPrefiro gastar uma baba de dinheiro pra ir pra cancun doq pra Disney por exemplo : n\n```", "```py\nFeature coefficients for category e\nI : 0.36688604\n! : 0.29588525\nDisney : 0.14954419\n\" : 0.07897427\nto : 0.07378086\n```", "```py\nde : -0.08864114\n( : -0.10818647\n*&^INTERCEPT%$^&** : -0.17089337\n```", "```py\n*&^INTERCEPT%$^&** : -0.03874782\n```", "```py\nFeature coefficients for category e\n' : -0.00003809\nFeature coefficients for category n\n```", "```py\nFeature coefficients for category e\nI : 0.36688604\n! : 0.29588525\nDisney : 0.14954419\n\nI : 0.40189501\n! : 0.31387376\nDisney : 0.18255271\n```", "```py\nboolean noninformativeIntercept = false;\ndouble priorVariance = 2;\nRegressionPrior prior \n  = RegressionPrior.gaussian(priorVariance,\n    noninformativeIntercept);\n```", "```py\nI : 0.38866670\n! : 0.27367013\nDisney : 0.22699340\n```", "```py\nAnnealingSchedule annealingSchedule\n    = AnnealingSchedule.exponential(0.00025,0.999);\n  double minImprovement = 0.000000001;\n  int minEpochs = 10;\n  int maxEpochs = 20;\n```", "```py\n    public class ContainsNumberFeatureExtractor implements FeatureExtractor<CharSequence> {\n      @Override\n      public Map<String,Counter> features(CharSequence text) {\n             ObjectToCounterMap<String> featureMap \n             = new ObjectToCounterMap<String>();\n        if (text.toString().matches(\".*\\\\d.*\")) {\n          featureMap.set(\"CONTAINS_NUMBER\", 1);\n        }\n        return featureMap;  }\n    ```", "```py\n    public static void main(String[] args) {\n      FeatureExtractor<CharSequence> featureExtractor \n             = new ContainsNumberFeatureExtractor();\n      System.out.println(featureExtractor.features(\"I have a number 1\"));\n    }\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.ContainsNumberFeatureExtractor\n\n    ```", "```py\n    CONTAINS_NUMBER=1\n\n    ```", "```py\n    public static void main(String[] args) {\n       int min = 2;\n      int max = 4;\n      TokenizerFactory tokenizerFactory \n         = new NGramTokenizerFactory(min,max);\n      FeatureExtractor<CharSequence> tokenFeatures \n    = new TokenFeatureExtractor(tokenizerFactory);\n    ```", "```py\n    FeatureExtractor<CharSequence> numberFeatures \n    = new ContainsNumberFeatureExtractor();\n    ```", "```py\n    FeatureExtractor<CharSequence> joinedFeatureExtractors \n      = new AddFeatureExtractor<CharSequence>(\n              tokenFeatures,numberFeatures);\n    ```", "```py\n    String input = \"show me 1!\";\n    Map<String,? extends Number> features \n       = joinedFeatureExtractors.features(input);\n    System.out.println(features);\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.CombinedFeatureExtractor\n\n    ```", "```py\n    {me =1.0,  m=1.0, me 1=1.0, e =1.0, show=1.0,  me =1.0, ho=1.0, ow =1.0, e 1!=1.0, sho=1.0,  1=1.0, me=1.0, how =1.0, CONTAINS_NUMBER=1.0, w me=1.0,  me=1.0, how=1.0,  1!=1.0, sh=1.0, ow=1.0, e 1=1.0, w m=1.0, ow m=1.0, w =1.0, 1!=1.0}\n\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n  String trainingFile = args.length > 0 ? args[0] \n    : \"data/activeLearningCompleted/\"\n    + \"disneySentimentDedupe.2.csv\";\n  int numFolds = 10;\n  List<String[]> training \n    = Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));\n  String[] categories = Util.getCategories(training);\n  XValidatingObjectCorpus<Classified<CharSequence>> corpus \n  = Util.loadXValCorpus(training,numFolds);\nTokenizerFactory tokenizerFactory \n  = IndoEuropeanTokenizerFactory.INSTANCE;\nPrintWriter progressWriter = new PrintWriter(System.out,true);\nReporter reporter = Reporters.writer(progressWriter);\nreporter.setLevel(LogLevel.WARN);\nboolean storeInputs = true;\nConditionalClassifierEvaluator<CharSequence> evaluator \n    = new ConditionalClassifierEvaluator<CharSequence>(null, categories, storeInputs);\ncorpus.setNumFolds(0);\nLogisticRegressionClassifier<CharSequence> classifier = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);\nevaluator.setClassifier(classifier);\nSystem.out.println(\"!!!Testing on training!!!\");\nUtil.printConfusionMatrix(evaluator.confusionMatrix());\n}\n```", "```py\n    System.out.println(\"!!!Testing on training!!!\");\n    corpus.visitTrain(evaluator);\n    ```", "```py\n    !!!Testing on training!!!\n    reference\\response\n              \\p,n,o,\n             p 67,0,3,\n             n 0,30,2,\n             o 2,1,106,\n    ```", "```py\n    static int NUM_FOLDS = 10;\n    ```", "```py\n    //System.out.println(\"!!!Testing on training!!!\");\n    //corpus.visitTrain(evaluator);\n    ```", "```py\n    corpus.setNumFolds(numFolds);\n    for (int i = 0; i < numFolds; ++i) {\n     corpus.setFold(i);\n      LogisticRegressionClassifier<CharSequence> classifier \n         = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);\n      evaluator.setClassifier(classifier);\n     corpus.visitTest(evaluator);\n    }\n    ```", "```py\n    reference\\response\n              \\p,n,o,\n             p 45,8,17,\n             n 16,13,3,\n             o 18,3,88,\n    ```", "```py\n    reference\\response\n          \\p,n,o,\n        p 45,8,17,\n    ```", "```py\n    reference\\response\n              \\p,\n             p 45\n             n 16\n             o 18\n    ```", "```py\n    Util.printConfusionMatrix(evaluator.confusionMatrix());\n    Util.printPrecRecall(evaluator);\n\n    ```", "```py\n    reference\\response\n              \\p,n,o,\n             p 45,8,17,\n             n 16,13,3,\n             o 18,3,88,\n    Category p\n    Recall: 0.64\n    Prec  : 0.57\n    Category n\n    Recall: 0.41\n    Prec  : 0.54\n    Category o\n    Recall: 0.81\n    Prec  : 0.81\n    ```", "```py\n    Util.printPRcurve(evaluator);\n\n    ```", "```py\n    reference\\response\n              \\p,n,o,\n             p 45,8,17,\n             n 16,13,3,\n             o 18,3,88,\n    Category p\n    Recall: 0.64\n    Prec  : 0.57\n    Category n\n    Recall: 0.41\n    Prec  : 0.54\n    Category o\n    Recall: 0.81\n    Prec  : 0.81\n    PR Curve for Category: p\n      PRECI.   RECALL    SCORE\n    0.000000 0.000000 0.988542\n    0.500000 0.014286 0.979390\n    0.666667 0.028571 0.975054\n    0.750000 0.042857 0.967286\n    0.600000 0.042857 0.953539\n    0.666667 0.057143 0.942158\n    0.571429 0.057143 0.927563\n    0.625000 0.071429 0.922381\n    0.555556 0.071429 0.902579\n    0.600000 0.085714 0.901597\n    0.636364 0.100000 0.895898\n    0.666667 0.114286 0.891566\n    0.615385 0.114286 0.888831\n    0.642857 0.128571 0.884803\n    0.666667 0.142857 0.877658\n    0.687500 0.157143 0.874135\n    0.647059 0.157143 0.874016\n    0.611111 0.157143 0.871183\n    0.631579 0.171429 0.858999\n    0.650000 0.185714 0.849296\n    0.619048 0.185714 0.845691\n    0.636364 0.200000 0.810079\n    0.652174 0.214286 0.807661\n    0.666667 0.228571 0.807339\n    0.640000 0.228571 0.799474\n    0.653846 0.242857 0.753967\n    0.666667 0.257143 0.753169\n    0.678571 0.271429 0.751815\n    0.655172 0.271429 0.747515\n    0.633333 0.271429 0.745660\n    0.645161 0.285714 0.744455\n    0.656250 0.300000 0.738555\n    0.636364 0.300000 0.736310\n    0.647059 0.314286 0.705090\n    0.628571 0.314286 0.694125\n    ```", "```py\ncorpus.setNumFolds(0);\nLogisticRegressionClassifier<CharSequence> classifier \n  = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);\nAbstractExternalizable.compileTo(classifier, \n  new File(\"models/ClassifierBuilder.LogisticRegression\"));\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.LinguisticTuning\n\n    ```", "```py\n    Training on fold 0\n    ######################Printing features for category p NON_ZERO \n    ?: 0.52\n    !: 0.41\n    love: 0.37\n    can: 0.36\n    my: 0.36\n    is: 0.34\n    in: 0.29\n    of: 0.28\n    I: 0.28\n    old: 0.26\n    me: 0.25\n    My: 0.25\n    ?: 0.25\n    wait: 0.24\n    ?: 0.23\n    an: 0.22\n    out: 0.22\n    movie: 0.22\n    ?: 0.21\n    movies: 0.21\n    shirt: 0.21\n    t: 0.20\n    again: 0.20\n    Princess: 0.19\n    i: 0.19 \n    …\n    ######################Printing features for category o NON_ZERO \n    :: 0.69\n    /: 0.52\n    *&^INTERCEPT%$^&**: 0.48\n    @: 0.41\n    *: 0.36\n    (: 0.35\n    …\n    ######################Printing features for category n ZERO\n    ```", "```py\n    Category p\n    Recall: 0.64\n    Prec  : 0.57\n    ```", "```py\n    TokenizerFactory tokenizerFactory \n      = IndoEuropeanTokenizerFactory.INSTANCE;\n    tokenizerFactory = new   LowerCaseTokenizerFactory(tokenizerFactory);\n    ```", "```py\n    Category p\n    Recall: 0.69\n    Prec  : 0.59\n    ```", "```py\n    Training on fold 0\n    ######################Printing features for category p NON_ZERO \n    ?: 0.53\n    my: 0.49\n    love: 0.43\n    can: 0.41\n    !: 0.39\n    i: 0.35\n    is: 0.31\n    of: 0.28\n    wait: 0.27\n    old: 0.25\n    ♥: 0.24\n    an: 0.22\n    ```", "```py\n    Category p\n    Recall: 0.67\n    Prec  : 0.58\n    ```", "```py\n    TokenizerFactory tokenizerFactory \n      = new NGramTokenizerFactory(2,4);\n    tokenizerFactory \n    = new LowerCaseTokenizerFactory(tokenizerFactory);\n    ```", "```py\n    Category p\n    Recall: 0.71\n    Prec  : 0.64\n    ```", "```py\n    #########Printing features for category p NON_ZERO \n    ea: 0.20\n    !!: 0.20\n    ov: 0.17\n    n : 0.16\n    ne: 0.15\n     ?: 0.14\n    al: 0.13\n    rs: 0.13\n    ca: 0.13\n    ! : 0.13\n    ol: 0.13\n    lo: 0.13\n     m: 0.13\n    re : 0.12\n    so: 0.12\n    i : 0.12\n    f : 0.12\n     lov: 0.12 \n    ```", "```py\n    Category n\n    Recall: 0.41\n    Prec  : 0.72\n    ```", "```py\n    False Positives for p\n    *<category> is truth category\n\n    I was really excited for Disney next week until I just read that it's \"New Jersey\" week. #noooooooooo\n     p 0.8434727204351016\n     o 0.08488521562829848\n    *n 0.07164206393660003\n\n    \"Why worry? If you've done the best you can, worrying won't make anything better.\" ~Walt Disney\n     p 0.4791823543407749\n    *o 0.3278392260935065\n     n 0.19297841956571868\n    ```", "```py\n    Cant sleep so im watching.. Beverley Hills Chihuahua.. Yep thats right, I'm watching a Disney film about talking dogs.. FML!!!\n     p 0.6045997587907997\n     o 0.3113342571409484\n    *n 0.08406598406825164\n    ```", "```py\n    reference\\response\n              \\p,n,o,\n             p 45,8,17,\n             n 16,13,3,\n             o 18,3,88,\n    Category p\n    Recall: 0.64\n    Prec  : 0.57\n    Category n\n    Recall: 0.41\n    Prec  : 0.54\n    Category o\n    Recall: 0.81\n    Prec  : 0.81\n    ```", "```py\n    reference\\response\n              \\p,n,o,\n             p 50,3,17,\n             n 14,13,5,\n             o 14,2,93,\n    Category p\n    Recall: 0.71\n    Prec  : 0.64\n    Category n\n    Recall: 0.41\n    Prec  : 0.72\n    Category o\n    Recall: 0.85\n    Prec  : 0.81\n    ```", "```py\nCategory p\nRecall: 0.64\nPrec  : 0.57\nCategory n\nRecall: 0.41\nPrec  : 0.54\nCategory o\nRecall: 0.81\nPrec  : 0.81\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3/RunClassifier\n    Data is: data/freshDisney.csv model is: models/ClassifierBuilder.LogisticRegression\n    No annotations found, not evaluating\n    writing scored output to data/freshDisney.csv\n\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3/RunClassifier data/freshDisneyAnnotated.csv\n\n    ```", "```py\n    Data is: data/freshDisneyAnnotated.csv model is: models/ClassifierBuilder.LogisticRegression\n    reference\\response\n     \\p,n,o,\n     p 141,25,0,\n     n 39,37,0,\n     o 51,28,0,\n    Category p\n    Recall: 0.85\n    Prec  : 0.61\n    Category n\n    Recall: 0.49\n    Prec  : 0.41\n    Category o\n    Recall: 0.00\n    Prec  : NaN\n\n    ```", "```py\n    PR Curve for Category: p\n      PRECI.   RECALL    SCORE\n    1.000000 0.006024 0.976872\n    1.000000 0.012048 0.965248\n    1.000000 0.018072 0.958461\n    1.000000 0.024096 0.947749\n    1.000000 0.030120 0.938152\n    1.000000 0.036145 0.930893\n    1.000000 0.042169 0.928653\n    …\n    0.829268 0.204819 0.781308\n    0.833333 0.210843 0.777209\n    0.837209 0.216867 0.776252\n    0.840909 0.222892 0.771287\n    0.822222 0.222892 0.766425\n    0.804348 0.222892 0.766132\n    0.808511 0.228916 0.764918\n    0.791667 0.228916 0.761848\n    0.795918 0.234940 0.758419\n    0.780000 0.234940 0.755753\n    0.784314 0.240964 0.755314\n    …\n    0.649746 0.771084 0.531612\n    0.651515 0.777108 0.529871\n    0.653266 0.783133 0.529396\n    0.650000 0.783133 0.528988\n    0.651741 0.789157 0.526603\n    0.648515 0.789157 0.526153\n    0.650246 0.795181 0.525740\n    0.651961 0.801205 0.525636\n    0.648780 0.801205 0.524874\n    ```", "```py\n    PR Curve for Category: n\n      PRECI.   RECALL    SCORE\n    1.000000 0.013158 0.981217\n    0.500000 0.013158 0.862016\n    0.666667 0.026316 0.844607\n    0.500000 0.026316 0.796797\n    0.600000 0.039474 0.775489\n    0.500000 0.039474 0.768295\n    …\n    0.468750 0.197368 0.571442\n    0.454545 0.197368 0.571117\n    0.470588 0.210526 0.567976\n    0.485714 0.223684 0.563354\n    0.500000 0.236842 0.552538\n    0.486486 0.236842 0.549950\n    0.500000 0.250000 0.549910\n    0.487179 0.250000 0.547843\n    0.475000 0.250000 0.540650\n    0.463415 0.250000 0.529589\n    ```", "```py\npublic class ThresholdedClassifier<E> implements  ScoredClassifier<E> {\n\n  ConditionalClassifier<E> mNonThresholdedClassifier;\n\n  public ThresholdedClassifier (ConditionalClassifier<E> classifier) {\n    mNonThresholdedClassifier = classifier;\n  }\n```", "```py\n@Override\npublic ScoredClassification classify(E input) {\n  ConditionalClassification classification \n    = mNonThresholdedClassifier.classify(input);\n  List<ScoredObject<String>> scores \n      = new ArrayList<ScoredObject<String>>();\n  for (int i = 0; i < classification.size(); ++i) {\n    String category = classification.category(i);     Double score = classification.score(i);\n     if (category.equals(\"p\") && score < .76d) {\n       score = 0.0;\n     }\n    if (category.equals(\"n\") && score < .549d) {\n       score = 0.0;\n     }\n     ScoredObject<String> scored \n      = new ScoredObject<String>(category,score);\n    scores.add(scored);\n  }\n  ScoredClassification thresholded \n    = ScoredClassification.create(scores);\n  return thresholded;\n}\n```", "```py\njava -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3/ThresholdedClassifier data/freshDisneyAnnotated.csv \nData is: data/freshDisneyAnnotated.csv model is: models/ClassifierBuilder.LogisticRegression\n\nreference\\response\n \\p,n,o,\n p 38,14,114,\n n 5,19,52,\n o 5,5,69,\nCategory p\nRecall: 0.23\nPrec  : 0.79\nCategory n\nRecall: 0.25\nPrec  : 0.50\nCategory o\nRecall: 0.87\nPrec  : 0.29\n\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar: com.lingpipe.cookbook.chapter3.ActiveLearner \n\n    ```", "```py\n    reference\\response\n              \\p,n,o,\n             p 7,0,1,\n             n 1,0,3,\n             o 2,0,11,\n    Category p\n    Recall: 0.88\n    Prec  : 0.70\n    Category n\n    Recall: 0.00\n    Prec  : NaN\n    Category o\n    Recall: 0.85\n    Prec  : 0.73\n    Writing to file: data/activeLearning/disneySentimentDedupe.1.csv\n    Done, now go annotate and save with same file name\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar: com.lingpipe.cookbook.chapter3.ActiveLearner \n\n    ```", "```py\n    First file: data/activeLearning2/disneySentimentDedupe.0.csv\n    Reading from file data/activeLearning2/disneySentimentDedupe.1.csv\n    reference\\response\n              \\p,n,o,\n             p 17,1,20,\n             n 9,1,5,\n             o 9,1,51,\n    Category p\n    Recall: 0.45\n    Prec  : 0.49\n    Category n\n    Recall: 0.07\n    Prec  : 0.33\n    Category o\n    Recall: 0.84\n    Prec  : 0.67\n    Corpus is: 114\n    Writing to file: data/activeLearning2/disneySentimentDedupe.2.csv\n    Done, now go annotate and save with same file name\n    ```", "```py\n    First file:  data/activeLearning2/disneySentimentDedupe.0.csv\n    Reading from file data/activeLearning2/disneySentimentDedupe.2.csv\n    reference\\response\n              \\p,n,o,\n             p 45,8,17,\n             n 16,13,3,\n             o 18,3,88,\n    Category p\n    Recall: 0.64\n    Prec  : 0.57\n    Category n\n    Recall: 0.41\n    Prec  : 0.54\n    Category o\n    Recall: 0.81\n    Prec  : 0.81\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n  String fileName = args.length > 0 ? args[0] \n    : \"data/activeLearning/disneySentimentDedupe.0.csv\"; \n  System.out.println(\"First file:  \" + fileName);\n  String latestFile = getLatestEpochFile(fileName);\n```", "```py\nList<String[]> data \n  = Util.readCsvRemoveHeader(new File(latestFile));\nint numFolds = 10;\nXValidatingObjectCorpus<Classified<CharSequence>> corpus \n  = Util.loadXValCorpus(data,numFolds);\nString[] categories = Util.getCategoryArray(corpus);\n```", "```py\nPrintWriter progressWriter = new PrintWriter(System.out,true);\nboolean storeInputs = true;\nConditionalClassifierEvaluator<CharSequence> evaluator \n  = new ConditionalClassifierEvaluator<CharSequence>(null, categories, storeInputs);\nTokenizerFactory tokFactory \n  = IndoEuropeanTokenizerFactory.INSTANCE;\n```", "```py\nfor (int i = 0; i < numFolds; ++i) {\n  corpus.setFold(i);\n  final LogisticRegressionClassifier<CharSequence> classifier \n    = Util.trainLogReg(corpus,tokFactory, progressWriter);\n  evaluator.setClassifier(classifier);\n  corpus.visitTest(evaluator);\n}\n```", "```py\nfinal ObjectToDoubleMap<String[]> accumulator \n  = new ObjectToDoubleMap<String[]>();\n```", "```py\nfor (String category : categories) {\nList<Classified<CharSequence>> inCategory\n   = evaluator.truePositives(category);    \ninCategory.addAll(evaluator.falseNegatives(category));\n```", "```py\nfor (Classified<CharSequence> testCase : inCategory) {\n   CharSequence text = testCase.getObject();\n  ConditionalClassification classification \n    = (ConditionalClassification)                  testCase.getClassification();\n  double score = classification.conditionalProbability(0);\n  String[] xFoldRow = new String[Util.TEXT_OFFSET + 1];\n  xFoldRow[Util.SCORE] = String.valueOf(score);\n  xFoldRow[Util.GUESSED_CLASS] = classification.bestCategory();\n  xFoldRow[Util.ANNOTATION_OFFSET] = category;\n  xFoldRow[Util.TEXT_OFFSET] = text.toString();\n  accumulator.set(xFoldRow,score);\n}\n```", "```py\nUtil.printConfusionMatrix(evaluator.confusionMatrix());\nUtil.printPrecRecall(evaluator);  \n```", "```py\ncorpus.setNumFolds(0);\nfinal LogisticRegressionClassifier<CharSequence> classifier\n  = Util.trainLogReg(corpus,tokFactory,progressWriter);\n```", "```py\nfor (String[] csvData : data) {\n   if (!csvData[Util.ANNOTATION_OFFSET].equals(\"\")) {\n    continue;\n   }\n   ScoredClassification classification = classifier.classify(csvData[Util.TEXT_OFFSET]);\n   csvData[Util.GUESSED_CLASS] = classification.category(0);\n   double estimate = classification.score(0);\n   csvData[Util.SCORE] = String.valueOf(estimate);\n   accumulator.set(csvData,estimate);\n  }\n```", "```py\nString outfile = incrementFileName(latestFile);\nUtil.writeCsvAddHeader(accumulator.keysOrderedByValueList(), \n        new File(outfile));    \nSystem.out.println(\"Corpus size: \" + corpus.size());\nSystem.out.println(\"Writing to file: \" + outfile);\nSystem.out.println(\"Done, now go annotate and save with same\" \n          + \" file name\");\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.InterAnnotatorAgreement\n\n    ```", "```py\n    data/disney_e_n.csv treated as truth \n    data/disney1_e_n.csv treated as response\n    Disagreement: n x e for: When all else fails #Disney\n    Disagreement: e x n for: 昨日の幸せな気持ちのまま今日はLANDにいっ\n    reference\\response\n     \\e,n,\n     e 10,1,\n     n 1,9, \n    Category: e Precision: 0.91, Recall: 0.91 \n    Category: n Precision: 0.90, Recall: 0.90\n\n    ```", "```py\nBaseClassifierEvaluator<CharSequence> evaluator \n  = new BaseClassifierEvaluator<CharSequence>(null, \n                categories, storeInputs);\n```", "```py\nevaluator.addClassification(truthCategory, \n          responseClassification, text);\n```"]