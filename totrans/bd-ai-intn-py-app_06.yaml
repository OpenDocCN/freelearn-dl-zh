- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI/ML Application Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the landscape of intelligent applications evolves, their architectural design
    becomes pivotal for efficiency, scalability, operability, and security. This chapter
    provides a guide on key topics to consider as you embark on creating robust and
    responsive AI/ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter begins with **data modeling**, examining how to organize data in
    a way that maximizes effectiveness for three different consumers: humans, applications,
    and AI models. You will learn about **data storage**, considering the impact of
    different data types and determining the best storage technology. You will estimate
    storage needs and determine the best MongoDB Atlas cluster configuration for your
    example application.'
  prefs: []
  type: TYPE_NORMAL
- en: As you learn about **data flow**, you will explore the detailed movement of
    data through ingestion, processing, and output to maintain integrity and velocity.
    This chapter also addresses **data lifecycle management**, including updates,
    aging, and retention, ensuring that data remains relevant and compliant.
  prefs: []
  type: TYPE_NORMAL
- en: Security concerns are stretched further for AI/ML applications due to the risks
    of exposing data or logic to AI models. This chapter discusses security measures
    and **role-based access control** (**RBAC**) to protect sensitive data and logic
    integrity. You will also learn the best principles for data storage, flow, modeling,
    and security, providing practical advice to avoid common pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, you will use a fictitious news application called **MongoDB
    Developer News** (**MDN**), which will be like Medium.com, equipping you to create
    intelligent applications by using a practical example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freshness and retention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and RBAC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the prerequisites to follow along with the code in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A MongoDB Atlas cluster `M0` tier (free) should be sufficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI account and API key with access to the `text-embedding-3-large` model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Python 3 working environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed Python libraries for MongoDB, LangChain, and OpenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atlas Search indexes and Vector Search indexes created on the MongoDB Atlas
    cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section delves into the diverse types of data required by AI/ML systems,
    including structured, unstructured, and semi-structured data, and how these are
    applied to MDN’s news articles. The following are short descriptions of each to
    set a basic understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured data** conforms to a predefined schema and is traditionally stored
    in relational databases for transactional information. It powers systems of engagement
    and intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unstructured data** includes binary assets, such as PDFs, images, videos,
    and others. Object stores such as Amazon S3 allow storing these under a flexible
    directory structure at a lower cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-structured data**, such as JSON documents, allow each document to define
    its schema, accommodating both common and unique data points, or even the absence
    of some data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDN will store news articles, subscriber profiles, billing information, and
    more. For simplicity, in this chapter, you will focus on the data about each news
    article and related binary content (which would be images). *Figure 6**.1* describes
    the data model of the `articles` collection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Schema for the articles collection'
  prefs: []
  type: TYPE_NORMAL
- en: The articles collection represents a news article with metadata, including creation
    details, tags, and contributors. All documents feature a title, summary, body
    content in HTML and plain text, and associated media elements such as images.
  prefs: []
  type: TYPE_NORMAL
- en: Enriching data with embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete the MDN data model, you need to consider data that will also be
    represented and stored via embeddings. **Text embeddings** for article titles
    and summaries will enable semantic search, while **image embeddings** will help
    find similar artwork used across articles. *Table 6.1* describes the data fields,
    embedding models to use, and their vector sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Field(s)** | **Embedding model** | **Vector size** |'
  prefs: []
  type: TYPE_TB
- en: '| Text | `title` | OpenAI `text-embedding-3-large` | 1,024 |'
  prefs: []
  type: TYPE_TB
- en: '| Text | `summary` |'
  prefs: []
  type: TYPE_TB
- en: '| Image | `contents` | OpenAI CLIP | 768 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.1: Embeddings for the articles collection'
  prefs: []
  type: TYPE_NORMAL
- en: Each article has a title and summary. Instead of embedding them separately,
    you will concatenate them and create one text embedding for simplicity. Ideally,
    for images, you would store the embedding with each content object in the `contents`
    array. However, support for fields inside arrays of objects for vector indexes
    is not available today in MongoDB Atlas and leads to the **anti-pattern of bloated
    documents**. The best practice is to store image embeddings in a separate collection
    and use the **extended reference schema design pattern**. You can learn more about
    indexing arrays with MongoDB, bloated documents, and the extended reference pattern
    from the links given in the *Further Reading* chapter of this book. *Figure 6**.2*
    shows the updated data model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Schema for articles with embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.2* shows the corresponding vector indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| `articles` | `article_content_embeddings` |'
  prefs: []
  type: TYPE_TB
- en: '| `semantic_embedding_vix` | `content_embedding_vix` |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.2: Vector search index definitions'
  prefs: []
  type: TYPE_NORMAL
- en: Considering search use cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before finalizing the data model, let’s consider search use cases for articles,
    and adapt the model once more. Here are some broader search use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`title` and `summary` fields for text search, and the `brand` and `subscription_type`
    fields for filtering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tags` field. You will also need a vector search index to cover the `title
    +` `summary` embedding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_id`, `brand`, and `subscription_type` fields from the `articles` collection
    into the `article_content_embeddings` collection. Since there is already an `_id`
    field in this collection, you can create a composite primary key that includes
    the `_id` of the article and the `_id` of the content. *Figure 6**.3* shows the
    updated data model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B22495_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Updated schema for articles with embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.3* shows updated vector indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| `articles` | `article_content_embeddings` |'
  prefs: []
  type: TYPE_TB
- en: '| `semantic_embedding_vix` | `content_embedding_vix` |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.3: Updated vector search index definitions'
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.4* shows the new text search index.'
  prefs: []
  type: TYPE_NORMAL
- en: '| `articles` |'
  prefs: []
  type: TYPE_TB
- en: '| `lexical_six` |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.4: Text search index definition'
  prefs: []
  type: TYPE_NORMAL
- en: You learned about writing vector search queries in [*Chapter 4*](B22495_04.xhtml#_idTextAnchor061)*,
    Embedding Models*. To learn more about hybrid search queries, you can refer to
    the tutorial at [https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand your data model and the indexes required, you need to
    consider the number of articles MDN will bear (including the sizes of embeddings
    and indexes), peak daily times, and more to determine the overall storage and
    database cluster requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will perform sizing, which is an educated estimate, for
    storage requirements. You will consider not just volume size and speed, but also
    several other aspects of the database cluster that are needed for harnessing the
    data of your application while following expected data access patterns.
  prefs: []
  type: TYPE_NORMAL
- en: MDN plans to publish 100 articles daily. Keeping the articles from the last
    5 years, the number of articles would total 182,500\. With 48 million subscribers
    and 24 million daily active users, peak access occurs for 30 minutes daily across
    three major time zones, as shown in *Figure 6**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: MDN subscriber time zones and peak times'
  prefs: []
  type: TYPE_NORMAL
- en: First, you will estimate the total data size. Each article has one 1,024-dimension
    embedding for semantic search and five 768-dimension embeddings for image search,
    totaling 40 KB uncompressed (dimensions use the double type). With the `title`,
    `summary`, `body` (with and without markup), and other fields, the average article
    size will be about 300 KB uncompressed.
  prefs: []
  type: TYPE_NORMAL
- en: Five years of articles will require about 100 GB uncompressed. With MongoDB’s
    **WiredTiger** compression (Snappy, zlib, and zstd are also available as compression
    options), this reduces to about 50 GB on disk. The defined vector indexes add
    about 3.6 GB. Images and binary assets will be stored in Amazon S3\. For simplicity,
    you will not estimate the size of search and traditional indexes. You can safely
    say that MDN will need 80 to 100 GB on disk in MongoDB Atlas, which is very manageable
    by today’s cloud computing standards.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you will determine the most suitable MongoDB Atlas cluster configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the type of database cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MongoDB Atlas provides two main cluster types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Replica sets** have a primary node for writes and secondary nodes for high
    availability, which can also be used for reads. These sets scale vertically and
    can also scale horizontally for reads by adding more nodes in the same or different
    cloud regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sharded clusters** consist of multiple shards, each being a part of the overall
    dataset, and each being a replica set. They scale vertically and horizontally
    for both reads and writes. Shards can be placed in different cloud regions to
    enhance data locality and compliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, how can you determine whether a replica set is sufficient or a sharded cluster
    is needed? Key factors include the size of the dataset or the throughput of applications
    that can challenge the capacity of a single server. For example, high query rates
    can exhaust the server’s CPU capacity and working set sizes larger than the system’s
    RAM can stress the I/O capacity of disk drives. MDN publishes 100 articles per
    day, so sharding is not necessary for this reason.
  prefs: []
  type: TYPE_NORMAL
- en: Other reasons for sharding include data governance and compliance and **recovery
    point objective** (**RPO**) and **recovery time objective** (**RTO**) policies,
    which are key metrics in disaster recovery and business continuity planning. None
    of these are applicable to MDN.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the small number of writes per second and manageable data size,
    it makes sense to use a replica set. You will now need to determine the amount
    of RAM and IOPS needed; both are key components for fast response times.
  prefs: []
  type: TYPE_NORMAL
- en: Determining IOPS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MDN is a low-write, high-read use case. With only 100 articles added per day,
    there is minimal pressure on the storage system for writes. *Table 6.5* shows
    the storage and IOPS options provided by MongoDB Atlas.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Storage types** | **Lowest IOPS/storage** | **Highest IOPS/storage** |'
  prefs: []
  type: TYPE_TB
- en: '| Standard IOPS | 3,000 IOPS/10 GB | 12,288 IOPS/4 TB 16,000 IOPS/14 TB**Extended
    storage enabled |'
  prefs: []
  type: TYPE_TB
- en: '| Provisioned IOPS | 100 IOPS/10 GB | 64,000 IOPS/4 TB |'
  prefs: []
  type: TYPE_TB
- en: '| NVMe | 100,125 100% random read IOPS35,000 write IOPS 380 GB | 3,300,000
    100% random read IOPS1,400,000 write IOPS 4,000 GB |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.5: MongoDB Atlas storage types on AWS'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 6**.4*, there will be a 30-minutes peak period, during which
    24 million users are expected to be active daily. So, you need to provision 6,000
    IOPS, as shown in *Table 6.6*. This is based on subscriber distribution, memory
    versus disk reads, and each article requiring 3 IOPS for disk reads (150 KB compressed
    ÷ 64 KB I/O size of Amazon EBS).
  prefs: []
  type: TYPE_NORMAL
- en: '| **Region** | **Allocation** | **DAU** | **20% reads** **from disk** | **Disk
    reads/sec during** **peak time** | **IOPS required** |'
  prefs: []
  type: TYPE_TB
- en: '| `AMER`^ | 40% | 9,600,000 | 1,920,000 | 1,067 | 3,200^ |'
  prefs: []
  type: TYPE_TB
- en: '| `EMEA`^ | 20% | 4,800,000 | 960,000 | 533 | 1,600^ |'
  prefs: []
  type: TYPE_TB
- en: '| `APAC` | 25% | 6,000,000 | 1,200,000 | 667 | 2,000 |'
  prefs: []
  type: TYPE_TB
- en: '| `LATAM`^ | 15% | 3,600,000 | 720,000 | 400 | 1,200^ |'
  prefs: []
  type: TYPE_TB
- en: '| ^ Zones overlapping at peak time |  |  |  | Peak IOPS | 6,000 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.6: MDN global subscriber distribution'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum standard IOPS on any Atlas cluster on AWS is 3,000\. To achieve
    6,000 IOPS you would need to use an Atlas `M50` tier with 2TB disk, which feels
    over-provisioned and would not provide low latency to all readers if deployed
    in a single cloud region. To address this, MDN will deploy the application stack
    in major geographies, enabling regional provisioning, workload distribution, and
    local reads for an optimal customer experience.
  prefs: []
  type: TYPE_NORMAL
- en: With MongoDB Atlas, you can place vector search nodes across regions. The `S40`
    tier offers 26,875 read IOPS, which is sufficient for this example, and a 2-node
    minimum per region, which ensures high availability.
  prefs: []
  type: TYPE_NORMAL
- en: While Vector Search nodes will handle lexical, semantic, and image search, the
    full JSON document must be fetched from a MongoDB data node after matching. To
    fully support local reads, we must provision read-only nodes in the same regions
    and meet IOPS requirements. We can do this with the Atlas `M40` tier. Having determined
    the IOPS needed, you now need to estimate RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Determining RAM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For data nodes, the Atlas `M40` tier provides 16 GB of RAM. The MongoDB WiredTiger
    storage engine reserves 50% of (RAM - 1 GB) for its cache. With documents averaging
    300 KB in size, the cache can hold approximately 28,000 documents. Keep in mind
    that traditional index sizes might slightly reduce this number. Given the addition
    of 100 new articles daily, the cache on an `M40` tier can accommodate data for
    about 280 days, or roughly 9 months, which is more than sufficient for this example.
  prefs: []
  type: TYPE_NORMAL
- en: The Search `S40` tier offers 16 GB of RAM, 2 vCPUs, and 100 GB of storage. The
    HNSW graph, or the vector index, must fit in the memory.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You learned about **HNSW** or **hierarchical navigable small worlds** in [*Chapter
    5*](B22495_05.xhtml#_idTextAnchor115), *Vector Databases*.
  prefs: []
  type: TYPE_NORMAL
- en: One article uses 1 x 1,024 vector + 5 x 768 vectors = 19.5 KB. With 3.5 GB needed
    for 182,500 articles, 16 GB of RAM is more than sufficient for vector search and
    leaves room for the lexical search index. The `S30` tier, which offers 4 GB of
    RAM, 1 vCPU, and 50 GB storage, is less costly, but note that more CPUs allows
    more concurrent searches.
  prefs: []
  type: TYPE_NORMAL
- en: Final cluster configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have now determined the cluster configuration for MDN. *Table 6.7* describes
    the MDN global cloud architecture, detailing the distribution of Atlas nodes across
    different regions. The `AMER` region, identified as the primary region, uses `M40`
    tier nodes and `S30` vector search nodes to serve writes and searches for the
    Americas, while the `EMEA`, `APAC`, and `LATAM` regions use `M40` read-only nodes
    and `S30` vector search nodes to serve local searches only for their respective
    region. Each region will need a deployment of the MDN application stack, as pictured
    in the global map in *Table 6.7*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Region** | **Atlas base** **tier nodes** | **Atlas** **read-only nodes**
    | **Atlas Vector** **Search nodes** |'
  prefs: []
  type: TYPE_TB
- en: '| `AMER`(primary region) | `M40`(three included) |  | `S30` x2 |'
  prefs: []
  type: TYPE_TB
- en: '| `EMEA` |  | `M40` x2 | `S30` x2 |'
  prefs: []
  type: TYPE_TB
- en: '| `APAC` |  | `M40` x2 | `S30` x2 |'
  prefs: []
  type: TYPE_TB
- en: '| `LATAM` |  | `M40` x2 | `S30` x2 |'
  prefs: []
  type: TYPE_TB
- en: '| **MDN global** **cloud architecture**![](img/B22495_Table_6.7.jpg) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.7: MongoDB Atlas cluster configuration for MDN'
  prefs: []
  type: TYPE_NORMAL
- en: Performance and availability versus cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Notice that additional read-only nodes were not provisioned in the `AMER` region,
    using the two secondary nodes as read-only instead. This saves costs due to MDN’s
    low write profile, despite potential resource competition. Provisioning only one
    `M40` read-only node in other regions saves more costs but increases latency during
    maintenance windows, as reads will be rerouted.
  prefs: []
  type: TYPE_NORMAL
- en: To protect against a complete `AMER` outage while adhering to best practices,
    consider provisioning five nodes across three regions and deploying the application
    stack in the two regions with two electable nodes each.
  prefs: []
  type: TYPE_NORMAL
- en: Data flow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data flow** involves the movement of data through a system, affecting the
    accuracy, relevance, and speed of the results delivered to consumers, which, in
    turn, influences their engagement. This section explores design considerations
    for handling data sources, processing data, prompting LLMs, and embedding models
    to enrich data using MDN as an example. *Figure 6**.5* illustrates this flow.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Typical data flow in an AI/ML application'
  prefs: []
  type: TYPE_NORMAL
- en: Let's us begin with the design for handling data sources. Data can be ingested
    into MongoDB Atlas either statically (at rest) from files as it is, or dynamically
    (in motion), allowing for continuous updates, data transformation, and logic execution.
  prefs: []
  type: TYPE_NORMAL
- en: Handling static data sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest way to import static data is to use `mongoimport`, which supports
    JSON, CSV, and TSV formats. It is ideal for initial loads or bulk updates as it
    can handle large datasets. Moreover, increasing the number of insertion workers
    to match the host’s vCPUs can boost import speed.
  prefs: []
  type: TYPE_NORMAL
- en: '`mongoimport` can also be used dynamically to update externally sourced data.
    You can build invocation commands at runtime and execute them as out-of-process
    tasks. Some video game companies use this method to update player profiles with
    purchase data from mobile app stores.'
  prefs: []
  type: TYPE_NORMAL
- en: Using MDN as an example, users can provide their GitHub ID when subscribing.
    With GitHub’s API, you can create a list of the programming languages used in
    the repositories that users own or have contributed to. A scheduled job can fetch
    this data periodically. The list of languages can then imported and merged into
    their profiles to recommend articles later. *Table 6.8* demonstrates how you can
    do this.
  prefs: []
  type: TYPE_NORMAL
- en: '| `github-20240719.json` |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `mdn.subscribers` |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| The `mongoimport` invocation to merge data matching on the `github_id` field
    |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `mdn.subscribers` after merge |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.8: Example of using mongoimport to merge data'
  prefs: []
  type: TYPE_NORMAL
- en: While `mongoimport` is a versatile tool for various data import needs, it does
    not support continuous synchronization, logic execution, or data transformations.
    You will now explore some methods that do support these functions.
  prefs: []
  type: TYPE_NORMAL
- en: Storing operational data enriched with vector embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When original representations are stored or updated, their corresponding vector
    embeddings must be refreshed to accurately reflect the content. This can be done
    in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronously**: Obtains the updated vector embedding before the database
    operation, writing both data and embedding together. This method is suitable for
    fast, simple embedding models or when the model is locally hosted. However, it
    may fail if the response times of the embedding model vary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronously**: Ensures immediate consistency of primary data and allows
    for prompting the embedding model afterward. While this offers scalability and
    handles unpredictable models, it introduces latency during which embeddings are
    temporarily outdated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can keep embeddings up to date asynchronously in MongoDB using the following
    four methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kafka connector**: You can facilitate data flow from Apache Kafka into MongoDB
    collections through the Kafka connector. It is a Confluent-verified connector
    and allows data to flow from Apache Kafka topics into MongoDB as a **data sink**
    and publishes changes from MongoDB to Kafka topics as a **data source**. To keep
    embeddings up to date, you would use the sink connector and develop a post-processor
    in Java. You can learn more about sink post-processors here: [https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors](https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atlas Stream Processing**: This method handles complex data streams with
    the same query API as MongoDB Atlas databases. It enables continuous aggregation
    and includes schema validation for message integrity and timely issue detection.
    Processed data can be written to Atlas collections, and they are integrated into
    Atlas projects and independent of Atlas clusters. Atlas Stream Processing logic
    is programmed in JavaScript using MongoDB aggregation syntax. For an example of
    using Atlas Stream Processing to handle embedding data, see [https://www.mongodb.com/solutions/solutions-library/rag-applications](https://www.mongodb.com/solutions/solutions-library/rag-applications).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atlas Triggers**: Atlas Triggers execute application and database logic by
    responding to events or following predefined schedules. Each Trigger listens for
    specific event types and is linked to an Atlas Function. When a matching event
    occurs, the Trigger fires and passes the event object to the linked Function.
    Triggers can respond to various events, such as specific operations in a collection,
    authentication events such as user creation or deletion, and scheduled times.
    They are fully managed instances of change streams but limited to JavaScript.
    For an example of using Atlas Triggers to keep embeddings up to date, see [https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/](https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change streams**: This method provides real-time access to data changes.
    Applications can subscribe to changes in a collection, database, or entire deployment
    and react immediately, with events processed in order and being resumable? Using
    the aggregation framework, change streams allow filtering and transforming notifications.
    They can be used with any programming language supported by an official MongoDB
    driver. However, they are not fully managed, requiring a running host to be maintained
    alongside the main application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that this book is written for Python developers, you will learn how to
    use a change stream written in Python. *Table 6.9* shows a Python 3 change stream
    using LangChain and OpenAI to embed the title and summary of an MDN article. It
    is triggered for new articles or changes to the title or summary following the
    data model from *Figure 6**.3* and the vector index from *Table 6.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Table 6.9: Change stream written in Python to set or update embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to handle the data flow for setting or updating
    embeddings, you will learn about data freshness and retention, which are essential
    for delivering relevant and timely content.
  prefs: []
  type: TYPE_NORMAL
- en: Freshness and retention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fresh data and effective retention strategies ensure that your content is relevant
    and delivered on time. **Freshness** keeps users engaged with the latest articles,
    comments, and recommendations. **Retention strategies** manage the data lifecycle,
    preserving valuable historical data for analytics while purging obsolete data.
    This section explores methods for ensuring up-to-date content and efficient data
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time updates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary concern is to ingest and update new data in real time, making it
    available across all cloud regions. For the news site, this means new articles
    and their vector embeddings should be promptly persisted and replicated for global
    access.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this with a distributed data model and application, use an ACID transaction
    to ensure that the article and its content embeddings are written together as
    a single unit. For an example of creating MongoDB transactions in Python, see
    [https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2](https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, balance data reliability, consistency, and performance in a distributed
    setup using MongoDB’s tunable consistency with `writeConcern`, `readConcern`,
    and `readPreference`. These modifiers help to ensure data integrity and quick
    access. The following is an explanation of these modifiers, but for a deeper understanding,
    you can visit [https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/](https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/):'
  prefs: []
  type: TYPE_NORMAL
- en: '`writeConcern:majority` ensures data consistency and durability by acknowledging
    write operations only after data is written to the majority of replica set members,
    reducing the risk of data loss during failures. It is the default write concern.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readConcern:majority` provides read consistency by ensuring that read operations
    return the most recent data acknowledged by the majority of the replica set members,
    providing a consistent view of the data across the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`readPreference:nearest` optimizes latency by directing read operations to
    the replica set member with the lowest network latency. For MDN, this minimizes
    response times by allowing each regional application deployment to read from the
    nearest MongoDB data and vector nodes, and balancing consistency and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have learned how to ensure data availability and speed, the next
    focus is on data lifecycle management, a key aspect of data freshness and retention.
  prefs: []
  type: TYPE_NORMAL
- en: Data lifecycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data lifecycle** refers to the various stages data goes through from creation
    to deletion, and how it may traverse and change systems or storage formats, including
    when data is archived or deleted. As latest content is added, older content may
    become less relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, older articles can be moved to an archive database or cold storage,
    reducing storage costs and optimizing active database performance. However, moving
    data to cold storage may reduce search capabilities compared to the operational
    database. Here are three approaches for handling the data lifecycle, along with
    their trade-offs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**All data in the operational cluster**: Keeping all data in the operational
    cluster is the most performant but costly approach, suitable for scenarios where
    most data is frequently accessed, such as global online games, authentication
    providers, or financial platforms. MongoDB Atlas supports this with sharded clusters
    and global clusters. Global clusters allocate *data zones* to cloud regions for
    capacity management and data locality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active and historic operational data clusters**: This involves using high-performance
    hardware for recent data and less capable hardware for older data, balancing functionality,
    and cost savings. With MongoDB Atlas, data can be moved from active to historic
    cluster(s) using Cluster-to-Cluster Sync and TTL indexes. Other platforms such
    as Apache Kafka, Confluent, and Striim also support this method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Active data cluster and historical storage**: Full historical data can be
    offloaded to cold storage while retaining key fields in the operational cluster,
    allowing for full or limited query and search capabilities. For MDN, this ensures
    that users can find historical articles through lexical semantic searches, with
    full articles stored in cold storage and accessed when needed. With MongoDB Atlas,
    this can be achieved using Online Archive and Data Federation. **Online Archive**
    automatically moves data from the cluster to lower-cost cloud storage based on
    the set expiration. **Data Federation** allows transparent querying of both clusters
    and the archive, regardless of the source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This section covered data lifecycle management, emphasizing how data is managed
    from creation to archival. You learned about three strategies: maintaining all
    data in the operational cluster for maximum performance, separating active and
    historical data to balance cost and performance, and offloading historical data
    to cold storage while retaining some search functionality. Now, you will learn
    about upgrading embedding models.'
  prefs: []
  type: TYPE_NORMAL
- en: Adopting new embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI superseded the `text-search-davinci-*-001` model with `text-embedding-ada-002`
    on December 15, 2022, and subsequently with `text-embedding-small/large` on January
    25, 2024\. It is likely that by the time you read this book, these models will
    be replaced too.
  prefs: []
  type: TYPE_NORMAL
- en: As you learned in the [*Chapter 4*](B22495_04.xhtml#_idTextAnchor061), *Embedding
    Models*, embeddings from one model are not compatible with another. Re-embedding
    previously indexed data may be necessary as newer models are adopted. This is
    a resource-intensive activity that requires design considerations upfront.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to choose an approach toward adopting new embedding models. You
    can either continue using the existing vector fields and perform lengthy all-or-nothing
    upgrades, double-embed for a period, or implement a gradual upgrade. Let''s explore
    these three approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use existing vector fields**: This approach keeps the application code intact
    but requires downtime to re-embed data and replace vector indexes. This approach
    is suitable if the re-embedding and re-indexing time fits within your allowable
    downtime windows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Double-embed temporarily**: This approach double embeds fields for new or
    modified data using the old and new model. It uses a background job to add new
    embeddings for data that is not modified. When all data has double embeddings,
    the application will be updated and deployed to use the new embeddings. Once stable,
    the deprecated vectors and indexes can be removed with another background job.
    Ensure sufficient disk space and memory for when two sets of vectors coexist.
    This approach is suitable if the downtime windows are small and only accommodate
    application deployment times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter` type of MongoDB’s vector indexes (shown in *Table 6.3*), you can introduce
    a new field to distinguish between documents with old and new vectors and implement
    the union. Eventually, old vectors and indexes can be dropped, and you can remove
    unneeded logic. This approach is suitable if no downtime is allowed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By addressing these three main concerns—data ingestion and real-time updates,
    managing the data lifecycle and aging, and upgrading embedding models—your application
    can ensure that its data remains fresh and relevant, providing an optimal platform
    and striving for the best user experience. Now, you will learn about security
    and its considerations for AI-intensive applications.
  prefs: []
  type: TYPE_NORMAL
- en: Security and RBAC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Security measures** protect data from unauthorized access and breaches, while
    RBAC ensures appropriate access levels based on roles. Here are key security and
    RBAC strategies to protect data integrity and privacy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data encryption and secure storage**: Encrypting data at rest and in transit
    is crucial for securing an application. Encryption at rest protects data from
    unauthorized access, while encryption in transit secures data as it moves between
    users and the application. MongoDB Atlas offers built-in integration with **AWS
    Key Management Service** (**AWS KMS**) for encryption at rest and TLS/SSL out
    of the box for data in transit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access controls and user authentication**: RBAC manages permissions, ensuring
    that users access only necessary data and functionalities. In the case of MDN,
    separate roles, such as editors and readers, require various levels of access.
    Different database users on MongoDB can be set up with distinct levels of permissions
    following the principle of least privilege. For example, only the application
    identity used by the microservice that embeds data would have write permissions
    to the collections where embeddings are stored, while the application identity
    used by human actors would only have read permissions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and auditing**: Continuous monitoring and auditing detect and
    respond to security incidents in real time. Monitoring tools and audit logs track
    user activities and identify unusual access patterns. MongoDB Atlas offers advanced
    monitoring and alerting capabilities, allowing administrators to set up alerts
    for suspicious activities. Regularly reviewing audit logs ensures compliance with
    security policies and provides insights for improving security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data backup and recovery**: Maintain data integrity and availability with
    regular backups to minimize downtime and loss during security breaches or incidents.
    MongoDB Atlas offers automated backup solutions with snapshots, ensuring quick
    recovery. If encryption at rest is enabled (for example, AWS KMS), embeddings
    and operational data are encrypted under the same key in both volumes and backups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While there are many security-related concerns, the ones just covered should
    suffice to start building AI applications. Ensuring security is a continuous effort
    that organizations must adopt and enforce to maintain compliance, foster user
    trust, and safeguard application integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for AI/ML application design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers best practices for the five concerns covered in this chapter—data
    modeling, data storage, data flow, data freshness and retention, and security
    and RBAC. These guidelines will help ensure that your application is efficient,
    scalable, and secure, providing a solid foundation for building reliable and high-performing
    AI apps. Here are the top two best practices for each aspect of your AI/ML application
    design.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data modeling**: The following techniques ensure efficiency and performance
    for handling embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Embeddings in separate collections**: Store embeddings in a separate collection
    to avoid bloated documents, especially when multiple embeddings and nested indexing
    limitations are involved. Duplicate fields to ensure efficient filtering and maintain
    performant searches.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid search**: Combine semantic and lexical searches using reciprocal rank
    fusion. This hybrid approach boosts search functionality by leveraging the strengths
    of both.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data storage**: To optimize database cluster sizing, implement the following
    best practices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sufficient IOPS and RAM based on peak usage**: Calculate required IOPS based
    on peak access times and application read/write patterns. Ensure data and search
    nodes have enough RAM to handle the caching and indexing needs of the most requested
    data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local reads**: Deploying nodes across regions helps minimize read latency
    and enhances the user experience. Ensure that each region has all the nodes required
    to fully serve data locally.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data flow**: Consider the following strategies for harnessing data flow effectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Asynchronous embedding updates**: Ensure primary data consistency by updating
    vector embeddings asynchronously. This method accommodates scalability and unpredictable
    model response times, although it introduces temporary latency.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic data handling**: Leverage technologies such as change streams, Atlas
    Triggers, Kafka, and Atlas Stream Processing to handle continuous updates, transformations,
    and logic execution.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data freshness and retention**: The following best practices can ensure that
    your application is relevant and prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Up-to-date embedding models**: Embeddings from one model are not compatible
    with another. Plan for model upgrades during downtime if possible, or consider
    gradual upgrades, which are architecturally complex but require no downtime. Leverage
    MongoDB’s flexible data model to transition between embeddings.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data tiering**: Implement a data aging strategy by moving older data to an
    archive cluster or cold storage while keeping recent data in high-performance
    clusters. Use broader MongoDB Atlas features such as Online Archive, Data Federation,
    and more for effective data tiering.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and RBAC**: Following are the best practices for ensuring the security
    of your data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**RBAC**: Assign role-based permissions and follow the **principle of least
    privilege** (**PoLP**), ensuring that users and entities access only necessary
    data and actions. For instance, code embedding data should have write access only
    to embedding collections.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encryption and storage**: Turn on encryption at rest and integrate with KMS
    to ensure that all data volumes and backups are encrypted with your own key.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing these best practices boosts the efficiency, scalability, and security
    of your AI/ML applications. Though just a starting point, these guidelines lay
    a solid foundation for building reliable, high-performing systems. With these
    best practices, you can navigate the complexities of modern AI and prepare your
    applications for long-term success and adaptability in a rapidly evolving tech
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered critical architectural considerations for developing intelligent
    applications. You learned about data modeling and how to evolve your model to
    fulfill use cases, address technical limitations, and consider patterns and anti-patterns.
    This approach ensures that data is not only useful but also accessible and optimally
    utilized across various components of your AI/ML system.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage was another key aspect of this chapter, focusing on the selection
    of appropriate storage technologies based on different data types and the specific
    needs of the application. It highlighted the importance of accurately estimating
    storage requirements and other aspects of choosing the right MongoDB Atlas cluster
    configuration. The fictitious example of the MDN application served as a practical
    case study, illustrating how to apply these principles in a real-world scenario.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also explored the flow of data through ingestion, processing, and
    output to ensure data integrity and maintain the velocity of data operations.
    This chapter also addressed data lifecycle management, including the importance
    of data freshness and retention. You learned strategies for managing updates and
    changing embedding models used by your application.
  prefs: []
  type: TYPE_NORMAL
- en: Security is a paramount concern in AI/ML applications, and you learned brief
    but important points about protecting the integrity of data and application logic.
    Concluding with a compilation of best practices, this chapter summarized key principles
    from data modeling, storage, flow, and security, offering practical advice to
    avoid common pitfalls and enhance the development of robust AI/ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will explore different AI/ML frameworks, Python libraries,
    and publicly available APIs and other tools.
  prefs: []
  type: TYPE_NORMAL
- en: Part 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Building Your Python Application: Frameworks, Libraries, APIs, and Vector Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This following set of chapters will equip you with the necessary tools for AI
    development through detailed instructions and examples on enhancing developer
    and user experience with Python and retrieval-augmented generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part of the book includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B22495_07.xhtml#_idTextAnchor162), *Useful Frameworks, Libraries,
    and APIs*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B22495_08.xhtml#_idTextAnchor180), *Implementing Vector Search
    in AI Applications*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
