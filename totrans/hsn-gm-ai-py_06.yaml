- en: Exploring SARSA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索SARSA
- en: In this chapter, we continue with our focus on **Temporal Difference Learning**
    (**TDL**) and expand on it from TD (0) to multi-step TD and beyond. We will look
    at a new method of **Reinforcement Learning** (**RL**) called SARSA, explore what
    it is, and how it differs from Q-learning. From there, we will look at a few examples
    with new continual control learning environments from Gym. Then, we will move
    to a deeper understanding of TDL and introduce concepts called **TD lambda** (λ)
    and **eligibility traces**. Finally, we will finish off this chapter by looking
    at an example of SARSA.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续关注**时间差分学习**（**TDL**），并从TD(0)扩展到多步TD以及更远。我们将研究一种新的**强化学习**（**RL**）方法，称为SARSA，探索它是什么，以及它与Q学习的区别。从那里，我们将查看一些来自Gym的新持续控制学习环境示例。然后，我们将更深入地理解TDL，并介绍称为**TD
    lambda**（λ）和**资格痕迹**的概念。最后，我们将通过查看SARSA的示例来完成本章。
- en: 'For this chapter, we will extend our discussion of TDL and uncover **State
    Action Reward State Action** (**SARSA**), continuous action spaces, TD (λ), eligibility
    traces, and on-policy learning. Here is an overview of what we will cover in this
    chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将扩展我们对TDL的讨论，并揭示**状态-动作-奖励-状态-动作**（**SARSA**），连续动作空间，TD(λ)，资格痕迹和在线策略学习。以下是本章我们将涵盖的内容概述：
- en: Exploring SARSA on-policy learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索SARSA在线策略学习
- en: Using continuous spaces with SARSA
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SARSA与连续空间
- en: Extending continuous spaces
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展连续空间
- en: Working with TD (λ) and eligibility traces
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TD(λ)和资格痕迹进行工作
- en: Understanding SARSA (λ)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解SARSA(λ)
- en: This chapter is very much a continuation of [Chapter 4](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml),
    *Temporal Difference Learning*. Please read that chapter before this one. In the
    next section, we continue right where we left off in the last chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章在很大程度上是[第4章](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml)，*时间差分学习*的延续。请在阅读本章之前阅读那一章。在下一节中，我们将继续上一章结束的地方。
- en: Exploring SARSA on-policy learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索SARSA在线策略学习
- en: SARSA, which is the process this method emulates. That is, the algorithm works
    by moving to a state, then choosing an action, receiving a reward, and then moving
    to the next state action. This makes SARSA an on-policy method, that is, the algorithm
    works by learning and deciding with the same policy. This differs from Q-learning,
    as we saw in [Chapter 4](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml), *Temporal
    Difference Learning*, where Q is a form of off-policy learner.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA，这是该方法模拟的过程。也就是说，算法通过移动到状态，然后选择一个动作，获得奖励，然后移动到下一个状态动作来工作。这使得SARSA成为一个在线策略方法，即算法通过使用相同的策略学习和决策。这与我们在[第4章](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml)，*时间差分学习*中看到的Q作为离线策略学习器的方法不同。
- en: 'The following diagram shows the difference in backup diagrams for Q-learning
    and SARSA:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Q学习和SARSA的回溯图之间的差异：
- en: '![](img/cdf4a700-7c4c-40b3-bfd1-59bfa0021c1c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cdf4a700-7c4c-40b3-bfd1-59bfa0021c1c.png)'
- en: Backup diagrams for Q and SARSA
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Q和SARSA的回溯图
- en: 'Recall that our Q-learner is an off-policy learner. That is, it requires the
    algorithm to update the policy or Q table offline and then later make decisions
    from that. However, if we want to tackle the TDL problem beyond one step or TD
    (0), then we need to have an on-policy learner. Our learning agent or algorithm
    must be able to update its policy in between whatever number of TD steps we may
    be looking at. This also requires us to update our Q update equation with a new
    SARSA update equation, as shown here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的Q学习器是一个离线策略学习器。也就是说，它需要算法离线更新策略或Q表，然后从那里做出决策。然而，如果我们想解决超过一步或TD(0)的TDL问题，那么我们需要有一个在线策略学习器。我们的学习代理或算法必须能够在观察到的任何数量的TD步骤之间更新其策略。这也要求我们更新我们的Q更新方程，使用新的SARSA更新方程，如下所示：
- en: '![](img/29924182-4562-4609-a89f-9c2186148fa3.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/29924182-4562-4609-a89f-9c2186148fa3.png)'
- en: 'Recall that our Q-learning equation was like so:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的Q学习方程是这样的：
- en: '![](img/550158be-5e57-49e7-ba97-21bc604ee66c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/550158be-5e57-49e7-ba97-21bc604ee66c.png)'
- en: 'In the previous equation, we have the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们有以下内容：
- en: '![](img/4606930b-f445-4204-b857-b84ec0ae33a0.png) The current state-action
    quality being updated'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![当前状态-动作质量正在更新](img/4606930b-f445-4204-b857-b84ec0ae33a0.png)'
- en: '![](img/3078a024-51a4-4f8b-9bfa-28230622897d.png) The learning rate'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![学习率](img/3078a024-51a4-4f8b-9bfa-28230622897d.png)'
- en: '![](img/4215b7a4-a539-4dd9-96e3-863b899564a6.png) The reward for the next state'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![下一个状态奖励](img/4215b7a4-a539-4dd9-96e3-863b899564a6.png)'
- en: '![](img/7322b8ce-8148-4521-b8cb-be1b4418663c.png) Gamma, the discount factor'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![伽马，折扣因子](img/7322b8ce-8148-4521-b8cb-be1b4418663c.png)'
- en: '[![](img/ce011eeb-f862-4cb7-94a3-f1469aab8d2a.png)] The maximum best or greedy
    action'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/ce011eeb-f862-4cb7-94a3-f1469aab8d2a.png)最大最佳或贪婪动作'
- en: 'We can further visualize this as shown in the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这一点进一步可视化，如下面的图所示：
- en: '![](img/63ee3993-33e5-4c08-b278-48e01c59b336.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/63ee3993-33e5-4c08-b278-48e01c59b336.png)'
- en: SARSA diagram and equation
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA图和方程
- en: Notice how that funny *max* term is gone now in SARSA and we use the expectation
    now and not just the best. This has to do with the action selection strategy.
    If you recall in Q-learning, we always used the max or best action according to
    averaging rewards. Recall that Q-learning assumes that you average the maximum
    reward. Instead, we want to select the action the agent perceives to be the one
    that will return the best possible returns. Hopefully, you have also noticed how
    we have progressed from speaking about rewards to value, state actions, state
    values, and now returns, where a return represents the perceived value for an
    action. We will discuss maximizing returns in more detail later in this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在SARSA中那个有趣的*max*项现在消失了，我们现在使用期望值而不是仅仅使用最佳值。这与动作选择策略有关。如果你还记得在Q-learning中，我们总是使用最大或最佳动作，根据平均奖励来选择。回想一下，Q-learning假设你平均最大奖励。相反，我们希望选择代理认为将返回最佳可能回报的动作。希望你也注意到了，我们是如何从谈论奖励到价值、状态动作、状态价值，再到现在的回报的，其中回报代表对动作的感知价值。我们将在本章后面更详细地讨论最大化回报。
- en: In the next section, we will learn how to solve a new type of problem called
    **continuous action spaces**. Then, we will look at how to use SARSA to solve
    a new Gym environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何解决一种称为**连续动作空间**的新类型问题。然后，我们将探讨如何使用SARSA来解决一个新的Gym环境。
- en: Using continuous spaces with SARSA
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SARSA的连续空间
- en: Up until now, we have been exploring the **finite Markov Decision Process**
    or **finite MDP**. These types of problems are all well and good for simulation
    and toy problems, but they don't show us how to tackle real-world problems. Real-world
    problems can be broken down or discretized into finite MDPs, but real problems
    are not finite. Real problems are infinite, that is, they define no discrete simple
    states such as showering or having breakfast. Infinite MDPs model problems in
    what we call continuous space or continuous action space, that is, in problems
    where we think of a state as a single point in time and state defined as a slice
    of that time. Hence, the discrete task of **eat breakfast** could be broken down
    to each time step including individual chewing actions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在探索**有限马尔可夫决策过程**或**有限MDP**。这类问题对于模拟和玩具问题来说都很好，但它们并没有展示我们如何解决现实世界的问题。现实世界的问题可以被分解或离散化为有限MDP，但现实问题并不是有限的。现实问题是无限的，也就是说，它们不定义像洗澡或吃早餐这样的离散简单状态。无限MDP在连续空间或连续动作空间中建模问题，也就是说，在我们将状态视为时间点的问题中，状态被定义为时间的切片。因此，**吃早餐**的离散任务可以分解为包括个别咀嚼动作的每个时间步。
- en: Solving an infinite MDP or continuous space problem is not trivial with our
    current toolset, but it will require us to apply discretization tricks. Applying
    discretization or breaking the continuous space into discrete spaces will make
    this problem solvable with our current toolset. In [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml),
    *Going Deep with DQN*, we will look to apply deep learning to a continuous action
    space, which allows us to solve these environments without using these discretization
    tricks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们当前的工具集解决无限MDP或连续空间问题并不简单，但我们需要应用离散化技巧。应用离散化或把连续空间分解成离散空间将使这个问题可以用我们的当前工具集解决。在[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)“深入DQN”，我们将探讨如何将深度学习应用于连续动作空间，这样我们就可以不使用这些离散化技巧来解决这些环境。
- en: Many continuous RL environments have more environmental states than atoms in
    the observable universe, and yes, that is a very big number. We have managed to
    solve these problems by applying deep learning and hence deep RL starting in [Chapter
    6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going Deep with DQN*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 许多连续强化学习环境的环境状态比可观测宇宙中的原子还多，是的，这是一个非常大的数字。我们已经通过应用深度学习，从[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)“深入DQN”开始，解决了这些问题。
- en: The code for this chapter was originally sourced from this GitHub repository: [https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym](https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym). It
    looks like the author, Shrinand Thakkar, has since moved on to other pursuits
    and did not complete this excellent work as he intended.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码最初来源于这个GitHub仓库：[https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym](https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym)。看起来作者Shrinand
    Thakkar已经转向了其他追求，并且没有完成他原本打算完成的这项优秀的工作。
- en: 'Open `Chapter_5_1.py` and follow the exercise shown here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Chapter_5_1.py`并遵循这里显示的练习：
- en: 'The full source code for the listing is as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出的完整源代码如下：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Moving past the imports, we will look at the hyperparameter initialization
    code:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过导入部分，我们将查看超参数初始化代码：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We start the code block with the instantiation of a new environment, `MountainCar-v0`,
    which is in a continuous space environment. We then see the `Q_table` table is
    initialized with all zeros. Then, we set values for the learning rate, `alpha`;
    the discount factor, `gamma`; and the number of `episodes`. Also, we see a new
    list called `buckets` constructed. We will cover what buckets do shortly.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从创建一个新的环境`MountainCar-v0`开始，它位于连续空间环境中。然后我们看到`Q_table`表被初始化为全零。然后，我们设置了学习率`alpha`、折现因子`gamma`和剧集数`episodes`的值。我们还看到构建了一个新的列表`buckets`。我们将在稍后介绍`buckets`的作用。
- en: 'From there, jump to the end of the code. We want a high-level overview of what
    the code does first. Take a look at the episode `for` loop, shown here:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那里跳到代码的末尾。我们首先想要对代码的功能有一个高级概述。看看这里显示的剧集`for`循环：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code is the episode loop code and very much follows the pattern
    that we have seen in several previous chapters. The one major point of difference
    here is the way the algorithm/agent seemingly picks an action twice, as can be
    seen in the following block of code:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码是剧集循环代码，非常遵循我们在前几章中看到的模式。这里的一个主要不同点是算法/智能体似乎选择了两次动作，如下面的代码块所示：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The difference from Q-learning here is that the agent in SARSA is on-policy,
    that is, the action it picks needs to also decide its next action. Recall, in
    Q-learning, the agent works off-policy, that is, it takes an action from a previously
    learned policy. Again, this also goes back to TD (0) or one step, where the algorithm
    is still only looking one step ahead.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与Q-learning相比，这里的区别在于SARSA中的智能体是按策略的，也就是说，它选择的行为也需要决定其下一个行为。回想一下，在Q-learning中，智能体是离策略的，也就是说，它从一个之前学习过的策略中采取行动。同样，这也回到了TD(0)或一步，其中算法仍然只看一步。
- en: 'At this point, let''s run the algorithm to see how this works. A few examples
    of the car climbing the hill can be seen here:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个阶段，让我们运行算法来看看它是如何工作的。这里可以看到一些汽车爬山的示例：
- en: '![](img/6216cdb5-09f4-4b27-8309-332948c02e30.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6216cdb5-09f4-4b27-8309-332948c02e30.png)'
- en: Example output from Chapter_5_1.py
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`Chapter_5_1.py`的示例输出
- en: From the preceding screenshot, we can see that the agent is climbing the hill.
    Let the agent continue climbing until it reaches the flag; it should almost get
    there. Now, this is cool and fairly powerful stuff, but even more so considering
    we can do this by assuming our infinite MDP (continuous space) is controllable
    in discrete steps and hence a finite MDP. To do that, we have to learn how to
    discretize a continuous action space and we will see how to do that in the next
    section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图，我们可以看到智能体正在爬山。让智能体继续爬山直到它到达旗帜；它应该几乎要到达那里了。现在，这很酷，而且相当强大，但考虑到我们可以通过假设我们的无限MDP（连续空间）可以在离散步骤中控制，因此是一个有限MDP，这一点更加突出。为了做到这一点，我们必须学习如何离散化连续的动作空间，我们将在下一节中看到如何做到这一点。
- en: Discretizing continuous state spaces
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散化连续状态空间
- en: 'RL is limited to discrete spaces or what we learned previously as a finite
    MDP. A finite MDP describes a discrete set of steps or states with an action to
    move between states decided by a probability. The infinite version of this may
    define an infinite number of state-actions between any set of states. Hence, a
    basketball player moving from one end of the court to score a basket describes
    an infinite MDP or continuous space. That is, for each point in time, the ball
    player could be in an infinite number of positions, dribbling or not dribbling,
    or shooting the ball and so on. Likewise, in the `MountainCar` environment, the
    car can be moving up or down the hill in either direction at any point in time.
    This makes the `MountainCar` environment a continuous state space, but just barely.
    Fortunately, we can use a clever trick to discretize the state space as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）局限于离散空间，或者我们之前所学的有限马尔可夫决策过程（MDP）。一个有限MDP描述了一组离散的步骤或状态，以及一个通过概率决定在状态间移动的动作。这个过程的无限版本可能在任意一组状态之间定义无限数量的状态-动作。因此，一个篮球运动员从球场一端移动到另一端投篮，描述了一个无限MDP或连续空间。也就是说，对于每一个时间点，球员可能处于无限多个位置，运球或不运球，投篮等等。同样，在`MountainCar`环境中，汽车可以在任何时间点向上或向下移动，无论是哪个方向。这使得`MountainCar`环境成为一个连续状态空间，但仅仅是刚刚好。幸运的是，我们可以使用一个巧妙的技巧来离散化状态空间，如下所示：
- en: '![](img/dcb85386-c50e-4a98-b235-c5646f7333e5.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dcb85386-c50e-4a98-b235-c5646f7333e5.png)'
- en: Example discretization of MountainCar
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 山地车示例离散化
- en: In the preceding diagram, we have overlaid a grid on top of the environment
    to represent state spaces the cart may be in. For the sample, a 4 x 4 grid is
    used, but in our code, we will use a much larger grid. Doing this allows us to
    capture the position of the cart as if it was on a grid. Since the goal of this
    task is to move the cart up the hill, then discretizing the space by applying
    a gridding technique works quite well. In more complex continuous spaces, your
    grid may represent multiple dimensions in space or across other features. Fortunately,
    we won't have to worry about those complex mathematics when we learn how to apply
    deep learning to continuous spaces.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们在环境中叠加了一个网格，以表示小车可能处于的状态空间。对于这个示例，使用了一个4x4的网格，但在我们的代码中，我们将使用一个更大的网格。这样做可以让我们捕捉到小车在网格上的位置。由于这个任务的目标是将小车移动到山上，因此通过应用网格技术来离散化空间效果相当好。在更复杂的连续空间中，你的网格可能代表空间中的多个维度或其他特征。幸运的是，当我们学习如何将深度学习应用于连续空间时，我们不必担心那些复杂的数学问题。
- en: 'Now that we understand how the space is discretized, let''s jump back to the
    sample code in `Chapter_5_1.py` and review how this works in the following exercise:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了空间是如何离散化的，让我们回到`Chapter_5_1.py`中的示例代码，并回顾以下练习中它是如何工作的：
- en: 'We will start by picking up where we last left off. At the last point, we were
    just updating the `Q_table` table with the following line inside the episode `for`
    loop:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从上次结束的地方开始。在上一个点，我们只是在事件`for`循环中用以下行更新`Q_table`表：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This calls the `update_SARSA` function, shown here:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这调用了一个名为`update_SARSA`的函数，如下所示：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For now, ignore the `Q_table` update code and instead focus on the highlighted
    calls to `to_discrete_states`. These calls take an observation as input. An observation
    denotes the cart''s absolute position in the *x,y* coordinates. This is where
    we discretize the state using the following function:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，忽略`Q_table`更新代码，而是专注于高亮的`to_discrete_states`调用。这些调用接受一个观察值作为输入。观察值表示小车在*x,y*坐标中的绝对位置。这就是我们使用以下函数来离散化状态的地方：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `to_discrete_states` function returns the grid interval the cart is currently
    in. Back in the `update_SARSA` function, we change the interval list back to a
    tuple with the following:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`to_discrete_states`函数返回小车当前所在的网格区间。在`update_SARSA`函数中，我们使用以下方式将区间列表转换回一个元组：'
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Run the sample as you normally would again, just to confirm it works as expected.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行示例，只是为了确认它按预期工作。
- en: This simple method of discretization works well for this task but can quickly
    fall down or become overtly complex depending on the complexity of the environment.
    Before we move on to other matters, we want to return and look at how we update
    `Q_table` with SARSA in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的离散化方法对于这个任务来说效果很好，但根据环境的复杂度，可能会迅速失效或变得过于复杂。在我们继续其他话题之前，我们想要回顾一下下一节中如何使用SARSA更新`Q_table`。
- en: Expected SARSA
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预期SARSA
- en: Vanilla SARSA is quite similar to Q-learning in terms of how we choose values.
    It will generally just use an epsilon-greedy max action strategy, not unlike what
    we used previously; however, what we find, especially when working on-policy,
    is that the algorithm needs to be more selective. Now, this is very much the goal
    of all RL, but, in this particular case, we manage these trade-offs a bit better
    by introducing an expectation. When we combine this with SARSA, we call it **expected
    SARSA**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择值方面，Vanilla SARSA与Q-learning非常相似。它通常会使用epsilon-greedy最大动作策略，这与我们之前使用的方法类似；然而，我们发现，尤其是在按策略工作的情况下，算法需要更加选择性地进行。现在，这确实是所有强化学习（RL）的目标，但在这个特定的情况下，我们通过引入期望来更好地管理这些权衡。当我们结合SARSA时，我们称之为**期望SARSA**。
- en: 'In expected SARSA, we assume an unknown learning rate alpha, and hence an unknown
    exploration rate epsilon as well. Instead, we equate the learning rate alpha and
    exploration rate epsilon using functions based on assigned rewards. We assign
    a reward of one timepoint for each time step and then calculate the new alpha
    and epsilon based on those. Open `Chapter_5_2.py` back up and let''s see how this
    works by following the exercise here:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在期望SARSA中，我们假设一个未知的学习率alpha，以及一个未知的探索率epsilon。相反，我们使用基于分配奖励的函数将学习率alpha和探索率epsilon等同起来。我们为每个时间步分配一个时间点奖励，然后根据这些计算新的alpha和epsilon。打开`Chapter_5_2.py`文件，让我们通过以下练习来查看这是如何工作的：
- en: 'The two functions of code we are interested in are shown here:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们感兴趣的代码的两个函数在此处显示：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The two functions, `expect_epsilon` and `expect_alpha`, calculate an expectation
    or ratio based on the rewards returned so far, `t`, where `t` equals the total
    time the cart has been moving in the environment.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这两个函数，`expect_epsilon`和`expect_alpha`，根据到目前为止返回的奖励计算期望或比率，其中`t`等于小车在环境中移动的总时间。
- en: 'We can focus on how `expect_epsilon` is used by looking at the `get_action`
    function shown here:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过查看此处显示的`get_action`函数来关注`expect_epsilon`的使用：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`get_action` returns the action based on the observation (*x* and *y* positions
    of the cart). It does this by first checking whether a random action is to be
    sampled or, instead, the best action. We determine the probability of this by
    using the `expect_epsilon` equation, which calculates epsilon based on the total
    episode time playing the environment. This effectively means the epsilon in this
    example will range between 0.001 and 0.0015; see whether you can figure that out
    in the code.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`get_action`函数根据观察到的信息（小车*x*和*y*位置）返回动作。它是通过首先检查是否要采样随机动作，或者相反，选择最佳动作来做到这一点的。我们通过使用`expect_epsilon`方程来确定这种概率，该方程根据在环境中玩的总游戏时间计算epsilon。这意味着在这个例子中，epsilon的范围将在0.001和0.0015之间；看看你是否能在代码中找出这一点。'
- en: 'Next, we do something similar to calculate `alpha` shown in the `update_SARSA`
    function. The single line where this is used is shown again:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们做类似的事情来计算`update_SARSA`函数中显示的`alpha`。再次显示使用此功能的单行：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code should be familiar by now as it looks like our regular policy
    update equation, except, in this instance, we are tuning the value for `alpha`
    using an expectation based on the current time on the task. You can also think
    of this in some ways as a secondary reward.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前的代码现在应该很熟悉了，因为它看起来像我们的常规策略更新方程，只是在这次实例中，我们使用基于当前任务时间的期望来调整`alpha`的值。你也可以从某些方面将其视为一种次级奖励。
- en: 'Run the code again and let it finish to completion. Notice the output as we
    will use that as a comparison soon:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行代码并让它完成。注意输出，因为我们很快会将其用作比较：
- en: '![](img/951548fc-9997-456b-8e53-026009355a53.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/951548fc-9997-456b-8e53-026009355a53.png)'
- en: The output of returns/rewards over training time
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间内的回报/奖励输出
- en: The plot shows the accumulated rewards/time the cart spends in the environment.
    The cost is awarded a time reward for each time slice it remains in the environment,
    where if the cart remains stationary or relatively still for more than a few time
    slices, the episode is over. Therefore, the more time the cart stays in the environment
    also equates to more movement.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了小车在环境中累积的奖励/时间。每次小车在环境中停留的时间片都会获得时间奖励，如果小车在几个时间片内保持静止或相对静止，则游戏结束。因此，小车在环境中停留的时间越长，也等于移动得越多。
- en: Continuous states or continuous observations are not the only things we need
    to concern ourselves when considering real-time problems. In the real world, we
    also deal with continuous action spaces as well. Currently, we have been looking
    at problems with discrete action spaces, that is, environments that use arbitrary
    discrete actions to control the agent. These actions are typically up, down, left,
    and right. However, for the real world, we need finer control and often categorize
    actions as turn left by amount *x* or right by amount *y*. By adding continuous
    action spaces, our RL algorithms become less robotic and provide finer control.
    Discretizing discrete action spaces into continuous action spaces is more difficult
    and not something we will concern ourselves with. Instead, we will look at how
    to convert another more popular continuous action space we use for deep RL in
    the following section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑实时问题时，我们不仅需要关注连续状态或连续观察。在现实世界中，我们还要处理连续动作空间。目前，我们一直在研究具有离散动作空间的问题，即使用任意离散动作来控制智能体的环境。这些动作通常是上、下、左和右。然而，对于现实世界，我们需要更精细的控制，通常将动作分类为左转*x*度或右转*y*度。通过添加连续动作空间，我们的强化学习算法变得更加灵活，提供了更精细的控制。将离散动作空间离散化到连续动作空间更困难，这不是我们关注的重点。相反，我们将探讨如何在下一节中转换另一个更流行的连续动作空间，该空间用于深度强化学习。
- en: Extending continuous spaces
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展连续空间
- en: Typically, we leave problems with large observation spaces to be tackled with
    deep learning. Deep learning, as we will learn, is very well-suited to such problems.
    However, deep learning is not without its own issues and it is sometimes prudent
    to try and solve an environment without deep learning. Now, not all environments
    will discretize well, as we mentioned previously, but we do want to look at another
    example. The next example we will look at is the infamous Cart Pole environment,
    which is almost always tackled with deep RL, primarily because it uses a continuous
    action space with four dimensions. Keep in mind that our previous observation
    spaces only had one dimension, and, in our last example, we only had two.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会将具有大观察空间的问题留给深度学习来解决。正如我们将学习的，深度学习非常适合这类问题。然而，深度学习并非没有问题，有时尝试在没有深度学习的情况下解决环境是明智的。现在，并非所有环境都能很好地离散化，正如我们之前提到的，但我们确实想看看另一个例子。下一个我们将要查看的例子是臭名昭著的滑车杆环境，它几乎总是使用深度强化学习来解决，主要是因为它使用了一个具有四个维度的连续动作空间。记住，我们之前的观察空间只有一个维度，而在我们上一个例子中，我们只有两个维度。
- en: Being able to convert an agent's observation space can be a useful trick especially
    in more abstract game environments. Remember, good game mechanics are often more
    about being fun rather than accurate. This certainly applies to some AI elements
    in games.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 能够转换智能体的观察空间可以是一个有用的技巧，尤其是在更抽象的游戏环境中。记住，好的游戏机制往往更注重乐趣而非准确性。这当然也适用于游戏中的一些AI元素。
- en: 'You can find the specifics of the observation and state spaces by going to
    the environment''s GitHub page if it has one. Most of the more popular environments
    have their own page. The **Cart Pole** and **Mountain Car** observation and action
    spaces are shown in the following excerpts:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境有GitHub页面，你可以通过访问该页面找到观察和状态空间的详细信息。大多数更受欢迎的环境都有自己的页面。以下摘录显示了**滑车杆**和**山地车**的观察和动作空间：
- en: '![](img/1c95f106-1c22-4245-bcc0-04a35c90a30e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c95f106-1c22-4245-bcc0-04a35c90a30e.png)'
- en: Spaces of Mountain Car and Cart Pole environments
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 山地车和滑车杆环境的空间
- en: The preceding excerpts show a comparison of the **Mountain Car** versus the
    **Cart Pole** environments. Both environments use discrete action spaces, which
    is good. However, the **Cart Pole** environment uses a 4-dimensional observation
    space with values shown in the ranges in the table in the screenshot. This can
    be a little tricky and it will be helpful to understand how multidimensional observation
    spaces work in more detail.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述摘录显示了**山地车**与**滑车杆**环境的比较。这两个环境都使用离散动作空间，这是好的。然而，**滑车杆**环境使用了一个具有四个维度的观察空间，其值在截图中的表格中显示。这可能有点棘手，了解多维观察空间如何工作将非常有帮助。
- en: 'Open `Chapter_5_3.py` and follow this exercise to see how our last example
    can be converted into **Cart Pole**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 `Chapter_5_3.py` 并按照这个练习来查看我们的上一个例子是如何转换为**滑车杆**的：
- en: 'For the most part, the code is identical to the last two examples, so we only
    need to look at the differences. We will start with the environment construction
    section at the top, as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大部分代码与最后两个示例相同，所以我们只需要查看差异。我们将从顶部的环境构建部分开始，如下所示：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This constructs the infamous **Cart Pole** environment. Again, switching environments
    is easy but your code has to adapt to the observation and action spaces. **Cart
    Pole** and **Mountain Car** share the same observation/action space types. That
    is, its observation space is continuous but with a discrete action space.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这构建了臭名昭著的 **Cart Pole** 环境。再次强调，切换环境很容易，但你的代码必须适应观察和动作空间。**Cart Pole** 和 **Mountain
    Car** 具有相同的观察/动作空间类型。也就是说，其观察空间是连续的，但动作空间是离散的。
- en: 'Next, we will look and see how this affects our `Q_table` table initialization
    with the code here:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看并了解这如何影响我们在此处代码中的 `Q_table` 表的初始化：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice how the table is now configured with four dimensions at size 20\. Previously,
    this was just two dimensions of size 20\. Go back and check the last code examples
    for comparison if you need to.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意现在表格是如何配置为四个维度，每个维度大小为 20。之前，这仅仅是两个维度，每个维度大小为 20。如果你需要比较，请回过头去检查最后的代码示例。
- en: 'With more dimensions added to the `Q_table` table, that means we also need
    to add more dimensions to our discretization buckets, as shown here:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随着 `Q_table` 表中维度的增加，这也意味着我们需要在我们的离散化桶中添加更多维度，如下所示：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Again, we increase the `buckets` array from two dimensions to four, all of size
    `20`. We are arbitrarily using a size of 20 but we could use a larger or smaller
    value.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们将 `buckets` 数组从两个维度增加到四个，所有维度的大小都是 `20`。我们任意使用大小为 20，但我们也可以使用更大的或更小的值。
- en: 'The last thing we need to do is redefine the boundaries of the environment''s
    observations. Recall we were able to extract this information from the GitHub
    page. This is the table that shows the min/max values in the ranges. The line
    of code we are interested in is just inside the `to_discrete_states` function,
    as shown here:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最后需要做的是重新定义环境观察的边界。回想一下，我们能够从 GitHub 页面提取出这些信息。这是显示范围中最小/最大值的表格。我们感兴趣的代码行就在
    `to_discrete_states` 函数内部，如下所示：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The line is highlighted and declares the `max_range` variable. `max_range` sets
    the max value along each dimension in the observation space. We populate this
    with the values from the table and, in the case of infinity, we use six 9s (999999),
    which often works for the upper limits of values with infinity.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一行被突出显示并声明了 `max_range` 变量。`max_range` 设置观察空间中每个维度的最大值。我们用表中的值填充这个变量，在无穷大的情况下，我们使用六个
    9（999999），这通常适用于具有无穷大的值的上限。
- en: 'Next we, need to update the axis dimensions we use for indexing into the `Q_table`
    table, as shown in the code here:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要更新用于索引 `Q_table` 表的轴维度，如下所示代码所示：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, notice how we are now indexing to the four dimensions
    and the action.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的代码中，请注意我们现在正在对四个维度和动作进行索引。
- en: 'Run the code as you normally would and observe the output; an example is shown
    here:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规运行代码并观察输出；这里有一个示例：
- en: '![](img/1deb404f-bcf1-472c-bbbc-72cda2095a13.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1deb404f-bcf1-472c-bbbc-72cda2095a13.png)'
- en: Example Chapter_5_3.py
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：Chapter_5_3.py
- en: Eventually, SARSA using a discretized observation space can solve the `CartPole`
    environment. This one may take a while to learn so be patient, but the agent will
    learn to balance the pole on the cart. You should have a fairly good understanding
    of how discretization works and SARSA at TD (0). In the next section, we will
    look at looking ahead/behind more than one step.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，使用离散化观察空间的 SARSA 可以解决 `CartPole` 环境。这个可能需要一段时间来学习，所以请耐心等待，但智能体将学会在车上平衡杆子。你应该对离散化如何工作以及
    TD (0) 中的 SARSA 有相当好的理解。在下一节中，我们将探讨向前/向后看超过一步的情况。
- en: Working with TD (λ) and eligibility traces
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 TD (λ) 和资格迹（eligibility traces）一起工作
- en: Up until now, we have looked at the forward view or what the agent perceives
    to be as the next best reward or state. In **MC**, we looked at the entire episode
    and then used those values to reverse calculate returns. For TDL methods such
    as Q-learning and SARSA, we looked a single step ahead or what we referred to
    as TD (0). However, we want our agents to be able to take into account several
    steps, *n*, in advance. If we can do this, then surely our agent will be able
    to make better decisions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注的是前视或代理感知到的下一个最佳奖励或状态。在**MC**中，我们查看整个剧集，然后使用这些值来反向计算回报。对于Q学习、SARSA等TDL方法，我们查看单步前瞻或我们所说的TD（0）。然而，我们希望我们的代理能够提前考虑*n*步。如果我们能这样做，那么我们的代理肯定能够做出更好的决策。
- en: 'As we have seen previously, we can average returns across steps using a discount
    factor, gamma. However, at this point, we need to more careful about how we average
    or collect returns. Instead, we can define the averaging of all returns over an
    infinite number of steps forward as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，我们可以使用折现因子gamma对步骤间的回报进行平均。然而，在这个阶段，我们需要更加小心地处理回报的平均或收集方式。相反，我们可以将所有回报在无限多个步骤前进行平均定义为以下内容：
- en: '![](img/5b74115d-fb63-4f38-900c-1544016e227c.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5b74115d-fb63-4f38-900c-1544016e227c.png)'
- en: 'In the preceding equation, we have the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，我们有以下内容：
- en: '![](img/4f80d35d-500d-4f12-8e6b-5331bc40a819.png) This is the weighted average
    of all returns.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/4f80d35d-500d-4f12-8e6b-5331bc40a819.png) 这是所有回报的加权平均值。'
- en: '![](img/2267d3ca-2648-4c06-8402-c357416c3e78.png) This is the return of individual
    episodes from *t* to *t+n.*'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/2267d3ca-2648-4c06-8402-c357416c3e78.png) 这是从*t*到*t+n*的单个剧集回报。'
- en: '![](img/c43f9843-b9df-4acd-bab4-82f9de0d26c5.png) Lambda, a weight value between
    [0,1].'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/c43f9843-b9df-4acd-bab4-82f9de0d26c5.png) Lambda，一个介于[0,1]之间的权重值。'
- en: 'Since lambda is less than one, as values of *n* increase, the amount of contribution
    to the final average return becomes smaller. This is due to raising lambda (λ)
    to the power of *n* as in the preceding equation. Again, this is the same principle
    as using the discount factor, gamma. Now that we are thinking in terms of *n*
    steps or what we will refer to as lambda, we can revisit how this looks in the
    following diagram:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于lambda小于1，随着*n*的增加，对最终平均回报的贡献量会逐渐减小。这是由于在前一个方程中将lambda（λ）提升到*n*的幂次所致。再次强调，这与使用折现因子gamma的原理相同。现在，当我们从*n*步或我们所说的lambda的角度思考时，我们可以回顾以下图表中的情况：
- en: '![](img/e8eeae23-6867-4dae-b031-7c4ae7f8e6c5.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8eeae23-6867-4dae-b031-7c4ae7f8e6c5.png)'
- en: Progression of TD (λ)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: TD（λ）的进展
- en: 'To find the general solution for *n* time steps, where *n* is an unknown we
    call lambda (λ), we need to determine a general solution for finding lambda, that
    is, the value of lambda that generalizes the problem. We can do that by first
    assuming that any episode will end at time step, *t*, and then rewriting our previous
    equation as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到*n*时间步长的通用解，其中*n*是我们称为lambda（λ）的未知数，我们需要确定一个寻找lambda的通用解，即lambda的值可以泛化问题。我们可以通过首先假设任何剧集将在时间步长*t*结束，然后按照以下方式重写我们之前的方程来实现这一点：
- en: '![](img/91d7517b-2145-4403-8129-2417c549de74.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/91d7517b-2145-4403-8129-2417c549de74.png)'
- en: When a value  of 0 for lambda is used, this represents TD (0). A value of 1
    for lambda represents MC or a need for a complete episode lookahead. However,
    it is complicated to implement this form of lookahead models and, intuitively,
    looking ahead is a very small part of what biological animals learn. In fact,
    our primary source of learning is experience, and that is exactly what we start
    to consider in the next section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当lambda的值为0时，这代表TD（0）。lambda的值为1代表MC或需要完整的剧集前瞻。然而，实现这种前瞻模型比较复杂，并且直观上看，前瞻只是生物动物学习过程中的一个非常小的部分。实际上，我们学习的主要来源是经验，这正是我们在下一节将要考虑的内容。
- en: Backward views and eligibility traces
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后视和资格痕迹
- en: 'Do you recall the last time you found a coin on the floor or street? After
    you picked up the coin, did you think to yourself: a) "I knew looking down all
    that time would pay off," or b) "Wow, I found a coin, how did I do that?" In fact,
    in most cases, it would be option *b*, that is, we learned something was good
    and then thought back to how we discovered it. The moment of brilliance in option
    *a* is akin to believing in quantum particles, atoms, and bacteria. This is no
    different in RL, and what we find is that it is often more useful to look back
    at what happened in the past; however, not so far back as to be a past event as
    in MC.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得上一次你在地板或街上找到硬币的时间吗？在你捡起硬币后，你是否想过：a) "我知道低头看那么久会得到回报，" 或 b) "哇，我找到了一个硬币，我是怎么做到的？"
    事实上，在大多数情况下，会是选项 *b*，也就是说，我们学到某件事是好的，然后回想起我们是怎样发现它的。选项 *a* 中的精彩时刻类似于相信量子粒子、原子和细菌。在强化学习（RL）中，情况也是如此，我们发现回顾过去发生的事情通常更有用；然而，不要回溯得太远，以至于成为像蒙特卡洛（MC）那样的过去事件。
- en: 'We can use TDL to take a backward look at the returns for several steps. However,
    we can''t just use an absolute value for the state transitions. Instead, we need
    to determine the predicted error for each step back using the following equation:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 TDL 来回顾几步的回报。然而，我们不能仅仅使用状态转换的绝对值。相反，我们需要使用以下方程确定每步回溯的预测误差：
- en: '![](img/861c5097-ba1a-4a3e-9c13-5981e348933a.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/861c5097-ba1a-4a3e-9c13-5981e348933a.png)'
- en: 'In the preceding equation, we have the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们有以下：
- en: '[![](img/416226c6-0388-4019-8e63-6a8526bd534f.png)] This is the TD error or
    delta.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/416226c6-0388-4019-8e63-6a8526bd534f.png)] 这是 TD 错误或 delta。'
- en: '[![](img/c9aab2e3-e3f9-4eb9-8d97-c56a31de0381.png)] The value function, which
    can be further defined by the following:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[![](img/c9aab2e3-e3f9-4eb9-8d97-c56a31de0381.png)] 值函数，它可以进一步定义为以下：'
- en: '![](img/53d9aed7-2b64-4394-9d80-619ceecbbba0.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53d9aed7-2b64-4394-9d80-619ceecbbba0.png)'
- en: 'We can further define the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步定义以下：
- en: '![](img/2a0de061-f7eb-4694-a5c6-ded805cd9141.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a0de061-f7eb-4694-a5c6-ded805cd9141.png)'
- en: '![](img/6accd72f-5deb-4f13-a909-fdb806eac621.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6accd72f-5deb-4f13-a909-fdb806eac621.png)'
- en: In the preceding equation, ![](img/fc2406d8-23d4-48ac-9816-0c095a62b1dd.png)
    assigns the full value of 1 when the state is at *s.*
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![](img/fc2406d8-23d4-48ac-9816-0c095a62b1dd.png) 当状态处于 *s.* 时赋予完整的值
    1。
- en: '*E* denotes the eligibility factor or the amount the value should be considered
    in the TD error. What is happening here is that the value function is being updated
    by the number of TD errors over *n* steps, but, instead of looking forward, we
    look backward. Much like all things in RL, it seems this has to be applied across
    several variations of algorithms. For *n* step TDL or TD (λ), we have three variations
    we concern ourselves with. They are Tabular TD (λ), SARSA (λ), and Q (λ). Each
    algorithm variation in pseudocode is shown in the following diagram:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*E* 表示资格因子或值在 TD 错误中应考虑的量。这里发生的情况是，值函数正在通过 *n* 步的 TD 错误数量进行更新，但我们不是向前看，而是向后看。就像强化学习（RL）中的所有事情一样，这似乎需要在算法的多个变体中应用。对于
    *n* 步 TDL 或 TD (λ)，我们有三个我们关注的变体。它们是表格式 TD (λ)、SARSA (λ) 和 Q (λ)。以下图表显示了伪代码中的每个算法变体：'
- en: '![](img/0090720b-6a78-4a7a-97d2-90b2c9f4dac4.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0090720b-6a78-4a7a-97d2-90b2c9f4dac4.png)'
- en: TD (λ), SARSA (λ), and Q (λ)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: TD (λ), SARSA (λ), 和 Q (λ)
- en: Each algorithm has a slight variation in the way it calculates values and TD
    errors. In the next section, we will look at a full implementation of SARSA (λ)
    in code.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 每个算法在计算值和 TD 错误的方式上都有细微的差别。在下一节中，我们将查看 SARSA (λ) 的完整代码实现。
- en: Understanding SARSA (λ)
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 SARSA (λ)
- en: 'We could, of course, implement TD (λ) using the tabular online method, which
    we haven''t covered yet, or with Q-learning. However, since this is a chapter
    on SARSA, it only makes sense that we continue with that theme throughout. Open
    `Chapter_5_4.py` and follow the exercise:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以使用表格式在线方法实现 TD (λ)，这是我们还没有覆盖的，或者使用 Q-learning。然而，由于这是关于 SARSA 的章节，我们继续这一主题是合情合理的。打开
    `Chapter_5_4.py` 并遵循练习：
- en: 'The code is quite similar to our previous examples, but let''s review the full
    source code, as follows:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码与我们的前例相当相似，但让我们回顾一下完整的源代码，如下所示：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The top section of code is quite similar with some notable differences. Notice
    the initialization of the `MountainCar` environment and the `Q_table` table setup
    using the following code:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的上半部分与之前的例子相当相似，但有几个显著的不同之处。注意以下代码初始化了 `MountainCar` 环境和 `Q_table` 表格设置：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice how we increase the number of discretized states from 20 x 20 to 65 x
    65 as we initialize the `Q_table` table.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们是如何在初始化`Q_table`表时将离散状态的数量从20 x 20增加到65 x 65。
- en: 'The next major difference now is the calculation of eligibility using lambda.
    We can find this code in the bottom episode `for` loop, as shown in the following
    code:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在的主要区别是使用lambda计算可选性。我们可以在下面的代码中的底部`for`循环中找到此代码，如下所示：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The calculation for eligibility is done in the highlighted lines. Notice how
    we multiply `eligibility` by `lambda` and `gamma`, then add one for the current
    state. This value is then passed into the `update_SARSA` function, as follows:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选性的计算在突出显示的行中进行。注意我们是如何将`eligibility`乘以`lambda`和`gamma`，然后为当前状态加一的。然后将此值传递到`update_SARSA`函数中，如下所示：
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice how we now update the `Q_table` table based on a determination of `td_error`
    and `eligibility`. In other words, we take into consideration now how current
    the information is and how much it was valued in the past.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们现在是根据`td_error`和`eligibility`的确定来更新`Q_table`表的。换句话说，我们现在考虑信息的当前性和过去的价值。
- en: 'Run the code example again as you normally would and watch the agent play the
    task. The training output for this task is shown in the following diagram:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规再次运行代码示例，并观察智能体如何完成任务。此任务的训练输出如下所示：
- en: '![](img/ca6f8852-14ba-4f90-bf02-da2fb54b8bcc.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca6f8852-14ba-4f90-bf02-da2fb54b8bcc.png)'
- en: Output plot of rewards for SARSA (λ)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA（λ）的奖励输出图
- en: It may take a few minutes to generate the plot shown in the preceding diagram,
    so please be patient. Be sure to note how this compares with the previous examples
    we already ran in this chapter. You did run all of the sample exercises to completion,
    right? Notice how the output of returns/rewards of the time on each episode increases
    quicker and converges quicker.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 生成前面图表所示的图形可能需要几分钟时间，所以请耐心等待。务必注意这与我们在本章中已经运行过的先前示例相比如何。你真的完成了所有的样本练习吗？注意每个时间段的回报/奖励输出是如何更快地增加并更快地收敛的。
- en: We want to look at one more complex example that puts our use of discretization
    to the extreme in the next example.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想在下一个例子中查看一个更复杂的例子，将我们对离散化的使用推向极致。
- en: SARSA lambda and the Lunar Lander
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA lambda和Lunar Lander
- en: As the algorithms we develop get more complicated, their capabilities also get
    more powerful. However, there are limits and it is important to understand the
    limits of any technology. To test those limits, we want to look at an example
    that pushes them. For this particular case, we will look at the Lunar Lander environment
    from Gym. This environment is modeled after the old classic arcade game of the
    same name, where the object is to land a lunar module on the surface of the moon.
    In this environment, the observation space is described in eight dimensions and
    the action space in four. As we will see, this can quickly go beyond our current
    computational limits.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们开发的算法变得越来越复杂，它们的性能也越来越强大。然而，它们有局限性，了解任何技术的局限性都很重要。为了测试这些限制，我们想看看一个能够推动它们的例子。对于这个特定的情况，我们将查看Gym中的Lunar
    Lander环境。这个环境是根据同名经典街机游戏建模的，目标是将登月舱降落在月球表面。在这个环境中，观察空间由八个维度描述，动作空间由四个维度描述。我们将看到，这很快就会超出我们当前的计算限制。
- en: The `LunarLander` environment requires the installation of a special module
    called `Box2D`. This is essentially a graphics package.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`LunarLander`环境需要安装一个名为`Box2D`的特殊模块。这本质上是一个图形包。'
- en: 'Follow the exercise in the next section to set up and run the advanced `Box2D`
    modules for Gym:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 按照下一节的练习来设置和运行Gym的高级`Box2D`模块：
- en: 'Follow these steps for Windows (Anaconda):'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照以下步骤在Windows（Anaconda）上操作：
- en: 'Open an Anaconda Prompt as an administrator. Run the following command:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以管理员身份打开Anaconda Prompt。运行以下命令：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: SWIG is a requirement of `Box2D`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: SWIG是`Box2D`的要求。
- en: 'Next, run the following command to install Box2D:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行以下命令安装Box2D：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Follow these steps for Mac/Linux (or Windows without Anaconda):'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照以下步骤在Mac/Linux（或没有Anaconda的Windows）上操作：
- en: 'Open a Python shell and run the following command:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Python shell并运行以下命令：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If you encounter issues, consult the instructions for the Windows installation.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果遇到问题，请参阅Windows安装说明。
- en: 'This installation now allows you to run all of the more advanced Box2D environments.
    These are far more game-like and interesting to train on as well. Open up `Chapter_5_5.py`
    and follow the exercise to set up and train SARSA on Lunar Lander:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的安装允许你运行所有更高级的Box2D环境。这些环境在游戏性和训练上都要有趣得多。打开`Chapter_5_5.py`并按照练习设置和训练月球着陆器上的SARSA：
- en: 'The source code for `Chapter_5_5.py` is almost identical to `Chapter_5_4.py`
    aside from minor differences in setting up the discrete states. We will first
    look at how we set up the `Q_table` table with the following code:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Chapter_5_5.py`的源代码几乎与`Chapter_5_4.py`相同，除了在设置离散状态方面有一些细微的差异。我们将首先查看如何使用以下代码设置`Q_table`表：'
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Notice how we went from values of 65 steps down to 5\. The last value denotes
    the action space size and this has gone from three in `MountainCar` to four for
    `LunarLander`. However, with eight dimensions, we have to be careful about the
    size of the array. Hence, we need to limit each step size to five, in this example.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们是如何从65步的值减少到5。最后一个值表示动作空间的大小，它从`MountainCar`中的三个增加到`LunarLander`中的四个。然而，由于有八个维度，我们必须小心数组的尺寸。因此，在这个例子中，我们需要将每个步长限制为五。
- en: 'Next, we initialize the `buckets` state:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化`buckets`状态：
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Again, initialized to a size of three for the eight dimensions.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，初始化为三个大小，用于八个维度。
- en: 'Then, we set the `max_range` values for the maximum values we want our step
    to span, like so:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们设置`max_range`值，这是我们想要我们的步骤跨越的最大值，如下所示：
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We use a value of 100 here to denote some arbitrary max value. Altering or tweaking
    these values could improve training efficiency.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里使用100这个值来表示某个任意的最大值。改变或调整这些值可能会提高训练效率。
- en: 'Next, we need to expand the `Q_table` indexing to include 8 dimensions, like
    so:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要扩展`Q_table`索引以包括8个维度，如下所示：
- en: '[PRE26]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Be aware of the limits we are applying to the agent in this example. We are
    effectively making the agent see in big sections, where each section or axis feature
    is only divided into three slices. It is surprising how effective this method
    can be.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意在这个例子中我们对代理施加的限制。我们实际上让代理以大块的方式观察，其中每个块或轴特征只分为三个部分。这个方法的有效性令人惊讶。
- en: 'Run the sample and let it go to completion. Yes, this one will take a while
    but it is worth it. An example output from the Lunar Lander environment can be
    seen in the following diagram:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例并让它完成。是的，这个会花一些时间，但这是值得的。以下是一个来自月球着陆器环境的示例输出：
- en: '![](img/ef2e7437-e1be-48ee-887e-ac6c42f2b973.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef2e7437-e1be-48ee-887e-ac6c42f2b973.png)'
- en: Example output from Chapter_5_5.py
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`Chapter_5_5.py`的示例输出
- en: In the last example, we briefly looked at using SARSA on another continuous
    observation space environment, the Lunar Lander. While it can be fun to play with
    these environments and see how our discretization can manage an infinite MDP adequately,
    it is time we moved on to using the big guns of deep learning to manage continuous
    observation spaces. From the output of rewards, we can see that the example does
    not converge at all. This is likely because the discretization is not fine enough;
    perhaps you can improve on that?
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，我们简要地探讨了在另一个连续观察空间环境——月球着陆器上使用SARSA。虽然在这些环境中玩耍并观察我们的离散化如何适当地处理无限MDP很有趣，但现在是时候转向使用深度学习的强大工具来处理连续观察空间了。从奖励输出中我们可以看到，这个例子根本就没有收敛。这很可能是由于离散化不够精细；也许你可以在这方面有所改进？
- en: The discretization process in this example is not optimal and could certainly
    be improved upon with some DP methods.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，离散化过程并不最优，并且可以使用一些DP方法进行改进。
- en: Deep learning networks applied to RL allow us to tackle enormous continuous
    observation and action spaces. As such, discretization of spaces won't be needed
    regularly going forward but it can be a useful trick or advantage for simpler
    problems.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习网络应用于强化学习（RL）使我们能够处理巨大的连续观察和动作空间。因此，在未来的常规操作中，我们可能不需要经常进行空间离散化，但对于更简单的问题，它可能是一个有用的技巧或优势。
- en: This completes this chapter and I encourage you to move on and explore the exercises
    to improve your own learning.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了这一章，我鼓励你继续前进并探索练习，以提升你自己的学习。
- en: Exercises
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'These exercises are here for you to use and learn from. Attempt at least 2-3,
    and the more you do, the easier later chapters will also be:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习是为了让你使用和学习而提供的。至少尝试2-3个，你做得越多，后面的章节也会越容易：
- en: What is the difference between an online and offline policy agent?
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在线策略代理和离线策略代理之间有什么区别？
- en: Tune the hyperparameters for any or all of the examples in this chapter, including
    the new hyperparameter, `lambda`.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整本章中任何或所有示例的超参数，包括新的超参数`lambda`。
- en: Change the discretization steps in any example that uses discretization and
    see what effect it has on training.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在任何使用离散化的示例中更改离散化步骤，并观察它对训练的影响。
- en: Use example `Chapter_5_3.py`, **SARSA(0)**, and adapt it to another Gym environment
    that uses a continuous observation space and discrete action space.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用示例 `Chapter_5_3.py`，**SARSA(0**)，并将其适配到另一个使用连续观察空间和离散动作空间的Gym环境。
- en: Use example `Chapter_5_4.py`, **SARSA(λ)**, and adapt it to another Gym environment
    that uses a continuous observation space and discrete action space.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用示例 `Chapter_5_4.py`，**SARSA(λ**)，并将其适配到另一个使用连续观察空间和离散动作空间的Gym环境。
- en: There is a hyperparameter shown in the code that is not used. Which parameter
    is it?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码中显示了一个未使用的超参数。它是哪个参数？
- en: Use example `Chapter_5_5.py`, **SARSA(λ)**, Lunar Lander and optimize the discretization
    so that it performs better. For example, you are still limited by array dimensions
    but you can increase or decrease some more important dimensions.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用示例 `Chapter_5_5.py`，**SARSA(λ**)，月球着陆器，并优化离散化以使其表现更佳。例如，你仍然受限于数组维度，但你可以增加或减少一些更重要的维度。
- en: Use example `Chapter_5_5.py`, **SARSA(λ)**, Lunar Lander and optimize the `max_range`
    values so that it performs better. For example, instead of setting all values
    to 999, check whether certain values can be narrowed or need expanding.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用示例 `Chapter_5_5.py`，**SARSA(λ**)，月球着陆器，并优化`max_range`值以使其表现更佳。例如，而不是将所有值都设置为999，检查是否某些值可以缩小或需要扩展。
- en: Update an example to work with a continuous action environment. This will require
    you to discrete the action space.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新一个示例以适应连续动作环境。这需要你将动作空间离散化。
- en: Convert one of the samples into Q-learning, that is, it uses an offline policy.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其中一个示例转换为Q-learning，即它使用离线策略。
- en: Feel free to also explore more on your own. We barely scratched the surface
    of the intricacies of these methods. Finally, we come to our summary in the next
    section.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 随意探索更多内容。我们仅仅触及了这些方法复杂性的表面。最后，我们将在下一节中总结。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: For this chapter, we continued exploring TD learning. We looked at an example
    of an online TD (0) method called **SARSA**. Then, we looked at how we can discretize
    an observation space to tackle harder problems but still use the same toolset.
    From there, we looked at how we could tackle harder continuous space problems
    such as `CartPole`. After that, we revisited TDL and then looked to *n* step forward
    views, decided that was less than optimal, and then moved to backward views and
    eligibility traces, which led to us uncovering TD (λ), SARSA(λ), and Q (λ). Using
    SARSA(λ), we were able to solve the `MountainCar` environment in far less time.
    Finally, we wanted to tackle a far more difficult environment, `LunarLander` using
    SARSA(λ) without deep learning.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们继续探索TD学习。我们查看了一个名为**SARSA**的在线TD (0)方法的示例。然后，我们探讨了如何将观察空间离散化以解决更难的问题，但仍然使用相同的工具集。从那里，我们探讨了如何解决更难的连续空间问题，例如`CartPole`。之后，我们回顾了TDL，然后转向*n*步前瞻视角，认为这不够理想，然后转向后向视角和资格痕迹，这导致我们发现了TD
    (λ)、SARSA(λ)和Q (λ)。使用SARSA(λ)，我们能够在远少于预期的时间内解决`MountainCar`环境。最后，我们想要使用SARSA(λ)而不使用深度学习来解决一个更困难的`LunarLander`环境。
- en: In the next chapter, we look at introducing deep learning and escalate ourselves
    to deep reinforcement learners.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨引入深度学习，并提升自己成为深度强化学习者。
