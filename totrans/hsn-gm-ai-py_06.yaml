- en: Exploring SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we continue with our focus on **Temporal Difference Learning**
    (**TDL**) and expand on it from TD (0) to multi-step TD and beyond. We will look
    at a new method of **Reinforcement Learning** (**RL**) called SARSA, explore what
    it is, and how it differs from Q-learning. From there, we will look at a few examples
    with new continual control learning environments from Gym. Then, we will move
    to a deeper understanding of TDL and introduce concepts called **TD lambda** (λ)
    and **eligibility traces**. Finally, we will finish off this chapter by looking
    at an example of SARSA.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, we will extend our discussion of TDL and uncover **State
    Action Reward State Action** (**SARSA**), continuous action spaces, TD (λ), eligibility
    traces, and on-policy learning. Here is an overview of what we will cover in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring SARSA on-policy learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using continuous spaces with SARSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending continuous spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with TD (λ) and eligibility traces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding SARSA (λ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter is very much a continuation of [Chapter 4](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml),
    *Temporal Difference Learning*. Please read that chapter before this one. In the
    next section, we continue right where we left off in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring SARSA on-policy learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SARSA, which is the process this method emulates. That is, the algorithm works
    by moving to a state, then choosing an action, receiving a reward, and then moving
    to the next state action. This makes SARSA an on-policy method, that is, the algorithm
    works by learning and deciding with the same policy. This differs from Q-learning,
    as we saw in [Chapter 4](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml), *Temporal
    Difference Learning*, where Q is a form of off-policy learner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the difference in backup diagrams for Q-learning
    and SARSA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdf4a700-7c4c-40b3-bfd1-59bfa0021c1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Backup diagrams for Q and SARSA
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that our Q-learner is an off-policy learner. That is, it requires the
    algorithm to update the policy or Q table offline and then later make decisions
    from that. However, if we want to tackle the TDL problem beyond one step or TD
    (0), then we need to have an on-policy learner. Our learning agent or algorithm
    must be able to update its policy in between whatever number of TD steps we may
    be looking at. This also requires us to update our Q update equation with a new
    SARSA update equation, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29924182-4562-4609-a89f-9c2186148fa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Recall that our Q-learning equation was like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/550158be-5e57-49e7-ba97-21bc604ee66c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4606930b-f445-4204-b857-b84ec0ae33a0.png) The current state-action
    quality being updated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3078a024-51a4-4f8b-9bfa-28230622897d.png) The learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4215b7a4-a539-4dd9-96e3-863b899564a6.png) The reward for the next state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7322b8ce-8148-4521-b8cb-be1b4418663c.png) Gamma, the discount factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/ce011eeb-f862-4cb7-94a3-f1469aab8d2a.png)] The maximum best or greedy
    action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can further visualize this as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63ee3993-33e5-4c08-b278-48e01c59b336.png)'
  prefs: []
  type: TYPE_IMG
- en: SARSA diagram and equation
  prefs: []
  type: TYPE_NORMAL
- en: Notice how that funny *max* term is gone now in SARSA and we use the expectation
    now and not just the best. This has to do with the action selection strategy.
    If you recall in Q-learning, we always used the max or best action according to
    averaging rewards. Recall that Q-learning assumes that you average the maximum
    reward. Instead, we want to select the action the agent perceives to be the one
    that will return the best possible returns. Hopefully, you have also noticed how
    we have progressed from speaking about rewards to value, state actions, state
    values, and now returns, where a return represents the perceived value for an
    action. We will discuss maximizing returns in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to solve a new type of problem called
    **continuous action spaces**. Then, we will look at how to use SARSA to solve
    a new Gym environment.
  prefs: []
  type: TYPE_NORMAL
- en: Using continuous spaces with SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have been exploring the **finite Markov Decision Process**
    or **finite MDP**. These types of problems are all well and good for simulation
    and toy problems, but they don't show us how to tackle real-world problems. Real-world
    problems can be broken down or discretized into finite MDPs, but real problems
    are not finite. Real problems are infinite, that is, they define no discrete simple
    states such as showering or having breakfast. Infinite MDPs model problems in
    what we call continuous space or continuous action space, that is, in problems
    where we think of a state as a single point in time and state defined as a slice
    of that time. Hence, the discrete task of **eat breakfast** could be broken down
    to each time step including individual chewing actions.
  prefs: []
  type: TYPE_NORMAL
- en: Solving an infinite MDP or continuous space problem is not trivial with our
    current toolset, but it will require us to apply discretization tricks. Applying
    discretization or breaking the continuous space into discrete spaces will make
    this problem solvable with our current toolset. In [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml),
    *Going Deep with DQN*, we will look to apply deep learning to a continuous action
    space, which allows us to solve these environments without using these discretization
    tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Many continuous RL environments have more environmental states than atoms in
    the observable universe, and yes, that is a very big number. We have managed to
    solve these problems by applying deep learning and hence deep RL starting in [Chapter
    6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going Deep with DQN*.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter was originally sourced from this GitHub repository: [https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym](https://github.com/srnand/Reinforcement-Learning-using-OpenAI-Gym). It
    looks like the author, Shrinand Thakkar, has since moved on to other pursuits
    and did not complete this excellent work as he intended.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Chapter_5_1.py` and follow the exercise shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for the listing is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving past the imports, we will look at the hyperparameter initialization
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We start the code block with the instantiation of a new environment, `MountainCar-v0`,
    which is in a continuous space environment. We then see the `Q_table` table is
    initialized with all zeros. Then, we set values for the learning rate, `alpha`;
    the discount factor, `gamma`; and the number of `episodes`. Also, we see a new
    list called `buckets` constructed. We will cover what buckets do shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From there, jump to the end of the code. We want a high-level overview of what
    the code does first. Take a look at the episode `for` loop, shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is the episode loop code and very much follows the pattern
    that we have seen in several previous chapters. The one major point of difference
    here is the way the algorithm/agent seemingly picks an action twice, as can be
    seen in the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The difference from Q-learning here is that the agent in SARSA is on-policy,
    that is, the action it picks needs to also decide its next action. Recall, in
    Q-learning, the agent works off-policy, that is, it takes an action from a previously
    learned policy. Again, this also goes back to TD (0) or one step, where the algorithm
    is still only looking one step ahead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, let''s run the algorithm to see how this works. A few examples
    of the car climbing the hill can be seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6216cdb5-09f4-4b27-8309-332948c02e30.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_5_1.py
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot, we can see that the agent is climbing the hill.
    Let the agent continue climbing until it reaches the flag; it should almost get
    there. Now, this is cool and fairly powerful stuff, but even more so considering
    we can do this by assuming our infinite MDP (continuous space) is controllable
    in discrete steps and hence a finite MDP. To do that, we have to learn how to
    discretize a continuous action space and we will see how to do that in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Discretizing continuous state spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RL is limited to discrete spaces or what we learned previously as a finite
    MDP. A finite MDP describes a discrete set of steps or states with an action to
    move between states decided by a probability. The infinite version of this may
    define an infinite number of state-actions between any set of states. Hence, a
    basketball player moving from one end of the court to score a basket describes
    an infinite MDP or continuous space. That is, for each point in time, the ball
    player could be in an infinite number of positions, dribbling or not dribbling,
    or shooting the ball and so on. Likewise, in the `MountainCar` environment, the
    car can be moving up or down the hill in either direction at any point in time.
    This makes the `MountainCar` environment a continuous state space, but just barely.
    Fortunately, we can use a clever trick to discretize the state space as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcb85386-c50e-4a98-b235-c5646f7333e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example discretization of MountainCar
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we have overlaid a grid on top of the environment
    to represent state spaces the cart may be in. For the sample, a 4 x 4 grid is
    used, but in our code, we will use a much larger grid. Doing this allows us to
    capture the position of the cart as if it was on a grid. Since the goal of this
    task is to move the cart up the hill, then discretizing the space by applying
    a gridding technique works quite well. In more complex continuous spaces, your
    grid may represent multiple dimensions in space or across other features. Fortunately,
    we won't have to worry about those complex mathematics when we learn how to apply
    deep learning to continuous spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand how the space is discretized, let''s jump back to the
    sample code in `Chapter_5_1.py` and review how this works in the following exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by picking up where we last left off. At the last point, we were
    just updating the `Q_table` table with the following line inside the episode `for`
    loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This calls the `update_SARSA` function, shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For now, ignore the `Q_table` update code and instead focus on the highlighted
    calls to `to_discrete_states`. These calls take an observation as input. An observation
    denotes the cart''s absolute position in the *x,y* coordinates. This is where
    we discretize the state using the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `to_discrete_states` function returns the grid interval the cart is currently
    in. Back in the `update_SARSA` function, we change the interval list back to a
    tuple with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Run the sample as you normally would again, just to confirm it works as expected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This simple method of discretization works well for this task but can quickly
    fall down or become overtly complex depending on the complexity of the environment.
    Before we move on to other matters, we want to return and look at how we update
    `Q_table` with SARSA in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Expected SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vanilla SARSA is quite similar to Q-learning in terms of how we choose values.
    It will generally just use an epsilon-greedy max action strategy, not unlike what
    we used previously; however, what we find, especially when working on-policy,
    is that the algorithm needs to be more selective. Now, this is very much the goal
    of all RL, but, in this particular case, we manage these trade-offs a bit better
    by introducing an expectation. When we combine this with SARSA, we call it **expected
    SARSA**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In expected SARSA, we assume an unknown learning rate alpha, and hence an unknown
    exploration rate epsilon as well. Instead, we equate the learning rate alpha and
    exploration rate epsilon using functions based on assigned rewards. We assign
    a reward of one timepoint for each time step and then calculate the new alpha
    and epsilon based on those. Open `Chapter_5_2.py` back up and let''s see how this
    works by following the exercise here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two functions of code we are interested in are shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The two functions, `expect_epsilon` and `expect_alpha`, calculate an expectation
    or ratio based on the rewards returned so far, `t`, where `t` equals the total
    time the cart has been moving in the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can focus on how `expect_epsilon` is used by looking at the `get_action`
    function shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`get_action` returns the action based on the observation (*x* and *y* positions
    of the cart). It does this by first checking whether a random action is to be
    sampled or, instead, the best action. We determine the probability of this by
    using the `expect_epsilon` equation, which calculates epsilon based on the total
    episode time playing the environment. This effectively means the epsilon in this
    example will range between 0.001 and 0.0015; see whether you can figure that out
    in the code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we do something similar to calculate `alpha` shown in the `update_SARSA`
    function. The single line where this is used is shown again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code should be familiar by now as it looks like our regular policy
    update equation, except, in this instance, we are tuning the value for `alpha`
    using an expectation based on the current time on the task. You can also think
    of this in some ways as a secondary reward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code again and let it finish to completion. Notice the output as we
    will use that as a comparison soon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/951548fc-9997-456b-8e53-026009355a53.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of returns/rewards over training time
  prefs: []
  type: TYPE_NORMAL
- en: The plot shows the accumulated rewards/time the cart spends in the environment.
    The cost is awarded a time reward for each time slice it remains in the environment,
    where if the cart remains stationary or relatively still for more than a few time
    slices, the episode is over. Therefore, the more time the cart stays in the environment
    also equates to more movement.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous states or continuous observations are not the only things we need
    to concern ourselves when considering real-time problems. In the real world, we
    also deal with continuous action spaces as well. Currently, we have been looking
    at problems with discrete action spaces, that is, environments that use arbitrary
    discrete actions to control the agent. These actions are typically up, down, left,
    and right. However, for the real world, we need finer control and often categorize
    actions as turn left by amount *x* or right by amount *y*. By adding continuous
    action spaces, our RL algorithms become less robotic and provide finer control.
    Discretizing discrete action spaces into continuous action spaces is more difficult
    and not something we will concern ourselves with. Instead, we will look at how
    to convert another more popular continuous action space we use for deep RL in
    the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Extending continuous spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Typically, we leave problems with large observation spaces to be tackled with
    deep learning. Deep learning, as we will learn, is very well-suited to such problems.
    However, deep learning is not without its own issues and it is sometimes prudent
    to try and solve an environment without deep learning. Now, not all environments
    will discretize well, as we mentioned previously, but we do want to look at another
    example. The next example we will look at is the infamous Cart Pole environment,
    which is almost always tackled with deep RL, primarily because it uses a continuous
    action space with four dimensions. Keep in mind that our previous observation
    spaces only had one dimension, and, in our last example, we only had two.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to convert an agent's observation space can be a useful trick especially
    in more abstract game environments. Remember, good game mechanics are often more
    about being fun rather than accurate. This certainly applies to some AI elements
    in games.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the specifics of the observation and state spaces by going to
    the environment''s GitHub page if it has one. Most of the more popular environments
    have their own page. The **Cart Pole** and **Mountain Car** observation and action
    spaces are shown in the following excerpts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c95f106-1c22-4245-bcc0-04a35c90a30e.png)'
  prefs: []
  type: TYPE_IMG
- en: Spaces of Mountain Car and Cart Pole environments
  prefs: []
  type: TYPE_NORMAL
- en: The preceding excerpts show a comparison of the **Mountain Car** versus the
    **Cart Pole** environments. Both environments use discrete action spaces, which
    is good. However, the **Cart Pole** environment uses a 4-dimensional observation
    space with values shown in the ranges in the table in the screenshot. This can
    be a little tricky and it will be helpful to understand how multidimensional observation
    spaces work in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Chapter_5_3.py` and follow this exercise to see how our last example
    can be converted into **Cart Pole**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the most part, the code is identical to the last two examples, so we only
    need to look at the differences. We will start with the environment construction
    section at the top, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This constructs the infamous **Cart Pole** environment. Again, switching environments
    is easy but your code has to adapt to the observation and action spaces. **Cart
    Pole** and **Mountain Car** share the same observation/action space types. That
    is, its observation space is continuous but with a discrete action space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will look and see how this affects our `Q_table` table initialization
    with the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the table is now configured with four dimensions at size 20\. Previously,
    this was just two dimensions of size 20\. Go back and check the last code examples
    for comparison if you need to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With more dimensions added to the `Q_table` table, that means we also need
    to add more dimensions to our discretization buckets, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Again, we increase the `buckets` array from two dimensions to four, all of size
    `20`. We are arbitrarily using a size of 20 but we could use a larger or smaller
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The last thing we need to do is redefine the boundaries of the environment''s
    observations. Recall we were able to extract this information from the GitHub
    page. This is the table that shows the min/max values in the ranges. The line
    of code we are interested in is just inside the `to_discrete_states` function,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The line is highlighted and declares the `max_range` variable. `max_range` sets
    the max value along each dimension in the observation space. We populate this
    with the values from the table and, in the case of infinity, we use six 9s (999999),
    which often works for the upper limits of values with infinity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next we, need to update the axis dimensions we use for indexing into the `Q_table`
    table, as shown in the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, notice how we are now indexing to the four dimensions
    and the action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and observe the output; an example is shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1deb404f-bcf1-472c-bbbc-72cda2095a13.png)'
  prefs: []
  type: TYPE_IMG
- en: Example Chapter_5_3.py
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, SARSA using a discretized observation space can solve the `CartPole`
    environment. This one may take a while to learn so be patient, but the agent will
    learn to balance the pole on the cart. You should have a fairly good understanding
    of how discretization works and SARSA at TD (0). In the next section, we will
    look at looking ahead/behind more than one step.
  prefs: []
  type: TYPE_NORMAL
- en: Working with TD (λ) and eligibility traces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, we have looked at the forward view or what the agent perceives
    to be as the next best reward or state. In **MC**, we looked at the entire episode
    and then used those values to reverse calculate returns. For TDL methods such
    as Q-learning and SARSA, we looked a single step ahead or what we referred to
    as TD (0). However, we want our agents to be able to take into account several
    steps, *n*, in advance. If we can do this, then surely our agent will be able
    to make better decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen previously, we can average returns across steps using a discount
    factor, gamma. However, at this point, we need to more careful about how we average
    or collect returns. Instead, we can define the averaging of all returns over an
    infinite number of steps forward as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b74115d-fb63-4f38-900c-1544016e227c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f80d35d-500d-4f12-8e6b-5331bc40a819.png) This is the weighted average
    of all returns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2267d3ca-2648-4c06-8402-c357416c3e78.png) This is the return of individual
    episodes from *t* to *t+n.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c43f9843-b9df-4acd-bab4-82f9de0d26c5.png) Lambda, a weight value between
    [0,1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since lambda is less than one, as values of *n* increase, the amount of contribution
    to the final average return becomes smaller. This is due to raising lambda (λ)
    to the power of *n* as in the preceding equation. Again, this is the same principle
    as using the discount factor, gamma. Now that we are thinking in terms of *n*
    steps or what we will refer to as lambda, we can revisit how this looks in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8eeae23-6867-4dae-b031-7c4ae7f8e6c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Progression of TD (λ)
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the general solution for *n* time steps, where *n* is an unknown we
    call lambda (λ), we need to determine a general solution for finding lambda, that
    is, the value of lambda that generalizes the problem. We can do that by first
    assuming that any episode will end at time step, *t*, and then rewriting our previous
    equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91d7517b-2145-4403-8129-2417c549de74.png)'
  prefs: []
  type: TYPE_IMG
- en: When a value  of 0 for lambda is used, this represents TD (0). A value of 1
    for lambda represents MC or a need for a complete episode lookahead. However,
    it is complicated to implement this form of lookahead models and, intuitively,
    looking ahead is a very small part of what biological animals learn. In fact,
    our primary source of learning is experience, and that is exactly what we start
    to consider in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Backward views and eligibility traces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Do you recall the last time you found a coin on the floor or street? After
    you picked up the coin, did you think to yourself: a) "I knew looking down all
    that time would pay off," or b) "Wow, I found a coin, how did I do that?" In fact,
    in most cases, it would be option *b*, that is, we learned something was good
    and then thought back to how we discovered it. The moment of brilliance in option
    *a* is akin to believing in quantum particles, atoms, and bacteria. This is no
    different in RL, and what we find is that it is often more useful to look back
    at what happened in the past; however, not so far back as to be a past event as
    in MC.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use TDL to take a backward look at the returns for several steps. However,
    we can''t just use an absolute value for the state transitions. Instead, we need
    to determine the predicted error for each step back using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/861c5097-ba1a-4a3e-9c13-5981e348933a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](img/416226c6-0388-4019-8e63-6a8526bd534f.png)] This is the TD error or
    delta.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/c9aab2e3-e3f9-4eb9-8d97-c56a31de0381.png)] The value function, which
    can be further defined by the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/53d9aed7-2b64-4394-9d80-619ceecbbba0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can further define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a0de061-f7eb-4694-a5c6-ded805cd9141.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/6accd72f-5deb-4f13-a909-fdb806eac621.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/fc2406d8-23d4-48ac-9816-0c095a62b1dd.png)
    assigns the full value of 1 when the state is at *s.*
  prefs: []
  type: TYPE_NORMAL
- en: '*E* denotes the eligibility factor or the amount the value should be considered
    in the TD error. What is happening here is that the value function is being updated
    by the number of TD errors over *n* steps, but, instead of looking forward, we
    look backward. Much like all things in RL, it seems this has to be applied across
    several variations of algorithms. For *n* step TDL or TD (λ), we have three variations
    we concern ourselves with. They are Tabular TD (λ), SARSA (λ), and Q (λ). Each
    algorithm variation in pseudocode is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0090720b-6a78-4a7a-97d2-90b2c9f4dac4.png)'
  prefs: []
  type: TYPE_IMG
- en: TD (λ), SARSA (λ), and Q (λ)
  prefs: []
  type: TYPE_NORMAL
- en: Each algorithm has a slight variation in the way it calculates values and TD
    errors. In the next section, we will look at a full implementation of SARSA (λ)
    in code.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SARSA (λ)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We could, of course, implement TD (λ) using the tabular online method, which
    we haven''t covered yet, or with Q-learning. However, since this is a chapter
    on SARSA, it only makes sense that we continue with that theme throughout. Open
    `Chapter_5_4.py` and follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is quite similar to our previous examples, but let''s review the full
    source code, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The top section of code is quite similar with some notable differences. Notice
    the initialization of the `MountainCar` environment and the `Q_table` table setup
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we increase the number of discretized states from 20 x 20 to 65 x
    65 as we initialize the `Q_table` table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next major difference now is the calculation of eligibility using lambda.
    We can find this code in the bottom episode `for` loop, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The calculation for eligibility is done in the highlighted lines. Notice how
    we multiply `eligibility` by `lambda` and `gamma`, then add one for the current
    state. This value is then passed into the `update_SARSA` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we now update the `Q_table` table based on a determination of `td_error`
    and `eligibility`. In other words, we take into consideration now how current
    the information is and how much it was valued in the past.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code example again as you normally would and watch the agent play the
    task. The training output for this task is shown in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ca6f8852-14ba-4f90-bf02-da2fb54b8bcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Output plot of rewards for SARSA (λ)
  prefs: []
  type: TYPE_NORMAL
- en: It may take a few minutes to generate the plot shown in the preceding diagram,
    so please be patient. Be sure to note how this compares with the previous examples
    we already ran in this chapter. You did run all of the sample exercises to completion,
    right? Notice how the output of returns/rewards of the time on each episode increases
    quicker and converges quicker.
  prefs: []
  type: TYPE_NORMAL
- en: We want to look at one more complex example that puts our use of discretization
    to the extreme in the next example.
  prefs: []
  type: TYPE_NORMAL
- en: SARSA lambda and the Lunar Lander
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the algorithms we develop get more complicated, their capabilities also get
    more powerful. However, there are limits and it is important to understand the
    limits of any technology. To test those limits, we want to look at an example
    that pushes them. For this particular case, we will look at the Lunar Lander environment
    from Gym. This environment is modeled after the old classic arcade game of the
    same name, where the object is to land a lunar module on the surface of the moon.
    In this environment, the observation space is described in eight dimensions and
    the action space in four. As we will see, this can quickly go beyond our current
    computational limits.
  prefs: []
  type: TYPE_NORMAL
- en: The `LunarLander` environment requires the installation of a special module
    called `Box2D`. This is essentially a graphics package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the exercise in the next section to set up and run the advanced `Box2D`
    modules for Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps for Windows (Anaconda):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open an Anaconda Prompt as an administrator. Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: SWIG is a requirement of `Box2D`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, run the following command to install Box2D:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow these steps for Mac/Linux (or Windows without Anaconda):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open a Python shell and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If you encounter issues, consult the instructions for the Windows installation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This installation now allows you to run all of the more advanced Box2D environments.
    These are far more game-like and interesting to train on as well. Open up `Chapter_5_5.py`
    and follow the exercise to set up and train SARSA on Lunar Lander:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for `Chapter_5_5.py` is almost identical to `Chapter_5_4.py`
    aside from minor differences in setting up the discrete states. We will first
    look at how we set up the `Q_table` table with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we went from values of 65 steps down to 5\. The last value denotes
    the action space size and this has gone from three in `MountainCar` to four for
    `LunarLander`. However, with eight dimensions, we have to be careful about the
    size of the array. Hence, we need to limit each step size to five, in this example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we initialize the `buckets` state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Again, initialized to a size of three for the eight dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we set the `max_range` values for the maximum values we want our step
    to span, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We use a value of 100 here to denote some arbitrary max value. Altering or tweaking
    these values could improve training efficiency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we need to expand the `Q_table` indexing to include 8 dimensions, like
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Be aware of the limits we are applying to the agent in this example. We are
    effectively making the agent see in big sections, where each section or axis feature
    is only divided into three slices. It is surprising how effective this method
    can be.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the sample and let it go to completion. Yes, this one will take a while
    but it is worth it. An example output from the Lunar Lander environment can be
    seen in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ef2e7437-e1be-48ee-887e-ac6c42f2b973.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_5_5.py
  prefs: []
  type: TYPE_NORMAL
- en: In the last example, we briefly looked at using SARSA on another continuous
    observation space environment, the Lunar Lander. While it can be fun to play with
    these environments and see how our discretization can manage an infinite MDP adequately,
    it is time we moved on to using the big guns of deep learning to manage continuous
    observation spaces. From the output of rewards, we can see that the example does
    not converge at all. This is likely because the discretization is not fine enough;
    perhaps you can improve on that?
  prefs: []
  type: TYPE_NORMAL
- en: The discretization process in this example is not optimal and could certainly
    be improved upon with some DP methods.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning networks applied to RL allow us to tackle enormous continuous
    observation and action spaces. As such, discretization of spaces won't be needed
    regularly going forward but it can be a useful trick or advantage for simpler
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: This completes this chapter and I encourage you to move on and explore the exercises
    to improve your own learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These exercises are here for you to use and learn from. Attempt at least 2-3,
    and the more you do, the easier later chapters will also be:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between an online and offline policy agent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for any or all of the examples in this chapter, including
    the new hyperparameter, `lambda`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the discretization steps in any example that uses discretization and
    see what effect it has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use example `Chapter_5_3.py`, **SARSA(0)**, and adapt it to another Gym environment
    that uses a continuous observation space and discrete action space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use example `Chapter_5_4.py`, **SARSA(λ)**, and adapt it to another Gym environment
    that uses a continuous observation space and discrete action space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a hyperparameter shown in the code that is not used. Which parameter
    is it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use example `Chapter_5_5.py`, **SARSA(λ)**, Lunar Lander and optimize the discretization
    so that it performs better. For example, you are still limited by array dimensions
    but you can increase or decrease some more important dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use example `Chapter_5_5.py`, **SARSA(λ)**, Lunar Lander and optimize the `max_range`
    values so that it performs better. For example, instead of setting all values
    to 999, check whether certain values can be narrowed or need expanding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update an example to work with a continuous action environment. This will require
    you to discrete the action space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert one of the samples into Q-learning, that is, it uses an offline policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to also explore more on your own. We barely scratched the surface
    of the intricacies of these methods. Finally, we come to our summary in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, we continued exploring TD learning. We looked at an example
    of an online TD (0) method called **SARSA**. Then, we looked at how we can discretize
    an observation space to tackle harder problems but still use the same toolset.
    From there, we looked at how we could tackle harder continuous space problems
    such as `CartPole`. After that, we revisited TDL and then looked to *n* step forward
    views, decided that was less than optimal, and then moved to backward views and
    eligibility traces, which led to us uncovering TD (λ), SARSA(λ), and Q (λ). Using
    SARSA(λ), we were able to solve the `MountainCar` environment in far less time.
    Finally, we wanted to tackle a far more difficult environment, `LunarLander` using
    SARSA(λ) without deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we look at introducing deep learning and escalate ourselves
    to deep reinforcement learners.
  prefs: []
  type: TYPE_NORMAL
