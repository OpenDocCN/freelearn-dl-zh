<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 1. Getting Started with Neural Networks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch01" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 1. Getting Started with Neural Networks</h1></div></div></div><p class="calibre11">In this chapter, we will introduce neural networks and what they are designed for. This chapter serves as a foundation layer for the subsequent chapters, while presenting the basic concepts for neural networks. In this chapter, we will cover the following:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Artificial neurons</li><li class="listitem">Weights and biases</li><li class="listitem">Activation functions</li><li class="listitem">Layers of neurons</li><li class="listitem">Neural network implementation in Java</li></ul></div><div class="calibre2" title="Discovering neural networks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Discovering neural networks</h1></div></div></div><p class="calibre11">By hearing <a id="id0" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the term <span class="strong1"><strong class="calibre12">neural networks</strong></span> we intuitively create a snapshot of a brain in our minds, and indeed that's right, if we consider the brain to be a big and natural neural network. However, what about <span class="strong1"><strong class="calibre12">artificial neural networks</strong></span> (<span class="strong1"><strong class="calibre12">ANNs</strong></span>)? Well, here comes an opposite word to natural, and the first thing now that comes into <a id="id1" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our head is an image of an artificial brain or a robot, given the term <span class="strong1"><em class="calibre16">artificial</em></span>. In this case, we also deal with creating a structure similar to and inspired by the human brain; therefore, this can be called artificial intelligence.</p><p class="calibre11">Now the reader newly introduced to ANN may be thinking that this book teaches how to build intelligent systems, including an artificial brain, capable of emulating the human mind using Java codes, isn't it? The amazing answer is yes, but of course, we will not cover the creation of artificial thinking machines such as those from the Matrix trilogy movies; the reader will be provided a walkthrough on the process of designing artificial neural network solutions capable of abstracting knowledge in raw data, taking advantage of the entire Java programming language framework.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Why artificial neural networks?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Why artificial neural networks?</h1></div></div></div><p class="calibre11">We cannot begin <a id="id2" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>talking about neural networks without understanding their origins, including the term as well. The terms neural networks (NN) and ANN are used as synonyms in this book, despite NNs being more general, covering the natural neural networks as well. So, what actually is an ANN? Let's explore a little of the history of this term.</p><p class="calibre11">In the 1940s, the neurophysiologist Warren McCulloch and the mathematician Walter Pitts designed the first mathematical implementation of an artificial neuron combining the neuroscience foundations with mathematical operations. At that time, the human brain was being studied largely to understand its hidden and mystery behaviors, yet within the field of neuroscience. The natural neuron structure was known to have a nucleus, dendrites receiving incoming signals from other neurons, and an axon activating a signal to other neurons, as shown in the following figure:</p><div class="mediaobject"><img src="Images/B05964_01_01.jpg" alt="Why artificial neural networks?" class="calibre18"/></div><p class="calibre11">The novelty of McCulloch and Pitts was the math component included in the neuron model, supposing a neuron as a simple processor summing all incoming signals and activating a new signal to other neurons:</p><div class="mediaobject"><img src="Images/B05964_01_02.jpg" alt="Why artificial neural networks?" class="calibre19"/></div><p class="calibre11">Furthermore, considering that the brain is composed of billions of neurons, each one interconnected <a id="id3" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with another tens of thousands, resulting in some trillions of connections, we are talking about a giant network structure. On the basis of this fact, McCulloch and Pitts designed a simple model for a single neuron, initially to simulate the human vision. The available calculators or computers at that time were very rare, but capable of dealing with mathematical operations quite well; on the other hand, even tasks today such as vision and sound recognition are not easily programmed without the use of special frameworks, as opposed to the mathematical operations and functions. Nevertheless, the human brain can perform sound and image recognition more efficiently than complex mathematical calculations, and this fact really intrigues scientists and researchers.</p><p class="calibre11">However, one known fact is that all complex activities that the human brain performs are based on learned knowledge, so as a solution to overcome the difficulty that conventional algorithmic approaches face in addressing these tasks easily solved by humans, an ANN is designed to have the capability to learn how to solve some task by itself, based on its stimuli (data):</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Tasks Quickly Solvable by Humans</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Tasks Quickly Solvable by Computers</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre28"><td class="calibre29">
<p class="calibre26">Classification of images</p>
<p class="calibre26">Voice recognition</p>
<p class="calibre26">Face identification</p>
<p class="calibre26">Forecast events on the basis of experience</p>
</td><td class="calibre29">
<p class="calibre26">Complex calculation</p>
<p class="calibre26">Grammatical error correction</p>
<p class="calibre26">Signal processing</p>
<p class="calibre26">Operating system management</p>
</td></tr></tbody></table></div><div class="calibre2" title="How neural networks are arranged"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec06" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How neural networks are arranged</h2></div></div></div><p class="calibre11">By taking into<a id="id4" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> account the human brain characteristics, it can be said that the ANN is a nature-inspired approach, and so is its structure. One neuron connects to a number of others that connect to another <a id="id5" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of neurons, thus being a highly interconnected structure. Later in this book, it will be shown that this connectivity between neurons accounts for the capability of learning, since every connection is configurable according to the stimuli and the desired goal.</p></div><div class="calibre2" title="The very basic element – artificial neuron"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec07" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The very basic element – artificial neuron</h2></div></div></div><p class="calibre11">Let's explore the most basic artificial neural element – the artificial neuron. Natural neurons have <a id="id6" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>proven to be signal processors since they receive micro signals in the dendrites that can trigger a signal in the axon depending <a id="id7" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>on their strength or magnitude. We can then think of a neuron as having a signal collector in the inputs and an activation unit in the output that can trigger a signal that will be forwarded to other neurons, as shown in the following figure:</p><div class="mediaobject"><img src="Images/B05964_01_03.jpg" alt="The very basic element – artificial neuron" class="calibre30"/></div><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip02" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">In natural neurons, there is a threshold potential that when reached, fires the axon and propagates the signal to the other neurons. This firing behavior is emulated with activation functions, which has proven to be useful in representing nonlinear behaviors in the neurons.</p></div></div></div><div class="calibre2" title="Giving life to neurons – activation function"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Giving life to neurons – activation function</h2></div></div></div><p class="calibre11">This activation<a id="id8" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> function is what fires the neuron's output, based on the sum of all incoming signals. Mathematically it adds <a id="id9" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>nonlinearity to neural network processing, thereby providing the artificial neuron nonlinear behaviors, which will be very useful in emulating the nonlinear nature of natural neurons. An activation function is usually bounded between two values at the output, therefore being a nonlinear function, but in some special cases, it can be a linear function.</p><p class="calibre11">Although any function can be used as activation, let's concentrate on common used ones:</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Function</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Equation</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Chart</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Sigmoid</p>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/B05964_01_04_01.jpg" alt="Giving life to neurons – activation function" class="calibre32"/></div>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/B05964_01_04.jpg" alt="Giving life to neurons – activation function" class="calibre33"/></div>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">Hyperbolic tangent</p>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/B05964_01_05_01.jpg" alt="Giving life to neurons – activation function" class="calibre32"/></div>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/B05964_01_05.jpg" alt="Giving life to neurons – activation function" class="calibre35"/></div>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Hard limiting threshold</p>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/b05964_01_06_01.jpg" alt="Giving life to neurons – activation function" class="calibre36"/></div>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/b05964_01_06.jpg" alt="Giving life to neurons – activation function" class="calibre35"/></div>
</td></tr><tr class="calibre37"><td class="calibre29">
<p class="calibre26">Linear</p>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/b05964_01_07_01.jpg" alt="Giving life to neurons – activation function" class="calibre38"/></div>
</td><td class="calibre29">
<div class="mediaobject"><img src="Images/b05964_01_07.jpg" alt="Giving life to neurons – activation function" class="calibre39"/></div>
</td></tr></tbody></table></div><p class="calibre11">In these equations <a id="id10" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and charts the <a id="id11" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>coefficient a can be chosen as a setting for the activation function.</p></div><div class="calibre2" title="The flexible values – weights"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The flexible values – weights</h2></div></div></div><p class="calibre11">While the neural <a id="id12" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>network structure can be fixed, weights represent the connections between neurons and they have the capability to amplify or attenuate incoming neural signals, thus modifying them and having the power to influence a neuron's output. Hence a neuron's activation will not be dependent on only the inputs, but on the weights too. Provided that the inputs come from other neurons or from the external world (stimuli), the weights are considered to be a neural network's established connections between its neurons. Since the weights are an internal neural network component and influence its outputs, they can be considered as neural network knowledge, provided that changing the weights will change the neural network's outputs, that is, its answers to external stimuli.</p></div><div class="calibre2" title="An extra parameter – bias"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>An extra parameter – bias</h2></div></div></div><p class="calibre11">It is useful for <a id="id13" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the artificial neuron to have an independent component that adds an extra signal to the activation function: the <span class="strong1"><strong class="calibre12">bias</strong></span>. This <a id="id14" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parameter acts like an input, except for the fact that it is stimulated by one fixed value (usually 1), which is multiplied by an associated weight. This feature helps in the neural network knowledge representation as a more purely nonlinear system, provided that when all inputs are zero, that neuron won't necessarily produce a zero at the output, instead it can fire a different value according to the bias associated weight.</p></div><div class="calibre2" title="The parts forming the whole – layers"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec11" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The parts forming the whole – layers</h2></div></div></div><p class="calibre11">In order to<a id="id15" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> abstract levels of processing, as our mind does, neurons are organized in layers. The input layer receives direct stimuli from the outside <a id="id16" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>world, and the output layers fire actions that will have a direct influence on the outside world. Between these layers, there are a number of hidden layers, in the sense that they are invisible (hidden) from the outside world. In artificial neural networks, a layer has the same inputs and activation function for all its composing neurons, as shown in the following figure:</p><div class="mediaobject"><img src="Images/B05964_01_08.jpg" alt="The parts forming the whole – layers" class="calibre40"/></div><p class="calibre11">Neural networks can be composed of several linked layers, forming the so-called <span class="strong1"><strong class="calibre12">multilayer networks</strong></span>. Neural <a id="id17" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>layers can then be classified as <span class="strong1"><em class="calibre16">Input</em></span>, <span class="strong1"><em class="calibre16">Hidden</em></span>, or <span class="strong1"><em class="calibre16">Output</em></span>.</p><p class="calibre11">In practice, an <a id="id18" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>additional neural layer enhances the <a id="id19" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neural network's capacity to represent more complex knowledge.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">Every neural network has at least an input/output layer irrespective of the number of layers. In the case of a multilayer network, the layers between the input and the <a id="id20" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>output are called <span class="strong1"><strong class="calibre12">hidden</strong></span>
</p></div></div></div><div class="calibre2" title="Learning about neural network architectures"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec12" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning about neural network architectures</h2></div></div></div><p class="calibre11">A neural network <a id="id21" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can have different layouts, depending on how the <a id="id22" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neurons or layers are connected to each other. Each neural network architecture is designed for a specific goal. Neural networks can be applied to a number of problems, and depending on the nature of the problem, the neural network should be designed in order to address this problem more efficiently.</p><p class="calibre11">Neural network architectures classification is two-fold:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Neuron connections</li><li class="listitem">Monolayer networks</li><li class="listitem">Multilayer networks</li><li class="listitem">Signal flow</li><li class="listitem">Feedforward networks</li><li class="listitem">Feedback networks</li></ul></div></div><div class="calibre2" title="Monolayer networks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec13" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Monolayer networks</h2></div></div></div><p class="calibre11">In this<a id="id23" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> architecture, all neurons are<a id="id24" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> laid out in the same level, forming one single layer, as shown in the following figure:</p><div class="mediaobject"><img src="Images/B05964_01_09.jpg" alt="Monolayer networks" class="calibre41"/></div><p class="calibre11">The <a id="id25" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neural network receives the input<a id="id26" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> signals and feeds them into the neurons, which in turn produce the output signals. The neurons can be highly connected to each other with or without recurrence. Examples of these architectures are the single-layer perceptron, Adaline, self-organizing map, Elman, and Hopfield neural networks.</p></div><div class="calibre2" title="Multilayer networks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec14" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Multilayer networks</h2></div></div></div><p class="calibre11">In this <a id="id27" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>category, neurons are divided into<a id="id28" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> multiple layers, each layer corresponding to a parallel layout of neurons that shares the same input data, as shown in the following figure:</p><div class="mediaobject"><img src="Images/B05964_01_10.jpg" alt="Multilayer networks" class="calibre42"/></div><p class="calibre11">Radial basis<a id="id29" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> functions and multilayer <a id="id30" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>perceptrons are good examples of this architecture. Such networks are really useful for approximating real data to a function especially designed to represent that data. Moreover, because they have multiple layers of processing, these networks are adapted to learn from nonlinear data, being able to separate it or determine more easily the knowledge that reproduces or recognizes this data.</p></div><div class="calibre2" title="Feedforward networks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec15" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Feedforward networks</h2></div></div></div><p class="calibre11">The flow <a id="id31" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the signals in neural networks <a id="id32" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can be either in only one direction or in recurrence. In the first case, we call the neural network architecture feedforward, since the input signals are fed into the input layer; then, after being processed, they are forwarded to the next layer, just as shown in the figure in the multilayer section. Multilayer perceptrons and radial basis functions are also good examples of feedforward networks.</p></div><div class="calibre2" title="Feedback networks"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch01lvl2sec16" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Feedback networks</h2></div></div></div><p class="calibre11">When the<a id="id33" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> neural network has some kind <a id="id34" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of internal recurrence, it means that the signals are fed back in a neuron or layer that has already received and processed that signal, the network is of the feedback type. See the following figure of feedback networks:</p><div class="mediaobject"><img src="Images/B05964_01_11.jpg" alt="Feedback networks" class="calibre43"/></div><p class="calibre11">The special reason to add recurrence in the network is the production of a dynamic behavior, particularly when the network addresses problems involving time series or pattern recognition, which require an internal memory to reinforce the learning process. However, such networks<a id="id35" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> are particularly difficult to<a id="id36" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> train, because there will eventually be a recursive behavior during the training (for example, a neuron whose outputs are fed back into its inputs), in addition to the arrangement of data for training. Most of the feedback networks are single layer, such as Elman and Hopfield networks, but it is possible to build a recurrent multilayer network, such as echo and recurrent multilayer perceptron networks.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="From ignorance to knowledge – learning process"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>From ignorance to knowledge – learning process</h1></div></div></div><p class="calibre11">Neural networks<a id="id37" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> learn by adjusting the connections between the neurons, namely the weights. As mentioned in the neural structure section, weights represent the neural network knowledge. Different weights cause the network to produce different results for the same inputs. So, a neural network can improve its results by adapting its weights according to a learning rule. The general schema of learning is depicted in the following figure:</p><div class="mediaobject"><img src="Images/B05964_01_12.jpg" alt="From ignorance to knowledge – learning process" class="calibre44"/></div><p class="calibre11">The process <a id="id38" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>depicted in the previous figure is called <span class="strong1"><strong class="calibre12">supervised learning</strong></span> because there is a desired output, but neural networks can also learn by the input data, without any desired output (supervision). In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Getting Neural Networks to Learn">Chapter 2</a>, <span class="strong1"><em class="calibre16">Getting Neural Networks to Learn</em></span>, we <a id="id39" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>are going to dive deeper into the neural network learning process.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Let the coding begin! Neural networks in practice"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec11" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Let the coding begin! Neural networks in practice</h1></div></div></div><p class="calibre11">In this book, we <a id="id40" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will cover the entire process<a id="id41" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of implementing a neural network by using the Java programming language. Java is an object-oriented programming language that was created in the 1990s by a small group of engineers from Sun Microsystems, later acquired by Oracle in the 2010s. Nowadays, Java is present in many devices that are part of our daily life.</p><p class="calibre11">In an object-oriented language, such as Java, we deal with classes and objects. A class is a blueprint of something in the real world, and an object is an instance of this blueprint, something like a car (class referring to all and any car) and my car (object referring to a specific car—mine). Java classes are usually composed of attributes and methods (or functions), that include <span class="strong1"><strong class="calibre12">objects-oriented programming</strong></span> (<span class="strong1"><strong class="calibre12">OOP</strong></span>) concepts. We are going to briefly <a id="id42" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>review all of these concepts without diving deeper into them, since the goal of this book is just to design and create neural networks from a practical point of view. Four concepts are relevant and need to be considered in this process:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><strong class="calibre12">Abstraction</strong></span>: The<a id="id43" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> transcription of a real-world problem or rule into a computer programming domain, considering only its relevant features and dismissing the details that often hinder development.</li><li class="listitem"><span class="strong1"><strong class="calibre12">Encapsulation</strong></span>: Analogous to a product encapsulation by which some relevant features <a id="id44" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>are disclosed openly (public methods), while others are kept hidden within their domain (private or protected), therefore avoiding misuse or excess of information.</li><li class="listitem"><span class="strong1"><strong class="calibre12">Inheritance</strong></span>: In the<a id="id45" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> real world, multiple classes of objects share attributes and methods in a hierarchical manner; for example, a vehicle can be a superclass for car and truck. So, in OOP, this concept allows one class to inherit all features from another one, thereby avoiding the rewriting of code.</li><li class="listitem"><span class="strong1"><strong class="calibre12">Polymorphism</strong></span>: Almost <a id="id46" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the same as inheritance, but with the difference that methods with the same signature present different behaviors on different classes.</li></ul></div><p class="calibre11">Using the neural network concepts presented in this chapter and the OOP concepts, we are now going to design the very first class set that implements a neural network. As could be seen, a neural network consists of layers, neurons, weights, activation functions, and biases. About layers, there are three types of them: input, hidden, and output. Each layer may have one or more neurons. Each neuron is connected either to a neural input/output or to<a id="id47" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> another neuron, and these connections<a id="id48" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> are known as weights.</p><p class="calibre11">It is important to highlight that a neural network may have many hidden layers or none, because the number of neurons in each layer may vary. However, the input and output layers have the same number of neurons as the number of neural inputs/outputs, respectively.</p><p class="calibre11">So, let's start implementing. Initially, we are going to define the following classes:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><strong class="calibre12">Neuron</strong></span>: Defines<a id="id49" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the artificial neuron</li><li class="listitem"><span class="strong1"><strong class="calibre12">NeuralLayer</strong></span>: Abstract<a id="id50" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> class that defines a layer of neurons</li><li class="listitem"><span class="strong1"><strong class="calibre12">InputLayer</strong></span>: Defines<a id="id51" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the neural input layer</li><li class="listitem"><span class="strong1"><strong class="calibre12">HiddenLayer</strong></span>: Defines <a id="id52" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the layers between input and output</li><li class="listitem"><span class="strong1"><strong class="calibre12">OutputLayer</strong></span>: Defines <a id="id53" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the neural output layer</li><li class="listitem"><span class="strong1"><strong class="calibre12">InputNeuron</strong></span>: Defines <a id="id54" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the neuron that is present at the neural network input</li><li class="listitem"><span class="strong1"><strong class="calibre12">NeuralNet</strong></span>: Combines <a id="id55" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>all previous classes into one ANN structure</li></ul></div><p class="calibre11">In addition to these classes, we should also define an <code class="literal">IActivationFunction</code> interface for activation functions. This is necessary because <code class="literal">Activation</code> functions will behave like methods, but they will need to be assigned as a neuron property. So we are going to define classes for activation functions that implement this interface:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Linear</li><li class="listitem">Sigmoid</li><li class="listitem">Step</li><li class="listitem">HyperTan</li></ul></div><p class="calibre11">Our first chapter coding is almost complete. We need to define two more classes. One for handling eventually thrown exceptions (<code class="literal">NeuralException</code>) and another to generate random numbers (<code class="literal">RandomNumberGenerator</code>). Finally, we are going to separate these classes into two packages:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><code class="literal">edu.packt.neuralnet</code>: For the neural network related classes (<code class="literal">NeuralNet</code>, <code class="literal">Neuron</code>, <code class="literal">NeuralLayer</code>, and so on)</li><li class="listitem"><code class="literal">edu.packt.neuralnet.math</code>: For the math related classes (<code class="literal">IActivationFunction</code>, <code class="literal">Linear</code>, and so on)</li></ul></div><p class="calibre11">To save space, we are not going to write the full description of each class, instead we are going to address the key features of most important classes. However, the reader is welcomed to<a id="id56" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> take a glance at the Javadoc documentation<a id="id57" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the code, in order to get more details on the implementation.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The neuron class"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec12" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The neuron class</h1></div></div></div><p class="calibre11">This is the <a id="id58" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>very foundation class for this chapter's code. According to the theory, an artificial neuron has the following attributes:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Inputs</li><li class="listitem">Weights</li><li class="listitem">Bias</li><li class="listitem">Activation function</li><li class="listitem">Output</li></ul></div><p class="calibre11">It is also important to define one attribute that will be useful in future examples, that is the output before activation function. We then have the implementation of the following properties:</p><div class="calibre2"><pre class="programlisting">public class Neuron {
  protected ArrayList&lt;Double&gt; weight;
  private ArrayList&lt;Double&gt; input;
  private Double output;
  private Double outputBeforeActivation;
  private int numberOfInputs = 0;
  protected Double bias = 1.0;
  private IActivationFunction activationFunction;
  …
}</pre></div><p class="calibre11">When instantiating a neuron, we need to specify how many inputs are going to feed values to it, and what should be its activation function. So let's take a look on the constructor:</p><div class="calibre2"><pre class="programlisting">public Neuron(int numberofinputs,IActivationFunction iaf){
    numberOfInputs=numberofinputs;
    weight=new ArrayList&lt;&gt;(numberofinputs+1);
    input=new ArrayList&lt;&gt;(numberofinputs);
    activationFunction=iaf;
}</pre></div><p class="calibre11">Note that we define one extra weight for the bias. One important step is the initialization of the neuron, that is, how the weights receive their first values. This is defined in the <code class="literal">init()</code> method, by which weights receive randomly generated values by the <code class="literal">RandomNumberGenerator static</code> class. Note the need to prevent an attempt to set a value outside the bounds of the weight array:</p><div class="calibre2"><pre class="programlisting">public void init(){
  for(int i=0;i&lt;=numberOfInputs;i++){
    double newWeight = RandomNumberGenerator.GenerateNext();
    try{
      this.weight.set(i, newWeight);
    }
    catch(IndexOutOfBoundsException iobe){
      this.weight.add(newWeight);
    }
  }
}</pre></div><p class="calibre11">Finally, let's take <a id="id59" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a look on how the output values are calculated in the <code class="literal">calc()</code> method:</p><div class="calibre2"><pre class="programlisting">public void calc(){
  outputBeforeActivation=0.0;
  if(numberOfInputs&gt;0){
    if(input!=null &amp;&amp; weight!=null){
      for(int i=0;i&lt;=numberOfInputs;i++){
        outputBeforeActivation+=(i==numberOfInputs?bias:input.get(i))*weight.get(i);
      }
    }
  }
  output=activationFunction.calc(outputBeforeActivation);
}</pre></div><p class="calibre11">Note that first, the products of all inputs and weights are summed (the bias multiplies the last weight <code class="literal">– i==numberOfInputs</code>), and this value is saved in the <code class="literal">outputBeforeActivation</code> property. The activation function calculates the neuron's output with this value.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The NeuralLayer class"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec13" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The NeuralLayer class</h1></div></div></div><p class="calibre11">In this class we are<a id="id60" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> going to group the neurons that are aligned in the same layer. Also, there is a need to define links between layers, since one layer forwards values to another. So the class will have the following properties:</p><div class="calibre2"><pre class="programlisting">public abstract class NeuralLayer {
  protected int numberOfNeuronsInLayer;
  private ArrayList&lt;Neuron&gt; neuron;
  protected IActivationFunction activationFnc;
  protected NeuralLayer previousLayer;
  protected NeuralLayer nextLayer;
  protected ArrayList&lt;Double&gt; input;
  protected ArrayList&lt;Double&gt; output;
  protected int numberOfInputs;
…
}</pre></div><p class="calibre11">Note that this class is abstract, the layer classes that can be instantiated are <code class="literal">InputLayer</code>, <code class="literal">HiddenLayer</code>, and <code class="literal">OutputLayer</code>. In order to create one layer, one must use one of these classes' constructors<a id="id61" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that work quite similar:</p><div class="calibre2"><pre class="programlisting">public InputLayer(int numberofinputs);
public HiddenLayer(int numberofneurons,IActivationFunction iaf,
int numberofinputs);
public OutputLayer(int numberofneurons,IActivationFunction iaf,
int numberofinputs);</pre></div><p class="calibre11">Layers are initialized and calculated as well as the neurons, they also implement the methods <code class="literal">init()</code> and <code class="literal">calc()</code>. The signature protected guarantees that only the subclasses can call or override these methods:</p><div class="calibre2"><pre class="programlisting">protected void init(){
  for(int i=0;i&lt;numberOfNeuronsInLayer;i++){
    try{
      neuron.get(i).setActivationFunction(activationFnc);
      neuron.get(i).init();
    }
    catch(IndexOutOfBoundsException iobe){
      neuron.add(new Neuron(numberOfInputs,activationFnc));
      neuron.get(i).init();
    }
  }
}
protected void calc(){
  for(int i=0;i&lt;numberOfNeuronsInLayer;i++){
    neuron.get(i).setInputs(this.input);
    neuron.get(i).calc();
    try{
      output.set(i,neuron.get(i).getOutput());
     }
     catch(IndexOutOfBoundsException iobe){
       output.add(neuron.get(i).getOutput());
     }
   }
  }</pre></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The ActivationFunction interface"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec14" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The ActivationFunction interface</h1></div></div></div><p class="calibre11">Before we <a id="id62" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>define the <code class="literal">NeuralNetwork</code> class, let's take a look at an example of Java code with interface:</p><div class="calibre2"><pre class="programlisting">public interface IActivationFunction {
  double calc(double x);
  public enum ActivationFunctionENUM {
    STEP, LINEAR, SIGMOID, HYPERTAN
  }
}</pre></div><p class="calibre11">The <code class="literal">calc()</code> signature method is used by a specific Activation Function that implements this interface, the <code class="literal">Sigmoid</code> function, for example:</p><div class="calibre2"><pre class="programlisting">public class Sigmoid implements IActivationFunction {
    private double a=1.0;
public Sigmoid(double _a){this.a=_a;}
@Override
    public double calc(double x){
        return 1.0/(1.0+Math.exp(-a*x));
    }
}</pre></div><p class="calibre11">This is one example of polymorphism, whereby a class or method may present different behavior, but yet under the same signature, allowing a flexible application.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The neural network class"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec15" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The neural network class</h1></div></div></div><p class="calibre11">Finally, let's<a id="id63" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> define the neural network class. It has been known so far that neural networks organize neurons in layers, and every neural network has at least two layers, one for gathering the inputs and one for processing the outputs, and a variable number of hidden layers. Therefore our <code class="literal">NeuralNet</code> class will have these properties, in addition to other properties similar to the neuron and the <code class="literal">NeuralLayer</code> classes, such as <code class="literal">numberOfInputs</code>, <code class="literal">numberOfOutputs</code>, and so on:</p><div class="calibre2"><pre class="programlisting">public class NeuralNet {
    private InputLayer inputLayer;
    private ArrayList&lt;HiddenLayer&gt; hiddenLayer;
    private OutputLayer outputLayer;
    private int numberOfHiddenLayers;
    private int numberOfInputs;
    private int numberOfOutputs;
    private ArrayList&lt;Double&gt; input;
    private ArrayList&lt;Double&gt; output;
…
}</pre></div><p class="calibre11">The constructor of this class has more arguments than the previous classes:</p><div class="calibre2"><pre class="programlisting">public NeuralNet(int numberofinputs,int numberofoutputs,
            int [] numberofhiddenneurons,IActivationFunction[] hiddenAcFnc,
            IActivationFunction outputAcFnc)</pre></div><p class="calibre11">Provided that the number of hidden layers is variable, we should take into account that there may be many hidden layers or none, and in each of them there will be a variable number <a id="id64" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of hidden neurons. So the best way to deal with this variability is to represent the quantity of neurons in each hidden layer as a vector of integers (argument <code class="literal">numberofhiddenlayers</code>). Moreover, one needs to define the activation functions for each hidden layer, and for the output layer as well, to that goal serve the arguments <code class="literal">hiddenActivationFnc</code> and <code class="literal">outputAcFnc</code>.</p><p class="calibre11">To save space in this chapter we are not going to show the full implementation of this constructor, but we can show the example for the definition of layers and the links between them. First, the input layer is defined observing the number of inputs:</p><div class="calibre2"><pre class="programlisting">input=new ArrayList&lt;&gt;(numberofinputs);
inputLayer=new InputLayer(numberofinputs);</pre></div><p class="calibre11">A hidden layer will be defined depending on its position, if it is right after the input layer, the definition is as follows:</p><div class="calibre2"><pre class="programlisting">hiddenLayer.set(i,new HiddenLayer(numberofhiddenneurons[i], hiddenAcFnc[i],
           inputLayer.getNumberOfNeuronsInLayer()));
    inputLayer.setNextLayer(hiddenLayer.get(i));</pre></div><p class="calibre11">Or else it will get the reference of the previous hidden layer:</p><div class="calibre2"><pre class="programlisting">hiddenLayer.set(i, new HiddenLayer(numberofhiddenneurons[i],             hiddenAcFnc[i],hiddenLayer.get(i-1).getNumberOfNeuronsInLayer()));
hiddenLayer.get(i-1).setNextLayer(hiddenLayer.get(i));</pre></div><p class="calibre11">As for the output layer, the definition is very similar to the latter case, except for the <code class="literal">OutputLayer</code> class and the fact that there may be no hidden layers:</p><div class="calibre2"><pre class="programlisting">if(numberOfHiddenLayers&gt;0){
  outputLayer=new OutputLayer(numberofoutputs,outputAcFnc,
    hiddenLayer.get(numberOfHiddenLayers-1).getNumberOfNeuronsInLayer() );
    hiddenLayer.get(numberOfHiddenLayers-1).setNextLayer(outputLayer);
}else{
    outputLayer=new OutputLayer(numberofinputs, outputAcFnc, numberofoutputs);
    inputLayer.setNextLayer(outputLayer);
}</pre></div><p class="calibre11">The <code class="literal">calc()</code> method executes the forwarding flow of signals from the input to the output end:</p><div class="calibre2"><pre class="programlisting">public void calc(){
  inputLayer.setInputs(input);
  inputLayer.calc();
  for(int i=0;i&lt;numberOfHiddenLayers;i++){
    HiddenLayer hl = hiddenLayer.get(i);
    hl.setInputs(hl.getPreviousLayer().getOutputs());
    hl.calc();
  }
  outputLayer.setInputs(outputLayer.getPreviousLayer().getOutputs());
  outputLayer.calc();
  this.output=outputLayer.getOutputs();
}</pre></div><p class="calibre11">In appendix C, we<a id="id65" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> present the reader the full documentation of the classes along with their UML class and package diagrams that will surely help as a reference for this book.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Time to play!"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec16" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Time to play!</h1></div></div></div><p class="calibre11">Now let's apply<a id="id66" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> these classes and get some results. The following code has a <code class="literal">test</code> class, a <code class="literal">main</code> method with an object of the <code class="literal">NeuralNet</code> class called <code class="literal">nn</code>. We are going to define a simple neural network with two inputs, one output, and one hidden layer containing three neurons:</p><div class="calibre2"><pre class="programlisting">public class NeuralNetConsoleTest {
  public static void main(String[] args) {
    RandomNumberGenerator.seed=0;
       
    int numberOfInputs=2;
    int numberOfOutputs=1;
    int[] numberOfHiddenNeurons= { 3 };
    IActivationFunction[] hiddenAcFnc = { new Sigmoid(1.0) } ;
    Linear outputAcFnc = new Linear(1.0);
    System.out.println("Creating Neural Network...");
    NeuralNet nn = new NeuralNet(numberOfInputs,numberOfOutputs,
          numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);
    System.out.println("Neural Network created!");
    nn.print();
    …
}</pre></div><p class="calibre11">Still in this code, let's<a id="id67" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> feed to the neural network two sets of data, and let's see what output it is going to produce:</p><div class="calibre2"><pre class="programlisting">  double [] neuralInput = { 1.5 , 0.5 };
  double [] neuralOutput;
  System.out.println("Feeding the values ["+String.valueOf(neuralInput[0])+" ; "+
                String.valueOf(neuralInput[1])+"] to the neural network");
  nn.setInputs(neuralInput);
  nn.calc();
  neuralOutput=nn.getOutputs();
  
  neuralInput[0] = 1.0;
  neuralInput[1] = 2.1;
  ...
  nn.setInputs(neuralInput);
  nn.calc();
  neuralOutput=nn.getOutputs();</pre></div><p class="calibre11">This code gives the following output:</p><div class="mediaobject"><img src="Images/B05964_01_13.jpg" alt="Time to play!" class="calibre45"/></div><p class="calibre11">It's relevant to<a id="id68" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> remember that each time that the code runs, it generates new pseudo random weight values, unless you work with the same seed value. If you run the code exactly as provided here, the same values will appear in console:</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch01lvl1sec17" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've seen an introduction to the neural networks, what they are, what they are used for, and their basic concepts. We've also seen a very basic implementation of a neural network in the Java programming language, wherein we applied the theoretical neural network concepts in practice, by coding each of the neural network elements. It's important to understand the basic concepts before we move on to advanced concepts. The same applies to the code implemented with Java.</p><p class="calibre11">In the next chapter, we will delve into the learning process of a neural network and explore the different types of leaning with simple examples.</p></div></div>



  </body></html>