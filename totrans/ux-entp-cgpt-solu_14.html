<html><head></head><body>
		<div><h1 id="_idParaDest-229" class="chapter-number"><a id="_idTextAnchor236" class="pcalibre pcalibre1 calibre6"/>11</h1>
			<h1 id="_idParaDest-230" class="calibre5"><a id="_idTextAnchor237" class="pcalibre pcalibre1 calibre6"/>Process</h1>
			<p class="calibre3">Not all companies are built the same way. The development process is running full steam, and time and resources were given to do the design steps at the right time and scale. Perfect. It would be best if you had written this chapter. Enterprise-scale problems come with enterprise-sized issues. The one that makes my head spin the most is the time it takes to incorporate design solutions into a live environment. <em class="italic">Time is the enemy of good</em>. And with each evolution of technology, the time to market and the iterative cycle have to be faster. Today’s user is not what it once was.</p>
			<p class="calibre3">Provide the best solution as soon as possible or risk losing customers. Thus, even in an Agile world (let’s include Scrum, Lean, and another iterative modern approach to software development when we mention Agile), its goal is to deliver quality promptly. Design and the efforts that go into it can cause headaches for engineering teams wanting to move quickly. Remember our discussion on cheap, fast, and good, choose two? Fast is required not to alienate customers, and customers no longer accept poor quality from enterprise solutions, so it must be good. Hence, it might not be cheap. Well, <em class="italic">cheap</em> here means expending resources to ensure that it is good. The previous chapters talked about how to make it good, but we need to apply these methods, practices, guidelines, and heuristics are applied in a way conducive to bringing quality work to market.</p>
			<p class="calibre3">In this chapter, we’ll discuss two areas to include conversational design in a software development organization:</p>
			<ul class="calibre7">
				<li class="calibre8">Incorporating design thinking into development</li>
				<li class="calibre8">Designing a content improvement life cycle</li>
			</ul>
			<h1 id="_idParaDest-231" class="calibre5"><a id="_idTextAnchor238" class="pcalibre pcalibre1 calibre6"/>Incorporating design thinking into development</h1>
			<p class="calibre3">Most of this book is about using the<a id="_idIndexMarker821" class="pcalibre pcalibre1 calibre6"/> steps needed to solve problems, considering user needs, refining problems, and creating and testing solutions. This is the essence of <strong class="bold">design thinking</strong>. You were probably doing this even without knowing its name. This hands-on approach emphasizes user empathy through the research methods covered in the book. However, in practical <a id="_idIndexMarker822" class="pcalibre pcalibre1 calibre6"/>terms, a design thinker remembers the user goals at every process step. Because Agile aligns so well with the iterative nature of Generative AI, this chapter exposes tricks to make generative AI successful in an enterprise software development organization. This works for companies who deliver enterprise tools and those who use enterprise tools to work with a mass audience. It should be clear that both have compelling use cases and challenges with design thinking in an agile AI world.</p>
			<p class="calibre3">This will sound harsh. If an organization is not using Agile or a form of iterative development, successful Generative AI will be out of reach. We live in a new, fast-paced world that needs a robust <em class="italic">iterative</em> approach to support it. I strongly encourage its consideration. There is a wealth of resources, from the Scaled Agile Framework discussed early on to the Agile Alliance and many more. AI tools are becoming fundamental in many development processes. Indeed, they should be deployed to customers as well. Tools for prompt engineering and fine-tuning were covered, but a wealth of AI add-ons can support backlog management, write better issues, or enrich development processes. Over 50 tools are marked as <em class="italic">AI apps</em> in the Atlassian Marketplace for Jira and Confluence. I can’t speak to how good they are, but certainly, in the end, some will be timesavers and workflow boons.</p>
			<p class="calibre3">Website:<a href="https://marketplace.atlassian.com/categories/artificial-intelligence" class="pcalibre pcalibre1 calibre6"> AI marketplace at Atlassian</a> (<a href="https://marketplace.atlassian.com/categories/artificial-intelligence" class="pcalibre pcalibre1 calibre6">https://marketplace.atlassian.com/categories/artificial-intelligence</a>)</p>
			<p class="calibre3">These tools are outside the scope of this book, but they help teams move towards more efficient processes. The key is that this is an ever-changing world. Adapt and learn how our content, customers, and AI work together, and then be able to make improvements to production quickly.</p>
			<p class="calibre3">I’d be surprised if anyone needs to watch this video. If you are new to Agile, take one of the three-day <a id="_idIndexMarker823" class="pcalibre pcalibre1 calibre6"/>Agile scrum classes. They can be life-changing, and there is a lot to learn.</p>
			<p class="calibre3">Video: <a href="https://www.agilealliance.org/agile101/agile-basics/introduction-to-agile/" class="pcalibre pcalibre1 calibre6">Introduction to Agile</a> (<a href="https://www.agilealliance.org/agile101/agile-basics/introduction-to-agile/" class="pcalibre pcalibre1 calibre6">https://www.agilealliance.org/agile101/agile-basics/introduction-to-agile/</a>)</p>
			<p class="calibre3">The software industry is able to transition to an AI-centric approach. The results from the 17th Annual State of Agile survey (2024 based on 2023 results), with 788 respondents, are compelling. Because the industry is overwhelmingly Agile, and all our recommendations can apply to any method, the focus is to reference Agile and how to address concerns along the way. 71% of surveyed companies use Agile. Some teams use other modern<a id="_idIndexMarker824" class="pcalibre pcalibre1 calibre6"/> approaches, such as <strong class="bold">DevOps</strong> (<strong class="bold">Development and Operations</strong> working as one team), <strong class="bold">Iterative</strong>, <strong class="bold">Lean</strong> (which works only on the <a id="_idIndexMarker825" class="pcalibre pcalibre1 calibre6"/>most critical items, with no multitasking), or <strong class="bold">Spiral</strong> (a four-phase repetitive process). Even<a id="_idIndexMarker826" class="pcalibre pcalibre1 calibre6"/> the <strong class="bold">waterfall</strong> approach (a complete phase with no going back) is <a id="_idIndexMarker827" class="pcalibre pcalibre1 calibre6"/>found in 28% of respondents.</p>
			<p class="calibre3">There are fans of Agile, but there are critics as well. The biggest complaints concern the relentless pace and story points. Story points are for sizing work, but some argue for the well-understood measurement in hours. Every significant change in someone’s lifestyle will be met with complaints. <em class="italic">Change is hard. Change is inevitable.</em> The pace of change in the technology world is constantly speeding up. And we don’t need to argue about story points. Do research on your own. The chapter focuses on how design assists an LLM solution come to life. And there is value in aligning with Agile. The most significant value is the alignment around iterative design. It is mandatory with Generative AI to have an iterative life cycle. But doing it well has challenges. From the scoring discussion in <a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em>, it is clear that design-related efforts have a place in Agile, but designer efforts do not fit nicely into a single sprint. Research and studies typically take time to write and run a plan but don’t fit in a week’s sprint. It turns out that is ok. There are answers. So, let me summarize <a id="_idIndexMarker828" class="pcalibre pcalibre1 calibre6"/>some tips to make design a successful part of a Generative AI iterative <em class="italic">Agile</em> process. Even without a strict Agile method, the principles and coaching can be applied within most organizations. Even our first suggestion about finding a sponsor is broader than just Agile.</p>
			<h2 id="_idParaDest-232" class="calibre9"><a id="_idTextAnchor239" class="pcalibre pcalibre1 calibre6"/>Find a sponsor</h2>
			<p class="calibre3">The Agile approach is perfect for an iterative AI care and feeding approach. Because of this, a business’s issues with Agile become issues with AI adoption. Look at <em class="italic">Figure 11</em><em class="italic">.1</em> from Digital AI’s 17th annual survey of Agile.</p>
			<div><div><img src="img/B21964_11_1.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.1 – The 17th Annual State of Agile survey – business issues with Agile (openart.ai)</p>
			<p class="calibre3">The survey covers a lot of ground, but organizational support is critical for Agile success. By extension, a Generative AI <a id="_idIndexMarker829" class="pcalibre pcalibre1 calibre6"/>solution needs organizational support. The three top issues (and others) relate to managerial issues. This is the latest survey.</p>
			<p class="calibre3">Article: <a href="https://digital.ai/resource-center/analyst-reports/state-of-agile-report/" class="pcalibre pcalibre1 calibre6">State of Agile Report</a> (<a href="https://digital.ai/resource-center/analyst-reports/state-of-agile-report/" class="pcalibre pcalibre1 calibre6">https://digital.ai/resource-center/analyst-reports/state-of-agile-report/</a>)</p>
			<p class="calibre3">The iterative care and feeding approach is typically radically different than a quarterly release schedule. It is a cultural shift. Even in an Agile organization, many ship products on a multiple of the Agile schedule. It is common to see two-week sprints but quarterly releases. And combining a slow cadence with a lack of leadership participation and support creates a problem. Get a sponsor who recognizes that the speed of change is critical to AI success and can help create a process conducive to rapid changes. Leadership support is essential as an organization recognizes the effort it takes to do AI well. Of course, people are critical, but tools help, too.</p>
			<h2 id="_idParaDest-233" class="calibre9"><a id="_idTextAnchor240" class="pcalibre pcalibre1 calibre6"/>Find the right tools and integrate Generative AI</h2>
			<p class="calibre3">Unsurprisingly, it is a massive undertaking to keep up with this level of change, manage huge suites of test cases, and deal with an entire suite of knowledge. It doesn’t involve a single tool but dozens, but at least start with a tool to manage the process and change. As discussed in <a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em> using Agile tracking tools, incorporating scoring, and working from a WSJF backlog manages change. It would be challenging to find a large enterprise that doesn’t have a knowledge management tool. Still, adaption might be necessary to feed the RAG solution accurately – that is, recognize that knowledge and data will need to adapt to improve the<a id="_idIndexMarker830" class="pcalibre pcalibre1 calibre6"/> results generated by ChatGPT. It isn’t magic but hard work that fills in the gaps. This might mean articles tuned to support the LLM, reworking existing knowledge, adapting APIs to include more context with calls, and using intermediate tools to improve the flow of information to the LLM for processing. And let’s not forget to mention using smaller models to do bespoke tasks a few times, feeding those results to other models in a chain to improve overall performance. It reminds me of a quote: “<em class="italic">Stick to the plan, stick to the plan, stick to the plan.</em>” This means making last-minute changes to processes can sometimes have spurious consequences. Another similar concept is “<em class="italic">Be religious... </em><em class="italic">at first.</em>”</p>
			<h2 id="_idParaDest-234" class="calibre9"><a id="_idTextAnchor241" class="pcalibre pcalibre1 calibre6"/>Be religious… at first</h2>
			<p class="calibre3">There is a reason the Agile manifesto works for millions of people, even as they grumble about its shortcomings. Sure, 15% of respondents in the Agile survey were “<em class="italic">not at all satisfied</em>” with Agile, but given our understanding of what Agile, Scrum, and the shared concept, experience suggests most of this is about not knowing who or what to blame for failures. Consider these and the accompanying principles as they can be applied. Once you become an expert, <em class="italic">then</em> adapt. At an organization, I went into one Wednesday team meeting where the VP of a group of about 800 people dictated, “<em class="italic">On Monday, we will start doing Agile.</em>” Then they proceeded to choose which elements to do or not do (including deciding that self-organizing teams weren’t something they could do, not knowing the meaning of “self-organizing”). Don’t let gross incompetence derail quality goals. Try to understand the intent of the principles of Agile before discarding them. Let me quote a few of the 12 basic principles of Agile:</p>
			<ul class="calibre7">
				<li class="calibre8"><em class="italic">Build projects around motivated individuals. Give them the environment and support they need, and trust them to get the </em><em class="italic">job done.</em></li>
				<li class="calibre8"><em class="italic">The most efficient and effective method of conveying information to and within a development team is </em><em class="italic">face-to-face conversation.</em></li>
				<li class="calibre8"><em class="italic">Working software is the primary measure </em><em class="italic">of progress.</em></li>
				<li class="calibre8"><em class="italic">Agile processes promote </em><em class="italic">sustainable development.</em><p class="calibre3">Article: <a href="http://agilemanifesto.org/" class="pcalibre pcalibre1 calibre6">The Agile Manifesto</a> (<a href="http://agilemanifesto.org/" class="pcalibre pcalibre1 calibre6">http://agilemanifesto.org/</a>)</p></li>
			</ul>
			<p class="calibre3">Our processes support building stuff: that is how we learn. Even in a world of video conferencing, people operate better with face-to-face meetings. And with the rapid pace of AI changes, sustainability needs to be built into our life cycle to avoid burning people out. Organizations love<a id="_idIndexMarker831" class="pcalibre pcalibre1 calibre6"/> to reduce risks. Reduce risk by following the process religiously until there is confidence that the unknowns are known and change is required. Risks create unknowns, especially in solutions such as an LLM, where what it will say at every turn is not predictable.</p>
			<h2 id="_idParaDest-235" class="calibre9"><a id="_idTextAnchor242" class="pcalibre pcalibre1 calibre6"/>Avoid “unknown unknowns”</h2>
			<p class="calibre3">This is so good it is worth repeating:</p>
			<p class="author-quote">“Reports that say that something hasn’t happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say, we know there are some things we do not know. But there are also unknown unknowns – the ones we don’t know we don’t know.”</p>
			<p class="author-quote">– Donald Rumsfeld, US Secretary of Defense (February 12, 2002)</p>
			<p class="calibre3">Dig into understanding and learning. Research, uncover the root causes of issues, and learn <em class="italic">how to learn</em> in a Generative AI world. See why design is so well positioned to uncover issues; design methods are intended to solve these issues. And always try to evolve and improve.</p>
			<h2 id="_idParaDest-236" class="calibre9"><a id="_idTextAnchor243" class="pcalibre pcalibre1 calibre6"/>Always evolve and improve</h2>
			<p class="calibre3">Retrospectives are not to be missed. They allow a content team to learn and give feedback on engineering issues that cause concern. Evolve to use better metrics (value and effort, not bugs) to drive improvement (measure the right things). Some metrics discussed in <a href="B21964_10_split_000.xhtml#_idTextAnchor216" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring and Evaluation</em>, are confusing and still evolving, but we have learned to understand how metrics change and work with them as they grow. Design thinking is fundamentally about listening and<a id="_idIndexMarker832" class="pcalibre pcalibre1 calibre6"/> learning, so designers should be best positioned to help a team develop and improve. And improving includes knowing how to define what needs to be done. This means having requirements.</p>
			<h2 id="_idParaDest-237" class="calibre9"><a id="_idTextAnchor244" class="pcalibre pcalibre1 calibre6"/>Agile does not mean “no requirements”</h2>
			<p class="calibre3">Specify what <em class="italic">others</em> need; don’t over-specify what <em class="italic">you</em> need (inter-module documentation, yes!). Agile is right:</p>
			<p class="author-quote">“Working software over comprehensive documentation… we value the items on the left more.”</p>
			<p class="calibre3">This doesn’t mean you don’t create test cases; that is a requirement to take steps forward without going backward, but that might be all that is needed. Consider the goals and let data science and engineering work to meet those goals. If an analysis identifies that a system needs better context recall, define new realistic goals. No one works in isolation on these projects. Focus on what others need. It doesn’t preclude writing a test plan for user research. Do what is required, but if you know what to do, consider whether it needs to be documented. This brings us to discussing others and how to work with a team.</p>
			<h2 id="_idParaDest-238" class="calibre9"><a id="_idTextAnchor245" class="pcalibre pcalibre1 calibre6"/>Team composition and location matters</h2>
			<p class="calibre3">Face-to-face conversation is the most efficient and effective method of conveying information to and within a development team. This is a big challenge for remote-only companies or any company these days. In the State of Agile survey, 91% of respondents said their teams were fully remote. However, some successful companies recognize a partial solution – they have teams whose members work in similar time zones. Having writers in Romania, QA in China, and PMs and designers in California would be wrong. Work to create content teams that can work together in real time to help them stay aligned on the work they have in progress.</p>
			<h2 id="_idParaDest-239" class="calibre9"><a id="_idTextAnchor246" class="pcalibre pcalibre1 calibre6"/>Manage Work in Progress (WIP) and technical debt</h2>
			<p class="calibre3">There is so much work and <a id="_idIndexMarker833" class="pcalibre pcalibre1 calibre6"/>so little time. Even with Generative AI, only so much gets done. Focus resources on the most valuable items addressed in <a href="B21964_04.xhtml#_idTextAnchor085" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 4</em></a>, <em class="italic">Scoring Stories</em>. Break up work into smaller pieces and deliver that value sooner. <em class="italic">Figure 11</em><em class="italic">.2</em> explains this with an analogy of how homes are built.</p>
			<div><div><img src="img/B21964_11_2.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Do not juggle excessive work in progress (Photoshop AI)</p>
			<p class="calibre3">It is straightforward – most people would rather be 100% done on 80% of the work than be 80% complete on 100% of the work. Work that is done has value. A realtor can sell four homes and then build on the next lot instead of having five homes that still are a work in progress and can’t be sold. This is why builders have phases. They build a set of homes (for efficiency), sell them, and then build the next set of homes. This reduces risk and increases value, and customers get the benefits sooner. It’s the same with content changes. Make incremental progress. This incremental progress usually means more extensive work is broken down into manageable pieces. <em class="italic">Figure 11</em><em class="italic">.3</em> jokingly represents organizing to create manageable, well-understood sets of efforts to address in order.</p>
			<div><div><img src="img/B21964_11_3.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Break down large projects into manageable pieces (openart.ai)</p>
			<p class="calibre3">The prioritization tools <a id="_idIndexMarker834" class="pcalibre pcalibre1 calibre6"/>discussed and an Agile process (or any methodology with backlog grooming) help focus on priorities. Do work that is worth doing. This work should provide the most customer value.</p>
			<h2 id="_idParaDest-240" class="calibre9"><a id="_idTextAnchor247" class="pcalibre pcalibre1 calibre6"/>Focus on customer value</h2>
			<p class="calibre3">There was an entire chapter on scoring user stories. By now, giving customers value by prioritizing the most essential work should be well understood. The more stories resonate with customers, the better their experience will be, and the more they will say good things about it (think Net Promoter Score), increasing users and usage. Then, invest to create even more value. One way to focus on customer value is to ensure the design process is part of the overall development process.</p>
			<h2 id="_idParaDest-241" class="calibre9"><a id="_idTextAnchor248" class="pcalibre pcalibre1 calibre6"/>Incorporate the design process into the dev process</h2>
			<p class="calibre3">There are few enterprise companies with a design-first approach. Design-first means more than just doing the design first; everyone has to do some design <em class="italic">first</em>. It is more about leading with a design mentality to create customer success, not driving solutions because engineering thinks it is cool or marketing wants something flashy. It has to deliver a user need and drive value. Also, as<a id="_idIndexMarker835" class="pcalibre pcalibre1 calibre6"/> mentioned earlier, it should be <strong class="bold">functional, usable, necessary, and engaging</strong> (<strong class="bold">FUN-E</strong> or <em class="italic">funny</em>, which is a good mnemonic). However, most product design people live in reality, so flexibility is required. The products built with Generative AI are not mature enough to survive on a three-to-six-month release schedule. Look for places to go faster and improve incrementally. In addition, designers, writers, linguists, and product managers will need to work with engineering, QA, and data science because many frameworks and processes need to be implemented. But do it with a design mindset so that new processes are also <em class="italic">FUN-E</em>.</p>
			<p class="calibre3">When incorporating GenAI or ChatGPT into processes, consider a content sprint focused on<a id="_idIndexMarker836" class="pcalibre pcalibre1 calibre6"/> knowledge bases (documents and data) and prompt engineering improvements. This Agile life cycle can coincide with traditional <strong class="bold">development</strong> (<strong class="bold">dev</strong>) sprints. The<a id="_idIndexMarker837" class="pcalibre pcalibre1 calibre6"/> concept of a content sprint will be introduced. Here are some suggestions for key individuals with responsibilities around designing Generative AI solutions in an Agile organization:</p>
			<ul class="calibre7">
				<li class="calibre8"><strong class="bold">Designers, writers, </strong><strong class="bold">and linguists</strong>:<ul class="calibre19"><li class="calibre8">Agile practices address design. However, most design efforts are done before entering a <em class="italic">dev</em> sprint. For example, user research won’t happen inside a one-week development sprint. Agile, Scaled Agile, and many frameworks have accommodations for the work needed to enter a sprint. The design team should have <em class="italic">design</em> sprints and processes to create the results so the content team or the dev team can do its job during their sprints. The concept of a content sprint is explained in the next section.</li><li class="calibre8">Deliver the design when it is 90% ready before a team starts a dev sprint. Reserve content and prompt changes for the content sprint. Items that span content and development are placed in the development sprint backlog. This allows more technical changes to be thoroughly tested.</li><li class="calibre8">Use GenAI tools to build GenAI products. Like the tools discussed for classification, synthetic data, and fine-tuning, there is an ever-increasing number of GenAI tools that can make work more efficient. GenAI tools also help with general practices for professionals, editing, brainstorming, thinking through problems, word choices, translation issues, and understanding components and interaction design best practices. Even a generic foundational model like ChatGPT 4o should be considered your intern for getting tasks done better.</li></ul></li>
				<li class="calibre8"><strong class="bold">Designers</strong>:<ul class="calibre19"><li class="calibre8">Get more involved in prioritization (to focus end-to-end and ensure that customer-critical items are completed in priority order).</li><li class="calibre8">Learn to identify the differences between prompt issues, fine-tuning gaps, knowledge or RAG issues, integration data issues, or context issues.</li><li class="calibre8">Actively resolve in-sprint issues promptly (i.e., to be responsive and to allocate time to be in the sprint). Being responsive to the dev team encourages them to become <a id="_idIndexMarker838" class="pcalibre pcalibre1 calibre6"/>dependent on product people. This is important because designers (writers, PMs, and even linguists) are better positioned to judge these content issues.</li></ul></li>
				<li class="calibre8"><strong class="bold">Design owners</strong>:<ul class="calibre19"><li class="calibre8">When entering the dev sprint, freeze the UI design that is expected for that sprint. Post the document, move additional significant changes to the next version, and only deal with the remaining 10% of detailed design changes within the sprint. Give the dev team design goals that are not moving.</li><li class="calibre8">Content sprints have a schedule for changes; <em class="italic">when in doubt, throw it out</em>. Keep what is trusted to work, and move the rest to the next sprint. Content sprints are like working with live wires; don’t be shocked at the results.</li></ul></li>
				<li class="calibre8"><strong class="bold">Developers and </strong><strong class="bold">data scientists</strong>:<ul class="calibre19"><li class="calibre8">Don’t do it alone; this is a team sport.</li><li class="calibre8">Learn and listen to feedback; some changes have unintended consequences. As the team starts to get a feel for the content, the data, and the context size, prototype and experiment with other releases or different models, to improve quality. Rely on the content people, such as designers, PMs, and writers, who are better positioned to judge the appropriateness and significance of issues with language and conversational interaction.</li></ul></li>
				<li class="calibre8"><strong class="bold">Scrum master</strong>:<ul class="calibre19"><li class="calibre8">Invite the designer to participate in <em class="italic">all</em> sprint scrum activities with active UI work. It is better to have teams focused on only UI and content work.</li><li class="calibre8">Design-related tasks in dev sprints should be assigned daily to ensure a fast turnaround for UI review/feedback/in-sprint design changes. No one wants surprises during the review at the end of the sprint.</li></ul></li>
				<li class="calibre8"><strong class="bold">Product owners</strong>:<ul class="calibre19"><li class="calibre8">Have the requirements handed to the designer before starting work. Work with designers during design sprints to deliver the “end-to-end” story for later dev sprints. Some changes require testing and research and will not fit into the same sprint as the dev. Scaled Agile is okay with this.</li><li class="calibre8">Keep the runway clean and in sync with the expected design needs so that the design can be provided before the dev sprint starts. Even content sprint teams might need infrastructure or want to try new models, and this would fall back on the <a id="_idIndexMarker839" class="pcalibre pcalibre1 calibre6"/>development sprints to provide.</li><li class="calibre8">Scale and scope stories correctly. Break up epics into smaller epics, stories, and tasks. Adopting a new LLM, for example, will span multiple phases and sprints to test, fine-tune, and validate.</li><li class="calibre8">Get content experts involved sooner rather than later. Before entering the sprint, agree on the language issues, support for product slang, outdated terms for products, discontinued product names, and synonyms for product names. Build this dictionary of terms to go beyond official terms. Remember, customers will use the language they are comfortable with.</li></ul></li>
				<li class="calibre8"><strong class="bold">Scrum masters/design owners</strong>: Any designer can be on up to three teams, typically two. Designers should “mostly” work ahead of dev sprints. In the dev track sprints, effort should be limited since a story should enter a sprint at least 90% ready. Content designers won’t have that luxury, as we are about to explain. They might have a 10% to 20% backlog from the week before, but this week’s work will be figured out <em class="italic">during</em> the sprint, and almost all the work will be done within the content update sprint.</li>
			</ul>
			<p class="calibre3">Now, it’s time to explain the a content improvement life cycle and a content sprint.</p>
			<h1 id="_idParaDest-242" class="calibre5"><a id="_idTextAnchor249" class="pcalibre pcalibre1 calibre6"/>Designing a content improvement life cycle</h1>
			<p class="calibre3"><a href="B21964_02_split_000.xhtml#_idTextAnchor031" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 2</em></a>, <em class="italic">Conducting Effective User Research</em>, discussed extensively<a id="_idIndexMarker840" class="pcalibre pcalibre1 calibre6"/> monitoring log files to improve the care and feeding life cycle. I can’t emphasize strongly enough how important it is to understand the metrics associated with the solution’s performance and adapt the solution to improve it. Although the data might differ for chat solutions versus recommender UIs, the process is the same for both. For behind-the-scenes work, figure out the feedback needed so that a continuous improvement process can be built.</p>
			<p class="calibre3">Let’s start with a vision of the content design team’s week once the product is in production. <em class="italic">Figure 11</em><em class="italic">.4</em> shows what a production content improvement team might do. Call the team a <em class="italic">production content team</em> or a <em class="italic">content/prompt sprint team</em>. Find a name that works so it is clear this is a slightly different beast from the more robust and complex <em class="italic">dev</em> sprint team.</p>
			<div><div><img src="img/B21964_11_4.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.4 – A weekly cadence for a content team’s continuous improvement lifecycle</p>
			<p class="calibre3">This life cycle may not be weekly early in the development phase; it could be a two- or three-week cadence. A week is excellent for an immature production system. It takes time to achieve this cadence. If an organization uses a two-week Agile development life cycle, start by matching that and improve the cadence over time. Alternatively, consider returning to a two-week approach when the product is more mature and changes slowly. It is not written in stone that this process needs to match the development cadence. There are some advantages, but it is worth considering the tradeoffs. This might be<a id="_idIndexMarker841" class="pcalibre pcalibre1 calibre6"/> the place to break that rule of thumb.</p>
			<p class="calibre3">Let’s now describe in detail the typical work week for a conversational analysis team. This can also work for recommender solutions, even without the same inputs.</p>
			<h2 id="_idParaDest-243" class="calibre9"><a id="_idTextAnchor250" class="pcalibre pcalibre1 calibre6"/>Inputs for conversational AIs</h2>
			<p class="calibre3">Logs are the primary input source for analysis but can be supplemented by surveys, bugs, or feedback. Since the logs represent what did happen, I would always check the logs against any anecdotal<a id="_idIndexMarker842" class="pcalibre pcalibre1 calibre6"/> feedback. A friend told me the other day that the most significant missing feature from Google Sheets was the ability to drag and drop rows. And I said, “You can drag and drop columns, so why can’t you drag and drop rows?” We went to Google Sheets, and drag and drop worked fine for rows, even on the mobile version. Unsurprisingly, this is an affordance issue, as discussed in the heuristic evaluation discussion in <a href="B21964_09_split_000.xhtml#_idTextAnchor190" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 9</em></a>, <em class="italic">Guidelines and Heuristics</em>. It is hard to know what can or can’t be done when there is no clear visual indicator of its support. Always check what people report. It is easy to be annoyed by something and only be confused by its real cause. Root-cause analysis can uncover the cause. Be willing to dig into the valid reason for a problem. Our user-centered design<a id="_idIndexMarker843" class="pcalibre pcalibre1 calibre6"/> methods are based on the concepts of <strong class="bold">root-cause analysis</strong>. This book follows this approach (define the problem, gather data, identify possible causes, develop and deploy solutions, and then monitor and verify). You can apply this to any AI solution, including recommender UIs.</p>
			<h2 id="_idParaDest-244" class="calibre9"><a id="_idTextAnchor251" class="pcalibre pcalibre1 calibre6"/>Inputs for recommender UIs</h2>
			<p class="calibre3">Recommender UIs don’t have log data like our conversational cousins. Earlier chapters covered feedback methods, but downstream metrics can proxy for usage data. Access and monitor this data to judge value and quality. For example, suppose a recommendation engine suggests <a id="_idIndexMarker844" class="pcalibre pcalibre1 calibre6"/>call follow-ups, email announcements, or product discounting. Analytics from the phone system, outbound email logs, or sales discount tools can be correlated with recommendations.</p>
			<p class="calibre3">In places where direct correlations cannot be created, secondary methods, such as user feedback, can be used to determine a user’s perception and gather written or verbal commentary. At least there is some data. Backend AIs are less lucky.</p>
			<h2 id="_idParaDest-245" class="calibre9"><a id="_idTextAnchor252" class="pcalibre pcalibre1 calibre6"/>Inputs for backend AIs</h2>
			<p class="calibre3">Since there is no UI, consider this a more challenging task to gather feedback. There are two suggestions to consider. The first would be to build feedback mechanisms pointing to issues in the results. The customer <a id="_idIndexMarker845" class="pcalibre pcalibre1 calibre6"/>will need to find out where the problem is. They will see something they don’t like, feel it is a bit off, or identify it as wrong. Let them do that, then determine the root cause. However, it is possible to locate the issue if enough data is gathered with the feedback (such as their usage history, tag-specific datasets, or screenshots they would see).</p>
			<p class="calibre3">The second relates to working out an AI monitoring system. Using ground truth examples, a secondary AI (not the same engine used for the solution) can check the work of the primary system. It might find results that don’t match what is expected or appear more than an order of magnitude outside of what is typical. This feeds into a continuous improvement system that starts on the first day of the week. The same foundational model could be used if fine-tuned for specific criteria, such as the checkpoint testing cycle in <em class="italic">Figure 11</em><em class="italic">.5</em>.</p>
			<p class="calibre3"> </p>
			<div><div><img src="img/B21964_11_5.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.5 – A checkpoint system for validating results to share</p>
			<p class="calibre3">This example allows us to fine-tune checkpoint models to pass judgments for specific criteria. It supports direct user input processes, or for a back-end solution, it would be a prompt generated on the customer’s behalf.</p>
			<p class="calibre3">Did the answer pass an ethical test (<strong class="bold">Judgment 1</strong>)? Do the facts match the product discussed (<strong class="bold">Judgment 2</strong>)? And in <strong class="bold">Judgement 3</strong>, does that answer fall within the company’s capabilities? If the results are promising, pass it on to the customer. If the answer is not good, gather more context or have them ask questions differently. In any case, the results of each of these models support the<a id="_idIndexMarker846" class="pcalibre pcalibre1 calibre6"/> primary model. Each of those failures creates an opportunity to learn. The output from models is part of what goes into the process for improvement each week. The idea is to work toward doing all of this on a weekly cadence. Let’s look at the work needed on the first day of the process.</p>
			<h2 id="_idParaDest-246" class="calibre9"><a id="_idTextAnchor253" class="pcalibre pcalibre1 calibre6"/>Monitoring Monday</h2>
			<p class="calibre3"><a href="B21964_02_split_000.xhtml#_idTextAnchor031" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 2</em></a><em class="italic">, Conducting Effective User Research</em>, covers log analysis. This is a valuable way of learning what is happening with customers. Manual tools were <a id="_idIndexMarker847" class="pcalibre pcalibre1 calibre6"/>provided to start, and methods were shared to prioritize the outcomes. Focus on the analysis on Monday or whatever day of the week is the start.</p>
			<p class="callout-heading">Is Monday the first day of the week?</p>
			<p class="callout">Maybe your country starts on Sunday. However, this won’t give you the alliteration of <em class="italic">Monitoring Monday</em>. This alliteration only works in some languages; Israel, Japan, and Saudi Arabia can start with <em class="italic">Surveying Sunday</em>. Come up with names that work in your language. I used the translation spreadsheet from the earlier chapter to translate Monitoring Mondays. Only one of the 30 languages happens even to be close to alliteration. If you are a word geek, it was Dutch, with the translation of “Maandag monitoren.” Consider making it fun and not feel like a structured march that people will oppose (a typical Agile complaint).</p>
			<p class="calibre3">There is only so much a team can do to review logs, also look at customer surveys, feedback messages, bug reports, failures from intermediate steps of multi-step validation models, and sales or service feedback. Filter for the issues to triage them over the next few days. Use this time to work on projects that carried over from last week.</p>
			<h2 id="_idParaDest-247" class="calibre9"><a id="_idTextAnchor254" class="pcalibre pcalibre1 calibre6"/>Analysis Tuesday (and Wednesday’s workup)</h2>
			<p class="calibre3">Understand, classify, and chunk results. For extensive collections of common issues, don’t write <a id="_idIndexMarker848" class="pcalibre pcalibre1 calibre6"/>20 stories; write one. This is where classification methods and third-party tools come into play, as discussed in <a href="B21964_02_split_000.xhtml#_idTextAnchor031" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 2</em></a>, <em class="italic">Conducting Effective User Research,</em> and <a href="B21964_03.xhtml#_idTextAnchor058" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 3</em></a>, <em class="italic">Identifying Optimal Use Cases for ChatGPT</em>. Work with the dev team to understand and segment issues into those that can be fixed this week and those that need devs’ help for improvements. Of course, investing in tools to make improvements is better without investing in development resources at every turn. This is the adage – <em class="italic">if you give someone a fish, you feed them for a day. If you teach them to fish, you feed them for a lifetime</em>. Dev can teach us to fish by providing tools that allow us to make improvements. Solutions like editing knowledge, prompt changes, fine-tuning examples, adding test cases, or other content changes can be considered through analysis. The analysis will also create work for researchers<a id="_idIndexMarker849" class="pcalibre pcalibre1 calibre6"/> and ample items for dev teams to address around fine-tuning, new models, better reflection or model-to-model integrations, and API work. All this is done to decide what the content team can deliver in the next few days and plan for what the dev team can provide in future sprints.</p>
			<h2 id="_idParaDest-248" class="calibre9"><a id="_idTextAnchor255" class="pcalibre pcalibre1 calibre6"/>Treatment Thursday and fault-finding Friday</h2>
			<p class="calibre3">I would love to have these on <a id="_idIndexMarker850" class="pcalibre pcalibre1 calibre6"/>different days, but figuring out solutions and testing to identify faults is an iterative process. Some readers will be laughing at the attempt to do all of this in a week. I get it; it is the goal, work towards it. Fixing and tests should be done over two days. Conflicts will be expected. Unsurprisingly, asking a model to do one thing can break something else, or two new solutions can conflict. And prompts can then get unwieldy. All hands are on deck<a id="_idIndexMarker851" class="pcalibre pcalibre1 calibre6"/> for solutions: writing or editing knowledge, updating prompts, improving fine-tuning, or incorporating a new API (and as discussed, this is likely to come from a previous sprint, as APIs don’t magically appear). Automation is needed to quickly rerun test cases and analyze results and metrics for this to work.</p>
			<p class="calibre3">Only some things can be fixed in days or hours, but some improvements are well within reason. Rapid improvements can happen for <em class="italic">content</em>:</p>
			<ul class="calibre7">
				<li class="calibre8">Adjusting instructions and prompts</li>
				<li class="calibre8">Training to improve grammar or to include company-specific language and initialisms</li>
				<li class="calibre8">Editing knowledge</li>
				<li class="calibre8">Creating knowledge (it’s hard to develop technical documentation quickly, but maybe FAQs are within reason)</li>
				<li class="calibre8">Adding learning examples</li>
				<li class="calibre8">Identifying context elements to include in instructions</li>
				<li class="calibre8">Fine-tuning</li>
				<li class="calibre8">Adding test cases</li>
			</ul>
			<p class="calibre3">This can be a challenge. The content team can’t make one change, test it and then make more changes when a testing run can take minutes or hours. It is also hard to debug issues with a batch of new changes. It is a trade-off. Sometimes, failed tests can prevent releasing a solution, while some solutions fail to materialize and require more or different work. Work can carry <a id="_idIndexMarker852" class="pcalibre pcalibre1 calibre6"/>over to next week or need to be referred to the dev team. Let’s explore carry-over work in more detail.</p>
			<h2 id="_idParaDest-249" class="calibre9"><a id="_idTextAnchor256" class="pcalibre pcalibre1 calibre6"/>What doesn’t fit into a week is still important</h2>
			<p class="author-quote">“If I had one hour to save the world, I would spend 55 minutes defining the problem and only five minutes finding the solution.”</p>
			<p class="author-quote">–Albert Einstein (not really)</p>
			<p class="calibre3">The “famous” quote from Albert Einstein isn’t from him (Google knows!), but the sentiment is still valid. Not all solutions are easy. However, understanding the solution is generally manageable if time is spent understanding the problems.</p>
			<p class="calibre3">When identifying problems that clearly won’t fit into a week or a sprint, evaluating their value is still important, as is getting a cost estimate and moving it into a backlog based on the Weighted Shortest Job First approach. By doing this on an ongoing basis, there will be<a id="_idIndexMarker853" class="pcalibre pcalibre1 calibre6"/> stories from previous weeks (or months) now being integrated into this week’s changes. Thus, these changes will impact the solution as tests are built. In true Agile fashion, if something is too big and disruptive to the process, it should not be picked up alongside other changes; use the traditional development and testing lifecycle for the more significant, more technically complex stories.</p>
			<p class="calibre3">For example, new test cases will be needed as a new API or integration becomes available. Because these former out-of-domain cases become in-domain solutions, this can cause new conflicts. For example, in an internal business solution that supports sales, inventory, and team staffing reports, new APIs that support marketing reports need to work with existing requests for other <em class="italic">reports</em>. Add training data to assist the system in differentiating these new marketing reports, rerun fine-tuning, update prompts, and refine FAQs to account for this feature. The previous out-of-domain test cases must move to valid in-domain test cases.</p>
			<p class="calibre3"><em class="italic">Figure 11</em><em class="italic">.6</em> shows adapting content sprint cadence based on the product’s stage. This is offered to <a id="_idIndexMarker854" class="pcalibre pcalibre1 calibre6"/>encourage thinking about the level of change at each phase of product development and the value of that change.</p>
			<div><div><img src="img/B21964_11_6.jpg" alt="" role="presentation" class="calibre4"/>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Cadence can change based on the stage of the product</p>
			<p class="calibre3">This suggests that the content sprint schedule adapts over time. This is a radical idea for Agile, but it might provide some value to consider. Since a team is self-organizing, they can make this schedule decision. Most enterprises do not believe a team can make this call, as they think self-organizing is a no-no. But there should be some flexibility. This isn’t suggesting changing often, just as the process matures. That is the Agile way.</p>
			<p class="calibre3">Notice a generic <em class="italic">quality</em> metric for the vertical axis. This could be any measure or all measures. Any specific metric can see dips when building and testing. In <a href="B21964_10_split_000.xhtml#_idTextAnchor216" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 10</em></a>, <em class="italic">Monitoring and Evaluation</em>, not all of OpenAI’s efforts to improve their enterprise use case were successful. Remember that even when a product is in production, efforts in dev will eventually come online and give opportunities for more significant improvements in quality than just what can be done from incremental improvements from a content perspective. Only so much can be done by editing prompts, improving tuning, adding new APIs, and working with our knowledge integration.</p>
			<p class="calibre3">Let’s break down each phase and the approach:</p>
			<ul class="calibre7">
				<li class="calibre8"><strong class="bold">Early development</strong>: There are no checks and balances at this stage; try to test changes regularly. Testing<a id="_idIndexMarker855" class="pcalibre pcalibre1 calibre6"/> frameworks and metrics will still be in development and need to be available to ensure progress (try not to take one step forward and two steps back). Manual testing will be typical. Significant changes will come from major investments, and it is okay if the sprint cycle for content doesn’t reflect development. Writing content, editing and reviewing updates, and bringing testing online takes time.</li>
				<li class="calibre8"><strong class="bold">Later development</strong>: As releases approach, changes will likely tighten and be on the same schedule as dev Agile teams. Two weeks are typical; some teams will be already on a one-week cadence. However, be aware that the Agile sprint length <a id="_idIndexMarker856" class="pcalibre pcalibre1 calibre6"/>doesn’t always equate to a release schedule, which is okay. Only when some of these significant investments pay off will there be major jumps in quality. <strong class="bold">Minimum Viable Product</strong> (<strong class="bold">MVP</strong>) expectations are<a id="_idIndexMarker857" class="pcalibre pcalibre1 calibre6"/> shown on the preceding chart at <strong class="bold">80%</strong>. An astute reader will notice we talked about 97% in other graphs. 97% is the <em class="italic">goal</em>; 80% is a release minimum. It would be prudent to show some repeated ability to maintain that level before going live. Some changes will hurt some quality measures. Significant changes in the product scope could reduce the quality of each interaction but increase the number of interactions that can be handled; learn to live with these bumps in the road.</li>
				<li class="calibre8"><strong class="bold">Early production</strong>: The goal is a complete update cycle in one week. By categorizing changes, there can be<a id="_idIndexMarker858" class="pcalibre pcalibre1 calibre6"/> opportunities to make faster changes with some adjustments, while more significant changes, such as access to a new API, take longer. Customers need to feel improvements. I use ChatGPT all day and see improvements monthly. One week is possible for some changes. Do what is best, but always with the focus on providing value to customers sooner.</li>
				<li class="calibre8"><strong class="bold">Mature production</strong>: As the solution matures, less value will be gained by incremental improvements. It might take the same effort to raise quality 10% from 80% to 90% as it does to go up 1% from 95% to 96%. This diminishing return is an indication to reconsider how time is spent. The cadence of changes can decrease because the <a id="_idIndexMarker859" class="pcalibre pcalibre1 calibre6"/>value of those changes is small. It may be time to reduce the story points the team can do in a content team and allocate those resources to new opportunities or the next-generation solution. Of course, this might be years, so cross that bridge when you come to it.</li>
			</ul>
			<p class="calibre3">No one is perfect, and we don’t need perfection. The cost of aiming for perfection can drive you out of business. Recall the failed Ford Pinto example from <a href="B21964_06_split_000.xhtml#_idTextAnchor134" class="pcalibre pcalibre1 calibre6"><em class="italic">Chapter 6</em></a><em class="italic">, Gathering Data—Content is King</em>. It would be best to make choices; don’t make bad choices. Use guardrails for high-value, high-risk answers. Value ethics, reduction in bias, and accuracy over feature expansion.</p>
			<p class="author-quote">“Perfection is not attainable. But if we chase perfection, we can catch excellence.”</p>
			<p class="author-quote">– Vince Lombardi</p>
			<p class="calibre3">As new models emerge, quality changes significantly, so consider how to re-adapt to a faster cadence. It is not uncommon to see quality hiccups as a function of new models, as with ChatGPT releases in the last two years, it is primarily a solid upward trajectory.</p>
			<p class="calibre3">The takeaway is to implement practices to improve quality quickly, keeping customers on the happy path. It is a labor of love to monitor regularly, gather feedback, learn about issues, classify to bundle improvements, research issues, test enhancements, and to start the process over again. Here are a few tips to overcome typical challenges:</p>
			<ul class="calibre7">
				<li class="calibre8">Monitoring log files <a id="_idIndexMarker860" class="pcalibre pcalibre1 calibre6"/>can be monotonous, so time-box the work. Reviewing 1,000 rows of logs is time-consuming and tedious. No one should do this every day, so do it for only a day. Alternatively, assign review and summarization to an LLM.</li>
				<li class="calibre8">Over time, monitoring will get faster. It is much faster (10x) to review 1,000 log files with 50 issues than with 200 issues since a human can quickly review conversations without issues but needs time to understand a single problem. ChatGPT models can also do some of this work, such as seeing what is available for automation, but do some monitoring manually to understand customers.</li>
				<li class="calibre8">Fixing only some things in one content iteration or sprint is okay. Some items are not fixed by prompt engineering, editing knowledge, or creating new content. Sometimes, new APIs are needed; in some organizations, they can take a long time, if ever. Classifying<a id="_idIndexMarker861" class="pcalibre pcalibre1 calibre6"/> issues helps because if multiple instances of the same problem appear, it is possible to advocate to address them. Put in placeholders to acknowledge a customer’s needs. A placeholder may include a link to a third-party resource or an explanation if nothing else is available. Good AI classifiers are available; a model can be created to classify issues.</li>
				<li class="calibre8">Enlist resources. Designers, analysts, content authors, product managers, engineers, quality assurance, and data scientists can understand and diagnose issues. Use these resources collaboratively to learn how to address a situation best. There could be more than one approach to something. Communicate, collaborate, and conclude based on the information gathered. Also, there can be more than one solution. A short-term solution with prompt engineering might be better addressed by improving the knowledge base in the long term. Implement one solution while waiting for the latter.</li>
				<li class="calibre8">Release management won’t want to make changes quickly. This is a tough one. Work within a system or find advocates to change it. It would be best to have a sponsor. There are different kinds of changes; work with a sponsor on the types of changes release management would approve. Build trust and expand this list over time. Give examples. If an online news organization had an article that said, “<em class="italic">Each new plane for the military will cost 200 million dollars,</em>” but it was 20 million dollars, no one would expect that <em class="italic">content</em> change to take a week to fix. And much of what we are working with is content. Consider classifying changes so that some refinements can be done weekly while others require more effort, testing, and time. All should be run through automated and manual testing as part of the process, so maybe that is enough.</li>
				<li class="calibre8">Garbage in, garbage<a id="_idIndexMarker862" class="pcalibre pcalibre1 calibre6"/> out. Don’t let lousy knowledge throw off the RAG model. Don’t feed the model only with generative data; use that sparingly if it can be helped. Inject new, high-value content and look for opportunities for improvement. No one will suggest editing or improving all articles every week; pick the right battles. Let data lead the charge. The articles with the most hits take priority. Use analytics to tailor editorial investment. Recognize and test for interactions between similar documents for different products. Look for places where databases have good contextual data that can add value to prompts, giving customers more customized and accurate experiences.</li>
				<li class="calibre8">Invest time in the process of handling supervised fine-tuning. With technical material, add well-known samples to the training. When inspecting logs, recognize that some human variety in interaction should be better understood. Consider whether FAQs already serve this purpose. Remember how prompts should look for these examples. A wealth of fine-tuning datasets are excellent examples of scope and scale, even if generic data is not of value to enterprise use cases. Here is one good example, with 200,000 robust prompts.<p class="calibre3">Article: <a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k" class="pcalibre pcalibre1 calibre6">Ultrachat 200,000-row data set</a> (<a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k" class="pcalibre pcalibre1 calibre6">https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</a>)</p><p class="calibre3">Build custom collections of tuned material, test cases, and templates to provide high-quality RAG results. The Generative AI revolution is not without cost. Over time, model costs will be less significant than overall life cycle costs.</p></li>
			</ul>
			<p class="calibre3">Now, let’s conclude with some key takeaways from the chapter.</p>
			<h1 id="_idParaDest-250" class="calibre5"><a id="_idTextAnchor257" class="pcalibre pcalibre1 calibre6"/>Conclusion</h1>
			<p class="author-quote">“Without continual growth and progress, such words as improvement, achievement, and success have no meaning.”</p>
			<p class="author-quote">– Benjamin Franklin</p>
			<p class="calibre3">When it comes to conversational solutions, no truer words were spoken. Software changes decades ago were measured in years and then, months or quarters, and now, weeks and days. This is expected, needed, and sometimes valuable. With processes to support continuous improvement and even delivery, it is a win-win. Tools and methods will improve quickly, allowing more time to tackle new projects, while customers will benefit from better solutions sooner.</p>
			<p class="calibre3">Up to this point, we have explored the entire life cycle of applying UX design thinking and processes to create exceptional enterprise ChatGPT solutions. Sometimes, we went too deep and left you with some homework. The technology is moving so fast; the hope is that 96% of the concepts discussed in this book will apply to any conversational AI platform or collection of tools. Models improve, and the tools become more robust. The methods and practices should still work until we to the mythical general-purpose artificial intelligence (GPAI), strong AI, or artificial general intelligence (AGI). This book will be relevant until an LLM doesn’t need a significant investment in creating and tuning, with a healthy dose of care and feeding. As such, we will wrap up our journey in the next chapter to send you on your way to conquer the next AI project.</p>
			<h1 id="_idParaDest-251" class="calibre5"><a id="_idTextAnchor258" class="pcalibre pcalibre1 calibre6"/>References</h1>
			<table id="table001-10" class="no-table-style">
				<colgroup class="calibre15">
					<col class="calibre16"/>
					<col class="calibre16"/>
				</colgroup>
				<tbody class="calibre17">
					<tr class="no-table-style1">
						<td class="no-table-style2">
							<div><div><img src="img/Image98994.jpg" alt="" role="presentation" class="calibre4"/>
								</div>
							</div>
						</td>
						<td class="no-table-style2">
							<p class="calibre3">The links, book recommendations, and GitHub files in this chapter are posted on the reference page.</p>
							<p class="calibre3">Web page: <a href="https://uxdforai.com/references#C11" class="pcalibre pcalibre1 calibre6">Chapter 11 References</a> (<a href="https://uxdforai.com/references#C11" class="pcalibre pcalibre1 calibre6">https://uxdforai.com/references#C11</a>)</p>
						</td>
					</tr>
				</tbody>
			</table>
		</div>
	</body></html>