- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multimodal Modular RAG for Drone Technology
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take generative AI to the next level with modular RAG in this chapter.
    We will build a system that uses different components or modules to handle different
    types of data and tasks. For example, one module processes textual information
    using LLMs, as we have done until the last chapter, while another module manages
    image data, identifying and labeling objects within images. Imagine using this
    technology in drones, which have become crucial across various industries, offering
    enhanced capabilities for aerial photography, efficient agricultural monitoring,
    and effective search and rescue operations. They even use advanced computer vision
    technology and algorithms to analyze images and identify objects like pedestrians,
    cars, trucks, and more. We can then activate an LLM agent to retrieve, augment,
    and respond to a user’s question.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will build a multimodal modular RAG program to generate
    responses to queries about drone technology using text and image data from multiple
    sources. We will first define the main aspects of modular RAG, multimodal data,
    multisource retrieval, modular generation, and augmented output. We will then
    build a multimodal modular RAG-driven generative AI system in Python applied to
    drone technology with LlamaIndex, Deep Lake, and OpenAI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Our system will use two datasets: the first one containing textual information
    about drones that we built in the previous chapter and the second one containing
    drone images and labels from Activeloop. We will use Deep Lake to work with multimodal
    data, LlamaIndex for indexing and retrieval, and generative queries with OpenAI
    LLMs. We will add multimodal augmented outputs with text and images. Finally,
    we will build performance metrics for the text responses and introduce an image
    recognition metric with GPT-4o, OpenAI’s powerful **Multimodal LLM** (**MMLLM**).
    By the end of the chapter, you will know how to build a multimodal modular RAG
    workflow leveraging innovative multimodal and multisource functionalities.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal modular RAG
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multisource retrieval
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI LLM-guided multimodal multisource retrieval
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Lake multimodal datasets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image metadata-based retrieval
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmented multimodal output
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining multimodal modular RAG.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What is multimodal modular RAG?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multimodal data combines different forms of information, such as text, images,
    audio, and video, to enrich data analysis and interpretation. Meanwhile, a system
    is a modular RAG system when it utilizes distinct modules for handling different
    data types and tasks. Each module is specialized; for example, one module will
    focus on text and another on images, demonstrating a sophisticated integration
    capability that enhances response generation with retrieved multimodal data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The program in this chapter will also be multisource through the two datasets
    we will use. We will use the LLM dataset on the drone technology built in the
    previous chapter. We will also use the Deep Lake multimodal VisDrone dataset,
    which contains thousands of labeled images captured by drones.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: We have selected drones for our example since drones have become crucial across
    various industries, offering enhanced capabilities for aerial photography, efficient
    agricultural monitoring, and effective search and rescue operations. They also
    facilitate wildlife tracking, streamline commercial deliveries, and enable safer
    infrastructure inspections. Additionally, drones support environmental research,
    traffic management, and firefighting. They can enhance surveillance for law enforcement,
    revolutionizing multiple fields by improving accessibility, safety, and cost-efficiency.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.1* contains the workflow we will implement in this chapter. It is
    based on the generative RAG ecosystem illustrated in *Figure 1.3* from *Chapter
    1*, *Why Retrieval-Augmented Generation?*. We added embedding and indexing functionality
    in the previous chapters, but this chapter will focus on retrieval and generation.
    The system we will build blurs the lines between retrieval and generation since
    the generator is intensively used for retrieving (seamless scoring and ranking)
    as well as generating in the chapter’s notebook.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a multimodal modular rag system  Description automatically generated](img/B31169_04_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: A multimodal modular RAG system'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aims to build an educational modular RAG question-answering system
    focused on drone technology. You can rely on the functionality implemented in
    the notebooks of the preceding chapters, such as Deep Lake for vectors in *Chapter
    2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*, and indices with
    LlamaIndex in *Chapter 3*, *Building* *Index-based RAG with LlamaIndex, Deep Lake,
    and OpenAI*. If necessary, take your time to go back to the previous chapters
    and have a look.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the multimodal, multisource, modular RAG ecosystem in this
    chapter, represented in *Figure 4.1*. We will use the titles and subsections in
    this chapter represented in italics. Also, each phase is preceded by its location
    in *Figure 4.1*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**(D4)** *Loading the LLM dataset* created in *Chapter 3*, which contains textual
    data on drones.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(D4)** *Initializing the LLM query engine* with a LlamaIndex vector store
    index using `VectorStoreIndex` and setting the created index for the query engine,
    which overlaps with **(G4)** as both a retriever and a generator with the OpenAI
    GPT model.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(G1)** Defining the *user input for multimodal modular RAG* for both the
    LLM query engine (for the textual dataset) and the multimodal query engine (for
    the `VisDrone` dataset).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the textual dataset has been loaded, the query engine has been created,
    and the user input has been defined as a baseline query for the textual dataset
    and the multimodal dataset, the process continues by generating a response for
    the textual dataset created in *Chapter 2*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: While *querying the textual dataset*, **(G1)**, **(G2)**, and **(G4)** overlap
    in the same seamless LlamaIndex process that retrieves data and generates content.
    The response is saved as `llm_response` for the duration of the session.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, the multimodal `VisDrone` dataset will be loaded into memory and queried:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**(D4)** The multimodal process begins by *loading and visualizing the multimodal
    dataset*. The program then continues by *navigating the multimodal dataset structure*,
    *selecting an image*, and *adding bounding boxes*.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The same process as for the textual dataset is then applied to the `VisDrone`
    multimodal dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**(D4)** *Building a multimodal query engine* with LlamaIndex by creating a
    vector store index based on `VisDrone` data using `VectorStoreIndex` and setting
    the created index for the query engine, which overlaps with **(G4)** as both a
    retriever and a generator with OpenAI GPT.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(G1)** The user input for the multimodal search engine is the same as the
    *user input for multimodal modular RAG* since it is used for both the LLM query
    engine (for the textual dataset) and the multimodal query engine (for the `VisDrone`
    dataset).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The multimodal `VisDrone` dataset will now be loaded and indexed, and the query
    engine is ready. The purpose of **(G1)** user input is for the LlamaIndex query
    engine to retrieve relevant documents from VisDrone using an LLM—in this case,
    an OpenAI model. Then, the retrieval functions will trace the response back to
    its source in the multimodal dataset to find the image of the source nodes. We
    are, in fact, using the query engine to reach an image through its textual response:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**(G1)**, **(G2)**, and **(G4)** overlap in a seamless LlamaIndex query when
    running a query on the `VisDrone` multimodal dataset.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing the response **(G4)** to find the source node and retrieve its image
    leads us back to **(D4)** for image retrieval. This leads to selecting and processing
    the image of the source node.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, we now have the textual and the image response. We can then
    build a summary and apply an accuracy performance metric after having visualized
    the time elapsed for each phase as we built the program:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**(G4)** We present a merged output with the LLM response and the augmented
    output with the image of the multimodal response in a *multimodal modular summary*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(E)** Finally, we create an *LLM performance metric* and a *multimodal performance
    metric*. We then sum them up as a *multimodal modular RAG performance metric*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can draw two conclusions from this multimodal modular RAG system:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The system we are building in this chapter is one of the many ways RAG-driven
    generative AI can be designed in real-life projects. Each project will have its
    specific needs and architecture.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rapid evolution from generative AI to the complexity of RAG-driven generative
    AI requires the corresponding development of seamlessly integrated cross-platform
    components such as LlamaIndex, Deep Lake, and OpenAI in this chapter. These platforms
    are also integrated with many other frameworks, such as Pinecone and LangChain,
    which we will discuss in *Chapter 6*, *Scaling RAG Bank Customer Data with Pinecone*.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s dive into Python and build the multimodal modular RAG program.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Building a multimodal modular RAG program for drone technology
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following sections, we will build a multimodal modular RAG-driven generative
    system from scratch in Python, step by step. We will implement:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex-managed OpenAI LLMs to process and understand text about drones
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Lake multimodal datasets containing images and labels of drone images taken
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functions to display images and identify objects within them using bounding
    boxes
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A system that can answer questions about drone technology using both text and
    images
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance metrics aimed at measuring the accuracy of the modular multimodal
    responses, including image analysis with GPT-4o
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, make sure you have created the LLM dataset in *Chapter 2* since we will
    be loading it in this section. However, you can read this chapter without running
    the notebook since it is self-contained with code and explanations. Now, let’s
    get to work!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Open the `Multimodal_Modular_RAG_Drones.ipynb` notebook in the GitHub repository
    for this chapter at [https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04](https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04).
    The packages installed are the same as those listed in the *Installing the environment*
    section of the previous chapter. Each of the following sections will guide you
    through building the multimodal modular notebook, starting with the LLM module.
    Let’s go through each section of the notebook step by step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Loading the LLM dataset
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will load the drone dataset created in *Chapter 3*. Make sure to insert
    the path to your dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will confirm that the dataset is loaded and will display the link
    to your dataset:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The program now creates a dictionary to hold the data to load it into a pandas
    DataFrame to visualize it:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output shows the text dataset with its structure: `embedding` (vectors),
    `id` (unique string identifier), `metadata` (in this case, the source of the data),
    and `text`, which contains the content:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31169_04_02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Output of the text dataset structure and content'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: We will now initialize the LLM query engine.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the LLM query engine
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As in *Chapter 3*, *Building Indexed-Based RAG with LlamaIndex, Deep Lake,
    and OpenAI*, we will initialize a vector store index from the collection of drone
    documents (`documents_llm`) of the dataset (`ds`). The `GPTVectorStoreIndex.from_documents()`
    method creates an index that increases the retrieval speed of documents based
    on vector similarity:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `as_query_engine()` method configures this index as a query engine with
    the specific parameters, as in *Chapter 3*, for similarity and retrieval depth,
    allowing the system to answer queries by finding the most relevant documents:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, the program introduces the user input.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: User input for multimodal modular RAG
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal of defining the user input in the context of the modular RAG system
    is to formulate a query that will effectively utilize both the text-based and
    image-based capabilities. This allows the system to generate a comprehensive and
    accurate response by leveraging multiple information sources:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this context, the user input is the *baseline*, the starting point, or a
    standard query used to assess the system’s capabilities. It will establish the
    initial frame of reference for how well the system can handle and respond to queries
    utilizing its available resources (e.g., text and image data from various datasets).
    In this example, the baseline is empirical and will serve to evaluate the system
    from that reference point.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Querying the textual dataset
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will run the vector query engine request as we did in *Chapter 3*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The execution time is satisfactory:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output content is also satisfactory:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The program now loads the multimodal drone dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Loading and visualizing the multimodal dataset
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the existing pubic VisDrone dataset available on Deep Lake: [https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/](https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/).
    We will *not* create a vector store but simply load the existing dataset in memory:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will display a link to the online dataset that you can explore with
    SQL, or natural language processing commands if you prefer, with the tools provided
    by Deep Lake:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s display the summary to explore the dataset in code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output provides useful information on the structure of the dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The structure contains images, boxes for the boundary boxes of the objects
    in the image, and labels describing the images and boundary boxes. Let’s visualize
    the dataset in code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output shows the images and their boundary boxes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31169_04_03.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Output showing boundary boxes'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s go further and display the content of the dataset in a pandas DataFrame
    to see what the images look like:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output in *Figure 4.4* shows the content of the dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_04_04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Excerpt of the VisDrone dataset'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 6,471 rows of images in the dataset and 3 columns:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The `image` column contains the image. The format of the image in the dataset,
    as indicated by the byte sequence `b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00...'`,
    is JPEG. The bytes `b'\xff\xd8\xff\xe0'` specifically signify the start of a JPEG
    image file.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `boxes` column contains the coordinates and dimensions of bounding boxes
    in the image, which are normally in the format `[x, y, width, height]`.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `labels` column contains the label of each bounding box in the `boxes` column.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can display the list of labels for the images:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output provides the list of labels, which defines the scope of the dataset:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With that, we have successfully loaded the dataset and will now explore the
    multimodal dataset structure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Navigating the multimodal dataset structure
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will select an image and display it using the dataset’s
    image column. To this image, we will then add the bounding boxes of a label that
    we will choose. The program first selects an image.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Selecting and displaying an image
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will select the first image in the dataset:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let’s display it with no bounding boxes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The image displayed contains trucks, pedestrians, and other types of objects:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31169_04_05.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Output displaying objects'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Now that the image is displayed, the program will add bounding boxes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Adding bounding boxes and saving the image
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have displayed the first image. The program will then fetch all the labels
    for the selected image:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output displays `value`, which contains the numerical indices of a label,
    and `text`, which contains the corresponding text labels of a label:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can display the values and the corresponding text in two columns:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output gives us a clear representation of the content of the labels of
    an image:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can group the class names (labels in plain text) of the images:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now group and display all the labels that describe the image:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can see all the classes the image contains:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The number of label classes sometimes exceeds what a human eye can see in an
    image.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now add bounding boxes. We first create a function to add the bounding
    boxes, display them, and save the image:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can add the bounding boxes for a specific label. In this case, we selected
    the `"truck"` label:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The image displayed now contains the bounding boxes for trucks:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![A truck with several trailers  Description automatically generated with medium
    confidence](img/B31169_04_06.png)Figure 4.6: Output displaying bounding boxes'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now activate a query engine to retrieve and obtain a response.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Building a multimodal query engine
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will query the VisDrone dataset and retrieve an image that
    fits the user input we entered in the *User input for multimodal modular RAG*
    section of this notebook. To achieve this goal, we will:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Create a vector index for each row of the `df` DataFrame containing the images,
    boxing data, and labels of the VisDrone dataset.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a query engine that will query the text data of the dataset, retrieve
    relevant image information, and provide a text response.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the nodes of the response to find the keywords related to the user input.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the nodes of the response to find the source image.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the bounding boxes of the source image to the image.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the image.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a vector index and query engine
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code first creates a document that will be processed to create a vector
    store index for the multimodal drone dataset. The `df` DataFrame we created in
    the *Loading and visualizing the multimodal dataset* section of the notebook on
    GitHub does not have unique indices or embeddings. We will create them in memory
    with LlamaIndex.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The program first assigns a unique ID to the DataFrame:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This line adds a new column to the `df` DataFrame called `doc_id`. It assigns
    unique identifiers to each row by converting the DataFrame’s row indices to strings.
    An empty list named `documents` is initialized, which we will use to create a
    vector index:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, the `iterrows()` method iterates through each row of the DataFrame, generating
    a sequence of index and row pairs:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`documents` is appended with all the records in the dataset, and a DataFrame
    is created:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The documents are now ready to be indexed with `GPTVectorStoreIndex`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The dataset is then seamlessly equipped with indices that we can visualize
    in the index dictionary:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output shows that an index has now been added to the dataset:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can now run a query on the multimodal dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Running a query on the VisDrone multimodal dataset
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now set `vector_store_index` as the query engine, as we did in the *Vector
    store index query engine* section in *Chapter 3*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can also run a query on the dataset of drone images, just as we did in *Chapter
    3* on an LLM dataset:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The execution time is satisfactory:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We will now examine the text response:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We can see that the output is logical and therefore satisfactory.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Drones use various sensors such as cameras, LiDAR, and GPS to identify and track
    objects like trucks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Processing the response
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now parse the nodes in the response to find the unique words in the
    response and select one for this notebook:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We found a unique word (`''truck''`) and its unique index, which will lead
    us directly to the image of the source of the node that generated the response:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We could select more words and design this function in many different ways depending
    on the specifications of each project.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: We will now search for the image by going through the source nodes, just as
    we did for an LLM dataset in the *Query response and source* section of the previous
    chapter. Multimodal vector stores and querying frameworks are flexible. Once we
    learn how to perform retrievals on an LLM and a multimodal dataset, we are ready
    for anything that comes up!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Let’s select and process the information related to an image.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Selecting and processing the image of the source node
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before running the image retrieval and displaying function, let’s first delete
    the image we displayed in the *Adding bounding boxes and saving the image* section
    of this notebook to make sure we are working on a new image:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We are now ready to search for the source image, call the bounding box, and
    display and save the function we defined earlier:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The program now goes through the source nodes with the keyword `"truck"` search,
    applies the bounding boxes, and displays and saves the image:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is satisfactory:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![An aerial view of a factory  Description automatically generated](img/B31169_04_07.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Displayed satisfactory output'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal modular summary
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have built a multimodal modular program step by step that we can now assemble
    in a summary. We will create a function to display the source image of the response
    to the user input, then print the user input and the LLM output, and display the
    image.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a function to display the source image saved by the multimodal
    retrieval engine:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we can display the user input, the LLM response, and the multimodal response.
    The output first displays the textual responses (user input and LLM response):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, the image is displayed with the bounding boxes for trucks in this case:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![An aerial view of a factory  Description automatically generated](img/B31169_04_08.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Output displaying boundary boxes'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: By adding an image to a classical LLM response, we augmented the output. Multimodal
    RAG output augmentation will enrich generative AI by adding information to both
    the input and output. However, as for all AI programs, designing a performance
    metric requires efficient image recognition functionality.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Performance metric
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Measuring the performance of a multimodal modular RAG requires two types of
    measurements: text and image. Measuring text is straightforward. However, measuring
    images is quite a challenge. Analyzing the image of a multimodal response is quite
    different. We extracted a keyword from the multimodal query engine. We then parsed
    the response for a source image to display. However, we will need to build an
    innovative approach to evaluate the source image of the response. Let’s begin
    with the LLM performance.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: LLM performance metric
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LlamaIndex seamlessly called an OpenAI model through its query engine, such
    as GPT-4, for example, and provided text content in its response. For text responses,
    we will use the same cosine similarity metric as in the *Evaluating the output
    with cosine similarity* section in *Chapter 2*, and the *Vector store index query
    engine* section in *Chapter 3*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation function uses `sklearn` and `sentence_transformers` to evaluate
    the similarity between two texts—in this case, an input and an output:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can now calculate the similarity between our baseline user input and the
    initial LLM response obtained:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output displays the user input, the text response, and the cosine similarity
    between the two texts:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The output is satisfactory. But we now need to design a way to measure the multimodal
    performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal performance metric
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the image returned, we cannot simply rely on the labels in the dataset.
    For small datasets, we can manually check the image, but when a system scales,
    automation is required. In this section, we will use the computer vision features
    of GPT-4o to analyze an image, parse it to find the objects we are looking for,
    and provide a description of that image. Then, we will apply cosine similarity
    to the description provided by GPT-4o and the label it is supposed to contain.
    GPT-4o is a multimodal generative AI model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first encode the image to simplify data transmission to GPT-4o. Base64
    encoding converts binary data (like images) into ASCII characters, which are standard
    text characters. This transformation is crucial because it ensures that the image
    data can be transmitted over protocols (like HTTP) that are designed to handle
    text data smoothly. It also avoids issues related to binary data transmission,
    such as data corruption or interpretation errors.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'The program encodes the source image using Python’s `base64` module:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We now create an OpenAI client and set the model to `gpt-4o`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The unique word will be the result of the LLM query to the multimodal dataset
    we obtained by parsing the response:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now submit the image to OpenAI GPT-4o:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We instructed the `system` and `user` roles to analyze images looking for our
    target label, `u_word`—in this case, `truck`. We then submitted the source node
    image to the model. The output that describes the image is satisfactory:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can now submit this response to the cosine similarity function by first
    adding an `"s"` to align with multiple trucks in a response:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output describes the image well but contains many other descriptions beyond
    the word “`truck`,” which limits its similarity to the input requested:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: A human observer might approve the image and the LLM response. However, even
    if the score was very high, the issue would be the same. Complex images are challenging
    to analyze in detail and with precision, although progress is continually made.
    Let’s now calculate the overall performance of the system.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal modular RAG performance metric
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To obtain the overall performance of the system, we will divide the sum of
    the LLM response and the two multimodal response performances by `2`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The result shows that although a human who observes the results may be satisfied,
    it remains difficult to automatically assess the relevance of a complex image:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The metric can be improved because a human observer sees that the image is relevant.
    This explains why the top AI agents, such as ChatGPT, Gemini, and Bing Copilot,
    always have a feedback process that includes thumbs up and thumbs down.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now sum up the chapter and gear up to explore how RAG can be improved
    even further with human feedback.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced us to the world of multimodal modular RAG, which uses
    distinct modules for different data types (text and image) and tasks. We leveraged
    the functionality of LlamaIndex, Deep Lake, and OpenAI, which we explored in the
    previous chapters. The Deep Lake VisDrone dataset further introduced us to drone
    technology for analyzing images and identifying objects. The dataset contained
    images, labels, and bounding box information. Working on drone technology involves
    multimodal data, encouraging us to develop skills that we can use across many
    domains, such as wildlife tracking, streamlining commercial deliveries, and making
    safer infrastructure inspections.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: We built a multimodal modular RAG-driven generative AI system. The first step
    was to define a baseline user query for both LLM and multimodal queries. We began
    by querying the Deep Lake textual dataset that we implemented in *Chapter 3*.
    LlamaIndex seamlessly ran a query engine to retrieve, augment, and generate a
    response. Then, we loaded the Deep Lake VisDrone dataset and indexed it in memory
    with LlamaIndex to create an indexed vector search retrieval pipeline. We queried
    it through LlamaIndex, which used an OpenAI model such as GPT-4 and parsed the
    text generated for a keyword. Finally, we searched the source nodes of the response
    to find the source image, display it, and merge the LLM and image responses into
    an augmented output. We applied cosine similarity to the text response. Evaluating
    the image was challenging, so we first ran image recognition with GPT-4o on the
    image retrieved to obtain a text to which we applied cosine similarity.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The journey into multimodal modular RAG-driven generative AI took us deep into
    the cutting edge of AI. Building a complex system was good preparation for real-life
    AI projects, which often require implementing multisource, multimodal, and unstructured
    data, leading to modular, complex systems. Thanks to transparent access to the
    source of a response, the complexity of RAG can be harnessed, controlled, and
    improved. We will see how we can leverage the transparency of the sources of a
    response to introduce human feedback to improve AI. The next chapter will take
    us further into transparency and precision in AI.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with *Yes* or *No*:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Does multimodal modular RAG handle different types of data, such as text and
    images?
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are drones used solely for agricultural monitoring and aerial photography?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the Deep Lake VisDrone dataset used in this chapter for textual data only?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can bounding boxes be added to drone images to identify objects such as trucks
    and pedestrians?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the modular system retrieve both text and image data for query responses?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is building a vector index necessary for querying the multimodal VisDrone dataset?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are the retrieved images processed without adding any labels or bounding boxes?
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the multimodal modular RAG performance metric based only on textual responses?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can a multimodal system such as the one described in this chapter handle only
    drone-related data?
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章所述的多模态系统是否只能处理与无人机相关的数据？
- en: Is evaluating images as easy as evaluating text in multimodal RAG?
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多模态 RAG 中评估图像是否像评估文本一样容易？
- en: References
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'LlamaIndex: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LlamaIndex: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/)'
- en: 'Activeloop Deep Lake: [https://docs.activeloop.ai/](https://docs.activeloop.ai/)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Activeloop Deep Lake: [https://docs.activeloop.ai/](https://docs.activeloop.ai/)'
- en: 'OpenAI: [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI: [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)'
- en: Further reading
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Retrieval-Augmented Multimodal Language Modeling, Yasunaga et al. (2023), [https://arxiv.org/pdf/2211.12561](https://arxiv.org/pdf/2211.12561)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Retrieval-Augmented Multimodal Language Modeling, Yasunaga 等人 (2023), [https://arxiv.org/pdf/2211.12561](https://arxiv.org/pdf/2211.12561)
- en: Join our community on Discord
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/rag](https://www.packt.link/rag)'
- en: '![](img/QR_Code50409000288080484.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code50409000288080484.png)'
