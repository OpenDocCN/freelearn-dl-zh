<html><head></head><body>
		<div id="_idContainer011">
			<h1 class="chapter-number" id="_idParaDest-21"><a id="_idTextAnchor021"/>2</h1>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor022"/>Building Blocks of Intelligent Applications</h1>
			<p>In the rapidly evolving landscape of software development, a new class of applications is emerging: intelligent applications. <strong class="bold">Intelligent applications</strong> are a superset of traditional full stack applications. These applications use <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) to deliver highly personalized, context-aware experiences that go beyond the capabilities of <span class="No-Break">traditional software.</span></p>
			<p>Intelligent applications understand complex, unstructured data and use this understanding to make decisions and create natural, <span class="No-Break">adaptive interactions.</span></p>
			<p>The goal of this chapter is to provide you with an overview of the logical and technical building blocks of intelligent applications. The chapter explores how intelligent applications extend the capability of traditional full-stack applications, the core structures that define them, and how these components function to create dynamic, context-aware experiences. By the end of this chapter, you will understand how these components fit together to form an <span class="No-Break">intelligent application.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>The building blocks of <span class="No-Break">intelligent applications</span></li>
				<li>LLMs as reasoning engines for <span class="No-Break">intelligent applications</span></li>
				<li>Vector embedding models and vector databases as semantic <span class="No-Break">long-term memory</span></li>
				<li>Model <span class="No-Break">hosting infrastructure</span></li>
			</ul>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor023"/>Technical requirements</h1>
			<p>This chapter is theoretical. It covers the logical components of intelligent applications and how they <span class="No-Break">fit together.</span></p>
			<p>This chapter assumes fundamental knowledge of traditional full stack application development components, such as servers, clients, databases, <span class="No-Break">and APIs.</span></p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor024"/>Defining intelligent applications</h1>
			<p>Traditional applications typically consist of a client-side user interface, a server-side backend, and a database for data storage and retrieval. They perform tasks following a strict set of instructions. Intelligent applications require a client, server, and database as well, but they augment the traditional stack with AI components. </p>
			<p>Intelligent applications stand out by understanding complex, unstructured data to enable natural, adaptive interactions and decision-making. Intelligent applications can engage in open-ended interactions, generate novel content, and make <span class="No-Break">autonomous decisions.</span></p>
			<p>Examples of intelligent applications include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Chatbots that provide natural language responses based on external data using <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>). For example, Perplexity.ai (<a href="https://www.perplexity.ai/"><span class="P---URL">https://www.perplexity.ai/</span></a>) is an AI-powered search engine and chatbot that provides users with AI-generated answers to their queries based on sources retrieved from <span class="No-Break">the web.</span></li>
				<li>Content generators that let you use natural language prompts to create media such as images, video, and audio. There are a variety of intelligent content generators focusing on different media types, such as Suno (<a href="https://suno.com/"><span class="P---URL">https://suno.com/</span></a>) for text-to-song, Midjourney (<a href="https://www.midjourney.com/home"><span class="P---URL">https://www.midjourney.com/home</span></a>) for text-to-image, and Runway (<a href="https://runwayml.com/"><span class="P---URL">https://runwayml.com/</span></a>) <span class="No-Break">for text-to-video.</span></li>
				<li>Recommendation systems that use customer data to provide personalized suggestions based on their preferences and history. These suggestions can be augmented with natural language to further personalize the customer experience. An example of this is Spotify’s AI DJ (<a href="https://support.spotify.com/us/article/dj/"><span class="P---URL">https://support.spotify.com/us/article/dj/</span></a>), which creates a personalized radio station, including LLM-generated DJ interludes, based on your <span class="No-Break">listening history.</span></li>
			</ul>
			<p>These examples are a few early glances at the new categories of intelligent applications that developers have only started to build. In the next section, you will learn more about the core components of <span class="No-Break">intelligent applications.</span></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor025"/>The building blocks of intelligent applications</h2>
			<p>At the heart of intelligent applications are two key <span class="No-Break">building blocks:</span></p>
			<ul>
				<li><strong class="bold">The reasoning engine</strong>: The reasoning engine is the brain of an intelligent application, responsible for understanding user input, generating appropriate responses, and making decisions based on available information. The reasoning engine is typically powered by <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>)—AI models that perform text completion. LLMs can understand user intent, generate human-like responses, and perform complex <span class="No-Break">cognitive tasks.</span></li>
				<li><strong class="bold">Semantic memory</strong>: Semantic memory refers to the application’s ability to store and retrieve information in a way that preserves its meaning and relationships, enabling the reasoning engine to access relevant context <span class="No-Break">as needed.</span><p class="list-inset">Semantic memory consists of two <span class="No-Break">core components:</span></p><ul><li><strong class="bold">AI vector embedding model</strong>: AI vector embedding models represent the semantic meaning of unstructured data, such as text or images, in large arrays <span class="No-Break">of numbers.</span></li><li><strong class="bold">Vector database</strong>: Vector databases efficiently store and retrieve vectors to support semantic search and <span class="No-Break">context retrieval.</span></li></ul></li>
			</ul>
			<p>The reasoning engine can retrieve and store relevant information from the semantic memory, using unstructured data to inform <span class="No-Break">its outputs.</span></p>
			<p>The LLMs and embedding models that power intelligent applications have different hardware requirements than traditional applications, especially at scale. Intelligent applications require specialized model hosting infrastructure that can handle the unique hardware and scalability requirements of AI workloads. Intelligent applications also incorporate continuous learning, safety monitoring, and human feedback to ensure quality <span class="No-Break">and integrity.</span></p>
			<p>LLMs are the vital organ for intelligent applications. The next section will provide a deeper understanding of the role of LLMs in <span class="No-Break">intelligent applications.</span></p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor026"/>LLMs – reasoning engines for intelligent apps</h1>
			<p>LLMs are the key technology of intelligent applications, unlocking whole new classes of AI-powered systems. These models are trained on vast amounts of text data to understand language, generate human-like text, answer questions, and engage <span class="No-Break">in dialogue.</span></p>
			<p>LLMs undergo continuous improvement with the release of new models. featuring billions or trillions of parameters and enhanced reasoning, memory, and <span class="No-Break">multi-modal capabilities.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor027"/>Use cases for LLM reasoning engines</h2>
			<p>LLMs have emerged as a powerful general-purpose technology for AI systems, analogous to the <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) in traditional computing. Much like CPUs, LLMs serve as general-purpose computational engines that can be programmed for many tasks and play a similar role in language-based reasoning and generation. The general-purpose nature of LLMs lets developers use their capabilities for a wide range of <span class="No-Break">reasoning tasks.</span></p>
			<p>A crop of techniques to leverage the diverse abilities of LLMs have emerged, <span class="No-Break">such as:</span></p>
			<ul>
				<li><strong class="bold">Prompt engineering</strong>: Using carefully crafted prompts, developers can steer LLMs to perform a wide range of language tasks. A key advantage of prompt engineering is its iterative nature. Since prompts are fundamentally just text, it’s easy to rapidly experiment with different prompts and see the results. Advanced prompt engineering techniques, such as chain-of-thought prompting (which encourages the model to break down its reasoning into a series of steps) and multi-shot prompting (which provides the model with example input/output pairs), can further enhance the quality and reliability of <span class="No-Break">LLM-generated text.</span></li>
				<li><strong class="bold">Fine-tuning</strong>: Fine-tuning involves starting with a pre-trained general-purpose model and further training it on a smaller dataset relevant to the target task. This can yield better results than prompt engineering alone, but it comes with certain caveats, such as being more expensive and time-consuming. You should only fine-tune after exhausting what you can achieve through <span class="No-Break">prompt engineering.</span></li>
				<li><strong class="bold">Retrieval augmentation</strong>: Retrieval augmentation interfaces LLMs with external knowledge, allowing them to draw on up-to-date, domain-specific information. In this approach, relevant information is retrieved from a knowledge base and injected into the prompt, enabling the LLM to generate contextually relevant outputs. Retrieval augmentation mitigates the limitations of the static pre-training of LLMs, keeping their knowledge updated and reducing the likelihood of the model hallucinating <span class="No-Break">incorrect information.</span></li>
			</ul>
			<p>With these techniques, you can use LLMs for a diverse array of tasks. The next section explores current use cases <span class="No-Break">for LLMs.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor028"/>Diverse capabilities of LLMs</h2>
			<p>While fundamentally <em class="italic">just</em> language models, LLMs have shown surprising emergent capabilities (<a href="https://arxiv.org/pdf/2307.06435"><span class="P---URL">https://arxiv.org/pdf/2307.06435</span></a>). As of writing in spring 2024, state-of-the-art language models are capable of performing tasks of the <span class="No-Break">following categories:</span></p>
			<ul>
				<li><strong class="bold">Text generation and completion</strong>: Given a prompt, LLMs can generate coherent continuations, making them useful for tasks such as content creation, text summarization, and <span class="No-Break">code completion.</span></li>
				<li><strong class="bold">Open-ended dialogue and chat</strong>: LLMs can engage in back-and-forth conversations, maintaining context and handling open-ended user queries and follow-up questions. This capability is foundational for chatbots, virtual assistants, tutoring systems, and <span class="No-Break">similar applications.</span></li>
				<li><strong class="bold">Question answering</strong>: LLMs can provide direct answers to user questions, perform research, and synthesize information to <span class="No-Break">address queries.</span></li>
				<li><strong class="bold">Classification and sentiment analysis</strong>: LLMs can classify text into predefined categories and assess sentiment, emotion, and opinion. This enables applications such as content moderation and customer <span class="No-Break">feedback analysis.</span></li>
				<li><strong class="bold">Data transformation and extraction</strong>: LLMs can map unstructured text into structured formats and extract key information, such as named entities, relationships, and events. This makes LLMs valuable for tasks such as data mining, knowledge graph construction, and <strong class="bold">robotic process </strong><span class="No-Break"><strong class="bold">automation</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RPA</strong></span><span class="No-Break">).</span></li>
			</ul>
			<p>As LLMs continue to grow in scale and sophistication, new capabilities are constantly emerging, often in surprising ways that were not directly intended by the original <span class="No-Break">training objective.</span></p>
			<p>For example, the ability of GPT-3 to generate functioning code was an unexpected discovery. With advancements in the field of LLMs, we can expect to see more impressive and versatile capabilities emerge, further expanding the potential of <span class="No-Break">intelligent applications.</span></p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor029"/>Multi-modal language models</h2>
			<p><strong class="bold">Multi-modal language models</strong> hold particular promise for expanding the capabilities of language models. Multi-modal models can process and generate images, speech, and video in addition to text, and have become an important component of <span class="No-Break">intelligent applications.</span></p>
			<p>Examples of new application categories made possible with multi-modal models include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Creating content based on multiple input types, such as a chatbot where users can provide both images and text <span class="No-Break">as inputs.</span></li>
				<li>Advanced data analysis, such as a medical diagnosis tool that analyzes X-rays along with <span class="No-Break">medical records.</span></li>
				<li>Real-time translation, taking audio or images of one language and translating it to <span class="No-Break">another language.</span></li>
			</ul>
			<p>Such examples highlight how multi-modal language models can enhance the possible use cases for <span class="No-Break">language models.</span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor030"/>A paradigm shift in AI development</h2>
			<p>The rise of LLMs represents a paradigm shift in the development of AI-powered applications. Previously, many reasoning tasks required specially trained models, which were time-intensive and computationally expensive to create. Developing these models often necessitated dedicated <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) engineering teams with <span class="No-Break">specialized expertise.</span></p>
			<p>In contrast, the general-purpose nature of LLMs allows most software engineers to leverage their capabilities through simple API calls and prompt engineering. While there is still an art and science to optimizing LLM-based workflows for production deployability, the process is significantly faster and more accessible compared to traditional <span class="No-Break">ML approaches.</span></p>
			<p>This shift has dramatically reduced the total cost of ownership and development timelines for AI-powered applications. NLP tasks that previously could take months of work by a sophisticated ML engineering team can now be achieved by a single software engineer with access to an LLM API and some prompt <span class="No-Break">engineering skills.</span></p>
			<p>Moreover, LLMs have unlocked entirely new classes of applications that were previously not possible or practical to develop. The ability of LLMs to understand and generate human-like text, engage in open-ended dialogue, and perform complex reasoning tasks has opened up a wide range of possibilities for intelligent applications <span class="No-Break">across industries.</span></p>
			<p>You’ll learn more about LLMs in <a href="B22495_03.xhtml#_idTextAnchor041"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Large Language Models</em>, which discusses their history and how <span class="No-Break">they operate.</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor031"/>Embedding models and vector databases – semantic long-term memory</h1>
			<p>In addition to the reasoning capabilities provided by LLMs, intelligent applications require semantic long-term memory for storing and <span class="No-Break">retrieving information.</span></p>
			<p>Semantic memory typically consists of two core components—AI vector embedding models and vector databases. Vector embedding models represent the semantic meaning of unstructured data, such as text or images, in large arrays of numbers. Vector databases efficiently store and retrieve these vectors to support semantic search and context retrieval. These components work together to enable the reasoning engine to access relevant context and information <span class="No-Break">as needed.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor032"/>Embedding models</h2>
			<p><strong class="bold">Embedding models</strong> are AI models that map text and other data types, such as images and audio, into high-dimensional vector representations. These vector representations capture the semantic meaning of the input data, allowing for efficient similarity comparisons and semantic search, typically using cosine similarity as the <span class="No-Break">distance metric.</span></p>
			<p>Embedding models encode semantic meaning into a machine-interpretable format. By representing similar concepts as nearby points in the vector space, embedding models let us measure the semantic similarity between pieces of unstructured data and perform semantic search across a <span class="No-Break">large corpus.</span></p>
			<p>Pre-trained embedding models are widely available and can be fine-tuned for specific domains or use cases. Compared to LLMs, embedding models tend to be more affordable and can run on limited hardware, making them accessible to a wider range <span class="No-Break">of applications.</span></p>
			<p>Some common applications of embedding models include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Semantic search and retrieval</strong>: Embedding models can be used as a component in larger AI systems to retrieve relevant context for LLMs, especially in RAG architectures. RAG is a particularly important use case for the intelligent applications discussed in this book and will be covered in more detail in <a href="B22495_08.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Implementing Vector Search in </em><span class="No-Break"><em class="italic">AI Applications</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Recommendation systems</strong>: By representing items and user preferences as embeddings, recommendation systems can identify similar items and generate <span class="No-Break">personalized recommendations.</span></li>
				<li><strong class="bold">Clustering and topic modeling</strong>: Embedding models can help discover latent topics and themes in large datasets, which can be useful for analyzing user interactions with intelligent applications, such as identifying frequently asked questions in <span class="No-Break">a chatbot.</span></li>
				<li><strong class="bold">Anomaly</strong><strong class="bold"> detection</strong>: By identifying outlier vectors that are semantically distant from the norm, embedding models can be used for anomaly detection in <span class="No-Break">various domains.</span></li>
				<li><strong class="bold">Analyzing relationships between entities</strong>: Embedding models can uncover hidden relationships and connections between entities based on their <span class="No-Break">semantic similarity.</span></li>
			</ul>
			<p>You will explore the technical details and practical considerations of embedding models in <a href="B22495_04.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <span class="No-Break"><em class="italic">Embedding Models</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>Vector databases</h2>
			<p><strong class="bold">Vector databases</strong> are specialized data stores optimized for storing and searching high-dimensional vectors. They provide fast, <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search capabilities that allow intelligent applications to quickly store and retrieve relevant information based on <span class="No-Break">spatial proximity.</span></p>
			<p>ANN search is necessary because performing exact similarity calculations against every vector in the database becomes computationally expensive as the database grows in size. Vector databases use algorithms, such as <strong class="bold">hierarchical navigable small worlds</strong> (<strong class="bold">HNSW</strong>), to efficiently find approximate nearest neighbors, making vector search feasible <span class="No-Break">at scale.</span></p>
			<p>In addition to ANN search, vector databases typically support filtering and exact search on metadata associated with the vectors. The exact functionality and performance of these features vary across different vector <span class="No-Break">database products.</span></p>
			<p>Vector databases provide an intelligent application with low-latency retrieval of relevant information given a query. Using the semantic meaning of the content for search, vector databases align with the way LLMs reason about information, enabling the application to apply the same unstructured data format for long-term memory as it does <span class="No-Break">for reasoning.</span></p>
			<p>In applications that use RAG, the vector database plays a crucial role. The application generates a query embedding, which is used to retrieve relevant context from the vector database. Multiple relevant chunks are then provided as context to the LLM, which uses this information to generate informed and <span class="No-Break">relevant responses.</span></p>
			<p>You will learn about the technical details and practical considerations of vector databases in <a href="B22495_05.xhtml#_idTextAnchor115"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Vector Databases</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>Model hosting</h2>
			<p>To implement AI models in your intelligent application, you must host them on computers, either in a data center or the cloud. This process is known as <strong class="bold">model hosting</strong>. Hosting AI models for applications presents a different set of requirements compared to hosting traditional software. Running AI models at scale requires powerful <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) and configuring the software environment to load and execute the <span class="No-Break">model efficiently.</span></p>
			<p>The key challenges in model hosting include high computational requirements and hardware costs, limited availability of GPU resources, complexity in managing and scaling the hosting infrastructure, and potential vendor lock-in or limited flexibility when using proprietary solutions. As a result, hardware and cost constraints must be factored into the application design process more <span class="No-Break">than ever.</span></p>
			<h3>Self-hosting models</h3>
			<p>The term <strong class="bold">self-hosting models</strong> refers to the practice of deploying and running AI models, such as LLMs, on an organization’s own infrastructure and hardware resources. In this approach, the organization is responsible for setting up and maintaining the necessary computational resources, software environment, and infrastructure required to load and execute <span class="No-Break">the models.</span></p>
			<p>Self-hosting AI models requires a significant upfront investment in specialized hardware, which can be cost-prohibitive for many organizations. Managing the model infrastructure also imposes an operational burden that requires ML expertise, which many software teams lack. This can divert the focus from the core application and <span class="No-Break">business logic.</span></p>
			<p>Scaling self-hosted models to ensure availability can be challenging, as models can be large and take time to load into memory. Organizations may need to provision significant excess capacity to handle peak loads. Additionally, maintaining and updating models is a complex task, as models can go stale over time and require retraining or fine-tuning. With the active research in the field, new models and techniques constantly emerge, making it difficult for organizations to <span class="No-Break">keep up.</span></p>
			<h3>Model hosting providers</h3>
			<p>The challenges associated with self-hosting have made model hosting providers a popular choice for intelligent <span class="No-Break">application development.</span></p>
			<p><strong class="bold">Model hosting providers</strong> are cloud-based services that offer a platform for deploying, running, and managing AI models, such as LLMs, on their infrastructure. These providers handle the complexities of setting up, maintaining, and scaling the infrastructure required to load and execute <span class="No-Break">the models.</span></p>
			<p>Model hosting providers offer <span class="No-Break">several benefits:</span></p>
			<ul>
				<li><strong class="bold">Outsourced hardware and infrastructure management</strong>: Model hosting providers handle provisioning, scaling, availability, security, and other infrastructure concerns, allowing application teams to focus on their <span class="No-Break">core product.</span></li>
				<li><strong class="bold">Cost efficiency and flexible pricing</strong>: With model hosting providers, organizations pay only for what they use and can scale resources up and down as needed, reducing <span class="No-Break">upfront investment.</span></li>
				<li><strong class="bold">Access to a wide range of models</strong>: Providers curate and host many state-of-the-art models, continuously integrating the latest research. They often add additional features and optimizations to the <span class="No-Break">raw models.</span></li>
				<li><strong class="bold">Support and expertise</strong>: Providers can offer consultation on model selection, prompt engineering, application architecture, and assistance with fine-tuning, data preparation, evaluation, and other aspects of <span class="No-Break">AI development.</span></li>
				<li><strong class="bold">Rapid prototyping and experimentation</strong>: Model hosting providers enable developers to quickly test different models and approaches, adapting to new developments in the fast-moving <span class="No-Break">AI/ML space.</span></li>
				<li><strong class="bold">Scalability and reliability</strong>: Providers build robust, highly available, and auto-scaling infrastructure to meet the demands of production-scale <span class="No-Break">intelligent applications.</span></li>
			</ul>
			<p>Examples of model hosting providers include those from model developers such as OpenAI, Anthropic, and Cohere, as well as offerings from cloud providers such as AWS Bedrock, Google Vertex AI, and Azure <span class="No-Break">AI Studio.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Your (soon-to-be) intelligent app</h1>
			<p>With LLMs, embedding models, vector databases, and model hosting, you have the key building blocks for creating intelligent applications. While the specific architecture will vary depending on your use case, a common <span class="No-Break">pattern emerges:</span></p>
			<ul>
				<li><strong class="bold">LLMs</strong> for reasoning <span class="No-Break">and generation</span></li>
				<li><strong class="bold">Embeddings</strong> and <strong class="bold">vector search</strong> for retrieval <span class="No-Break">and memory</span></li>
				<li><strong class="bold">Model hosting</strong> to serve these components <span class="No-Break">at scale</span></li>
			</ul>
			<p>This AI stack is integrated with traditional application components, such as backend services, APIs, frontend user interfaces, databases, and data pipelines. Additionally, intelligent applications often include components for AI-specific concerns, such as prompt management and optimization, data preparation and embedding generation, and AI safety, testing, <span class="No-Break">and monitoring.</span></p>
			<p>The rest of this section walks through an example architecture for a RAG-powered chatbot, showcasing how these components work together. The subsequent chapters will dive deeper into the end-to-end process of building production-grade <span class="No-Break">intelligent applications.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>Sample application – RAG chatbot</h2>
			<p>Consider a simple chatbot application that leverages RAG that lets users talk to <span class="No-Break">some documentation.</span></p>
			<p>There are seven key components of <span class="No-Break">this application:</span></p>
			<ul>
				<li><strong class="bold">Chatbot UI</strong>: A website with a simple chatbot UI that communicates with the <span class="No-Break">web server</span></li>
				<li><strong class="bold">Web server</strong>: A Python Flask server to manage conversations between the user and <span class="No-Break">the LLM</span></li>
				<li><strong class="bold">Data ingestion extract, transform, load (ETL) pipeline</strong>: A Python script that ingests data from the <span class="No-Break">data sources</span></li>
				<li><strong class="bold">Embedding model</strong>: The OpenAI <strong class="source-inline">text-embedding-3-small</strong> model, hosted <span class="No-Break">by OpenAI</span></li>
				<li><strong class="bold">LLM</strong>: The OpenAI <strong class="source-inline">gpt-4-turbo</strong> model, hosted <span class="No-Break">by OpenAI</span></li>
				<li><strong class="bold">Vector store</strong>: MongoDB Atlas <span class="No-Break">Vector Search</span></li>
				<li><strong class="bold">MongoDB Atlas</strong>: A database-as-a-service for <span class="No-Break">persisting conversations</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">This simple example application does not include evaluation or <span class="No-Break">observability modules.</span></p>
			<p>In this architecture, there are two key <span class="No-Break">data flows:</span></p>
			<ul>
				<li><strong class="bold">Chat interaction</strong>: The user communicates with the chatbot <span class="No-Break">with RAG</span></li>
				<li><strong class="bold">Data ingestion</strong>: Bringing data from its original sources into the <span class="No-Break">vector database</span></li>
			</ul>
			<p>In the chat interaction, the chatbot UI communicates with the chatbot web server, which in turn interacts with the LLM, embedding model, and vector store. This occurs for every message that the user sends to the chatbot. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> shows the data flow for the <span class="No-Break">chatbot application:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer009">
					<img alt="" role="presentation" src="image/B22495_02_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1: An example of a basic RAG chatbot conversation data flow</p>
			<p>The data flow illustrated in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> can be described <span class="No-Break">as follows:</span></p>
			<ol>
				<li>The user sends a message to the chatbot from the <span class="No-Break">web UI.</span></li>
				<li>The web UI creates a request to the server with the <span class="No-Break">user’s message.</span></li>
				<li>The web server sends a request to the embedding model API to create a vector embedding for the user query. The embedding model API responds with the corresponding <span class="No-Break">vector embedding.</span></li>
				<li>The web server performs a vector search in the vector database using the query vector embedding. The vector store responds with the matching vector <span class="No-Break">search results.</span></li>
				<li> The server constructs a message that the LLM will respond to. This message consists of a system prompt and a new message that includes the user’s original message and the content retrieved from the vector search. The LLM then responds to the <span class="No-Break">user message.</span></li>
				<li>The server saves the conversation state to <span class="No-Break">the database.</span></li>
				<li>The server returns the LLM-generated message to the user in a response to the original request from the <span class="No-Break">web UI.</span></li>
			</ol>
			<p>A data ingestion pipeline prepares and enriches data, generates embeddings using the embedding model, and populates the vector store and traditional database. This pipeline runs as a batch job every 24 hours. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.2</em> shows an example of a data <span class="No-Break">ingestion pipeline:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer010">
					<img alt="" role="presentation" src="image/B22495_02_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: An example of a RAG chatbot data ingestion ETL pipeline</p>
			<p>Let’s look at the data flow shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<ol>
				<li>The data ingestion ETL pipeline pulls in data from various <span class="No-Break">data sources.</span></li>
				<li>The ETL pipeline cleans the data into a consistent format. It also breaks the data into chunks <span class="No-Break">of data.</span></li>
				<li>The ETL pipeline calls the embedding model API to generate a vector embedding for each <span class="No-Break">data chunk.</span></li>
				<li>The ETL pipeline stores the chunks along with their vector embeddings in a <span class="No-Break">vector database.</span></li>
				<li>The vector database indexes the embeddings for use with <span class="No-Break">vector search.</span></li>
			</ol>
			<p>While a simple architecture like this can be used to build compelling prototypes, transitioning from prototype to production and continuously iterating on the application requires addressing many <span class="No-Break">additional considerations:</span></p>
			<ul>
				<li><strong class="bold">Data ingestion strategy</strong>: Acquiring, cleaning, and preparing the data that will be ingested into the vector store or database <span class="No-Break">for retrieval.</span></li>
				<li><strong class="bold">Advanced retrieval patterns</strong>: Incorporating techniques for efficient and accurate retrieval of relevant information from the vector store or database, such as combining semantic search with traditional filtering, AI-based reranking, and <span class="No-Break">query mutation.</span></li>
				<li><strong class="bold">Evaluation and testing</strong>: Adding modules for evaluating model outputs, testing end-to-end application flows, and monitoring for potential biases <span class="No-Break">or errors.</span></li>
				<li><strong class="bold">Scalability and performance optimization</strong>: Implementing optimizations such as caching, load balancing, and efficient resource management to handle increasing workloads and ensure <span class="No-Break">consistent responsiveness.</span></li>
				<li><strong class="bold">Security and privacy</strong>: Securing the application to ensure that users can only interact with data that they have permission to, so that user data is handled in accordance with relevant policies, standards, <span class="No-Break">and laws.</span></li>
				<li><strong class="bold">User experience and interaction design</strong>: Incorporating new generative AI interfaces and interaction patterns, such as streaming responses, answer confidence, and <span class="No-Break">source citation.</span></li>
				<li><strong class="bold">Continuous improvement and model updates</strong>: Building processes and systems to update AI models safely and reliably and hyperparameters in the <span class="No-Break">intelligent application.</span></li>
			</ul>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Implications of intelligent applications for software engineering</h2>
			<p>The rise of intelligent applications has significant implications for how software is made. Developing these intelligent applications requires an extension of traditional development skills. The AI engineer must possess an understanding of prompt engineering, vector search, and evaluation, as well as familiarity with the latest AI techniques and architectures. While a complete understanding of the underlying neural networks is not necessary, basic knowledge of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) <span class="No-Break">is helpful.</span></p>
			<p>Intelligent application development also introduces new challenges and considerations, such as data management and integration with AI components, testing and debugging of AI-driven functionality, and addressing the ethical, safety, and security implications of AI outputs. The compute-heavy nature of AI workloads also necessitates focusing on scalability and cost optimization. Developers building traditional software generally do not need to face <span class="No-Break">such concerns.</span></p>
			<p>To address these challenges, software development teams must adapt their processes and adopt novel approaches and best practices. This entails implementing AI governance, bridging the gap between software and ML/AI teams, and adjusting the development lifecycle for intelligent <span class="No-Break">app needs.</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>Summary</h1>
			<p>Intelligent applications represent a new paradigm in software development, combining AI with traditional application components to deliver highly personalized, context-aware experiences. This chapter details the core components of intelligent applications, highlighting the pivotal role of LLMs as reasoning engines. LLMs serve as versatile computational tools capable of performing diverse tasks, including chat, summarization, and classification, due to their <span class="No-Break">general-purpose design.</span></p>
			<p>Complementing these reasoning engines are embedding models and vector databases, which function as the semantic memory of intelligent applications. These components enable the reasoning engine to retrieve pertinent context and information as needed. Additionally, the hosting of AI models demands dedicated infrastructure, as their unique hardware requirements differ significantly from traditional software needs. Using building blocks such as LLMs, embedding models, vector databases, and model hosting infrastructure, developers can create applications that understand complex, unstructured data, engage in open-ended interactions, generate novel content, and make autonomous decisions. Building these intelligent applications demands a new set of tools, approaches, and <span class="No-Break">best practices.</span></p>
			<p>The next chapter will examine how LLMs work and the role they play in building <span class="No-Break">intelligent applications.</span></p>
		</div>
	

		<div>
			<div id="_idContainer012">
			</div>
		</div>
		<div id="_idContainer013">
			<h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>Part 1</h1>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor040"/>Foundations of AI: LLMs, Embedding Models, Vector Databases, and Application Design</h1>
			<p>This set of chapters provides in-depth and practical knowledge on the techniques and principles underpinning AI-intensive applications. You will progress quickly from fundamental concepts to real-world use cases and learn best practices for building your <span class="No-Break">AI solution.</span></p>
			<p>This part of the book includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B22495_03.xhtml#_idTextAnchor041"><em class="italic">Chapter 3</em></a>, <em class="italic">Large Language Models</em></li>
				<li><a href="B22495_04.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a>, <em class="italic">Embedding Models</em></li>
				<li><a href="B22495_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Vector Databases</em></li>
				<li><a href="B22495_06.xhtml#_idTextAnchor137"><em class="italic">Chapter 6</em></a>, <em class="italic">AI/ML Application Design</em></li>
			</ul>
		</div>
	</body></html>