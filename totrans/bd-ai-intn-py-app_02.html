<html><head></head><body>
		<div><h1 class="chapter-number" id="_idParaDest-21"><a id="_idTextAnchor021"/>2</h1>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor022"/>Building Blocks of Intelligent Applications</h1>
			<p>In the rapidly evolving landscape of software development, a new class of applications is emerging: intelligent applications. <strong class="bold">Intelligent applications</strong> are a superset of traditional full stack applications. These applications use <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) to deliver highly personalized, context-aware experiences that go beyond the capabilities of traditional software.</p>
			<p>Intelligent applications understand complex, unstructured data and use this understanding to make decisions and create natural, adaptive interactions.</p>
			<p>The goal of this chapter is to provide you with an overview of the logical and technical building blocks of intelligent applications. The chapter explores how intelligent applications extend the capability of traditional full-stack applications, the core structures that define them, and how these components function to create dynamic, context-aware experiences. By the end of this chapter, you will understand how these components fit together to form an intelligent application.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>The building blocks of intelligent applications</li>
				<li>LLMs as reasoning engines for intelligent applications</li>
				<li>Vector embedding models and vector databases as semantic long-term memory</li>
				<li>Model hosting infrastructure</li>
			</ul>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor023"/>Technical requirements</h1>
			<p>This chapter is theoretical. It covers the logical components of intelligent applications and how they fit together.</p>
			<p>This chapter assumes fundamental knowledge of traditional full stack application development components, such as servers, clients, databases, and APIs.</p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor024"/>Defining intelligent applications</h1>
			<p>Traditional applications typically consist of a client-side user interface, a server-side backend, and a database for data storage and retrieval. They perform tasks following a strict set of instructions. Intelligent applications require a client, server, and database as well, but they augment the traditional stack with AI components. </p>
			<p>Intelligent applications stand out by understanding complex, unstructured data to enable natural, adaptive interactions and decision-making. Intelligent applications can engage in open-ended interactions, generate novel content, and make autonomous decisions.</p>
			<p>Examples of intelligent applications include the following:</p>
			<ul>
				<li>Chatbots that provide natural language responses based on external data using <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>). For example, Perplexity.ai (<a href="https://www.perplexity.ai/">https://www.perplexity.ai/</a>) is an AI-powered search engine and chatbot that provides users with AI-generated answers to their queries based on sources retrieved from the web.</li>
				<li>Content generators that let you use natural language prompts to create media such as images, video, and audio. There are a variety of intelligent content generators focusing on different media types, such as Suno (<a href="https://suno.com/">https://suno.com/</a>) for text-to-song, Midjourney (<a href="https://www.midjourney.com/home">https://www.midjourney.com/home</a>) for text-to-image, and Runway (<a href="https://runwayml.com/">https://runwayml.com/</a>) for text-to-video.</li>
				<li>Recommendation systems that use customer data to provide personalized suggestions based on their preferences and history. These suggestions can be augmented with natural language to further personalize the customer experience. An example of this is Spotify’s AI DJ (<a href="https://support.spotify.com/us/article/dj/">https://support.spotify.com/us/article/dj/</a>), which creates a personalized radio station, including LLM-generated DJ interludes, based on your listening history.</li>
			</ul>
			<p>These examples are a few early glances at the new categories of intelligent applications that developers have only started to build. In the next section, you will learn more about the core components of intelligent applications.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor025"/>The building blocks of intelligent applications</h2>
			<p>At the heart of intelligent applications are two key building blocks:</p>
			<ul>
				<li><strong class="bold">The reasoning engine</strong>: The reasoning engine is the brain of an intelligent application, responsible for understanding user input, generating appropriate responses, and making decisions based on available information. The reasoning engine is typically powered by <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>)—AI models that perform text completion. LLMs can understand user intent, generate human-like responses, and perform complex cognitive tasks.</li>
				<li><strong class="bold">Semantic memory</strong>: Semantic memory refers to the application’s ability to store and retrieve information in a way that preserves its meaning and relationships, enabling the reasoning engine to access relevant context as needed.<p class="list-inset">Semantic memory consists of two core components:</p><ul><li><strong class="bold">AI vector embedding model</strong>: AI vector embedding models represent the semantic meaning of unstructured data, such as text or images, in large arrays of numbers.</li><li><strong class="bold">Vector database</strong>: Vector databases efficiently store and retrieve vectors to support semantic search and context retrieval.</li></ul></li>
			</ul>
			<p>The reasoning engine can retrieve and store relevant information from the semantic memory, using unstructured data to inform its outputs.</p>
			<p>The LLMs and embedding models that power intelligent applications have different hardware requirements than traditional applications, especially at scale. Intelligent applications require specialized model hosting infrastructure that can handle the unique hardware and scalability requirements of AI workloads. Intelligent applications also incorporate continuous learning, safety monitoring, and human feedback to ensure quality and integrity.</p>
			<p>LLMs are the vital organ for intelligent applications. The next section will provide a deeper understanding of the role of LLMs in intelligent applications.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor026"/>LLMs – reasoning engines for intelligent apps</h1>
			<p>LLMs are the key technology of intelligent applications, unlocking whole new classes of AI-powered systems. These models are trained on vast amounts of text data to understand language, generate human-like text, answer questions, and engage in dialogue.</p>
			<p>LLMs undergo continuous improvement with the release of new models. featuring billions or trillions of parameters and enhanced reasoning, memory, and multi-modal capabilities.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor027"/>Use cases for LLM reasoning engines</h2>
			<p>LLMs have emerged as a powerful general-purpose technology for AI systems, analogous to the <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) in traditional computing. Much like CPUs, LLMs serve as general-purpose computational engines that can be programmed for many tasks and play a similar role in language-based reasoning and generation. The general-purpose nature of LLMs lets developers use their capabilities for a wide range of reasoning tasks.</p>
			<p>A crop of techniques to leverage the diverse abilities of LLMs have emerged, such as:</p>
			<ul>
				<li><strong class="bold">Prompt engineering</strong>: Using carefully crafted prompts, developers can steer LLMs to perform a wide range of language tasks. A key advantage of prompt engineering is its iterative nature. Since prompts are fundamentally just text, it’s easy to rapidly experiment with different prompts and see the results. Advanced prompt engineering techniques, such as chain-of-thought prompting (which encourages the model to break down its reasoning into a series of steps) and multi-shot prompting (which provides the model with example input/output pairs), can further enhance the quality and reliability of LLM-generated text.</li>
				<li><strong class="bold">Fine-tuning</strong>: Fine-tuning involves starting with a pre-trained general-purpose model and further training it on a smaller dataset relevant to the target task. This can yield better results than prompt engineering alone, but it comes with certain caveats, such as being more expensive and time-consuming. You should only fine-tune after exhausting what you can achieve through prompt engineering.</li>
				<li><strong class="bold">Retrieval augmentation</strong>: Retrieval augmentation interfaces LLMs with external knowledge, allowing them to draw on up-to-date, domain-specific information. In this approach, relevant information is retrieved from a knowledge base and injected into the prompt, enabling the LLM to generate contextually relevant outputs. Retrieval augmentation mitigates the limitations of the static pre-training of LLMs, keeping their knowledge updated and reducing the likelihood of the model hallucinating incorrect information.</li>
			</ul>
			<p>With these techniques, you can use LLMs for a diverse array of tasks. The next section explores current use cases for LLMs.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor028"/>Diverse capabilities of LLMs</h2>
			<p>While fundamentally <em class="italic">just</em> language models, LLMs have shown surprising emergent capabilities (<a href="https://arxiv.org/pdf/2307.06435">https://arxiv.org/pdf/2307.06435</a>). As of writing in spring 2024, state-of-the-art language models are capable of performing tasks of the following categories:</p>
			<ul>
				<li><strong class="bold">Text generation and completion</strong>: Given a prompt, LLMs can generate coherent continuations, making them useful for tasks such as content creation, text summarization, and code completion.</li>
				<li><strong class="bold">Open-ended dialogue and chat</strong>: LLMs can engage in back-and-forth conversations, maintaining context and handling open-ended user queries and follow-up questions. This capability is foundational for chatbots, virtual assistants, tutoring systems, and similar applications.</li>
				<li><strong class="bold">Question answering</strong>: LLMs can provide direct answers to user questions, perform research, and synthesize information to address queries.</li>
				<li><strong class="bold">Classification and sentiment analysis</strong>: LLMs can classify text into predefined categories and assess sentiment, emotion, and opinion. This enables applications such as content moderation and customer feedback analysis.</li>
				<li><strong class="bold">Data transformation and extraction</strong>: LLMs can map unstructured text into structured formats and extract key information, such as named entities, relationships, and events. This makes LLMs valuable for tasks such as data mining, knowledge graph construction, and <strong class="bold">robotic process </strong><strong class="bold">automation</strong> (<strong class="bold">RPA</strong>).</li>
			</ul>
			<p>As LLMs continue to grow in scale and sophistication, new capabilities are constantly emerging, often in surprising ways that were not directly intended by the original training objective.</p>
			<p>For example, the ability of GPT-3 to generate functioning code was an unexpected discovery. With advancements in the field of LLMs, we can expect to see more impressive and versatile capabilities emerge, further expanding the potential of intelligent applications.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor029"/>Multi-modal language models</h2>
			<p><strong class="bold">Multi-modal language models</strong> hold particular promise for expanding the capabilities of language models. Multi-modal models can process and generate images, speech, and video in addition to text, and have become an important component of intelligent applications.</p>
			<p>Examples of new application categories made possible with multi-modal models include the following:</p>
			<ul>
				<li>Creating content based on multiple input types, such as a chatbot where users can provide both images and text as inputs.</li>
				<li>Advanced data analysis, such as a medical diagnosis tool that analyzes X-rays along with medical records.</li>
				<li>Real-time translation, taking audio or images of one language and translating it to another language.</li>
			</ul>
			<p>Such examples highlight how multi-modal language models can enhance the possible use cases for language models.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor030"/>A paradigm shift in AI development</h2>
			<p>The rise of LLMs represents a paradigm shift in the development of AI-powered applications. Previously, many reasoning tasks required specially trained models, which were time-intensive and computationally expensive to create. Developing these models often necessitated dedicated <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) engineering teams with specialized expertise.</p>
			<p>In contrast, the general-purpose nature of LLMs allows most software engineers to leverage their capabilities through simple API calls and prompt engineering. While there is still an art and science to optimizing LLM-based workflows for production deployability, the process is significantly faster and more accessible compared to traditional ML approaches.</p>
			<p>This shift has dramatically reduced the total cost of ownership and development timelines for AI-powered applications. NLP tasks that previously could take months of work by a sophisticated ML engineering team can now be achieved by a single software engineer with access to an LLM API and some prompt engineering skills.</p>
			<p>Moreover, LLMs have unlocked entirely new classes of applications that were previously not possible or practical to develop. The ability of LLMs to understand and generate human-like text, engage in open-ended dialogue, and perform complex reasoning tasks has opened up a wide range of possibilities for intelligent applications across industries.</p>
			<p>You’ll learn more about LLMs in <a href="B22495_03.xhtml#_idTextAnchor041"><em class="italic">Chapter 3</em></a>, <em class="italic">Large Language Models</em>, which discusses their history and how they operate.</p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor031"/>Embedding models and vector databases – semantic long-term memory</h1>
			<p>In addition to the reasoning capabilities provided by LLMs, intelligent applications require semantic long-term memory for storing and retrieving information.</p>
			<p>Semantic memory typically consists of two core components—AI vector embedding models and vector databases. Vector embedding models represent the semantic meaning of unstructured data, such as text or images, in large arrays of numbers. Vector databases efficiently store and retrieve these vectors to support semantic search and context retrieval. These components work together to enable the reasoning engine to access relevant context and information as needed.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor032"/>Embedding models</h2>
			<p><strong class="bold">Embedding models</strong> are AI models that map text and other data types, such as images and audio, into high-dimensional vector representations. These vector representations capture the semantic meaning of the input data, allowing for efficient similarity comparisons and semantic search, typically using cosine similarity as the distance metric.</p>
			<p>Embedding models encode semantic meaning into a machine-interpretable format. By representing similar concepts as nearby points in the vector space, embedding models let us measure the semantic similarity between pieces of unstructured data and perform semantic search across a large corpus.</p>
			<p>Pre-trained embedding models are widely available and can be fine-tuned for specific domains or use cases. Compared to LLMs, embedding models tend to be more affordable and can run on limited hardware, making them accessible to a wider range of applications.</p>
			<p>Some common applications of embedding models include the following:</p>
			<ul>
				<li><strong class="bold">Semantic search and retrieval</strong>: Embedding models can be used as a component in larger AI systems to retrieve relevant context for LLMs, especially in RAG architectures. RAG is a particularly important use case for the intelligent applications discussed in this book and will be covered in more detail in <a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>, <em class="italic">Implementing Vector Search in </em><em class="italic">AI Applications</em>.</li>
				<li><strong class="bold">Recommendation systems</strong>: By representing items and user preferences as embeddings, recommendation systems can identify similar items and generate personalized recommendations.</li>
				<li><strong class="bold">Clustering and topic modeling</strong>: Embedding models can help discover latent topics and themes in large datasets, which can be useful for analyzing user interactions with intelligent applications, such as identifying frequently asked questions in a chatbot.</li>
				<li><strong class="bold">Anomaly</strong><strong class="bold"> detection</strong>: By identifying outlier vectors that are semantically distant from the norm, embedding models can be used for anomaly detection in various domains.</li>
				<li><strong class="bold">Analyzing relationships between entities</strong>: Embedding models can uncover hidden relationships and connections between entities based on their semantic similarity.</li>
			</ul>
			<p>You will explore the technical details and practical considerations of embedding models in <a href="B22495_04.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a>, <em class="italic">Embedding Models</em>.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>Vector databases</h2>
			<p><strong class="bold">Vector databases</strong> are specialized data stores optimized for storing and searching high-dimensional vectors. They provide fast, <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search capabilities that allow intelligent applications to quickly store and retrieve relevant information based on spatial proximity.</p>
			<p>ANN search is necessary because performing exact similarity calculations against every vector in the database becomes computationally expensive as the database grows in size. Vector databases use algorithms, such as <strong class="bold">hierarchical navigable small worlds</strong> (<strong class="bold">HNSW</strong>), to efficiently find approximate nearest neighbors, making vector search feasible at scale.</p>
			<p>In addition to ANN search, vector databases typically support filtering and exact search on metadata associated with the vectors. The exact functionality and performance of these features vary across different vector database products.</p>
			<p>Vector databases provide an intelligent application with low-latency retrieval of relevant information given a query. Using the semantic meaning of the content for search, vector databases align with the way LLMs reason about information, enabling the application to apply the same unstructured data format for long-term memory as it does for reasoning.</p>
			<p>In applications that use RAG, the vector database plays a crucial role. The application generates a query embedding, which is used to retrieve relevant context from the vector database. Multiple relevant chunks are then provided as context to the LLM, which uses this information to generate informed and relevant responses.</p>
			<p>You will learn about the technical details and practical considerations of vector databases in <a href="B22495_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Vector Databases</em>.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>Model hosting</h2>
			<p>To implement AI models in your intelligent application, you must host them on computers, either in a data center or the cloud. This process is known as <strong class="bold">model hosting</strong>. Hosting AI models for applications presents a different set of requirements compared to hosting traditional software. Running AI models at scale requires powerful <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) and configuring the software environment to load and execute the model efficiently.</p>
			<p>The key challenges in model hosting include high computational requirements and hardware costs, limited availability of GPU resources, complexity in managing and scaling the hosting infrastructure, and potential vendor lock-in or limited flexibility when using proprietary solutions. As a result, hardware and cost constraints must be factored into the application design process more than ever.</p>
			<h3>Self-hosting models</h3>
			<p>The term <strong class="bold">self-hosting models</strong> refers to the practice of deploying and running AI models, such as LLMs, on an organization’s own infrastructure and hardware resources. In this approach, the organization is responsible for setting up and maintaining the necessary computational resources, software environment, and infrastructure required to load and execute the models.</p>
			<p>Self-hosting AI models requires a significant upfront investment in specialized hardware, which can be cost-prohibitive for many organizations. Managing the model infrastructure also imposes an operational burden that requires ML expertise, which many software teams lack. This can divert the focus from the core application and business logic.</p>
			<p>Scaling self-hosted models to ensure availability can be challenging, as models can be large and take time to load into memory. Organizations may need to provision significant excess capacity to handle peak loads. Additionally, maintaining and updating models is a complex task, as models can go stale over time and require retraining or fine-tuning. With the active research in the field, new models and techniques constantly emerge, making it difficult for organizations to keep up.</p>
			<h3>Model hosting providers</h3>
			<p>The challenges associated with self-hosting have made model hosting providers a popular choice for intelligent application development.</p>
			<p><strong class="bold">Model hosting providers</strong> are cloud-based services that offer a platform for deploying, running, and managing AI models, such as LLMs, on their infrastructure. These providers handle the complexities of setting up, maintaining, and scaling the infrastructure required to load and execute the models.</p>
			<p>Model hosting providers offer several benefits:</p>
			<ul>
				<li><strong class="bold">Outsourced hardware and infrastructure management</strong>: Model hosting providers handle provisioning, scaling, availability, security, and other infrastructure concerns, allowing application teams to focus on their core product.</li>
				<li><strong class="bold">Cost efficiency and flexible pricing</strong>: With model hosting providers, organizations pay only for what they use and can scale resources up and down as needed, reducing upfront investment.</li>
				<li><strong class="bold">Access to a wide range of models</strong>: Providers curate and host many state-of-the-art models, continuously integrating the latest research. They often add additional features and optimizations to the raw models.</li>
				<li><strong class="bold">Support and expertise</strong>: Providers can offer consultation on model selection, prompt engineering, application architecture, and assistance with fine-tuning, data preparation, evaluation, and other aspects of AI development.</li>
				<li><strong class="bold">Rapid prototyping and experimentation</strong>: Model hosting providers enable developers to quickly test different models and approaches, adapting to new developments in the fast-moving AI/ML space.</li>
				<li><strong class="bold">Scalability and reliability</strong>: Providers build robust, highly available, and auto-scaling infrastructure to meet the demands of production-scale intelligent applications.</li>
			</ul>
			<p>Examples of model hosting providers include those from model developers such as OpenAI, Anthropic, and Cohere, as well as offerings from cloud providers such as AWS Bedrock, Google Vertex AI, and Azure AI Studio.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Your (soon-to-be) intelligent app</h1>
			<p>With LLMs, embedding models, vector databases, and model hosting, you have the key building blocks for creating intelligent applications. While the specific architecture will vary depending on your use case, a common pattern emerges:</p>
			<ul>
				<li><strong class="bold">LLMs</strong> for reasoning and generation</li>
				<li><strong class="bold">Embeddings</strong> and <strong class="bold">vector search</strong> for retrieval and memory</li>
				<li><strong class="bold">Model hosting</strong> to serve these components at scale</li>
			</ul>
			<p>This AI stack is integrated with traditional application components, such as backend services, APIs, frontend user interfaces, databases, and data pipelines. Additionally, intelligent applications often include components for AI-specific concerns, such as prompt management and optimization, data preparation and embedding generation, and AI safety, testing, and monitoring.</p>
			<p>The rest of this section walks through an example architecture for a RAG-powered chatbot, showcasing how these components work together. The subsequent chapters will dive deeper into the end-to-end process of building production-grade intelligent applications.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>Sample application – RAG chatbot</h2>
			<p>Consider a simple chatbot application that leverages RAG that lets users talk to some documentation.</p>
			<p>There are seven key components of this application:</p>
			<ul>
				<li><strong class="bold">Chatbot UI</strong>: A website with a simple chatbot UI that communicates with the web server</li>
				<li><strong class="bold">Web server</strong>: A Python Flask server to manage conversations between the user and the LLM</li>
				<li><strong class="bold">Data ingestion extract, transform, load (ETL) pipeline</strong>: A Python script that ingests data from the data sources</li>
				<li><code>text-embedding-3-small</code> model, hosted by OpenAI</li>
				<li><code>gpt-4-turbo</code> model, hosted by OpenAI</li>
				<li><strong class="bold">Vector store</strong>: MongoDB Atlas Vector Search</li>
				<li><strong class="bold">MongoDB Atlas</strong>: A database-as-a-service for persisting conversations</li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">This simple example application does not include evaluation or observability modules.</p>
			<p>In this architecture, there are two key data flows:</p>
			<ul>
				<li><strong class="bold">Chat interaction</strong>: The user communicates with the chatbot with RAG</li>
				<li><strong class="bold">Data ingestion</strong>: Bringing data from its original sources into the vector database</li>
			</ul>
			<p>In the chat interaction, the chatbot UI communicates with the chatbot web server, which in turn interacts with the LLM, embedding model, and vector store. This occurs for every message that the user sends to the chatbot. <em class="italic">Figure 2</em><em class="italic">.1</em> shows the data flow for the chatbot application:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_02_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1: An example of a basic RAG chatbot conversation data flow</p>
			<p>The data flow illustrated in <em class="italic">Figure 2</em><em class="italic">.1</em> can be described as follows:</p>
			<ol>
				<li>The user sends a message to the chatbot from the web UI.</li>
				<li>The web UI creates a request to the server with the user’s message.</li>
				<li>The web server sends a request to the embedding model API to create a vector embedding for the user query. The embedding model API responds with the corresponding vector embedding.</li>
				<li>The web server performs a vector search in the vector database using the query vector embedding. The vector store responds with the matching vector search results.</li>
				<li> The server constructs a message that the LLM will respond to. This message consists of a system prompt and a new message that includes the user’s original message and the content retrieved from the vector search. The LLM then responds to the user message.</li>
				<li>The server saves the conversation state to the database.</li>
				<li>The server returns the LLM-generated message to the user in a response to the original request from the web UI.</li>
			</ol>
			<p>A data ingestion pipeline prepares and enriches data, generates embeddings using the embedding model, and populates the vector store and traditional database. This pipeline runs as a batch job every 24 hours. <em class="italic">Figure 2</em><em class="italic">.2</em> shows an example of a data ingestion pipeline:</p>
			<div><div><img alt="" role="presentation" src="img/B22495_02_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: An example of a RAG chatbot data ingestion ETL pipeline</p>
			<p>Let’s look at the data flow shown in <em class="italic">Figure 2</em><em class="italic">.2</em>:</p>
			<ol>
				<li>The data ingestion ETL pipeline pulls in data from various data sources.</li>
				<li>The ETL pipeline cleans the data into a consistent format. It also breaks the data into chunks of data.</li>
				<li>The ETL pipeline calls the embedding model API to generate a vector embedding for each data chunk.</li>
				<li>The ETL pipeline stores the chunks along with their vector embeddings in a vector database.</li>
				<li>The vector database indexes the embeddings for use with vector search.</li>
			</ol>
			<p>While a simple architecture like this can be used to build compelling prototypes, transitioning from prototype to production and continuously iterating on the application requires addressing many additional considerations:</p>
			<ul>
				<li><strong class="bold">Data ingestion strategy</strong>: Acquiring, cleaning, and preparing the data that will be ingested into the vector store or database for retrieval.</li>
				<li><strong class="bold">Advanced retrieval patterns</strong>: Incorporating techniques for efficient and accurate retrieval of relevant information from the vector store or database, such as combining semantic search with traditional filtering, AI-based reranking, and query mutation.</li>
				<li><strong class="bold">Evaluation and testing</strong>: Adding modules for evaluating model outputs, testing end-to-end application flows, and monitoring for potential biases or errors.</li>
				<li><strong class="bold">Scalability and performance optimization</strong>: Implementing optimizations such as caching, load balancing, and efficient resource management to handle increasing workloads and ensure consistent responsiveness.</li>
				<li><strong class="bold">Security and privacy</strong>: Securing the application to ensure that users can only interact with data that they have permission to, so that user data is handled in accordance with relevant policies, standards, and laws.</li>
				<li><strong class="bold">User experience and interaction design</strong>: Incorporating new generative AI interfaces and interaction patterns, such as streaming responses, answer confidence, and source citation.</li>
				<li><strong class="bold">Continuous improvement and model updates</strong>: Building processes and systems to update AI models safely and reliably and hyperparameters in the intelligent application.</li>
			</ul>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Implications of intelligent applications for software engineering</h2>
			<p>The rise of intelligent applications has significant implications for how software is made. Developing these intelligent applications requires an extension of traditional development skills. The AI engineer must possess an understanding of prompt engineering, vector search, and evaluation, as well as familiarity with the latest AI techniques and architectures. While a complete understanding of the underlying neural networks is not necessary, basic knowledge of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) is helpful.</p>
			<p>Intelligent application development also introduces new challenges and considerations, such as data management and integration with AI components, testing and debugging of AI-driven functionality, and addressing the ethical, safety, and security implications of AI outputs. The compute-heavy nature of AI workloads also necessitates focusing on scalability and cost optimization. Developers building traditional software generally do not need to face such concerns.</p>
			<p>To address these challenges, software development teams must adapt their processes and adopt novel approaches and best practices. This entails implementing AI governance, bridging the gap between software and ML/AI teams, and adjusting the development lifecycle for intelligent app needs.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>Summary</h1>
			<p>Intelligent applications represent a new paradigm in software development, combining AI with traditional application components to deliver highly personalized, context-aware experiences. This chapter details the core components of intelligent applications, highlighting the pivotal role of LLMs as reasoning engines. LLMs serve as versatile computational tools capable of performing diverse tasks, including chat, summarization, and classification, due to their general-purpose design.</p>
			<p>Complementing these reasoning engines are embedding models and vector databases, which function as the semantic memory of intelligent applications. These components enable the reasoning engine to retrieve pertinent context and information as needed. Additionally, the hosting of AI models demands dedicated infrastructure, as their unique hardware requirements differ significantly from traditional software needs. Using building blocks such as LLMs, embedding models, vector databases, and model hosting infrastructure, developers can create applications that understand complex, unstructured data, engage in open-ended interactions, generate novel content, and make autonomous decisions. Building these intelligent applications demands a new set of tools, approaches, and best practices.</p>
			<p>The next chapter will examine how LLMs work and the role they play in building intelligent applications.</p>
		</div>
	

		<div><div></div>
		</div>
		<div><h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>Part 1</h1>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor040"/>Foundations of AI: LLMs, Embedding Models, Vector Databases, and Application Design</h1>
			<p>This set of chapters provides in-depth and practical knowledge on the techniques and principles underpinning AI-intensive applications. You will progress quickly from fundamental concepts to real-world use cases and learn best practices for building your AI solution.</p>
			<p>This part of the book includes the following chapters:</p>
			<ul>
				<li><a href="B22495_03.xhtml#_idTextAnchor041"><em class="italic">Chapter 3</em></a>, <em class="italic">Large Language Models</em></li>
				<li><a href="B22495_04.xhtml#_idTextAnchor061"><em class="italic">Chapter 4</em></a>, <em class="italic">Embedding Models</em></li>
				<li><a href="B22495_05.xhtml#_idTextAnchor115"><em class="italic">Chapter 5</em></a>, <em class="italic">Vector Databases</em></li>
				<li><a href="B22495_06.xhtml#_idTextAnchor137"><em class="italic">Chapter 6</em></a>, <em class="italic">AI/ML Application Design</em></li>
			</ul>
		</div>
	</body></html>