- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Effective Prompt Engineering Techniques: Unlocking Wisdom Through AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt engineering emerged as a standout profession in 2023, captivating the
    tech industry with its profound impact on AI interactions and applications. But
    what sparked this surge in popularity? The answer lies in the nuanced and intricate
    nature of the discipline. Understanding the essentials of prompt engineering is
    crucial; it’s not just about communicating with the model; it’s about crafting
    prompts that guide the AI to understand the context and nuances of the task at
    hand. In the previous chapter, we learned about how we can add relevant context
    through RAG by searching through vector DB. Finally, a prompt needs to be crafted
    and sent to the LLMs. This leads to more accurate and relevant responses, turning
    a simple interaction into a robust tool for a variety of cloud-based applications.
    Whether it’s automating customer support, generating content, or analyzing data,
    the ability to fine-tune prompts is a game-changer, ensuring that the AI’s capabilities
    are fully leveraged in a controlled and purposeful manner.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter delves into the techniques for effective prompt engineering, offering
    strategies to refine interactions for optimal outcomes. This goes hand in hand
    with the ethical dimensions of prompt engineering, a topic of paramount importance
    in today’s tech landscape. It addresses the responsibility of developers to ensure
    that AI interactions are not only efficient and goal-oriented but also ethically
    sound and bias-free. Finally, the integration of prompt engineering with cloud
    solutions opens up a new realm of possibilities. It allows for scalable, efficient,
    and flexible AI solutions that can be seamlessly integrated into existing cloud
    infrastructure, revolutionizing how businesses and individuals interact with AI.
    In essence, this chapter is not just an instructional guide but is a cornerstone
    for building responsible and effective cloud-based GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will talk about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The essentials of prompt engineering with ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for effective prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ethical dimensions of prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Comic depiction of Prompt Engineer](img/chapter_5-joke.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Comic depiction of Prompt Engineer
  prefs: []
  type: TYPE_NORMAL
- en: The essentials of prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before discussing prompt engineering, it is important to first understand the
    foundational components of a prompt. In this section, we’ll delve into the key
    components of a prompt, such as ChatGPT prompts, completions, and tokens. Additionally,
    grasping what tokens are is pivotal to understanding the model’s constraints and
    managing costs.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT prompts and completions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A prompt is an input provided to LLMs, whereas completions refer to the output
    of LLMs. The structure and content of a prompt can vary based on the type of LLM
    (e.g., the text or image generation model), specific use cases, and the desired
    output of the language model.
  prefs: []
  type: TYPE_NORMAL
- en: Completions refer to the response generated by ChatGPT prompts; basically, it
    is an answer to your questions. Check out the following example to understand
    the difference between prompts and completions when we prompt ChatGPT with, “What
    is the capital of India?”
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – An image showing a sample LLM prompt and completion](img/B21443_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – An image showing a sample LLM prompt and completion
  prefs: []
  type: TYPE_NORMAL
- en: Based on the use case, we can leverage one of the two ChatGPT API calls, named
    **Completions** or **ChatCompletions**, to interact with the model. However, OpenAI
    recommends using the ChatCompletions API in the majority of scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Completions API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Completions API is designed to generate creative, free-form text. You provide
    a prompt, and the API generates text that continues from it. This is often used
    for tasks where you want the model to answer a question or generate creative text,
    such as for writing an article or a poem.
  prefs: []
  type: TYPE_NORMAL
- en: ChatCompletions API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ChatCompletions API is designed for multi-turn conversations. You send a
    series of messages instead of a single prompt, and the model generates a message
    as a response. The messages sent to the model include a role (which can be a **system**,
    **user**, or **assistant**) and the content of the message. The system role is
    used to set the behavior of the assistant, the user role is used to instruct the
    assistant, and the model’s responses are under the assistant role.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a sample ChatCompletions API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The main difference between the Completions API and ChatCompletions API is that
    the Completions API is designed for single-turn tasks, while the ChatCompletions
    API is designed to handle multiple turns in a conversation, making it more suitable
    for building conversational agents. However, the ChatCompletions API format can
    be modified to behave as a Completions API by using a single user message.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The CompletionsAPI, launched in June 2020, initially offered a freeform text
    interface for Open AI’s language models. However, experience has shown that structured
    prompts often yield better outcomes. The chat-based approach, especially through
    the ChatCompletions API, excels in addressing a wide array of needs, offering
    enhanced flexibility and specificity and reducing prompt injection risks. Its
    design supports multi-turn conversations and a variety of tasks, enabling developers
    to create advanced conversational experiences. Hence, Open AI announced that they
    would be deprecating some of the older models using Completions API and, in moving
    forward, they would be investing in the ChatCompletions API to optimize their
    efforts to use compute capacity. While the Completions API will remain accessible,
    it shall be labeled as “legacy” in the Open AI developer documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the concepts of tokens is essential, as it helps us better comprehend
    the restrictions, such as model limitations, and the aspect of cost management
    when utilizing ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: A ChatGPT token is a unit of text that ChatGPT’s language model uses to understand
    and generate language. In ChatGPT, a token is a sequence of characters that the
    model uses to generate new sequences of tokens and form a coherent response to
    a given prompt. The models use tokens to represent words, phrases, and other language
    elements. The tokens are not cut where the word starts or ends but can consist
    of trailing spaces, sub words and punctuations, too.
  prefs: []
  type: TYPE_NORMAL
- en: As stated on the OpenAI website, tokens can be thought of as pieces of words.
    Before the API processes the prompts, the input is broken down into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand tokens in terms of lengths, the following is used as a rule of
    thumb:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 token ~= 4 chars in English
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 token ~= ¾ words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 100 tokens ~= 75 words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1–2 sentences ~= 30 tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 paragraph ~= 100 tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1,500 words ~= 2048 tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 US page (8 ½” x 11”) ~= 450 tokens (assuming ~1800 characters per page)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, this famous quote from Thomas Edison (“Genius is one percent inspiration
    and ninety-nine percent perspiration.”) has **14** tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Tokenization of sentence](img/B21443_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Tokenization of sentence
  prefs: []
  type: TYPE_NORMAL
- en: We used the OpenAI **Tokenizer** tool to calculate the tokens; the tool can
    be found at [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer).
    An alternative way to tokenize text (programmatically) is to use the **Tiktoken
    library** on Github; this can be found at [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken).
  prefs: []
  type: TYPE_NORMAL
- en: Token limits in ChatGPT models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on the model, the token limits on the model will vary. As of Feb 2024,
    the token limit for the family of GPT-4 models ranges from 8,192 to 128,000 tokens.
    This means the sum of prompt and completion tokens for an API call cannot exceed
    32,768 tokens for the GPT-4-32K model. If the prompt is 30,000 tokens, the response
    cannot be more than 2,768 tokens. The GPT4-Turbo 128K is the most recent model
    as of Feb 2024, with 128,000 tokens, which is close to 300 pages of text in a
    single prompt and completion. This is a massive context prompt compared to its
    predecessor models.
  prefs: []
  type: TYPE_NORMAL
- en: Though this can be a technical limitation, there are creative ways to address
    the problem of limitation, such as using chunking and condensing your prompts.
    We discussed chunking strategies in [*Chapter 4*](B21443_04.xhtml#_idTextAnchor070),
    which can help you address token limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows various models and token limits:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Token Limit** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | 4,096 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo-16k | 16,384 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo-0613 | 4,096 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo-16k-0613 | 16,384 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 8,192 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-0613 | 32,768 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-32K | 32,768 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-32-0613 | 32,768 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-Turbo 128K | 128,000 |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.4 – Models and associated Token Limits
  prefs: []
  type: TYPE_NORMAL
- en: For the latest updates on model limits for newer versions of models, please
    check the OpenAI website.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens and cost considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cost of using ChatGPT or similar models via an API is often tied to the
    number of tokens processed, encompassing both the input prompts and the model’s
    generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of pricing, providers typically have a per-token charge, leading to
    a direct correlation between conversation length and cost; the more tokens processed,
    the higher the cost. The latest cost updates can be found on the OpenAI website.
  prefs: []
  type: TYPE_NORMAL
- en: From an optimization perspective, understanding this cost-token relationship
    can guide more efficient API usage. For instance, creating more succinct prompts
    and configuring the model for brief yet effective responses can help control token
    count and, consequently, manage expenses.
  prefs: []
  type: TYPE_NORMAL
- en: We hope you now have a good understanding of the key components of a prompt.
    Now, you are ready to learn about prompt engineering. In the next section, we
    will explore the details of prompt engineering and effective strategies, enabling
    you to maximize the potential of your prompt contents through the one-shot and
    few-shot learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt engineering is the art of crafting or designing prompts to unlock desired
    outcomes from large language models or AI systems. The concept of prompt engineering
    revolves around the fundamental idea that the quality of your response is intricately
    tied to the quality of the question you pose. By strategically engineering prompts,
    one can influence the generated outputs and improve the overall performance and
    usefulness of the system. In this section, we will learn about the necessary elements
    of effective prompt design, prompt engineering techniques, best practices, bonus
    tips, and tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of a good prompt design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Designing a good prompt is important because it significantly influences the
    output of a language model such as GPT. The prompt provides the initial context,
    sets the task, guides the style and structure of the response, reduces ambiguities
    and hallucinations, and supports the optimization of resources, thereby reducing
    costs and energy use. In this section, let’s understand the elements of good prompt
    design.
  prefs: []
  type: TYPE_NORMAL
- en: 'The foundational elements of a good prompt include instructions, questions,
    input data, and examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instructions**: The instructions in a prompt refer to the specific guidelines
    or directions given to a language model within the input text to guide the kind
    of response it should produce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Questions**: Questions in a prompt refer to queries or interrogative statements
    that are included in the input text. The purpose of these questions is to instruct
    the language model to provide a response or an answer to the query. In order to
    obtain the results, either the question or instruction is mandatory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input data**: The purpose of input data is to provide any additional supporting
    context when prompting the LLM. It could be used to provide new information the
    model has not previously been trained on for more personalized experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Examples**: The purpose of examples in a prompt is to provide specific instances
    or scenarios that illustrate the desired behavior or response from ChatGPT. You
    can input a prompt that includes one or more examples, typically in the form of
    input-output pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table shows how to build effective prompts using the aforementioned
    prompt elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sample** **Prompt Formula** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Questions + Instructions | How should I create a healthy meal plan for a
    week?Include a variety of nutrients and food groups, and explain the benefits
    of each meal choice. |'
  prefs: []
  type: TYPE_TB
- en: '| Instructions + Input Data | Provide a punchy title in less than 5 words for
    the paragraph below.{Jake finally took his brand-new Tesla for a spin on the coastal
    highway, the smooth hum of the electric motor filling the air as the scenic ocean
    views passed by.} |'
  prefs: []
  type: TYPE_TB
- en: '| Examples + Question | I enjoy movies such as Star Wars, Matrix, and Transformers.What
    other movies would you recommend? |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.5 – Sample Prompt formula consisting of prompt elements with examples
  prefs: []
  type: TYPE_NORMAL
- en: Prompt parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ChatGPT prompt parameters are variables that you can set in the API calls.
    They allow users to influence the model’s output, customizing the behavior of
    the model to better fit specific applications or contexts. The following table
    shows some of the most important parameters of a ChatGPT API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Description** | **Effect** **and Usage** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model | Determines the model to be used in the API. Larger models have higher
    costs and latency. | Select based on the task complexity, cost considerations,
    and acceptable latency. Always try to use the latest model version. |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature | Controls the randomness of the model’s responses. It can be
    set between 0 (more focused responses) and 2 (more diverse responses). | Lower
    values yield more deterministic responses, which is ideal for more formal or exact
    responses, such as in legal use cases. Higher values may result in more creative
    output but can also lead to hallucinations. |'
  prefs: []
  type: TYPE_TB
- en: '| Top_P (Nucleus Sampling) | Sets a cumulative probability threshold for the
    model’s responses. A value of 0.1 implies only the top 10% of probable tokens
    are considered. | Lower values yield more predictable and focused responses. OpenAI
    recommends using either Temperature or Top_p, not both. |'
  prefs: []
  type: TYPE_TB
- en: '| Max Tokens | Sets the maximum length of the generated response. This is useful
    for controlling the length of output and the cost. | Lower values lead to shorter
    responses, reduced latency, and potentially lower costs, while higher values allow
    for longer, more detailed responses. |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.6 – Essential Prompt Parameters
  prefs: []
  type: TYPE_NORMAL
- en: In this section, only the top parameters for building an effective prompt are
    highlighted. For a full list of parameters, refer to the OpenAI API reference
    ([https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference)).
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT roles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: System message
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the part where you design your metaprompts. Metaprompts help to set
    the initial context, theme, and behavior of the ChatGPT API to guide the model’s
    interactions with the user, thus setting roles or response styles for the assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Metaprompts are structured instructions or guidelines that dictate how the system
    should interpret and respond to user requests. These metaprompts are designed
    to ensure that the system’s outputs adhere to specific policies, ethical guidelines,
    or operational rules. They’re essentially “prompts about how to handle prompts,”
    guiding the system in generating responses, handling data, or interacting with
    users in a way that aligns with predefined standards.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table is a metaprompt framework that you can follow to design
    the ChatGPT system message:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Elements of** **a Metaprompt** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Task and Audience | Explain the intended audience for the application and
    expectations from the model. |'
  prefs: []
  type: TYPE_TB
- en: '| Helper Tools | Clarify how the model should address user queries and whether
    there are external tools, such as plugins, APIs, or code, that the model might
    need to consider utilizing. |'
  prefs: []
  type: TYPE_TB
- en: '| Scope of the Task | Clarify on how the model should respond if a question
    is out of scope, and set those guard rails. |'
  prefs: []
  type: TYPE_TB
- en: '| Posture and Tone | Setting postures and tones, such as professional, friendly,
    respectful, and motivational, help improve user experiences with chat application.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Format of Responses | Based on the requirements of the application, you can
    set the output format to be of a certain format. It could be a table of contents,
    a certain programming language, JSON, or XML. |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot Examples | Outline the challenging scenarios where the prompts are
    unclear or complex, providing the model with more insight into how to handle such
    situations. |'
  prefs: []
  type: TYPE_TB
- en: '| Chain-of-Thought Reasoning | Demonstrate the reasoning process to guide the
    model in taking the necessary steps to produce the desired results. |'
  prefs: []
  type: TYPE_TB
- en: '| Guardrails to Address Specific Harm | Establish clear boundaries to address
    and prevent any potential harm that has been recognized and deemed important for
    the given scenario.For example, if jailbreaking attempts are detected, you must
    have clear guardrails to address those jailbreaking attempts in the system message.
    We will learn more about jailbreaking in [*Chapter 8*](B21443_08.xhtml#_idTextAnchor163).
    |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.7 – Elements of a Metaprompt
  prefs: []
  type: TYPE_NORMAL
- en: User
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The messages from the user serve as prompts or remarks that the assistant is
    expected to react to or engage with. what is it establishes the anticipated scope
    of queries that may come from the user.
  prefs: []
  type: TYPE_NORMAL
- en: Assistant
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While not mandatory, assistant messages can be included to illustrate the preferred
    conduct or response patterns of the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at an example; if you are developing an application for a fitness
    AI assistant, a very well-rounded system message might look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following link provides great examples of “Act As” prompts for you to leverage
    in your system message: *Awesome ChatGPT* *Prompts* ([https://github.com/f/awesome-chatgpt-prompts/](https://github.com/f/awesome-chatgpt-prompts/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for effective prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past two years, a wide array of prompt-engineering techniques have been
    developed. This section focuses on the essential ones, offering key strategies
    that you might find indispensable for daily interactions with ChatGPT and other
    LLM-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: N-shot prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: N-shot prompting is a term used in the context of training large language models,
    particularly for **zero-shot** or **few-shot** learning tasks. It is also called
    in-context learning and refers to the technique of providing the model with example
    prompts along with corresponding responses during training to steer the model’s
    behavior to provide more accurate responses.
  prefs: []
  type: TYPE_NORMAL
- en: The “N” in “N-shot” refers to the number of example prompts provided to the
    model. For instance, in a one-shot learning scenario, only one example prompt
    and its response are given to the model. In an N-shot learning scenario, multiple
    example prompts and responses are provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'While ChatGPT works great with zero-shot prompting, it may sometimes be useful
    to provide examples for a more accurate response. Let’s see some examples of zero-shot
    and few-shot prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Techniques** | **Prompt Example** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Zero-shot prompting**: No additional examples are provided in line with
    the prompt. | System message: You are an AI assistant who determines the sentiment
    of the customer review provided.(No examples responses provided in the form of
    assistant response, hence it is called zero-shot prompting.) |'
  prefs: []
  type: TYPE_TB
- en: '| **Few-Shot Prompting**: A few examples are provided in line with the prompt.
    | System message: You are an AI assistant who determines the sentiment of the
    customer review provided.Example 1:User: The product is miserable.Assistant: NegativeExample
    2:User: This shirt is made from very good material.Assistant: Positive |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.8 – N-shot prompting examples
  prefs: []
  type: TYPE_NORMAL
- en: Chain-of-thought (CoT) prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chain-of-thought prompting refers to a sequence of intermediate reasoning steps,
    significantly boosting the capability of large language models to tackle complex
    reasoning tasks. By presenting a few chain-of-thought demonstrations as examples
    in the prompts, the models proficiently handle intricate reasoning tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Chain-of-Thought Prompting Examples](img/B21443_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Chain-of-Thought Prompting Examples
  prefs: []
  type: TYPE_NORMAL
- en: Figure sourced from [https://arxiv.org/pdf/2201.11903.pdf](https://arxiv.org/pdf/2201.11903.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Program-aided language (PAL) models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Program-aided language** (**PAL**) models, also called **program-of-thought
    prompting** (**PoT**), is a technique that incorporates additional task-specific
    instructions, pseudo-code, rules, or programs alongside the free-form text to
    guide the behavior of a language model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Program-aided language prompting examples](img/B21443_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Program-aided language prompting examples
  prefs: []
  type: TYPE_NORMAL
- en: Figure sourced from [https://arxiv.org/abs/2211.10435](https://arxiv.org/abs/2211.10435).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, although we have not explored all prompt engineering techniques
    (only the most important ones), we want to convey to our readers that there are
    numerous variants of these techniques, as illustrated in the following figure
    from the research paper *A Systematic Survey of prompt engineering in Large Language
    Models: Techniques and Applications* ([https://arxiv.org/pdf/2402.07927.pdf](https://arxiv.org/pdf/2402.07927.pdf)).
    This paper provides an extensive inventory of prompt engineering strategies across
    various application areas, showcasing the evolution and breadth of this field
    over the last four years:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Taxonomy of prompt engineering techniques across multiple application
    domains](img/B21443_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Taxonomy of prompt engineering techniques across multiple application
    domains
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering best practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following list, we outline additional best practices to optimize and
    enhance your experience with prompt creation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clarity and precision for accurate responses**: Ensure that prompts are clear,
    concise, and specific, avoiding ambiguity or multiple interpretations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Bad Prompt** | **Good Prompt** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Tell me about World War 1 | How did World War 1 start, and who won it? |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5.12 – Best practice: clarity and precision'
  prefs: []
  type: TYPE_NORMAL
- en: '**Descriptive**: Be descriptive so that ChatGPT can understand your intent:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Bad Prompt** | **Good Prompt** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Write a poem about India. | Write a poem about India focusing on its cultural
    diversity, deciduous cuisine, beautiful wildlife, nature, technology innovation,
    and film industry. |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5.13 – Best practice: be descriptive'
  prefs: []
  type: TYPE_NORMAL
- en: '**Format the output**: Mention the format of the output, which can be bullet
    points, paragraphs, sentences, tables, and languages, such as XML, HTML, and JSON.
    Use examples to articulate the desired output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adjust the Temperature and Top_p parameters for creativity**: As indicated
    in the parameters section, modifying the **Temperatures** and **Top_p** can significantly
    influence the variability of the model’s output. In scenarios that call for creativity
    and imagination, raising the temperature proves beneficial. On the other hand,
    when dealing with legal applications that demand a reduction in hallucinations,
    a lower temperature becomes advantageous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use syntax as separators in prompts**: In this example, for a more effective
    output, use “”” or ### to separate instruction and input data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the text below to Spanish
  prefs: []
  type: TYPE_NORMAL
- en: 'Text: “””'
  prefs: []
  type: TYPE_NORMAL
- en: '{text input here}'
  prefs: []
  type: TYPE_NORMAL
- en: “””
  prefs: []
  type: TYPE_NORMAL
- en: '**Order of the prompt elements matter**: It has been found, in certain instances,
    that giving an instruction before an example can improve the quality of your outputs.
    Additionally, the order of examples can affect the output of prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use guiding words**: This helps steer the model toward a specific structure,
    such as the text highlighted in the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a basic Python function that
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Requests the user to enter a temperature in Celsius
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2\. Converts the Celsius temperature to Fahrenheit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'def ctf():'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instead of saying what not to provide, give alternative recommendations**:
    Provide an alternative path if ChatGPT is unable to perform a task, such as in
    the following highlighted message:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'System Message: You are an AI nutrition consultant that provides nutrition
    consultation based on health and wellness goals of the customer Please note that
    any questions or inquiries beyond the scope of nutrition consultation will NOT
    be answered and instead will receive the response: “Sorry! This question falls
    outside my domain of expertise!”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Customer: How do I invest in 401K?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nutrition AI Assistant: “Sorry! This question falls outside my domain of expertise!”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Provide example-based prompts**: This helps the language model learn from
    specific instances and patterns. Start with a zero-shot, then a few-shot, and
    if neither of them works, then fine-tune the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ask ChatGPT to provide citations/sources**: When asking ChatGPT to provide
    information, you can ask it to answer only using reliable sources and to cite
    the sources:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Prompts** | **ChatGPT Completion** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| When was the steam engine invented?Answer using only reliable sources and
    cite the sources | The steam engine was invented during the late 17th century.
    The first practical steam engine is credited to Thomas Savery, an English military
    engineer and inventor, who patented his design in 1698\. Savery’s steam engine
    was primarily used for pumping water out of mines.Source: Encyclopædia Britannica.
    (2021, September 2). Thomas Savery. In Encyclopædia Britannica. [https://www.britannica.com/biography/Thomas-Savery](https://www.britannica.com/biography/Thomas-Savery).
    |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5.14 – Best practice: provide citations'
  prefs: []
  type: TYPE_NORMAL
- en: '**Break down a complex task into simpler tasks**: See the following example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Bad Prompt** | **Effective Prompt** |'
  prefs: []
  type: TYPE_TB
- en: '| Provide a summary of the following and provide key points from the summary
    | Provide a summary of the following text: [Insert long text here]Summarize the
    key points from the following passage: [Insert long text here] |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 5.15 – Best practice: break down a complex task'
  prefs: []
  type: TYPE_NORMAL
- en: Bonus tips and tricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following list provides some helpful bonus tips and tricks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<begin>`, `<end>`, and `<|endofprompt|>`, that determine the beginning and
    end of prompts can help separate the different elements of a prompt. This can
    help generate high-quality output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use of languages**: Though ChatGPT performs best with English, it can be
    used to generate responses in several other languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Obtaining the most accurate, up-to-date information**: This can be achieved
    by using the grounding process with a **retrieval augmented generation** (**RAG**)
    architecture and plugins, as discussed in [*Chapter 4*](B21443_04.xhtml#_idTextAnchor070)
    already. This helps in addressing the knowledge cutoff limitation of LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical guidelines for prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompt engineering is a critical stage where AI behavior is molded, and incorporating
    ethics at this level helps ensure that AI language models are developed and deployed
    responsibly. It promotes fairness, transparency, and user trust while avoiding
    potential risks and negative societal impact.
  prefs: []
  type: TYPE_NORMAL
- en: 'While [*Chapter 4*](B21443_04.xhtml#_idTextAnchor070) delved further into constructing
    ethical generative AI solutions, in this section, our focus will be on briefly
    discussing the integration of ethical approaches at the prompt engineering level:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diverse and** **representative data**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When fine-tuning the model with few-shot examples, use training data that represent
    diverse perspectives and demographics.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the AI language model is intended for healthcare, the training data should
    cover medical cases from different demographics and regions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, if a user poses a question to the LLM, such as, “Can you describe
    some global traditional festivals?” the response should offer a comprehensive
    view that encompasses a multitude of countries rather than focusing on just one.
    This can be ensured by including diverse few-shot examples in the prompts.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias detection** **and mitigation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and address biases in the model’s outputs to ensure fairness.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing debiasing techniques to reduce gender or racial biases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that generated content related to sensitive topics is neutral and unbiased.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, if a user asks the LLM, “What is the gender of a nurse?” improperly
    trained models might default to “female” due to biases in their training data.
    To address this, it’s vital to incorporate few-shot examples that emphasize nurses
    can be of any gender, be it male or female.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduce misinformation** **and disinformation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As AI language models can inadvertently generate false or misleading information
    due to model “hallucinations,” implement measures to minimize the spread of misinformation
    and disinformation through carefully crafted prompts and responses.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, based on the guidelines from the prompt engineering section and
    [*Chapter 3*](B21443_03.xhtml#_idTextAnchor052)’s grounding techniques, system
    prompts should clearly state their scope, such as, “Your scope is XYZ.” If a user
    asks about something outside this, such as ABC, the system should have a set response.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy and** **data security**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When engineering prompts, one must prioritize user privacy and data security.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineers should be transparent about data usage, gain user consent,
    and implement safeguards to protect sensitive information.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, when crafting prompts, system messages, or providing few-shot examples,
    it is essential to exclude personal user data such as social security numbers,
    credit card details, and passwords.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content moderation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement mechanisms to filter out harmful or inappropriate content.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use profanity filters to prevent offensive language. Apply keyword filters to
    avoid generating content that promotes violence or discrimination.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if someone asks, “How to create a bomb?”, the LLM should not answer.
    Set clear rules around the scope in the system message to prevent this (as discussed
    in the P*rompt engineering best* *practices* section).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User consent** **and control**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure users are aware of AI interactions and have control over them.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly inform users that they are interacting with an AI language model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, whenever a user initiates a chat with an LLM, they should receive
    a notification that says, “You are now conversing with an LLM,” or a similar message.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular audits** **and testing**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct routine audits and tests regarding prompts to identify and address ethical
    issues.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, users should try various versions of a prompt to verify diverse
    responses, protect user privacy, and follow content moderation guidelines. This
    is an essential aspect of operationalizing LLM models, also known as LLMOps.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education** **and training**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train prompt engineers and developers about ethical AI practices on an ongoing
    basis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethics guidelines** **and policies**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop clear guidelines and policies for prompt engineering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish an ethics charter that outlines the principles followed in prompt
    engineering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining a content safety policy that prohibits harmful or offensive outputs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft’s **Responsible AI** team has been a trailblazer in terms of steering
    the AI revolution with ethical practices. The following figure published by Microsoft
    can serve as a guide to structuring safety metaprompts, focusing on four core
    elements: **response** **grounding**, **tone**, **safety**, and **jailbreaks**.
    This approach is instrumental in implementing a robust safety system within the
    application layer. However, in [*Chapter 9*](B21443_09.xhtml#_idTextAnchor184),
    we will delve into more detail regarding the best practices of responsible AI
    for generative AI applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Metaprompt best practices from Microsoft](img/B21443_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Metaprompt best practices from Microsoft
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, in this chapter, we have outlined the fundamentals of prompt engineering,
    offering insights into how to formulate effective prompts that maximize the potential
    of LLMs. Additionally, we have examined prompt engineering from an ethical perspective.
    Thus far, in this book, we have explored the essential elements and methodologies
    necessary for constructing a solid generative AI framework. In the next chapter,
    we will integrate these concepts with application development strategies for generative
    AI involving agents. We will also discuss methods for operationalizing these strategies
    through LLMOps, which stands as a critical component in the automation process.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Introduction to Prompt* *Engineering*: [https://tinyurl.com/azu5ubma](https://tinyurl.com/azu5ubma)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prompt Engineering* *Guide*: [https://www.promptingguide.ai/](https://www.promptingguide.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Awesome ChatGPT* *prompts*: [https://github.com/f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Best practices for prompt engineering with Open* *AI:* [https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Azure OpenAI Service: Azure OpenAI | Microsoft* *Learn:* [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*System Message* *Framework:* [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/system-message)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ethics in Prompt* *Engineering*: [https://promptengineeringai.in/ethics-of-prompt-engineering/#:~:text=Prompt%20engineering%20should%20respect%20user,their%20data%20is%20being%20used](https://promptengineeringai.in/ethics-of-prompt-engineering/#:~:text=Prompt%20engineering%20should%20respect%20user,their%20data%20is%20being%20used).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ethics of Prompt Engineeering and its potential* *Implications:* [https://promptengineering.guide/article/The_ethics_of_prompt_engineering_and_its_potential_implications.html](https://promptengineering.guide/article/The_ethics_of_prompt_engineering_and_its_potential_implications.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Systematic Survey of Prompt Engineering in Large Language Models: Techniques
    and* *Applications*: ([https://arxiv.org/pdf/2402.07927.pdf](https://arxiv.org/pdf/2402.07927.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chain of Thought* *Prompting*: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: Developing, Operationalizing, and Scaling Generative AI Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore important concepts such as agents, copilots,
    and autonomous agents, alongside discussing prominent application development
    frameworks such as Semantic Kernel and LangChain, as well as the agent collaboration
    framework AutoGen, which are currently very popular. This discussion aims to guide
    you in creating strong autonomous generative AI applications. We will also concentrate
    on strategies for deploying these generative AI applications in a live production
    environment and scaling them efficiently for a large enterprise-wide scenario,
    considering the existing rate limits of **Large Language Model** (**LLM**) APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21443_06.xhtml#_idTextAnchor117), *Developing and Operationalizing
    LLM-Based Cloud Applications: Exploring Dev Frameworks and LLMOps*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21443_07.xhtml#_idTextAnchor143), *Deploying ChatGPT in the
    Cloud: Architecture Design and Scaling Strategies*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
