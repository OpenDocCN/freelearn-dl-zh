- en: '*Chapter 4*: Implementing Sensors'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we discussed in the previous chapter, a character AI system needs to be
    aware of its surrounding environment. For example, **Non-Player Characters** (**NPCs**)
    need to know where the obstacles are, the direction the player is looking, whether
    they are in the player''s sight, and a lot more. The quality of the AI of our
    NPCs depends, for the most part, on the information they can get from the environment.
    Sensor mistakes are apparent to the player: we''ve all experienced playing a video
    game and laughing at an NPC that clearly should have seen us, or, on the other
    hand, been frustrated because an NPC spotted us from behind a wall.'
  prefs: []
  type: TYPE_NORMAL
- en: Video game characters usually get the input information required by their underlying
    AI decision-making algorithms from sensory information. For simplicity, in this
    chapter, we will consider *sensory information* as any kind of data coming from
    the game world. If there's not enough information, characters might show unusual
    behaviors, such as choosing the wrong places to take cover, idling, or looping
    in strange actions without knowing how to proceed. A quick search for AI glitches
    on YouTube opens the door to a vast collection of common funny behaviors of AI,
    even in AAA games.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing sensory systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering what a sensory system is and how to implement two senses—sight and
    touch—in Unity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a demo where we can see our sensory system in action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you just need Unity3D 2022\. You can find the example project
    described in this chapter in the `Chapter 4` folder in the book repository: [https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter04](https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter04).'
  prefs: []
  type: TYPE_NORMAL
- en: Basic sensory systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An AI sensory system emulates senses such as sight, hearing, and even smell
    to get information from other GameObjects. In such a system, the NPCs need to
    examine the environment and check for such senses periodically based on their
    particular interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a minimal sensory system, we have two principal elements: **aspect** (also
    called **event emitters**) and **sense** (also called **event senses**). Every
    sense can perceive only a specific aspect; for instance, an NPC with just the
    sense of hearing can only perceive the sound (one of the aspects) emitted by another
    GameObject, or a zombie NPC can use its sense of smell to prey on the player''s
    brain. As in real life, we do not need a single sense for every NPC; they can
    have sight, smell, and touch all at once.'
  prefs: []
  type: TYPE_NORMAL
- en: In our demo, we'll implement a base interface, called `Sense`, that we'll use
    to implement custom senses. In this chapter, we'll implement sight and touch senses.
    Sight is what we use to see the world around them; if our AI character sees an
    enemy, we receive an event in our code, and we act accordingly by doing some action
    in response. Likewise, with touch, when an enemy gets too close, we want to be
    able to sense that. Finally, we'll implement a minimal `Aspect` class that our
    senses can perceive.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s get started by setting up our scene:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we add a plane as a floor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's create a few walls to block the line of sight from our AI character to
    the enemy. We make these out of short—but wide—cubes that we group under an empty
    GameObject called **Obstacles**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we add a directional light to see what is going on in our scene.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We represent the player with a tank, similar to what we used earlier, and we
    represent the NPCs with simple cubes. We also have a **Target** object to show
    us where the tank is moving in our scene. Our Scene hierarchy should look similar
    to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The setup of the example''s Hierarchy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17984_04_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The setup of the example's Hierarchy
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s position the tank, AI character, and walls randomly in our scene.
    First, make sure to increase the size of the plane to something that looks good.
    Fortunately, in this demo, all the objects are locked on the plane, and there
    is no simulated gravity so that nothing can fall off the plane. Also, be sure
    to adjust the camera so that we can have a clear view of the following scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – The space that our tank and player wander in'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17984_04_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – The space that our tank and player wander in
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the basics set up, let's look at how to implement the tank,
    AI character, and aspects for our player character.
  prefs: []
  type: TYPE_NORMAL
- en: The player's tank and the aspect class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Target** object is a simple sphere object with the mesh render disabled.
    We have also created a point light and made it a child of our Target object. Make
    sure that the light is centered, or it will not be very helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following code in the `Target.cs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Attach this script to the Target object. The script detects the mouse-click
    event and then, using the raycasting technique, detects the mouse-click location
    on the plane in the 3D space, and updates the Target object's position in our
    scene. We will have a look at the player's tank in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: The player's tank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The player's tank is the simple model we used in [*Chapter 2*](B17984_02_Epub.xhtml#_idTextAnchor100),
    *Finite State Machines*, with a non-kinematic `Rigidbody` component. We need the
    `Rigidbody` component to generate trigger events whenever we do collision detection
    with AI characters and environment objects. Finally, we need to assign the **Player**
    tag to our tank.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can easily see from its name, the `PlayerTank` script controls the player''s
    tank. The following is the code for the `PlayerTank.cs` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This script retrieves the Target position on the map and updates the tank''s
    destination point and direction accordingly. The result of the preceding code
    is shown in the following panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – The properties of our Tank object'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17984_04_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – The properties of our Tank object
  prefs: []
  type: TYPE_NORMAL
- en: After we assign the preceding script to the tank, be sure to assign the Target
    object to the `targetTransform` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Aspect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, let's take a look at the `Aspect` class. `Aspect` is an elementary class
    with just one public property, called `aspectName`. That's all the variables we
    need in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever our AI character senses something, we''ll check this against `aspectName`
    to see whether it''s the aspect that the AI has been looking for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Attach this aspect script to our player's tank and set the `aspectName` property
    as `Player`.
  prefs: []
  type: TYPE_NORMAL
- en: AI characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, the AI characters roam around the scene in a random direction.
    They have two senses: sight and touch. The sight sense checks whether the enemy
    aspect is within a set visible range and distance. Touch detects whether the enemy
    aspect has collided with the `Box Collider` around the character. As we have seen
    previously, our player''s tank has the `Player` aspect. Consequently, these senses
    are triggered when they detect the player''s tank.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, let''s look at the script we use to move the NPCs around:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `Wander` script generates a new random position in a specified range whenever
    an AI character reaches its current destination point. Then, the `Update` method
    rotates the NPCs and moves them toward their new destination. Attach this script
    to our AI character so that it can move around in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Sense
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Sense` class is the interface of our sensory system that the other custom
    senses can implement. It defines two virtual methods, `Initialize` and `UpdateSense`,
    executed from the `Start` and `Update` methods, respectively, and that we can
    override when implementing custom senses as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The basic properties of this script are the intervals between two consecutive
    sensing operations and the name of the aspect it should look for. This script
    is not attached to any objects; instead, we use it as a base for specific senses,
    such as `Sight` and `Touch`.
  prefs: []
  type: TYPE_NORMAL
- en: Sight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Sight` sense detects whether a specific aspect is within the perception
    field of the character. If it perceives anything, it takes the specified action
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We need to implement the `Initialize` and `UpdateSense` methods of the parent
    `Sense` class, respectively. Then, in the `DetectAspect` method, we first check
    the angle between the player and the AI's current direction. Then, if it's in
    the field-of-view range, we shoot a ray in the direction of the player's tank.
    The length of the ray is the value in the visible distance property.
  prefs: []
  type: TYPE_NORMAL
- en: The `Raycast` method returns when it first hits another object. Then, we check
    this against the aspect component and the aspect name. In this way, even if the
    player is in the visible range, the AI character will not see the player if they
    hide behind a wall.
  prefs: []
  type: TYPE_NORMAL
- en: The `OnDrawGizmos` method draws lines based on the perspective field (determined
    by the view angle and viewing distance) to see the AI character's line of sight
    in the editor window during playtesting. Attach this script to the AI character,
    and ensure to set the aspect name to `Enemy`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method can be illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`OnDrawGizmos` is an event function that we can use when we want to draw gizmos
    in the scene. `DrawLine`, `DrawIcon`, and `DrawSphere`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'They are a handy way to quickly provide some visual feedback to our algorithms.
    You can learn more about the gizmo functions by following this link: [https://docs.unity3d.com/ScriptReference/Gizmos.html](https://docs.unity3d.com/ScriptReference/Gizmos.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Touch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another sense we''re going to implement is `Touch`, which is triggered when
    the player entity is within a specific range of the AI entity as shown in the
    following code block. Our AI character has a box collider component, and its `Is
    Trigger` flag is on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We need to implement the `OnTriggerEnter` event fired whenever the collider
    component collides with another collider component. Since our tank entity also
    has collider and `Rigidbody` components, a collision event occurs as soon as the
    colliders of the AI character and the player's tank coincide.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the box collider of our enemy AI that we are
    using to implement the `Touch` sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – The collider component around our player'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17984_04_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – The collider component around our player
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see how our AI character is set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Properties of our player'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17984_04_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Properties of our player
  prefs: []
  type: TYPE_NORMAL
- en: Inside the `OnTriggerEnter` method, we access the aspect component of the other
    collider entity and check whether the name of the aspect is the same aspect that
    this AI character is looking for. For demonstration purposes, we print out in
    the console that the character detects the enemy aspect by the `Touch` sense.
    In a real game, we would not print the event but rather trigger other actions,
    such as turning to face an enemy and then chasing, attacking, and so on. Let's
    move on to testing our game.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, play the game in Unity3D and move the player's tank near the wandering
    AI character by clicking on the ground. You should see the **Enemy touch detected**
    message in the console log window whenever our AI character gets close to our
    player's tank.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Our player and tank in action'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17984_04_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Our player and tank in action
  prefs: []
  type: TYPE_NORMAL
- en: The previous screenshot shows an AI agent with touch and perspective senses
    looking for an enemy aspect. Move the player's tank in front of the AI character,
    and you'll get the *Enemy detected* message. If you go into the editor view while
    running the game, you should see the rendered debug drawings thanks to the `OnDrawGizmos`
    method implemented in the `Sight sense` class.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the concept of using sensors in implementing game AI,
    and we implemented two senses, `Sight` and `Touch`, for our AI character. The
    sensory system is just the first element of the decision-making system of a whole
    AI system. For example, we can use the sensory system to control the execution
    of a behavior system or change the state of a Finite State Machine once we have
    detected an enemy within the AI's line of sight.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover how to apply behavior tree systems in [*Chapter 9*](B17984_09_Epub.xhtml#_idTextAnchor487),
    *Behavior Trees*. In the meantime, in the next chapter, we'll look at how to implement
    flocking behaviors in Unity3D, as well as how to implement Craig Reynold's flocking
    algorithm.
  prefs: []
  type: TYPE_NORMAL
