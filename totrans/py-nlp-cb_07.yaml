- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualizing Text Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is dedicated to creating visualizations for the different aspects
    of NLP work, much of which we have done in previous chapters. Visualizations are
    important when working with NLP tasks, as they help us to easier see the big picture
    of the work accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: We will create different types of visualizations, including visualizations of
    grammar details, parts of speech, and topic models. After working through this
    chapter, you will be well equipped to create compelling images to show and explain
    the outputs of various NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the recipes you will find in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the dependency parse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing parts of speech
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing NER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a confusion matrix plot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing word clouds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing topics from Gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing topics from BERTopic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the following packages in this chapter: `spacy`, `matplotlib`,
    `wordcloud`, and `pyldavis`. They are part of the `poetry` environment and the
    `requirements.txt` file.'
  prefs: []
  type: TYPE_NORMAL
- en: We will be using two datasets in this chapter. The first is the BBC news dataset,
    located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json)
    and [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is used in this book with permission from the researchers. The
    original paper associated with this dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Derek Greene and Pádraig Cunningham. “Practical Solutions to the Problem of
    Diagonal Dominance in Kernel Document Clustering,” in Proc. 23rd International
    Conference on Machine Learning (ICML’06), 2006.
  prefs: []
  type: TYPE_NORMAL
- en: All rights, including copyright, in the text content of the original articles
    are owned by the BBC.
  prefs: []
  type: TYPE_NORMAL
- en: The second is the Sherlock Holmes text, located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/sherlock_holmes.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/sherlock_holmes.txt).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the dependency parse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use the `displaCy` library and visualize
    the dependency parse. It shows us the grammatical relations between words in a
    piece of text, usually a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Details about how to create a dependency parse can be found in [*Chapter 2*](B18411_02.xhtml#_idTextAnchor042),
    in the *Getting the dependency parse* recipe. We will create two visualizations,
    one for a short text and another for a long multi-sentence text.
  prefs: []
  type: TYPE_NORMAL
- en: After working through this recipe, you will be able to create visualizations
    of grammatical structures with different options for formatting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `displaCy` library is part of the `spacy` package. You need at least version
    2.0.12 of the `spacy` package for `displaCy` to work. The version in the `poetry`
    environment and `requirements.txt` is 3.6.1.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.1_dependency_parse.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.1_dependency_parse.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To visualize the dependency parse, we will use the functionality of the `displaCy`
    package to first show one sentence, and then two sentences together:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the language utilities file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the input text and process it using the small model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now define different visualization options. The `render` command, we
    provide these options as an argument. We set the `jupyter` parameter to `True`
    for the visualization to work correctly in the notebook. You can omit the argument
    for non-Jupyter visualizations. We set the `style` parameter to `''dep''`, as
    we would like to have a dependency parse output. The output is a visual representation
    of the dependency parse:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is shown in *Figure 7**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18411_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Dependency parse visualization
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we save the visualization to a file. We first import the **Path**
    object from the **pathlib** package. We then initialize a string with the path
    where we want to save the file and create a **Path** object. We use the same **render**
    command, this time saving the output in a variable and setting the **jupyter**
    parameter to **False**. We then use the **output_path** object and write the output
    to the corresponding file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create the dependency parse and save it at `../data/dep_parse_viz.svg`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s define a longer text and process it using the small model. This
    way, we will be able to see how **displaCy** deals with longer texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we visualize the new text. This time, we have to input a list of sentences
    from the processed **spacy** object to indicate that there is more than one sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output should look like in *Figure 7**.2*. We see that the output for the
    second sentence starts on a new line.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Several sentences dependency parse visualization
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing parts of speech
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we visualize part of speech counts. Specifically, we count the
    number of infinitives and past or present verbs in the book *The Adventures of
    Sherlock Holmes*. This can give us an idea about whether the text mostly talks
    about past or present events. We could imagine that similar tools could be used
    to evaluate the quality of a text; for example, a book with very few adjectives
    but many nouns would not work very well as a fiction book.
  prefs: []
  type: TYPE_NORMAL
- en: After working through this recipe, you will be able to use the `matplotlib`
    package to create bar plots of different verb types, which are tagged using the
    `spacy` package.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `spacy` package for text analysis and the `matplotlib` package
    to create the graph. They are part of the `poetry` environment and the `requirements.txt`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.2_parts_of_speech.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.2_parts_of_speech.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a function that will count the number of verbs by tense and
    plot each on a bar graph:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the file and language utilities files. The language utilities notebook
    loads the **spacy** model, and the file utilities notebook loads the **read_text_file**
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the text of the Sherlock Holmes book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we define the verb tag lists, one for present tense and one for past
    tense. We do not define another list, but use it in the next step, and that is
    the infinitive verb, which only has one tag, **VB**. If you went through the *Part-of-speech
    tagging* recipe in [*Chapter 1*](B18411_01.xhtml#_idTextAnchor013), you will notice
    that the tags are different from the **spacy** tags used there. These tags are
    more detailed and use the **tag_** attribute instead of the **pos_** attribute
    that is used in the simplified tagset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the **visualize_verbs** function. The input to the
    function is the text and the **spacy** model. We check each token’s **tag_** attribute
    and add the counts of present, past, and infinitive verbs to a dictionary. We
    then use the **pyplot** interface to plot those counts in a bar graph. We use
    the **bar** function to define the bar graph. The first argument lists the *x*
    coordinates of the bars. The next argument is a list of heights of the bars. We
    also set the **align** parameter to “center” and provide the colors for the bars
    using the **color** parameter. The **xticks** function sets the labels for the
    *x* axis. Finally, we use the **show** function to display the resulting plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the **visualize_verbs** function on the text of the Sherlock Holmes book
    using the small **spacy** model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create the graph in *Figure 7**.3*. We see that most of the verbs
    in the book are past tense, which makes sense for a novel. However, there is also
    a sizable number of present tense verbs, which could be part of direct speech.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Infinitive, past, and present verbs in The Adventures of Sherlock
    Holmes
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`displacy` package to create compelling and easy-to-read images.'
  prefs: []
  type: TYPE_NORMAL
- en: After working through this recipe, you will be able to create visualizations
    of named entities in a text using different formatting options and save the results
    in a file.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `displaCy` library is part of the `spacy` package. You need at least version
    2.0.12 of the `spacy` package for `displaCy` to work. The version in the `poetry`
    environment and `requirements.txt` file is 3.6.1.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.3_ner.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.3_ner.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use `spacy` to parse the sentence and then the `displacy` engine to
    visualize the named entities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import both **spacy** and **displacy**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the language utilities file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the text to process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we process the text using the small model. This gives us a **Doc**
    object. We then modify the object to contain a title. This title will be part
    of the NER visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we set up color options for the visualization display. We set green for
    the **ORG**-labeled text and yellow for the **PERSON**-labeled text. We then set
    the **options** variable, which contains the colors. Finally, we use the **render**
    command to display the visualization. As arguments, we provide the **Doc** object
    and the options we previously defined. We also set the **style** argument to **"ent"**,
    as we would like to display just entities. We set the **jupyter** argument to
    **True** in order to display directly in the notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output should look like that in *Figure 7**.4*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Named entities visualization
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we save the visualization to an HTML file. We first define the **path**
    variable. Then, we use the same **render** command, but we set the **jupyter**
    argument to **False** this time and assign the output of the command to the **html**
    variable. We then open the file, write the HTML, and close the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create an HTML file with the entities visualization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Creating a confusion matrix plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with machine learning models, for example, NLP classification models,
    creating a confusion matrix plot can be a very good tool to see the mistakes that
    the model makes to then further refine it. The model “confuses” one class for
    another, hence the name **confusion matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: After working through this recipe, you will be able to create an SVM model,
    evaluate it, and then create a confusion matrix visualization that will tell you
    in detail which mistakes the model makes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create an SVM classifier for the BBC news dataset using the sentence
    transformer model as the vectorizer. We will then use the `ConfusionMatrixDisplay`
    object to create a more informative confusion matrix. The classifier is the same
    as in the [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106) recipe *Using SVMs for
    supervised* *text classification*.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_train.json)
    and [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/data/bbc_test.json).
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.4_confusion_matrix.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.4_confusion_matrix.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary packages and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the simple classifier utilities file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the training and test data and shuffle the training data. We shuffle
    the data so that there are no long sequences of one class, which might either
    bias the model during training or exclude large chunks of some classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load the transformer model and create the **get_sentence_vector**
    function. The function takes as arguments the text and the model, then creates
    and returns the vector. The **encode** method takes in a list of text, so in order
    to encode one piece of text, we need to put it into a list, and then get the first
    element of the **return** object, since the model also returns a list of encoding
    vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we create the **train_classifier** function. The function takes in vectorized
    input and the correct answers. It then creates and trains an SVC object and returns
    it. It could take a few minutes to finish training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we train and test the classifier. First, we create a list with
    the target labels. We then create a **vectorize** function that uses the **get_sentence_vector**
    function but specifies the model to use. We then use the **create_train_test_data**
    function from the simple classifier utilities file to get the vectorized input
    and labels for both the training and test sets. This function takes in the training
    and test dataframes, the vectorizing method, and the name of the column where
    the text is located. The results are the vectorized training and test data and
    the true labels for both. Then, we use the **train_classifier** function to create
    a trained SVM classifier. We print the classification report for the training
    data and use the **test_classifier** function to print the classification report
    for the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we create a mapping from number labels to text labels and then create
    a new column in the test dataframe that shows the text label prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create a confusion matrix using the **sklearn** **confusion_matrix**
    function. The function takes as input the true labels, the predictions, and the
    names of the categories. We then create a **ConfusionMatrixDisplay** object that
    takes in that confusion matrix and the names to display. We then create the confusion
    matrix plot using the object and display it using the **matplotlib** library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is shown in *Figure 7**.5*. The resulting plot clearly shows which
    classes have overlaps and their number. For example, it is easy to see that there
    are two examples that are predicted to be about business but are actually about
    politics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Confusion matrix visualization
  prefs: []
  type: TYPE_NORMAL
- en: Constructing word clouds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word clouds are a nice visualization tool to quickly see topics that are prevalent
    in a text. They can be used at the preliminary data analysis stage and for illustration
    purposes. A distinguishing feature of word clouds is that larger-font words signify
    a more frequent topic, while smaller-font words signify less frequent topics.
  prefs: []
  type: TYPE_NORMAL
- en: After working through this recipe, you will be able to create word clouds from
    a text and also apply a picture mask on top of the word cloud, which makes for
    a cool image.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the text of the book *The Adventures of Sherlock Holmes* and the
    picture mask we will use is a silhouette of Sherlock Holmes’ head.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `wordcloud` package for this recipe. In order to display the
    image, we need the `matplotlib` package as well. They are both part of the `poetry`
    environment and the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.5_word_clouds.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.5_word_clouds.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary packages and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the file utilities notebook. We will use the **read_text_file** function
    from this notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the book text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define the **create_wordcloud** function. The function takes
    as arguments the text to be processed, stopwords, the filename of where to save
    the result, and whether to apply a mask over the image (**None** by default).
    It creates the **WordCloud** object, saves it to the file, and then outputs the
    resulting plot. The options that we provide to the **WordCloud** object are the
    minimum font size, the maximum font size, the width and height, the maximum number
    of words, and the background color:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the **create_wordcloud** function on the text of the Sherlock Holmes book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will save the result in the file located at `data/sherlock_wc.png` and
    create the visualization displayed in *Figure 7.6* (your results might look slightly
    different).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18411_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Sherlock Holmes word cloud visualization
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also apply a mask to the word cloud. Here, we will apply a Sherlock
    Holmes silhouette to the word cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the additional imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the mask image and save it as a **numpy** array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the function on the text of the Sherlock Holmes book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will save the result in the file located at `data/sherlock_mask.png` and
    create the visualization shown in *Figure 7**.7* (your result might be slightly
    different):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18411_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Word cloud with mask
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please see the `wordcloud` docs, [https://amueller.github.io/word_cloud/](https://amueller.github.io/word_cloud/),
    for more options.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing topics from Gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will visualize the **Latent Dirichlet Allocation** (**LDA**)
    topic model that we created in [*Chapter 6*](B18411_06.xhtml#_idTextAnchor156).
    The visualization will allow us to quickly see words that are most relevant to
    a topic and the distances between topics.
  prefs: []
  type: TYPE_NORMAL
- en: After working through this recipe, you will be able to load an existing LDA
    model and create a visualization for its topics, both in Jupyter and saved as
    an HTML file.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `pyLDAvis` package to create the visualization. It is available
    in the `poetry` environment and the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: We will load the model we created in [*Chapter 6*](B18411_06.xhtml#_idTextAnchor156)
    and then use the `pyLDAvis` package to create the topic model visualization.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.6_topics_gensim.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter07/7.6_topics_gensim.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary packages and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the paths to the model files. The model was trained in [*Chapter 6*](B18411_06.xhtml#_idTextAnchor156):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load the objects that these paths point to. If you get a **FileNotFoundError**
    error at this step, it means that you have not created the dictionary, corpus,
    and model files. In that case, go back to [*Chapter 6*](B18411_06.xhtml#_idTextAnchor156),
    the *LDA topic modeling with Gensim* recipe, and create the model and the accompanying
    files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we create the **PreparedData** object using the preceding files and save
    the visualization as HTML. The object is required for the visualization methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we enable the Jupyter **display** option and display the visualization
    in the notebook. You will see the topics and the words that are important for
    each topic. To select a particular topic, hover over it with the mouse. You will
    see the most important words for each topic change while hovering over them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create the visualization in *Figure 7**.8* (your results might vary):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – LDA model visualization
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using `pyLDAvis`, it is also possible to visualize models created using `sklearn`.
    See the package documentation for more information: [https://github.com/bmabey/pyLDAvis](https://github.com/bmabey/pyLDAvis).'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing topics from BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will create and visualize a BERTopic model on the BBC data.
    There are several visualizations available with the BERTopic package, and we will
    use several of them.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a topic model in a similar fashion as in [*Chapter
    6*](B18411_06.xhtml#_idTextAnchor156), in the *Topic modeling using BERTopic*
    recipe. However, unlike in [*Chapter 6*](B18411_06.xhtml#_idTextAnchor156), we
    will not limit the number of topics created, and resulting in more than the 5
    original topics in the data. It will allow for more interesting visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `BERTopic` package to create the visualization. It is available
    in the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary packages and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the language utilities file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we create a list of training documents from the dataframe object. We then
    initialize a representation model object. Here, we use the **KeyBERTInspired**
    object, which uses BERT to extract the keywords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This object creates the names (representations) for the topics; it does a better
    job than the default version, which contains lots of stopwords. We then create
    the main topic model object and fit it to the document set. In this recipe, in
    contrast to the *Topic modeling using BERTopic* recipe in [*Chapter 6*](B18411_06.xhtml#_idTextAnchor156),
    we do not place limits on the number of created topics. This will create a lot
    more topics:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we display the general topic visualization. It shows all 42 topics
    created. If you hover on each circle, you will see the topic representation or
    name. The representations consist of the top five words in the topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create the visualization in *Figure 7**.9* (your results might vary).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – BERTopic model visualization
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we create a visualization of the topic hierarchy. This hierarchy clusters
    the different topics together if they are related. We first create the hierarchy
    by using the **hierarchical_topics** function of the topic model object, and then
    pass it into the **visualize_hierarchy** function. The nodes that combine the
    different topics have their own names that you can see if you hover over them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create the visualization in *Figure 7**.10*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – BERTopic hierarchical visualization
  prefs: []
  type: TYPE_NORMAL
- en: If you hover over the nodes, you will see their names.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we create a bar chart with the top words for the topics. We specify
    the number of topics to show by using the **top_n_topics** argument that the **visualize_barchart**
    function of the topic model object takes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create a visualization similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – BERTopic word scores
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we create a visualization of individual documents in the training set.
    We provide the list of documents created in *step 4* to the **visualize_documents**
    function. It clusters the documents to the topics. You can see the documents if
    you hover over the individual circles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be a visualization similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.12 – BERTopic document visualization
  prefs: []
  type: TYPE_NORMAL
- en: If you hover over the nodes, you will see the text of the individual documents.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are additional visualization tools available through BERTopic. See the
    package documentation for more information: [https://maartengr.github.io/BERTopic/index.html](https://maartengr.github.io/BERTopic/index.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about **KeyBERTInspired**, see [https://maartengr.github.io/BERTopic/api/representation/keybert.html](https://maartengr.github.io/BERTopic/api/representation/keybert.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
