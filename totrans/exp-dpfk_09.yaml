- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Future of Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it sometimes might feel like we’re already living in the future with deepfakes
    and AI-generated images, the technology behind them is really just beginning to
    take off. As we move forward, the capabilities of these generative AIs will only
    become more powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is not unbounded futurism but will instead look at specific generative
    AIs and where they are improving. We will examine the following technologies and
    how they are changing. We’ll discuss the future of the following areas of AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving image quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-guided image generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating sound
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deepfakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recently, text generation models made a major impact when they came into the
    public consciousness with OpenAI’s success with ChatGPT in 2022\. However, text
    generation was among the first uses of AI. Eliza was the first **chatbot** ever
    developed, back in 1966, before all but the most technically inclined people had
    even seen a computer themselves. The personal computer wouldn’t even be invented
    for another 5 years, in 1971\. However, it’s only recently that truly impressive
    chatbots have been developed.
  prefs: []
  type: TYPE_NORMAL
- en: Recent developments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A type of model called **transformers** is responsible for the recent burst
    in language models. Transformers are neural networks that are comprised entirely
    of a layer called an **attention layer**. Attention layers work sort of like a
    spotlight, focusing on the part of the data that is most likely to be important.
    This lets transformers (and other models that use attention layers) be a lot deeper
    without losing “focus” (though that’s not actually the technical term for it,
    it’s a good metaphor for the effect).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the ability to make very deep models, transformers excel at tasks
    that need to pull meaning from complicated structures. This is a perfect match
    for language tasks and other long sequences where a word’s meaning depends heavily
    on those words around it (think about the difference between “*I ate an orange
    fruit*” and “*I ate an orange*”). Transformers are used heavily for tasks such
    as translation, but they also are excellent for other tasks such as answering
    questions, summarizing information, and the subject of this section: text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Most language tasks require not just the ability to “understand” the text but
    also to create new text. For example, in a summarizing task, you cannot simply
    take the important words from a document and copy them out to the user. The words
    would be meaningless, as they’d be in an arbitrary order and have none of the
    important context that language is built on. So, these models must also create
    realistic and understandable output in the form of properly constructed sentences
    for the user to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of debate as to whether you can use a uniquely human concept
    of understanding to describe anything a computer does. One famous example is the
    so-called **Chinese Room** thought experiment by John Searle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basics of the thought experiment are this: imagine a room where there is
    a huge array of filing cabinets. In each of these filing cabinets, there are cards
    that pair a certain input to a certain output. The room can take as input and
    output other cards on which Chinese characters are written. Inside the room, there
    is someone (or something) who knows no Chinese, but by matching the incoming cards
    with the cards in the filing cabinets, then copying the card’s output characters
    onto another card, can output perfect Chinese.'
  prefs: []
  type: TYPE_NORMAL
- en: In that context, can you say that the machine (or the person or thing inside
    the machine) “understands” Chinese? What about the room itself? The question has
    led to a lively debate among philosophers, computer scientists, linguists, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Building sentences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Eliza, that first chatbot, built its sentences by simply repeating back words
    from the input sentences into one of a number of pre-built sentences that were
    written carefully to let those words be put into them. This process of parroting
    parts of the user’s responses back was good enough to serve as a very simple Freudian
    counselor, which many people felt helped them to get through some situations.
  prefs: []
  type: TYPE_NORMAL
- en: Today’s generative models instead build sentences with extremely advanced sentence
    structures. They do this by building sentences in a way not unlike a jigsaw puzzle.
    The model uses a concept called a **beam** to search for the word that fits best
    into a given part of a sentence, much like how a jigsaw solver searches over a
    set of pieces to find the ones that best fit the available space. Unfortunately,
    this is where the metaphor becomes strained. A single beam keeps track of just
    one sequence, and unlike a normal jigsaw, a sentence can be made of a near-infinite
    combination of words. Because of this, a model needs to keep track of several
    beams as the best scoring beam may change as the sentence gets completed. More
    beams lead to a better-generated sentence, both **grammatically** and **semantically**
    (that is, both the structure and meaning) but require more computation as the
    model solves a larger number of sentences at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The future of text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we move into the future, these models will improve in two ways; computational
    availability will increase, making it feasible to get larger models with larger
    beam searches, and design efficiency will allow for better-designed models that
    can do more with the same resources. You can think of this as something like packing
    boxes into a truck: you can only fit so many boxes in a truck until you need to
    either get a bigger truck or find a way to make the boxes smaller. Transformers
    are an example of design efficiency, while the fact that computers always get
    faster and better is an example of computational availability.'
  prefs: []
  type: TYPE_NORMAL
- en: It may seem that text generation has endless grounds for improvement, but there
    are questions as to how sustainable the long-term growth of language models may
    be. There are concerns that we might reach the limits of **large language models**
    (**LLMs**) relatively quickly. Multiple constraints exist that may limit the models.
  prefs: []
  type: TYPE_NORMAL
- en: The first limit on language models is that the largest models already require
    weeks or months to train, spread across thousands of powerful computers. The biggest
    limitations of these types of models may be economics. There simply might not
    be enough demand to keep growing the models as the costs continue to skyrocket.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second major limit to language models is far harder to get past: there
    simply might not be enough data to keep growing. LLMs require a massive amount
    of data to be able to learn languages, and we are quickly approaching the point
    where they could be trained on the entire output of humanity’s written history
    and still not have enough data to fully train the model. Deepmind published ([https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556))
    a study of LLMs that found that the compute and data needed to completely train
    a model must scale up in tandem. That means that if you double the amount of compute
    a model needs to train, you should also double the data. This means that it’s
    entirely possible that models will soon be limited in how well they can learn
    language because they’ve been trained on everything ever written by humanity.'
  prefs: []
  type: TYPE_NORMAL
- en: Both of these problems have potential solutions, but they’re nowhere near as
    easy to come by as throwing more compute or smarter model designs at the problem.
    You might ask something such as “*but how can humans learn language without reading
    every text humanity has ever written*?” or something similar. The reason is that
    humans are fully “intelligent,” a term whose definition seems to change every
    time something that is not human gets close to it. In the end, it’s possible that
    we will be unable to get a computer to fully understand language until we can
    get them to “understand” in a way that philosophers cannot debate. Until then,
    language models need a lot more data than humans to reach high levels of language
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation may have been one of the first uses of AI, but arguably a more
    significant use until recently has been improving image quality.
  prefs: []
  type: TYPE_NORMAL
- en: Improving image quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The earliest known image that was taken through mechanical means is the *Niépce
    héliographie*, which was taken by Joseph Nicéphore Niépce in 1827\. It was taken
    through the window of his workshop by exposing a pewter plate covered in a thin
    layer of a concoction made from lavender and bitumen. This plate was exposed to
    sunlight for several days to create a blurry, monochrome image.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 9.1 – The \uFEFFNiépce Heliograph taken by Joseph Nicéphore Niépce\
    \ in 1827](img/B17535_09_001.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – The Niépce Heliograph taken by Joseph Nicéphore Niépce in 1827
  prefs: []
  type: TYPE_NORMAL
- en: 'Since then, images have gotten better at capturing reality, but the process
    has not been *perfected*. There are always various limitations that mean that
    the images aren’t quite accurate in color, cannot capture all the details, or
    cause distortions in the image. A truly perfect image is mindboggling to even
    consider: from a single perfect image, you’d be able to zoom into any atom even
    on the other side of the universe and our images simply cannot accomplish that
    level of detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Various tactics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, the limitations of the image are more than we’re willing to accept.
    In this case, we can try to “improve” the image by modifying it to look more like
    what we want. Some of these edits are relatively easy. Perhaps your photo was
    taken at an angle, and everything is crooked. In the age of physical photo prints,
    this could be fixed with nothing but a pair of scissors (and maybe a mat to hide
    the smaller size). Many cameras with a flash attached have built-in “red eye”
    removal, which masks the reflection of the flash in people’s eyes by firing the
    flash twice in quick succession, once to make your eyes adjust to the flash and
    the second to actually take the photo.
  prefs: []
  type: TYPE_NORMAL
- en: Now you can fix both of these problems in software, and you can fix a lot more
    than crooked photos or a bright light reflecting off the back of your eye. Modern
    editing software can even “fix” things that people would argue aren’t actually
    broken. There is an ongoing discussion in society whether the images on the covers
    of magazines should be “brushed up” to make the subject more attractive or whether
    the process creates more harm than good by perpetuating an unrealistic body image.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve reached the age of neural networks, the automatic improvement of photos
    has become easier than ever. There are numerous services and tools offering to
    improve photos, whether they have been newly taken or are older. They work on
    a variety of techniques but fall into several different categories, including
    **upscaling**, **color correction** (or **grading**), and **denoising**. Let’s
    look at each of these briefly.
  prefs: []
  type: TYPE_NORMAL
- en: Upscaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Upscaling involves taking an image and scaling it up. That is, making an image
    have a higher resolution. Modern upscaling AIs have the ability to not just increase
    the resolution but also the fidelity – they can make images both bigger and better.
    This innovation comes from learning from lots of other images about what objects,
    textures, and images look like both large and shrunk down. The model, after being
    fed an artificially downscaled image, is asked to recreate the original image
    and is then reviewed on how well it did.
  prefs: []
  type: TYPE_NORMAL
- en: Making an image larger and higher in fidelity can be a balancing act. It’s possible
    to make images *too* sharp or fill in data that shouldn’t be there. For example,
    you don’t want to add wrinkles to a child, but a photo of your grandmother without
    her wrinkles just wouldn’t look right. For this reason, most upscaling has some
    sort of slider or other control to set how much additional fidelity you want the
    AI to give your images, letting you choose case by case what level of detail should
    be added.
  prefs: []
  type: TYPE_NORMAL
- en: There is one type of upscaling that sidesteps this limitation, and that is **domain-specific
    upscaling**. A domain-specific upscaler is one that upscales only one type of
    content. A recently popular example of this is face upscaling. Humans are hardwired
    to see faces, and we will notice any issues with them very easily. But when you
    take an upscaler and train it on nothing but faces, it’s able to learn specifically
    about faces and get far higher fidelity results when doing that one task instead
    of having to worry about upscaling everything. This lets the AI learn that “old
    people” tend to have wrinkles, something that a general purpose upscaler just
    doesn’t have the space to learn.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques of domain-specific upscaling require additional processes such
    as detection and compositing to detect the objects that the upscaler can improve
    and ensure that they get put back into the image correctly. This does lead to
    a different potential problem where the objects are of a higher quality than the
    rest of the image, but unless the difference is drastic, people don’t seem to
    notice very much.
  prefs: []
  type: TYPE_NORMAL
- en: Color correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Color correction is a process that takes place in every camera device anywhere.
    Taking an image from the camera sensor and turning it into an image that we can
    look at requires many different processes, one of which is color correction –
    if for nothing else than to correct for the color of the lighting that the picture
    was taken in. Our eyes and brains automatically correct for light color, but cameras
    need to do it explicitly. Even once a photo is taken, that doesn’t mean that color
    correction is done. Most professional photographers take pictures in a “raw” format
    that allows them to set the colors exactly, without the loss of converting them
    multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: One lie that our cameras tell us is that an image is made up of many pixels
    that have red, blue, and green elements. In reality, our cameras take separate
    images of each color and then join them together in a process called **debayering**.
    In fact, most cameras cheat even more by having twice as many green sub-pixels
    as red or blue, as human eyes are more sensitive to details in green. The problem
    is that these pixels never line up perfectly when re-aligned, and poorly done
    debayering means that your images will have weird color artifacts at the edges
    of the colors.
  prefs: []
  type: TYPE_NORMAL
- en: However, color is not just an objective thing. Think about the film *The Matrix*.
    Have you ever noticed how green everything is inside the Matrix? That was done
    on purpose. This use of color is called “grading” (though confusingly, sometimes,
    correction is also called grading). Humans are weirdly connected to colors on
    an emotional level, and changing them can tweak our emotional connection to an
    image. For example, think about how we say “blue” to mean “sad” or how an image
    full of browns and reds might make you feel a chill like autumn has just begun.
  prefs: []
  type: TYPE_NORMAL
- en: Both of these color tasks can be performed by AI. There are AI tools out there
    that can convert an image to match a particular emotional style or to match another
    image that you have. Here, the trick for the AI is not in changing the colors
    of an image, it’s in changing them in a particular way that will be pleasing to
    the humans involved. You probably don’t want to turn your green shoes into red
    just because you ask the AI to turn the photo to fall colors, but the green leaves
    need to be adjusted.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Noise is another of the inevitable issues with pictures that today’s cameras
    do their best to hide, but it’s a part of any image you take – especially in dark
    conditions. Noise reduction in cameras usually works by applying a slight blur
    to all pixels in the image (often before the color correction happens). This can
    help to smooth out any noise coming from the sensor, but it can’t do a perfect
    job. Fancier techniques have become extremely common since modern cameras now
    have substantial computer power in them (and many are in our phones). In fact,
    most of today’s phone processors have some sort of AI acceleration just so that
    they can improve the photos being taken.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising can be done in many different ways, but a recent innovation is diffusion
    models. Diffusion models are really just using a **diffusion** process to train
    a neural network – one in which the image is given to the model with blur or noise
    added and it’s tasked with providing the original image. This process can be made
    deeper, by repeating the diffusion process multiple times and asking the model
    to clean it up each time. This allows a model to create incredibly detailed images
    from very little information. Unfortunately, information loss is information loss,
    and while diffusion models can create new information, restoring the original
    information is impossible without some kind of guidance. This type of AI denoising
    is actually responsible for one of the innovations we’ll talk about more in the
    *Text-guided image* *generation* section.
  prefs: []
  type: TYPE_NORMAL
- en: The future of image quality upgrading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a fundamental question that limits what an AI can do for image quality.
    Is it okay for the AI to “invent” a new detail that doesn’t belong there? If the
    answer is yes, then there is no practical limit to what AI upscaling can accomplish.
    It’s possible that we could have AI upscalers that generate the very atoms of
    a person’s skin if we zoom in far enough. Unfortunately, the answer is generally
    considered to be “no.” We are picky about our images and want them to be not just
    detailed, but accurate. This means that quality and detail will always be limited,
    and there isn’t a whole lot we can do to do to avoid that.
  prefs: []
  type: TYPE_NORMAL
- en: That’s not to say that it’s impossible to improve further. It’s possible that
    we might be able to make AIs capable of getting the details they need from other
    sources. For example, right now, we have video upscalers that can upscale a frame
    of video by borrowing data from the frames that surround them. This lets the AI
    look beyond the current frame to find patterns and details that it can copy back
    to the working frame. You might be able to upscale your old family home videos
    by providing photographs to fill in the missing quality. Additionally, you might
    have an AI that learns what a given person looked like at various points in their
    life (for example, your school yearbooks and family photos) that can then interpolate
    that data to fill in specific times of your video.
  prefs: []
  type: TYPE_NORMAL
- en: The quest for higher quality will never end, and people will always want to
    see the next improvement. That said, there are practical limits that prevent a
    given image from being improved forever. However, what if you don’t care about
    accuracy, and you just want the greatest quality image possible? Maybe an image
    that is created from just a sentence or two? Well, that’s what we’ll look at next.
  prefs: []
  type: TYPE_NORMAL
- en: Text-guided image generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text-guided image generation is an interesting category of generative AI. OpenAI
    had several developers release a paper called *Learning Transferable Visual Models
    From Natural Language Supervision* (https://arxiv.org/abs/2103.00020). Though
    I prefer the summary title they posted on their blog *CLIP: Connecting Text and
    Images*. CLIP was mentioned in [*Chapter 8*](B17535_08.xhtml#_idTextAnchor136),
    *Applying the Lessons of Deepfakes*, but we’ll talk about it some more here.'
  prefs: []
  type: TYPE_NORMAL
- en: CLIP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CLIP is actually a pair of neural network encoders. One is trained on images
    while the other is trained on text. So far, this isn’t very unusual. The real
    trick comes from how the two are linked. Essentially, both encoders are passed
    data from the same image; the image encoder gets the image, the text encoder gets
    the image’s description, and then the encoding they generate is compared to each
    other. This training methodology effectively trains two separate models to create
    the same output given related inputs.
  prefs: []
  type: TYPE_NORMAL
- en: That might still sound confusing, so let’s look at it another way. Imagine a
    room where there are hundreds of little boxes in a grid. Two men take turns in
    the room; one is given an image, and the other one is given a sentence. They are
    tasked with placing their objects in one of the boxes but with no information
    as to what is “correct”. They can study their own objects but then must pick one
    of the boxes. Then, they’re scored based on how close they were to each other’s
    placement and only on that similarity, with no assumptions that there was a better
    choice than the ones the two men picked.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that, eventually, they’ll settle on a set of rules where both the
    image and the text description of the object will be placed in the same boxes.
    This metaphor is very close to how it actually works, except instead of just putting
    the object in one box, they score the text and image on how they fit into each
    of the hundreds of boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'When fully trained, similar images should score similarly, similar text descriptions
    should be scored similarly, and matching image and text descriptions should be
    scored similarly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – An example of CLIP’s shared latent space (Mountain photo by
    Rohit Tandon via Unsplash)](img/B17535_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – An example of CLIP’s shared latent space (Mountain photo by Rohit
    Tandon via Unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Image generation with CLIP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, now we have a model that turns an image and its associated text into a **shared
    latent space**. How does this generate images? Well, that’s where other tricks
    come in. We’re going to gloss over the specifics here because there are so many
    competing models that have come out recently that all do things a little differently
    and writing about each one in depth would be a book of its own, but the basic
    idea is similar between them, so let’s go with that.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so to train the model, the first step is to get the CLIP latent. Some
    models use both the image and the text embeddings, some go for one or the other
    at random, while others pick just the text embedding. No matter which latent they
    use, the process is similar. After generating the CLIP embedding, a model is trained
    to turn that embedding into an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a quick summary of just a few of the big text-to-image models on the
    market right now:'
  prefs: []
  type: TYPE_NORMAL
- en: Stability AI’s **Stable Diffusion** uses a repeating diffusion process that
    starts with a random noise pattern (or, in image-to-image mode, an image) and
    then iterates the diffusion process over and over in the latent space and uses
    a final decoder to convert the embedding back into an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google’s **Imagen** works with a similar method, but the diffusion process occurs
    in the image’s pixel space, and there is no need for a final conversion back into
    an image space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI’s **Dall-E** is, instead, a transformer-based model that generates an
    image by passing the text representation through a number of attention layers
    and other layers to create the image output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effective result in each case is an image that should match the text given
    to guide the generation.
  prefs: []
  type: TYPE_NORMAL
- en: So, where will image generation go from here?
  prefs: []
  type: TYPE_NORMAL
- en: The future of image generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many other techniques have been developed and announced, and just keeping up
    with the various releases is a significant endeavor beyond the scope of this book.
    The really incredible thing about all these different techniques is that they
    were all developed in rapid succession. The pace seems to be continually accelerating
    and doubtless shortly after this book comes out, all of this information will
    be superseded by a new innovation.
  prefs: []
  type: TYPE_NORMAL
- en: This field is rapidly developing, so it’s impossible to predict where future
    limitations might show up. It’s possible that we’re near the tail end of this
    current burst of development, and we’ll see a quite few years before a new innovation.
    But it’s also possible that we’ll see a rapid development of new functionality
    that makes today’s image generators look primitive within a few months.
  prefs: []
  type: TYPE_NORMAL
- en: 'The limitations that abound right now are not about resolution, detail, or
    even quality: it’s simply a matter of learning the best way to get results. Thanks
    to the public distribution of Stable Diffusion, there is a huge community of people
    experimenting with the technology, and it’s nearly impossible to go more than
    a couple of days without some new technique or idea coming forward that promises
    new frontiers. Things such as **Textual Inversion**, **LoRA**, and **DreamBooth**
    offer new ways to tune Stable Diffusion results on consumer hardware, while new
    functionality such as **inpainting** and **outpainting** allow you to fill in
    or expand an image, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: As time goes on, we’ll get more comfortable with using these image-generation
    models, and we’ll be better able to design them to maximize their performance.
    At that point, we’ll be able to predict what limits image generation has. At this
    point, we’re just too close to the mountain directly in front of us to see the
    top.
  prefs: []
  type: TYPE_NORMAL
- en: Generating sound
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sound generation is another one of those fields we could keep subdividing down
    and down until all the room we have in the book is taken up by headings listing
    the different methods of sound generation. For the sake of brevity, we’ll group
    them all here and cover a few big subfields instead.
  prefs: []
  type: TYPE_NORMAL
- en: Voice swapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing that most people think about when they learn they can swap
    faces is the question of whether they can swap voices too. The answer is quite
    unsatisfying: yes, but you probably don’t want to. There are AIs out there that
    can swap voices but they all suffer from various problems: from sounding like
    a robot, to lacking inflection, to not matching the person involved, to being
    very expensive and exclusive. If you’re doing anything with even moderate production
    value, you’ll get much better use out of *natural* intelligence: finding an impressionist
    who can do an impersonation of the voice. AI technology is just not there (yet).
    Even *Fall*, a movie that famously did “voice deepfakes” actually just re-recorded
    the voices in the studio (a process called automated dialog replacement or looping),
    and then the AI was used to adjust their mouths to match the new dialog.'
  prefs: []
  type: TYPE_NORMAL
- en: Voice-swapping technology will undoubtedly improve, and there are companies
    that offer very high-quality voice models for various projects, but they’re very
    expensive and still a niche use case. However, if the demand remains, it’s inevitable
    that someone will come out with a new technique or improvement that will make
    high-quality voice swapping as easy as other deepfakes.
  prefs: []
  type: TYPE_NORMAL
- en: Text-guided music generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This technique works a lot like text-to-image generation. In fact, most of the
    implementations actually use a text-to-image generator under the hood. The basic
    technique is to turn audio into a spectrogram image of the audio and then train
    a text-to-image model on it. This means that the same image generators can be
    used to generate audio spectrograms that can then be converted back into an audio
    stream.
  prefs: []
  type: TYPE_NORMAL
- en: This technique is lossy and limits the quality of the output because the spectrogram
    image can only hold so much frequency. In addition, there is the problem that
    spectrogram images represent just a small slice of time. This means that generating
    the audio has to happen in small chunks a few seconds long. Riffusion (an audio
    generation model built on Stable Diffusion) was trained on spectrograms about
    8 seconds long. This means that even if you come up with tricks such as joining
    multiple spectrograms, you’re going to have a new generation every 8 seconds or
    so. It’s possible to increase both the time slice and frequency by increasing
    the resolution of the image underlying the generation, but the limits would still
    be there, just higher. It’s unlikely that that current technology will be able
    to generate full length songs of 3-4 minutes even with reasonable advances in
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – An example of a spectrogram made by Riffusion](img/B17535_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – An example of a spectrogram made by Riffusion
  prefs: []
  type: TYPE_NORMAL
- en: Music that has low-frequency resolution and changes significantly every few
    seconds is a far cry from what we, typically, expect as music listeners. It’s
    likely that someone will create a text-to-music generator that will not involve
    the audio-image conversion process, but that would be a new creation that will
    probably have its own quirks and is getting into the speculative futurism that
    we want to avoid in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The future of sound generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s all but inevitable that someone will create a tool or technique that will
    blow open the field of sound generation in the same ways that convolutional layers
    and diffusion models have done for images and transformers have done for language.
    When that happens, we’ll see a gold rush-style burst of innovation as people adopt
    and extend the new technology with the same improvements that we’ve seen in the
    image and text generation fields. Unfortunately, for now, we cannot rely on the
    great innovations of the future, so we must ground ourselves in the now or the
    near future.
  prefs: []
  type: TYPE_NORMAL
- en: Modern audio techniques rely on autoencoders or spectrograms to bring the audio
    into a form that is usable by the AI generation techniques that we have access
    to now. To this end, it’s possible that these methods could be improved to add
    more detail or information. For example, usually, spectrograms are done in black
    and white. What if color were added in a way that more than just linearly increased
    the amount that could be generated? For example, what if red were used for a repeating
    melody, blue for another motif, and green to show how the two interact at a much
    larger timescale? Could this, when paired with larger spectrograms, bring modern
    sound generation to the point where an entire high-quality song could be generated
    at once?
  prefs: []
  type: TYPE_NORMAL
- en: What if we spent the time to develop a coding system for a spoken voice that
    turned it into a written form that could be learned and trained into a text-generation
    model? If we could define an accent as some combination of how the written form
    gets turned into audio, could we make an AI that could copy accents like they
    can painting styles? There are a lot of ways that we might be able to modify audio
    into a form that our current AI models excel at that would enable improvements
    of current sound-generation AI across the board.
  prefs: []
  type: TYPE_NORMAL
- en: Not even counting the revolutionary improvements from a whole new paradigm type
    akin to transformers or convolutions, I think that there are years of substantial
    evolution available in audio if computer scientists were to put as much attention
    into audio as they have images and video recently.
  prefs: []
  type: TYPE_NORMAL
- en: Deepfakes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, this whole book has been about the past and present of deepfakes,
    so it makes sense to circle back to their future at the end. There is a lot we
    can learn about the future of deepfakes from the other AI mentioned in this chapter.
    This is because, sneakily, all the parts of this chapter have been building up
    to this section. Deepfakes are, after all, an image generation AI that works on
    domain-specific images with a shared embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Every area that we’ve explored in this chapter can be used to improve deepfakes,
    so let’s approach them one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Sound generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This one is quite simple and obvious. The next step after swapping a face would
    be to swap the voice too. If we could get a solid voice swap, then deepfakes would
    be taken to a whole new capability. Making music or other effects could also be
    useful if you were making a movie without any other people helping, but their
    utility would be otherwise limited (in deepfakes, other industries would doubtlessly
    have more use for them).
  prefs: []
  type: TYPE_NORMAL
- en: Text-guided image generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Right now, you could consider deepfakes to be “face-guided” and perhaps that
    is the best solution, as it lets an actor’s performance shine through, even if
    they’re wearing a different actor’s face. But text-guided postprocessing could
    definitely fill a need. There is a famous scene in the film *Blade Runner* where
    the detective, Deckard, examines an image using voice commands to explore the
    environment (before doing the impossible and shifting the view to the side, which
    could only happen by inventing new data and wouldn’t be usable for finding clues).
    This could be seen as a prototype workflow for image modification with text-guided
    image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of generating a whole new image, we could modify parts of it using masks
    or filters and combine the AI changes back with the original image. This lets
    us do things such as write (or say) “*make him smile more*” to generate modifications
    to our image without replacing the whole image. This leverages the power of text
    guidance for some things, while still letting the face-guided swap happen.
  prefs: []
  type: TYPE_NORMAL
- en: Improving image quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is easy to see how improving image quality could improve deepfakes: you
    could improve your input data or output results to be more accurate, of higher
    quality, or for color correction. Image quality is critical in creating a successful
    deepfake, but some sources simply don’t provide the data that you really need
    to get a quality deepfake. For example, Albert Einstein, unfortunately, passed
    away before HD video cameras could capture all the data that we want for a standard
    deepfake. Photos and some videos do exist that we could use, but those sources
    are low-quality, are mostly monochrome, and would generally make for a poor swap.
    If we could improve that data enough to train a model, we could then bootstrap
    to create a whole new view of Albert Einstein.'
  prefs: []
  type: TYPE_NORMAL
- en: In particular, domain-specific tools will be beneficial to deepfakes. There
    are already face-specific upscalers that can be used on Faceswap's output to increase
    the effective resolution of the deepfakes process. Especially when used with very
    high-quality videos, upscaling the output can bridge the gap between computational
    power and the resolution of the swapped face.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This one seems like it might be the least important for deepfakes, and in a
    way, the “generation” part is, yes. But techniques and technologies such as CLIP
    could be brought into deepfakes in new and unique ways. For example, what if we
    got rid of the general-purpose face encoder that is a part of all deepfakes and
    replaced it with a new encoder that was trained like CLIP, contrasting with various
    faces and data? We might be able to cut one of the largest parts of the model
    to run separately, allowing us to focus all our energy (and compute) on the decoder
    that builds the face back instead of wasting time and energy training an encoder
    just to give us an embedding to feed into the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The future of deepfakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I think that with all the innovations that have come to generative AI recently,
    we can be sure that deepfakes have not reached the end of the line. There are
    innovations in these other domains that are just waiting to be implemented in
    deepfakes. These improvements will allow deepfakes to do new and interesting things
    while still keeping the essence of what a deepfake is – the swapping of one face
    with another.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve not yet seen the end of deepfakes in movies. In fact, it seems likely
    that deepfakes in movies will only grow as the technology becomes more mature,
    capable, and available. It seems reasonable that entire actors may be deepfaked
    in the future just to keep a character from changing due to growing older or passing
    away. Imagine a sequel to *Gone with the Wind* done entirely with the original
    actors animated by AI under the complete direction of the director. It’s feasible
    that we may see that before too long if AI continues to grow and advance.
  prefs: []
  type: TYPE_NORMAL
- en: Deepfakes are not unbounded; there are still problems to solve, such as resolution,
    training time, and even data collection. But it’s now possible to make a deepfake
    face in a resolution higher than early Blu-rays with data collected from black-and-white
    movies, colorized and cleanly upscaled. Where will we be in 10 years? Or even
    20 years? How long until you don’t even bother to get out of bed to attend a video
    call where your full body is generated and matched to you?
  prefs: []
  type: TYPE_NORMAL
- en: The future of AI ethics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our guidelines from [*Chapter 3*](B17535_03.xhtml#_idTextAnchor054)*, Examining
    Deepfake Ethics and Dangers*, are still a solid foundation for ethics (after all,
    being nice to people never goes out of style), but as we move into the future,
    we’ll approach entirely new challenges that will need their own ethical guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: When, for example, is it acceptable to replace an actor with a deepfake? Is
    it acceptable to replace a hard-working human with an AI that is just puppeted
    by the director? What if it’s a role that nobody wants to (or can) play? *Gollum*
    in Peter Jackson’s *Lord of the Rings* was played by the incredibly talented character
    actor *Andy Serkis* (Serkis also played Snoke in Disney’s *Star Wars* sequels
    and many other digital-first characters). Would it be ethical to replace him with
    an AI?
  prefs: []
  type: TYPE_NORMAL
- en: Chances are that an entirely AI character would be considered ethical by many
    (probably not the actors, though). But what if a director decided that an actress’s
    voice wasn’t “girly” enough and replaced it with a squeaky valley girl's voice
    without the actress’s permission? If it’s the director’s creative vision to replace
    the actor playing the bad guy with a darker-skinned person? What about the example
    in the previous section where you replace your pajamas with a well-coordinated
    outfit for your video call? These questions are harder to answer, and it’s something
    that we’ll have to evaluate together as society moves forward.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI has a huge history and a tremendous future. We’re standing before
    a vast plane where anything is possible, and we just have to go toward it. That
    said, not everything is visible today, and we must temper our expectations. The
    main challenges are the limitations of our computers, time, and research. If we
    dedicate our time and efforts to solving some of AI’s limitations, we’ll inevitably
    come up with brand-new leaps that will help us move forward. Even without huge
    revolutionary improvements though, there are a lot of smaller evolutionary improvements
    that we can make to improve the capabilities of these models.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest driver of innovation is need. Having more and more people using
    generative AI and putting it toward novel uses will create the economic and social
    pushes that generative AI needs to continue being improved on into the future.
  prefs: []
  type: TYPE_NORMAL
- en: This book has all been about getting us to this point where we, the authors,
    can invite you to go forward and help AI move toward this generative future that,
    hopefully, we can all see just over that horizon.
  prefs: []
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
