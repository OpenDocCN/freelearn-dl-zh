<html><head></head><body>
		<div><h1 id="_idParaDest-153" class="chapter-number"><a id="_idTextAnchor152"/><st c="0">8</st></h1>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor153"/><st c="2">Similarity Searching with Vectors</st></h1>
			<p><st c="35">This chapter is all about the </st><strong class="bold"><st c="66">R</st></strong><st c="67"> or </st><strong class="bold"><st c="71">retrieval</st></strong><st c="80"> part of </st><strong class="bold"><st c="89">retrieval-augmented generation</st></strong><st c="119"> (</st><strong class="bold"><st c="121">RAG</st></strong><st c="124">). </st><st c="128">Specifically, we are going to talk about four areas related to similarity searches: </st><strong class="bold"><st c="212">indexing</st></strong><st c="220">, </st><strong class="bold"><st c="222">distance metrics</st></strong><st c="238">, </st><strong class="bold"><st c="240">similarity algorithms</st></strong><st c="261">, and </st><strong class="bold"><st c="267">vector search services</st></strong><st c="289">. With this in mind, in this chapter, we will cover </st><st c="341">the following:</st></p>
			<ul>
				<li><st c="355">Distance metrics versus similarity algorithms versus </st><st c="409">vector search</st></li>
				<li><st c="422">Vector space</st></li>
				<li><st c="435">Code lab 8.1 – Semantic </st><st c="460">distance metrics</st></li>
				<li><st c="476">Different search paradigms – sparse, dense, </st><st c="521">and hybrid</st></li>
				<li><st c="531">Code lab 8.2 – Hybrid search with a </st><st c="568">custom function</st></li>
				<li><st c="583">Code lab 8.3 – Hybrid search with </st><st c="618">LangChain’s EnsembleRetriever</st></li>
				<li><st c="647">Semantic search algorithms such as k-NN </st><st c="688">and ANN</st></li>
				<li><st c="695">Indexing techniques that enhance ANN </st><st c="733">search efficiency</st></li>
				<li><st c="750">Vector </st><st c="758">search options</st></li>
			</ul>
			<p><st c="772">By the end of this chapter, you should have a comprehensive understanding of how vector-based similarity searching operates and why it’s instrumental for the retrieval component in </st><st c="954">RAG systems.</st></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor154"/><st c="966">Technical requirements</st></h1>
			<p><st c="989">The code for this chapter is placed in the following GitHub </st><st c="1050">repository: </st><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_08 "><st c="1062">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_08</st></a></p>
			<p><st c="1159">Individual file names for each code lab are mentioned in the </st><st c="1221">respective sections.</st></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor155"/><st c="1241">Distance metrics versus similarity algorithms versus vector search</st></h1>
			<p><st c="1308">First, let’s distinguish the difference between distance metrics, similarity algorithms, and vector search. </st><st c="1417">A similarity algorithm can use different distance metrics, whereas a vector search can use different similarity algorithms. </st><st c="1541">They are all different concepts that ultimately form the retrieval component of your RAG system. </st><st c="1638">It is important to make the distinction between these concepts serving different purposes if you are going to understand how to properly implement and optimize your retrieval solution. </st><st c="1823">You can think of this as a hierarchy, as shown in </st><em class="italic"><st c="1873">Figure 8</st></em><em class="italic"><st c="1881">.1</st></em><st c="1883">:</st></p>
			<div><div><img src="img/B22475_08_01.jpg" alt="Figure 8.1 – Vector store, similarity algorithm, and distance metric hierarchy for two options each"/><st c="1885"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="2007">Figure 8.1 – Vector store, similarity algorithm, and distance metric hierarchy for two options each</st></p>
			<p><st c="2106">In </st><em class="italic"><st c="2110">Figure 8</st></em><em class="italic"><st c="2118">.1</st></em><st c="2120">, we are</st><a id="_idIndexMarker402"/><st c="2128"> only demonstrating two options for each, where each vector search has two different options for similarity algorithms, and then each similarity algorithm has two different options for distance metrics. </st><st c="2331">In reality, though, there are many more options at </st><st c="2382">each level.</st></p>
			<p><st c="2393">The key point here is that these terms are often used interchangeably or together as if they are the same thing, but they are very different parts of the overall similarity search mechanism. </st><st c="2585">If you make the mistake of confusing them, it makes it much more difficult to understand the overall concepts behind </st><st c="2702">similarity search.</st></p>
			<p><st c="2720">Now that we have cleared that up, we will talk about another concept that can help you understand the underpinnings of how similarity search works, the </st><st c="2873">vector space.</st></p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/><st c="2886">Vector space</st></h1>
			<p><st c="2899">The </st><a id="_idIndexMarker403"/><st c="2904">concept of a </st><strong class="bold"><st c="2917">vector space</st></strong><st c="2929"> is highly related to the vector similarity search, as the search is conducted within the vector space represented by the vectors. </st><st c="3060">Technically, a vector space is a mathematical construct that represents a collection of vectors in a high-dimensional space. </st><st c="3185">The dimensions of the vector space correspond to the number of features or attributes associated with each vector. </st><st c="3300">In this space, the vectors of text that are most similar have more similar embeddings and, therefore, are located closer to each other in the space. </st><st c="3449">You will hear the concept of vector space referred to often when talking in more technical ways about similarity searches. </st><st c="3572">Other </st><a id="_idIndexMarker404"/><st c="3578">common </st><a id="_idIndexMarker405"/><st c="3585">names for this </st><em class="italic"><st c="3600">space</st></em><st c="3605"> are </st><strong class="bold"><st c="3610">embedding space</st></strong><st c="3625"> or </st><strong class="bold"><st c="3629">latent space</st></strong><st c="3641">.</st></p>
			<p><st c="3642">The concept of vector space can be helpful in visualizing how the distance algorithms that find the nearest vectors to our user query embedding are working. </st><st c="3800">Ignoring the fact that these vectors are sometimes thousands of dimensions, we can picture them in a 2D space with its outer limits defined by the vectors in them, and the data points (little dots can be seen in the free PDF version) representing each of those vectors (see </st><st c="4074">Figure 8</st><st c="4082">.2). </st><st c="4087">There are little clusters of data points (small dots) in various places representing semantic similarities across the different data points. </st><st c="4228">When a search happens, a new query (X) appears in this imaginary space based on the user query vector dimensions, and the data points (little dots) that are closest to that query (X) are going to be the results of our retriever-orchestrated similarity search. </st><st c="4488">We take all of the data points (small dots) that we are going to retrieve in the search result, and we turn them into query results (</st><st c="4621">large dots):</st></p>
			<div><div><img src="img/B22475_08_02.jpg" alt="Figure 8.2 – ﻿2D representation of embeddings in a vector space with the X representing a query and large dots representing the closest embeddings from the dataset"/><st c="4634"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="4719">Figure 8.2 – 2D representation of embeddings in a vector space with the X representing a query and large dots representing the closest embeddings from the dataset</st></p>
			<p><st c="4881">Let’s talk </st><a id="_idIndexMarker406"/><st c="4893">through what we see here. </st><st c="4919">There are four results from the query (the large dots). </st><st c="4975">From our vantage point, in this 2D space, it looks like there are data points (small dots) that are closer to the query (X) than the query results (large dots). </st><st c="5136">Why is that? </st><st c="5149">You may remember that these dots were originally in a 1,536D space. </st><st c="5217">So if you imagine just adding one more dimension (height), where the dots spread toward you right out of this page, those query results (large dots) may actually be closer because they are all much higher than the data points (small dots) that seem closer. </st><st c="5474">Looking straight down at them, some data points (small dots) may appear closer, but it is a mathematical certainty that the query results (large dots) are the closer ones when taking all dimensions into account. </st><st c="5686">Expand your space to all 1,536 dimensions, and this situation becomes even more likely. </st></p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor157"/><st c="5774">Semantic versus keyword search</st></h1>
			<p><st c="5804">As we’ve already</st><a id="_idIndexMarker407"/><st c="5821"> said many times, vectors capture the meaning behind our data in a mathematical representation. </st><st c="5917">To find data points similar in meaning to a user query, we can search and retrieve the closest objects in a vector space such as the one we just showed. </st><st c="6070">This is known as </st><strong class="bold"><st c="6087">semantic</st></strong><st c="6095"> or </st><strong class="bold"><st c="6099">vector search</st></strong><st c="6112">. A semantic </st><a id="_idIndexMarker408"/><st c="6125">search, as opposed to keyword matching, is searching for documents that have similar semantic meaning, rather than just the same words. </st><st c="6261">As humans, we can say the same or similar things in so many different ways! </st><st c="6337">Semantic search can capture that aspect of our language because it assigns similar mathematical values to similar concepts, whereas keyword search focuses on specific word matching and often misses similar semantic meanings partially </st><st c="6571">or entirely.</st></p>
			<p><st c="6583">From a technical standpoint, semantic search utilizes the meaning of the documents we have vectorized that is mathematically embedded in the vector that represents it. </st><st c="6752">For math enthusiasts, you have to recognize the beauty of using a mathematical solution to solve </st><st c="6849">linguistic challenges!</st></p>
			<p><st c="6871">Let’s walk through an example to highlight how semantic </st><st c="6928">search works.</st></p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/><st c="6941">Semantic search example</st></h2>
			<p><st c="6965">Think of a </st><a id="_idIndexMarker409"/><st c="6977">simple example of semantic similarity, such as a review of a blanket product online where one customer says </st><st c="7085">the following:</st></p>
			<p><code><st c="7099">This blanket does such a great job maintaining a high cozy temperature </st></code><code><st c="7171">for me!</st></code></p>
			<p><st c="7178">Another customer </st><st c="7196">says this:</st></p>
			<p><code><st c="7206">I am so much warmer and snug using </st></code><code><st c="7242">this spread!</st></code></p>
			<p><st c="7254">While they are saying relatively similar things semantically, a keyword search would not grade this nearly as similar as a semantic search would. </st><st c="7401">Here, we introduce a third sentence representing a random comment online </st><st c="7474">for comparison:</st></p>
			<p><code><st c="7489">Taylor Swift was 34 years old </st></code><code><st c="7520">in 2024.</st></code></p>
			<p><st c="7528">The semantics of this random online comment are considered quite different from either of the last two sentences. </st><st c="7643">But let’s not take my word for it, let’s do the math in a notebook! </st><st c="7711">In the following code, we will review some of the most common distance metrics used as a foundational element of </st><st c="7824">semantic search.</st></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor159"/><st c="7840">Code lab 8.1 – Semantic distance metrics</st></h1>
			<p><st c="7881">The file you need to access from the GitHub repository is </st><st c="7940">titled </st><code><st c="7947">CHAPTER8-1_DISTANCEMETRICS.ipynb</st></code><st c="7979">.</st></p>
			<p><st c="7980">Our first code</st><a id="_idIndexMarker410"/><st c="7995"> lab in this chapter will focus on the different ways you can calculate the distance between vectors, giving you a hands-on view of the difference between each of these approaches. </st><st c="8176">We will use a brand new notebook called </st><code><st c="8216">CHAPTER8-1_DISTANCEMETRICS.ipynb</st></code><st c="8248"> that has separate code from what we have used up to this point. </st><st c="8313">We will install and import the packages we need, create the embeddings for the sentences we discussed, and then we will step through three types of distance metric formulas that are very common in NLP, generative AI, and </st><st c="8534">RAG systems.</st></p>
			<p><st c="8546">We first install the open source </st><code><st c="8580">sentence_transformers</st></code><st c="8601"> library that will set up our </st><st c="8631">embedding algorithm:</st></p>
			<pre class="source-code"><st c="8651">
%pip install sentence_transformers -q --user</st></pre>
			<p><st c="8696">The </st><code><st c="8701">sentence_transformers</st></code><st c="8722"> package provides an easy way to compute dense vector representations for sentences and paragraphs. </st><st c="8822">Next, we import some select packages that will aid our efforts to </st><st c="8888">measure distance:</st></p>
			<pre class="source-code"><st c="8905">
import numpy as npfrom sentence_transformers import SentenceTransformer</st></pre>
			<p><st c="8977">Here, we add the popular NumPy library that will provide the mathematical operations we need to perform our analysis of distances. </st><st c="9109">As mentioned previously, </st><code><st c="9134">sentence_transformers</st></code><st c="9155"> is imported so that we can create dense vector representations for our text. </st><st c="9233">This will give us the ability to create instances of a pre-trained </st><st c="9300">embedding model.</st></p>
			<p><st c="9316">In this next line, we define the transformer model we want </st><st c="9376">to use:</st></p>
			<pre class="source-code"><st c="9383">
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')</st></pre>
			<p><st c="9438">This </st><code><st c="9444">'paraphrase-MiniLM-L6-v2'</st></code><st c="9469"> model is one of the smaller models available through this package, which will hopefully make it more compatible across more computer environments that you may be using this code on. </st><st c="9652">If you want something more powerful, try the </st><code><st c="9697">'all-mpnet-base-v2'</st></code><st c="9716"> model, where the semantic search performance scores around </st><st c="9776">50% higher.</st></p>
			<p><st c="9787">We will take the sentences we mentioned previously and add them to a list we can reference in </st><st c="9882">our code:</st></p>
			<pre class="source-code"><st c="9891">
sentence = ['This blanket has such a cozy temperature for me!', 'I am so much warmer and snug using this spread!', 'Taylor Swift was 34 years old in 2024.']</st></pre>
			<p><st c="10048">We then</st><a id="_idIndexMarker411"/><st c="10056"> encode the sentences using our </st><st c="10088">SentenceTransformer model:</st></p>
			<pre class="source-code"><st c="10114">
embedding = model.encode(sentence)
print(embedding)
embedding.shape</st></pre>
			<p><st c="10182">The </st><code><st c="10187">model.encode</st></code><st c="10199"> function takes a list of strings and converts them into a list of embeddings. </st><st c="10278">Our output shows us the mathematical representations (vectors) of </st><st c="10344">our sentences:</st></p>
			<pre class="source-code"><st c="10358">
[[-0.5129604   0.6386722   0.3116684  ... </st><st c="10396">-0.5178649  -0.3977838  0.2960762 ][-0.07027415  0.23834501  0.44659805 ... </st><st c="10468">-0.38965416  0.20492953  0.4301296 ][ 0.5601178  -0.96016043  0.48343912 ... </st><st c="10541">-0.36059788  1.0021329  -0.5214774 ]]
(3, 384)</st></pre>
			<p><st c="10585">You’ll notice </st><code><st c="10600">(3, 384)</st></code><st c="10608"> coming from the </st><code><st c="10625">embedding.shape</st></code><st c="10640"> function. </st><st c="10651">Do you remember what that is telling us? </st><st c="10692">It says we have three vectors, all of which are 384 dimensions. </st><st c="10756">So now we know this particular SentenceTransformer model provides vectors </st><st c="10830">in 384D!</st></p>
			<p class="callout-heading"><st c="10838">Fun fact</st></p>
			<p class="callout"><st c="10847">You may be wondering whether you can use the </st><code><st c="10893">sentence_transformers</st></code><st c="10914"> library to generate embeddings for your RAG vector store as we have been doing with OpenAI’s embedding API. </st><st c="11023">The answer is a resounding yes! </st><st c="11055">This is a free alternative to using OpenAI’s embeddings API and the embeddings, especially if generated from the larger </st><code><st c="11175">'all-mpnet-base-v2'</st></code><st c="11194"> model. </st><st c="11202">You can use the </st><code><st c="11415">ada</st></code><st c="11418"> model is ranked 65</st><st c="11437">th</st><st c="11440"> and their “best” model, the </st><code><st c="11469">'text-embedding-3-large'</st></code><st c="11493"> model is ranked 14</st><st c="11512">th</st><st c="11515">. You can also fine-tune these models with your own data and potentially make it more effective for your RAG system than any paid API embedding service. </st><st c="11668">Finally, for any API service, you are reliant on it being available, which is not always the case. </st><st c="11767">Using the </st><code><st c="11777">sentence_transformers</st></code><st c="11798"> model locally makes it always available and 100% reliable. </st><st c="11858">Take a look at MTEB to find even better models that you can download and use in a </st><st c="11940">similar way.</st></p>
			<p><st c="11952">Okay, we now </st><a id="_idIndexMarker413"/><st c="11966">have an environment for us to start exploring </st><st c="12012">distance measures.</st></p>
			<p><st c="12030">There are many ways to calculate the distance between vectors. </st><st c="12094">Euclidean distance (L2), dot product, and cosine distance are the most common distance metrics used </st><st c="12194">in NLP.</st></p>
			<p><st c="12201">Let’s start with </st><strong class="bold"><st c="12219">Euclidean </st></strong><strong class="bold"><st c="12229">distance</st></strong><st c="12237"> (</st><strong class="bold"><st c="12239">L2</st></strong><st c="12241">).</st></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/><st c="12244">Euclidean distance (L2)</st></h2>
			<p><st c="12268">Euclidean distance </st><a id="_idIndexMarker414"/><st c="12288">calculates the shortest distance between </st><a id="_idIndexMarker415"/><st c="12329">two vectors. </st><st c="12342">When using this to score the distance, keep in mind that we are looking for what is closer, so a lower value indicates higher similarity (closeness in distance). </st><st c="12504">Let us calculate the Euclidean distance between </st><st c="12552">two vectors:</st></p>
			<pre class="source-code"><st c="12564">
def euclidean_distance(vec1, vec2):
    return np.linalg.norm(vec1 - vec2)</st></pre>
			<p><st c="12635">In this function, we are calculating the Euclidean distance between two vectors, </st><code><st c="12717">vec1</st></code><st c="12721"> and </st><code><st c="12726">vec2</st></code><st c="12730">. We first perform element-wise subtraction between the two vectors, and then we use NumPy’s </st><code><st c="12823">linalg.norm()</st></code><st c="12836"> function to calculate the Euclidean norm (also known as L2 norm) of the vector. </st><st c="12917">This function takes the square root of the sum of the squares of the vector elements. </st><st c="13003">Combined, these give us the Euclidean distance between the </st><st c="13062">two vectors.</st></p>
			<p><st c="13074">We call this function here for each of </st><st c="13114">the embeddings:</st></p>
			<pre class="source-code"><st c="13129">
print("Euclidean Distance: Review 1 vs Review 2:",
    euclidean_distance(embedding[0], embedding[1]))
print("Euclidean Distance: Review 1 vs Random Comment:",
    euclidean_distance(embedding[0], embedding[2]))
print("Euclidean Distance: Review 2 vs Random Comment:",
    euclidean_distance(embedding[1], embedding[2]))
Running this cell gives us this output:
Euclidean Distance: Review 1 vs Review 2: 4.6202903
Euclidean Distance: Review 1 vs Random Comment: 7.313547
Euclidean Distance: Review 2 vs Random Comment: 6.3389034</st></pre>
			<p><st c="13645">Take a moment </st><a id="_idIndexMarker416"/><st c="13660">and look around you for the closest </st><em class="italic"><st c="13696">thing</st></em><st c="13701"> to you. </st><st c="13710">Then look for something further away. </st><st c="13748">The thing that is closest to you is measured </st><a id="_idIndexMarker417"/><st c="13793">in a smaller distance. </st><st c="13816">One foot is closer than two feet, so in this case, when you want it to be closer, </st><code><st c="13898">1</st></code><st c="13899"> is a better score than </st><code><st c="13923">2</st></code><st c="13924">. When it comes to distance in semantic search, closer means it is more similar. </st><st c="14005">For these results, we want to see a lower score to say it is more similar. </st><code><st c="14080">Review 1</st></code><st c="14088"> and </st><code><st c="14093">Review 2</st></code><st c="14101"> have a Euclidean distance of </st><code><st c="14131">4.6202903</st></code><st c="14140">. Both reviews are significantly further from </st><code><st c="14186">Random Comment</st></code><st c="14200">. This shows how math is used to determine how semantically similar or dissimilar these texts are. </st><st c="14299">But as with most things in data science, we have several options for how to calculate these distances. </st><st c="14402">Let’s take a look at another approach, </st><strong class="bold"><st c="14441">dot product</st></strong><st c="14452">.</st></p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor161"/><st c="14453">Dot product (also called inner product)</st></h2>
			<p><st c="14493">The </st><a id="_idIndexMarker418"/><st c="14498">dot product is not technically a distance metric, as it measures the magnitude of the </st><a id="_idIndexMarker419"/><st c="14584">projection of one vector onto the other, which indicates similarity rather than distance. </st><st c="14674">However, it is a metric used for similar purposes as the other metrics mentioned here. </st><st c="14761">Since we are talking about magnitude and not closeness, a higher positive dot product value indicates more similarity. </st><st c="14880">And so, as the value goes lower, or even negative, this indicates less similarity. </st><st c="14963">Here we will print out the dot product of each of our </st><st c="15017">text strings:</st></p>
			<pre class="source-code"><st c="15030">
print("Dot Product: Review 1 vs Review 2:",
    np.dot(embedding[0], embedding[1]))
print("Dot Product: Review 1 vs Random Comment:",
    np.dot(embedding[0], embedding[2]))
print("Dot Product: Review 2 vs Random Comment:",
    np.dot(embedding[1], embedding[2]))</st></pre>
			<p><st c="15282">In this code, we </st><a id="_idIndexMarker420"/><st c="15300">are using a NumPy function that does all the dot product computations for us. </st><st c="15378">The output is </st><st c="15392">as follows:</st></p>
			<pre class="source-code"><st c="15403">
Dot Product: Review 1 vs Review 2: 12.270497
Dot Product: Review 1 vs Random Comment: -0.7654616
Dot Product: Review 2 vs Random Comment: 0.95240986</st></pre>
			<p><st c="15552">In our </st><a id="_idIndexMarker421"/><st c="15560">first comparison, </st><code><st c="15578">Review</st></code> <code><st c="15584">1</st></code><st c="15586"> and </st><code><st c="15591">Review 2</st></code><st c="15599">, we see a score of </st><code><st c="15619">12.270497</st></code><st c="15628">. The positive magnitude of the dot product (</st><code><st c="15673">12.270497</st></code><st c="15683">) suggests a relatively high similarity between </st><code><st c="15732">Review 1</st></code><st c="15740"> and </st><code><st c="15745">Review 2</st></code><st c="15753">. When we compare </st><code><st c="15771">Review</st></code> <code><st c="15777">1</st></code><st c="15779"> with </st><code><st c="15785">Random Comment</st></code><st c="15799">, we see a score of </st><code><st c="15819">-0.7654616</st></code><st c="15829">, and </st><code><st c="15835">Review 2</st></code><st c="15843"> versus </st><code><st c="15851">Random Comment</st></code><st c="15865"> gives us a </st><code><st c="15877">0.95240986</st></code><st c="15887"> dot product. </st><st c="15901">These low and negative values indicate that there is dissimilarity or misalignment between the two vectors. </st><st c="16009">These scores tell us that </st><code><st c="16035">Review 1</st></code><st c="16043"> and </st><code><st c="16048">Review 2</st></code><st c="16056"> are more similar to each other compared to their similarity with </st><code><st c="16122">Random Comment</st></code><st c="16136">.</st></p>
			<p><st c="16137">Let’s look at our last distance metric, </st><strong class="bold"><st c="16178">cosine distance</st></strong><st c="16193">.</st></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/><st c="16194">Cosine distance</st></h2>
			<p><st c="16210">Cosine distance </st><a id="_idIndexMarker422"/><st c="16227">measures the difference in directionality between</st><a id="_idIndexMarker423"/><st c="16276"> the vectors. </st><st c="16290">Given that this is another distance metric, we consider a lower value to indicate closer, more similar vectors. </st><st c="16402">First, we set up a function to calculate the cosine distance between </st><st c="16471">two vectors:</st></p>
			<pre class="source-code"><st c="16483">
def cosine_distance(vec1,vec2):
    cosine = 1 - abs((np.dot(vec1,vec2)/(
        np.linalg.norm(vec1)*np.linalg.norm(vec2))))
    return cosine</st></pre>
			<p><st c="16612">Notice that the formula for cosine distance contains elements from both of our previous metrics. </st><st c="16710">First, we use </st><code><st c="16724">np.dot(vec1, vec2)</st></code><st c="16742"> to calculate the dot product between the two vectors. </st><st c="16797">Then, we divide by the product of the magnitudes, using the same NumPy function we used for Euclidean distance to calculate the Euclidean norm. </st><st c="16941">In this case, though, we are calculating the Euclidean norm of each of the vectors (rather than the difference between</st><a id="_idIndexMarker424"/><st c="17059"> the vectors as we did with Euclidean distance) and </st><a id="_idIndexMarker425"/><st c="17111">then multiplying them. </st><st c="17134">Combined, we get the cosine similarity, which is then subtracted as an absolute value from </st><code><st c="17225">1</st></code><st c="17226"> to get the cosine distance. </st><st c="17255">Here, we call </st><st c="17269">this function:</st></p>
			<pre class="source-code"><st c="17283">
print("Cosine Distance: Review 1 vs Review 2:",
    cosine_distance(embedding[0], embedding[1]))
print("Cosine Distance: Review 1 vs Random Comment:",
    cosine_distance(embedding[0], embedding[2]))
print("Cosine Distance: Review 2 vs Random Comment:",
    cosine_distance(embedding[1], embedding[2]))</st></pre>
			<p><st c="17574">And this is what we see in </st><st c="17602">the output:</st></p>
			<pre class="source-code"><st c="17613">
Cosine Distance: Review 1 vs Review 2: 0.4523802399635315
Cosine Distance: Review 1 vs Random Comment: 0.970455639064312
Cosine Distance: Review 2 vs Random Comment: 0.9542623348534107</st></pre>
			<p><st c="17798">Just like with Euclidean distance, a lower distance value means closer, which means more similar. </st><st c="17897">Once again, the value measuring the distance between the two reviews indicates much closer and similar semantics compared to either of the reviews and the random comment. </st><st c="18068">However, it should be noted that </st><code><st c="18101">0.4523802399635315</st></code><st c="18119"> suggests more of a moderate similarity between </st><code><st c="18167">Review 1</st></code><st c="18175"> and </st><code><st c="18180">Review 2</st></code><st c="18188">. But the other two scores, </st><code><st c="18216">1.0295443572103977</st></code><st c="18234"> and </st><code><st c="18239">0.9542623348534107</st></code><st c="18257">, indicate a high dissimilarity between </st><st c="18297">the vectors.</st></p>
			<p><st c="18309">Sorry Taylor Swift, mathematically speaking, we have ample proof that you are not the semantic equivalent of a </st><st c="18421">warm blanket!</st></p>
			<p><st c="18434">Keep in mind that there are many other distance metrics and similarity scores you can use for text </st><a id="_idIndexMarker426"/><st c="18534">embeddings, including </st><strong class="bold"><st c="18556">Lin similarity</st></strong><st c="18570">, </st><strong class="bold"><st c="18572">Jaccard similarity</st></strong><st c="18590">, </st><strong class="bold"><st c="18592">Hamming distance</st></strong><st c="18608">, </st><strong class="bold"><st c="18610">Manhattan distance</st></strong><st c="18628">, and </st><strong class="bold"><st c="18634">Levenshtein distance</st></strong><st c="18654">. However, the </st><a id="_idIndexMarker427"/><st c="18669">three</st><a id="_idIndexMarker428"/><st c="18674"> metrics</st><a id="_idIndexMarker429"/><st c="18682"> listed</st><a id="_idIndexMarker430"/><st c="18689"> previously are the most commonly used for NLP and should give you a good start in understanding how these metrics </st><st c="18804">are calculated.</st></p>
			<p><st c="18819">Up to this point, we have discussed dense vectors, which represent semantic meaning, but not all models represent semantic meaning. </st><st c="18952">Some are literally just a count of words in the data we provide to it. </st><st c="19023">These vectors are called sparse vectors. </st><st c="19064">Let’s talk about the differences between these types of vectors and how we can use those differences to our advantage </st><st c="19182">in RAG.</st></p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor163"/><st c="19189">Different search paradigms – sparse, dense, and hybrid</st></h1>
			<p><st c="19244">There are different types of vectors, and this difference is important to this discussion because you need to use different types of vector searches depending on the type of vector you are searching. </st><st c="19445">Let’s talk in depth about the differences between these types </st><st c="19507">of vectors.</st></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/><st c="19518">Dense search</st></h2>
			<p><strong class="bold"><st c="19531">Dense search</st></strong><st c="19544"> (semantic search) uses</st><a id="_idIndexMarker431"/><st c="19567"> vector embedding representation</st><a id="_idIndexMarker432"/><st c="19599"> of data to perform search. </st><st c="19627">As we have talked about previously, this type of search allows you to capture and return semantically similar objects. </st><st c="19746">It relies on the meaning of the data in order to perform that query. </st><st c="19815">This sounds great in theory, but there are some limitations. </st><st c="19876">If the model we are using was trained on a completely different domain, the accuracy of our queries would be poor. </st><st c="19991">It is very dependent on the data it was </st><st c="20031">trained on.</st></p>
			<p><st c="20042">Searching for data that is a reference to something (such as serial numbers, codes, IDs, and even people’s names) will also yield poor results. </st><st c="20187">This is because there isn’t a lot of meaning in text like this, so no meaning is captured in the embeddings, and no meaning can be used to compare embeddings. </st><st c="20346">When searching for specific references like that, it is better for string or word matching. </st><st c="20438">We call this type of search keyword search or sparse search, and we will discuss </st><st c="20519">this next.</st></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor165"/><st c="20529">Sparse search</st></h2>
			<p><strong class="bold"><st c="20543">Sparse search</st></strong><st c="20557"> allows</st><a id="_idIndexMarker433"/><st c="20564"> you to utilize</st><a id="_idIndexMarker434"/><st c="20579"> keyword matching across all of your content. </st><st c="20625">It is called </st><strong class="bold"><st c="20638">sparse embedding</st></strong><st c="20654"> because </st><a id="_idIndexMarker435"/><st c="20663">text is embedded into vectors by counting how many times every unique word in your vocabulary occurs in the query and stored sentences. </st><st c="20799">This vector has mostly zeros because the likelihood of any given sentence containing every word in your vocabulary is low. </st><st c="20922">In mathematical terms, if an embedding contains mostly zeros, it is </st><st c="20990">considered sparse.</st></p>
			<p><st c="21008">One example could be </st><a id="_idIndexMarker436"/><st c="21030">using a </st><strong class="bold"><st c="21038">bag-of-words</st></strong><st c="21050">. A bag-of-words approach is where you count how many times a word occurs in the query and the data vector and then return objects with the highest matching word frequency. </st><st c="21223">This is the easiest way to do </st><st c="21253">keyword matching.</st></p>
			<p><st c="21270">A good example of a keyword-based </st><a id="_idIndexMarker437"/><st c="21305">algorithm is the </st><strong class="bold"><st c="21322">Best Matching 25</st></strong><st c="21338"> (</st><strong class="bold"><st c="21340">BM25</st></strong><st c="21344">) algorithm. </st><st c="21358">This very popular model performs really well when it comes to searching across many keywords. </st><st c="21452">The idea behind BM25 is that it counts the number of words within the phrase that you are passing in and then those that appear more than often are weighted as less important when the match occurs. </st><st c="21650">Words that are rare will score much higher. </st><st c="21694">Does this concept sound familiar? </st><st c="21728">It uses TF-IDF, one of the models we reviewed in the </st><st c="21781">last chapter!</st></p>
			<p><st c="21794">Having these two options brings up a challenging question, though: which one do we use? </st><st c="21883">What if we need semantic matching and keyword matching? </st><st c="21939">The great news is we do not have to choose; we can use both in what is called </st><strong class="bold"><st c="22017">hybrid searching</st></strong><st c="22033">! We will review that </st><st c="22055">concept next.</st></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/><st c="22068">Hybrid search</st></h2>
			<p><st c="22082">Hybrid search</st><a id="_idIndexMarker438"/><st c="22096"> allows you to make the most of both dense and sparse search techniques and then fuse the return rank results together. </st><st c="22216">With hybrid search, you </st><a id="_idIndexMarker439"/><st c="22240">are performing both vector/dense search and keyword/sparse search and then you combine </st><st c="22327">the results.</st></p>
			<p><st c="22339">This combination can be done based on a scoring system that measures how well each object matches the query using both dense and sparse searches. </st><st c="22486">What better way to illustrate how this approach works than to walk through a code lab with it? </st><st c="22581">In the next section, we will introduce you to BM25 to conduct your keyword/sparse search and then combine it with our existing retriever to form a </st><st c="22728">hybrid search.</st></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor167"/><st c="22742">Code lab 8.2 – Hybrid search with a custom function</st></h1>
			<p><st c="22794">The</st><a id="_idIndexMarker440"/><st c="22798"> file you need to access from the GitHub repository is </st><st c="22853">titled </st><code><st c="22860">CHAPTER8-2_HYBRID_CUSTOM.ipynb</st></code><st c="22890">.</st></p>
			<p><st c="22891">In this code lab, we are going to start with the notebook from </st><a href="B22475_05.xhtml#_idTextAnchor095"><em class="italic"><st c="22955">Chapter 5</st></em></a><st c="22964">: </st><code><st c="22967">CHAPTER5-3_BLUE_TEAM_DEFENDS.ipynb</st></code><st c="23001">. Note that we are not using the </st><a href="B22475_06.xhtml#_idTextAnchor114"><em class="italic"><st c="23034">Chapter 6</st></em></a><st c="23043"> or </st><em class="italic"><st c="23047">7</st></em><st c="23048"> code, which has a lot of miscellaneous code we won’t use going forward. </st><st c="23121">There is an added bonus in this code lab though; we are going to introduce some new elements that will carry us through the next couple of chapters, such as a new type of document loader for PDFs rather than web pages, a new larger document with more data to search, and a new text splitter. </st><st c="23413">We will also clean out any code we no longer need as a result of </st><st c="23478">these changes.</st></p>
			<p><st c="23492">Once we have updated the code for these changes, we can focus on the task at hand, which is to use BM25 to generate our sparse vectors, combining those vectors with the dense vectors we have already used to form a hybrid search approach. </st><st c="23731">We will use our previous vectorizer to generate our dense vectors. </st><st c="23798">Then, we will search using both sets of vectors, rerank the results accounting for documents that appear in both retrievals, and provide a final hybrid result. </st><st c="23958">BM25 has been around for several decades, but it is still a very effective bag-of-words algorithm based on TF-IDF, which we reviewed in the last chapter. </st><st c="24112">It is also very quick </st><st c="24134">to compute.</st></p>
			<p><st c="24145">One interesting aspect of combining the results from the two retrievers begs the question, how does it rank results from two relatively different search mechanisms? </st><st c="24311">Our dense vector search uses cosine similarity and provides a similarity score. </st><st c="24391">Our sparse vector is based on TF-IDF and using TF and IDF scores, which we reviewed in the previous chapter. </st><st c="24500">These are not comparable scores. </st><st c="24533">As it turns out, there are numerous algorithms we can use to perform the ranking among these two retrievers. </st><st c="24642">The one we will use is called </st><a id="_idIndexMarker441"/><st c="24672">the </st><strong class="bold"><st c="24676">Reciprocal Rank Fusion</st></strong><st c="24698"> (</st><strong class="bold"><st c="24700">RRF</st></strong><st c="24703">) algorithm. </st><st c="24717">This lab primarily focuses on building a function that mimics the ranking approach that the RRF takes, so that you can walk through and understand these </st><st c="24870">calculations yourself.</st></p>
			<p><st c="24892">We no longer need this package focused on parsing web pages, since we are changing from processing a web page to parsing a PDF. </st><st c="25021">Let’s start with removing </st><st c="25047">that code:</st></p>
			<pre class="source-code"><st c="25057">
%pip install beautifulsoup4</st></pre>
			<p><st c="25085">We do</st><a id="_idIndexMarker442"/><st c="25091"> need to install a new package for parsing PDFs, as we need a new package that will let us use the BM25 model with LangChain to generate the </st><st c="25232">sparse embeddings:</st></p>
			<pre class="source-code"><st c="25250">
%pip install PyPDF2 -q –user
%pip install rank_bm25</st></pre>
			<p><st c="25302">That will load both of these packages into our environment. </st><st c="25363">Remember to restart your kernel after </st><st c="25401">the installation!</st></p>
			<p><st c="25418">Next, remove this code from </st><st c="25447">the imports:</st></p>
			<pre class="source-code"><st c="25459">
from langchain_community.document_loaders import WebBaseLoader
import bs4
from langchain_experimental.text_splitter import SemanticChunker</st></pre>
			<p><st c="25598">As mentioned earlier, we no longer need the code for parsing web pages. </st><st c="25671">We are also removing our text splitter and will replace it with a </st><st c="25737">new one.</st></p>
			<p><st c="25745">Add this code to </st><st c="25763">the imports:</st></p>
			<pre class="source-code"><st c="25775">
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.retrievers import BM25Retriever</st></pre>
			<p><st c="25977">Here, we add </st><code><st c="25991">PdfReader</st></code><st c="26000"> for PDF extraction. </st><st c="26021">We add the </st><code><st c="26032">RecursiveCharacterTextSplitter</st></code><st c="26062"> text splitter, which will replace </st><code><st c="26097">SemanticChunker</st></code><st c="26112">. We add a new class that will help us manage and process our documents when dealing with LangChain. </st><st c="26213">Last, we add the </st><code><st c="26230">BM25Retriever</st></code><st c="26243"> loader that acts as a </st><st c="26266">LangChain retriever.</st></p>
			<p><st c="26286">Let’s next remove the web </st><st c="26313">parsing code:</st></p>
			<pre class="source-code"><st c="26326">
loader = WebBaseLoader(
    web_paths=("https://kbourne.github.io/chapter1.html",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title",
                "post-header")
        )
    ),
)
docs = loader.load()</st></pre>
			<p><st c="26532">We</st><a id="_idIndexMarker443"/><st c="26535"> are going to take the cell where we are defining our OpenAI variables and expand it to define all of the variables we use across the code; add this to the bottom of </st><st c="26701">that cell:</st></p>
			<pre class="source-code"><st c="26711">
pdf_path = "google-2023-environmental-report.pdf"
collection_name = "google_environmental_report"
str_output_parser = StrOutputParser()</st></pre>
			<p><st c="26847">This sets up some variables that we will discuss further when we use them in the following code. </st><st c="26945">Now, let’s add our code for processing </st><st c="26984">a PDF:</st></p>
			<pre class="source-code"><st c="26990">
pdf_reader = PdfReader(pdf_path)
text = ""
for page in pdf_reader.pages:
    text += page.extract_text()</st></pre>
			<p><st c="27091">We will talk more about LangChain document loading in </st><a href="B22475_11.xhtml#_idTextAnchor229"><em class="italic"><st c="27146">Chapter 11</st></em></a><st c="27156">, but for now, we wanted to introduce you to an alternative to just loading web pages. </st><st c="27243">Given the popularity of PDFs, this is probably going to be a common situation for you. </st><st c="27330">The catch is that you need to have the </st><code><st c="27369">google-2023-environmental-report.pdf</st></code><st c="27405"> file available in the same directory as your notebook. </st><st c="27461">You can download that file from the same repo you access all of the other code in this book. </st><st c="27554">This code pulls that file up and extracts the text across the pages, concatenating the text back together so that there is no text loss across </st><st c="27697">the pages.</st></p>
			<p><st c="27707">At this</st><a id="_idIndexMarker444"/><st c="27715"> point, we have a very large string representing all of the text in the PDF. </st><st c="27792">We now need to use a splitter to break the text into manageable chunks. </st><st c="27864">This is where we are going to switch from </st><code><st c="27906">SemanticChunker</st></code><st c="27921"> to </st><code><st c="27925">RecursiveCharacterTextSplitter</st></code><st c="27955">. This gives you a chance to work with a different LangChain splitter as well, which is another topic we will expand on in </st><a href="B22475_11.xhtml#_idTextAnchor229"><em class="italic"><st c="28078">Chapter 11</st></em></a><st c="28088">. First, remove </st><st c="28104">this one:</st></p>
			<pre class="source-code"><st c="28113">
text_splitter = SemanticChunker(OpenAIEmbeddings())
splits = text_splitter.split_documents(docs)</st></pre>
			<p><st c="28210">Then, add </st><st c="28221">this one:</st></p>
			<pre class="source-code"><st c="28230">
character_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". </st><st c="28313">", " ", ""],
    chunk_size=1000,
    chunk_overlap=200
)
splits = character_splitter.split_text(text)</st></pre>
			<p><code><st c="28407">RecursiveCharacterTextSplitter</st></code><st c="28438"> is a commonly used splitter that also saves us the cost of using the </st><code><st c="28508">OpenAI</st></code><st c="28514">embeddings API associated with </st><code><st c="28546">the SemanticChunker</st></code><st c="28565"> splitter object. </st><st c="28583">Combined with the larger PDF document we are now uploading, this splitter will give us more chunks to work with when looking at vector spaces and retrieval maps in the </st><st c="28751">next chapter.</st></p>
			<p><st c="28764">With new data and a new splitter, we also need to update our retriever-related code. </st><st c="28850">Let’s start with prepping </st><st c="28876">our documents:</st></p>
			<pre class="source-code"><st c="28890">
documents = [Document(page_content=text, metadata={
    "id": str(i)}) for i, text in enumerate(splits)]</st></pre>
			<p><st c="28991">Then, the retriever code needs to </st><st c="29026">be removed:</st></p>
			<pre class="source-code"><st c="29037">
vectorstore = Chroma.from_documents(
    documents=splits,embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()</st></pre>
			<p><st c="29160">Replace it </st><a id="_idIndexMarker445"/><st c="29172">with </st><st c="29177">this code:</st></p>
			<pre class="source-code"><st c="29187">
chroma_client = chromadb.Client()
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embedding_function,
    collection_name=collection_name,
    client=chroma_client
)
dense_retriever = vectorstore.as_retriever(
    search_kwargs={"k": 10})
sparse_retriever = BM25Retriever.from_documents(
    documents, k=10)</st></pre>
			<p><st c="29500">This code takes a moment to load, but once it does, we are setting up our Chroma DB vector store to better manage the documents we get from our PDF, with ID metadata added. </st><st c="29674">The original retriever is now called </st><code><st c="29711">dense_retriever</st></code><st c="29726">, which is a more descriptive and accurate name for it since it interacts with dense embeddings. </st><st c="29823">The new retriever, </st><code><st c="29842">sparse_retriever</st></code><st c="29858">, is based on BM25, which is conveniently available through LangChain as a retriever, giving us similar functionality to any other LangChain instance of retriever. </st><st c="30022">In both instances, we are ensuring that we are getting 10 results back by setting </st><code><st c="30104">k</st></code><st c="30105"> to </st><code><st c="30109">10</st></code><st c="30111">. Also, note that the </st><code><st c="30133">vectorstore</st></code><st c="30144"> object is using the </st><code><st c="30165">collection_name</st></code><st c="30180"> string we defined in the variables earlier in </st><st c="30227">the code.</st></p>
			<p class="callout-heading"><st c="30236">Fun fact</st></p>
			<p class="callout"><st c="30245">You should note though that we are not storing our sparse embeddings in a Chroma DB vector store like we are with the dense embeddings. </st><st c="30382">We pull the documents directly into the retriever, which keeps them in memory while we use them in our code. </st><st c="30491">In a more sophisticated application, we would likely want to handle this more thoroughly and store the embeddings in a more permanent vector store for future retrieval. </st><st c="30660">Even our Chroma DB is ephemeral in this code, which means we will lose it if we shut down the notebook kernel. </st><st c="30771">You can improve this situation using </st><code><st c="30808">vectorstore.persist()</st></code><st c="30829">, which will store the Chroma DB database locally in a </st><code><st c="30884">sqlite</st></code><st c="30890"> file. </st><st c="30897">These are advanced techniques that aren’t needed for this code lab, but look them up if you want to build a more robust vector store environment for your </st><st c="31051">RAG pipeline!</st></p>
			<p><st c="31064">In a moment, we </st><a id="_idIndexMarker446"/><st c="31081">will introduce you to the function that performs a hybrid search for you, so that you can step through and see what is happening. </st><st c="31211">Before reviewing it, though, let’s discuss how to approach it. </st><st c="31274">Keep in mind that this is a quick stab at trying to replicate the ranking algorithm that LangChain uses in its hybrid search mechanism. </st><st c="31410">The idea here is that this will let you walk through what is going on under the hood when you are doing a hybrid search using LangChain. </st><st c="31547">LangChain actually provides a</st><a id="_idIndexMarker447"/><st c="31576"> mechanism that will do all of this in one line of code! </st><st c="31633">This is </st><code><st c="31660">EnsembleRetriever</st></code><st c="31677"> performs a hybrid search in the same way our function does, but it employs a sophisticated ranking algorithm called the RRF algorithm. </st><st c="31813">This algorithm does the heavy lifting of determining how to rank all of the results, similar to how we just discussed our </st><st c="31935">function operation.</st></p>
			<p><st c="31954">We will step through the next function discussing each point and how that relates to the RFF algorithm that LangChain uses for the same purpose. </st><st c="32100">This is by far the biggest function we have used so far, but it is worth the effort! </st><st c="32185">Keep in mind that this is one function, which you can see in the code altogether. </st><st c="32267">Let’s start with the </st><st c="32288">function definition:</st></p>
			<pre class="source-code"><st c="32308">
def hybrid_search(query, k=10, dense_weight=0.5,
    sparse_weight=0.5):</st></pre>
			<p><st c="32377">Initially, we are going to take the weights for the dense and sparse results separately. </st><st c="32467">This matches the </st><code><st c="32484">EnsembleRetriever</st></code><st c="32501"> weight parameters from LangChain, which we will review in a moment, but this sets up this function to act exactly like that type of retriever. </st><st c="32645">We also have a </st><code><st c="32660">k</st></code><st c="32661"> value, indicating the total results we want the function to return. </st><st c="32730">The default of </st><code><st c="32745">k</st></code><st c="32746"> matches what the retrievers are set to return when they were initialized earlier in </st><st c="32831">the code.</st></p>
			<p><st c="32840">Our first step within the function is focused on retrieving the top-</st><code><st c="32909">k</st></code><st c="32911"> documents from both types </st><st c="32938">of retrievers:</st></p>
			<pre class="source-code"><st c="32952">
    dense_docs = dense_retriever.get_relevant_documents(
        query)[:k]
    dense_doc_ids = [doc.metadata[
        'id'] for doc in dense_docs]
    print("\nCompare IDs:")
    print("dense IDs: ", dense_doc_ids)
    sparse_docs = sparse_retriever.get_relevant_documents(
        query)[:k]
    sparse_doc_ids = [doc.metadata[
        'id'] for doc in sparse_docs]
    print("sparse IDs: ", sparse_doc_ids)
    all_doc_ids = list(set(dense_doc_ids + sparse_doc_ids))
    dense_reciprocal_ranks = {
        doc_id: 0.0 for doc_id in all_doc_ids}
    sparse_reciprocal_ranks = {
        doc_id: 0.0 for doc_id in all_doc_ids}</st></pre>
			<p><st c="33491">We start our</st><a id="_idIndexMarker448"/><st c="33504"> retrieval process by retrieving the top-</st><code><st c="33545">k</st></code><st c="33547"> documents from both dense search and sparse search. </st><st c="33600">Just like RRF, we start with retrieving the top documents from both dense search and sparse search based on their respective scoring mechanisms. </st><st c="33745">We also want to assign IDs to our content so that we can compare the results across retrievers, remove duplicates across results (by converting them to a set that removes all duplicates), and then create two dictionaries to store the reciprocal ranks of </st><st c="33999">each document.</st></p>
			<p><st c="34013">Next, we are going to calculate the reciprocal rank for </st><st c="34070">each document:</st></p>
			<pre class="source-code"><st c="34084">
for i, doc_id in enumerate(dense_doc_ids):
    dense_reciprocal_ranks[doc_id] = 1.0 / (i + 1)
for i, doc_id in enumerate(sparse_doc_ids):
    sparse_reciprocal_ranks[doc_id] = 1.0 / (i + 1)</st></pre>
			<p><st c="34266">This code will </st><a id="_idIndexMarker449"/><st c="34282">calculate the reciprocal rank for each document in dense and sparse search results and store them in the dictionaries we just created. </st><st c="34417">For each document, we calculate its reciprocal rank in each ranked list. </st><st c="34490">The reciprocal rank is the inverse of the document’s position in the ranked list (e.g., 1/rank). </st><st c="34587">The reciprocal rank is calculated as </st><code><st c="34624">1.0</st></code><st c="34627"> divided by the position of the document in the respective search results (1-based index). </st><st c="34718">Note that the similarity scores are not involved in this calculation. </st><st c="34788">As you might remember from previous discussions, our semantic search is ranking based on distance and BM25 is ranking based on relevance. </st><st c="34926">But RRF does not require these scores, which means we don’t need to worry about normalizing scores from the different retrieval methods to be on the same scale or directly comparable. </st><st c="35110">With RFF, it relies on the rank positions, making it easier to combine results from different scoring mechanisms. </st><st c="35224">It is important to note the impact this will have on your search, though. </st><st c="35298">You may have a scenario where from a semantic standpoint, you have a really </st><em class="italic"><st c="35374">close</st></em><st c="35379"> score (distance-wise) in your semantic search, but the highest-ranked result from your keyword search is still not that similar. </st><st c="35509">Using RFF with equal weights will result in these results having equal rankings and, therefore, equal value from the ranking standpoint, even though you would want the semantic result to have more weight. </st><st c="35714">You can adjust this using the </st><code><st c="35744">dense_weight</st></code><st c="35756"> and </st><code><st c="35761">sparse_weight</st></code><st c="35774"> parameters, but what if you have the reverse situation? </st><st c="35831">This is a downside of using RRF and hybrid search in general, which is why you will want to test to make sure this is the best solution for your </st><st c="35976">particular needs.</st></p>
			<p><st c="35993">Here, we sum the reciprocal ranks of each document across the ranked lists from dense search and </st><st c="36091">sparse search:</st></p>
			<pre class="source-code"><st c="36105">
combined_reciprocal_ranks = {doc_id:
    0.0 for doc_id in all_doc_ids}
for doc_id in all_doc_ids:
   combined_reciprocal_ranks[doc_id] = dense_weight *
       dense_reciprocal_ranks[doc_id] + sparse_weight *
       sparse_reciprocal_ranks[doc_id]</st></pre>
			<p><st c="36332">The RFF approach hinges on the idea that documents that are ranked highly by both retrieval methods are more likely to be relevant to the query. </st><st c="36478">By using reciprocal ranks, RRF gives more weight to documents that appear at the top of the ranked lists. </st><st c="36584">Note that we are </st><a id="_idIndexMarker450"/><st c="36601">weighting the sums using the weights we collected in the parameters. </st><st c="36670">That means this is the place where we can make a particular set of embeddings (dense or sparse) more influential in the </st><st c="36790">search results.</st></p>
			<p><st c="36805">This next line sorts the document IDs based on their combined reciprocal rank scores in </st><st c="36894">descending order:</st></p>
			<pre class="source-code"><st c="36911">
sorted_doc_ids = sorted(all_doc_ids, key=lambda doc_id:
    combined_reciprocal_ranks[doc_id], reverse=True)</st></pre>
			<p><st c="37016">Descending order is indicated by </st><code><st c="37050">reverse=True</st></code><st c="37062">. It uses the </st><code><st c="37076">sorted()</st></code><st c="37084"> function with a </st><code><st c="37101">key</st></code><st c="37104"> function that retrieves the combined reciprocal rank for each </st><st c="37167">document ID.</st></p>
			<p><st c="37179">Our next step is to iterate over the sorted document IDs and retrieve the corresponding documents from the dense and sparse </st><st c="37304">search results:</st></p>
			<pre class="source-code"><st c="37319">
    sorted_docs = []
    all_docs = dense_docs + sparse_docs
    for doc_id in sorted_doc_ids:
        matching_docs = [
            doc for doc in all_docs if doc.metadata[
                'id'] == doc_id]
            if matching_docs:
                doc = matching_docs[0]
                doc.metadata['score'] =
                    combined_reciprocal_ranks[doc_id]
                doc.metadata['rank'] =
                    sorted_doc_ids.index(doc_id) + 1
                if len(matching_docs) &gt; 1:
                    doc.metadata['retriever'] = 'both'
                elif doc in dense_docs:
                    doc.metadata['retriever'] = 'dense'
            else:
                 doc.metadata['retriever'] = 'sparse'
                 sorted_docs.append(doc)</st></pre>
			<p><st c="37822">We use </st><a id="_idIndexMarker451"/><st c="37830">this to indicate the source retriever, giving us a better sense of how each of our retrievers is impacting the results. </st><st c="37950">Retrieve the documents based on the sorted document IDs. </st><st c="38007">The resulting ranked list represents the hybrid search results, where documents that appear higher in both dense and sparse search rankings will have higher </st><st c="38164">combined scores.</st></p>
			<p><st c="38180">Finally, we return </st><st c="38200">the results:</st></p>
			<pre class="source-code"><st c="38212">
return sorted_docs[:k]</st></pre>
			<p><st c="38235">Note that </st><code><st c="38246">k</st></code><st c="38247"> was used for both retrievers, giving us twice as many results as we are asking for here. </st><st c="38337">So this is taking those results and cutting them in half, returning just the top-</st><code><st c="38418">k</st></code><st c="38420">. What this does in practice is if there are results in the lower half of these retrievers, such as rank #8, but they are in both results, it is likely to push those results into </st><st c="38599">the top-</st><code><st c="38607">k</st></code><st c="38609">.</st></p>
			<p><st c="38610">Next, we have to account for this new retriever mechanism in our LangChain chain. </st><st c="38693">Update the </st><code><st c="38704">rag_chain_with_source</st></code><st c="38725"> chain to use the </st><code><st c="38743">hybrid_search</st></code><st c="38756"> function to return </st><code><st c="38776">context</st></code> <st c="38783">like this:</st></p>
			<pre class="source-code"><st c="38794">
rag_chain_with_source = RunnableParallel(
    {"context": hybrid_search,
     "question": RunnablePassthrough()}
).assign(answer=rag_chain_from_docs)</st></pre>
			<p><st c="38935">This completes the code changes for the RAG pipeline to use hybrid search. </st><st c="39011">But we added all of this extra metadata that we want to show in our output and analysis. </st><st c="39100">An extra benefit of building this function ourselves is that it lets us print output that you normally wouldn’t be able to see if using LangChain’s </st><code><st c="39248">EnsembleRetriever</st></code><st c="39265">. Let’s take advantage and replace this cell where we call to the RAG pipeline. </st><st c="39345">Rather than using the final code in the past code labs, when processing our RAG pipeline, use </st><st c="39439">this code:</st></p>
			<pre class="source-code"><st c="39449">
user_query = "What are Google's environmental initiatives?"
</st><st c="39510">result = rag_chain_with_source.invoke(user_query)
relevance_score = result['answer']['relevance_score']
final_answer = result['answer']['final_answer']
retrieved_docs = result['context']
print(f"\nOriginal Question: {user_query}\n")
print(f"Relevance Score: {relevance_score}\n")
print(f"Final Answer:\n{final_answer}\n\n")
print("Retrieved Documents:")
for i, doc in enumerate(retrieved_docs, start=1):
    doc_id = doc.metadata['id']
    doc_score = doc.metadata.get('score', 'N/A')
    doc_rank = doc.metadata.get('rank', 'N/A')
    doc_retriever = doc.metadata.get('retriever', 'N/A')
    print(f"Document {i}: Document ID: {doc_id}
        Score: {doc_score} Rank: {doc_rank}
        Retriever: {doc_retriever}\n")
    print(f"Content:\n{doc.page_content}\n")</st></pre>
			<p><st c="40234">This code carries over</st><a id="_idIndexMarker452"/><st c="40257"> what we’ve used in previous chapters, such as the relevance score that we used in our security response. </st><st c="40363">We added a printout of each of the results from our retriever and the metadata we collected on them. </st><st c="40464">Here is a sample output with the first couple </st><st c="40510">of results:</st></p>
			<pre class="source-code"><st c="40521">
Compare IDs:
dense IDs:  ['451', '12', '311', '344', '13', '115', '67', '346', '66', '262']
sparse IDs:  ['150', '309', '298', '311', '328', '415', '139', '432', '91', '22']
Original Question: What are Google's environmental initiatives?
</st><st c="40758">Relevance Score: 5
Final Answer:
Google's environmental initiatives include partnering with suppliers to reduce energy consumption and GHG emissions, engaging with suppliers to report and manage emissions, empowering individuals to take action through sustainability features in products, working together with partners and customers to reduce carbon emissions, operating sustainably at their campuses, focusing on net-zero carbon energy, water stewardship, circular economy practices, and supporting various environmental projects and initiatives such as the iMasons Climate Accord, ReFED, and The Nature Conservancy. </st><st c="41377">They also work on sustainable consumption of public goods and engage with coalitions and sustainability initiatives to promote environmental sustainability.
</st><st c="41534">Retrieved Documents:
</st><strong class="bold"><st c="41555">Document 1: Document ID: 150 Score: 0.5 Rank: 1 Retriever: sparse</st></strong><st c="41620">
Content: sustainability, and we're partnering with them…
</st><strong class="bold"><st c="41678">Document 2: Document ID: 451 Score: 0.5 Rank: 2 Retriever: dense</st></strong><st c="41742">
Content: Empowering individuals: A parking lot full of electric vehicles lined up outside a Google office…
</st><strong class="bold"><st c="41850">Document 3: Document ID: 311 Score: 0.29166666666666663 Rank: 3 Retriever: both</st></strong><st c="41929">
Content: In 2022, we audited a subset of our suppliers to verify compliance for the following environmental…</st></pre>
			<p><st c="42038">While we are retrieving documents, we print out the document IDs so that we can see how many overlap. </st><st c="42141">Then, for each result, we print out the document ID, the ranking score, the rank, and what retriever produced that result (including </st><code><st c="42274">both</st></code><st c="42278"> if both retrieved it). </st><st c="42302">Note</st><a id="_idIndexMarker453"/><st c="42306"> that I cut off the full content here to only show the first 3 results of 10, as it was considerably longer in the output. </st><st c="42429">But if you run this in the notebook, you can see the </st><st c="42482">full output.</st></p>
			<p><st c="42494">If you look through the 10 results, the source retriever is </st><code><st c="42555">sparse</st></code><st c="42561">, </st><code><st c="42563">dense</st></code><st c="42568">, </st><code><st c="42570">both</st></code><st c="42574">, </st><code><st c="42576">sparse</st></code><st c="42582">, </st><code><st c="42584">dense</st></code><st c="42589">, </st><code><st c="42591">sparse</st></code><st c="42597">, </st><code><st c="42599">dense</st></code><st c="42604">, </st><code><st c="42606">dense</st></code><st c="42611">, </st><code><st c="42613">sparse</st></code><st c="42619">, and </st><code><st c="42625">sparse</st></code><st c="42631">. This is a relatively even distribution across the different searching mechanisms, including one result that came from both, pushing it further up the ranking. </st><st c="42792">The ranking scores were </st><code><st c="42816">0.5</st></code><st c="42819">, </st><code><st c="42821">0.5</st></code><st c="42824">, </st><code><st c="42826">0.29</st></code><st c="42830">, </st><code><st c="42832">0.25</st></code><st c="42836">, and </st><code><st c="42842">0.25</st></code><st c="42846">, </st><code><st c="42848">0.17</st></code><st c="42852">, </st><code><st c="42854">0.125</st></code><st c="42859">, </st><code><st c="42861">0.1</st></code><st c="42864">, </st><code><st c="42866">0.1</st></code><st c="42869">, </st><code><st c="42871">0.83</st></code><st c="42875">.</st></p>
			<p><st c="42876">This is the response we saw when we were just using </st><code><st c="42929">dense</st></code><st c="42934"> embeddings:</st></p>
			<pre class="source-code"><st c="42946">
Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, and promoting a circular economy. </st><st c="43200">They have reached a goal to help 1 billion people make more sustainable choices through their products and aim to collectively reduce 1 gigaton of carbon equivalent emissions annually by 2030. </st><st c="43393">Google also audits suppliers for compliance with environmental criteria and is involved in public policy and advocacy efforts. </st><st c="43520">Additionally, Google is a founding member of the iMasons Climate Accord, provided funding for the ReFED Catalytic Grant Fund to address food waste, and supported projects with The Nature Conservancy to promote reforestation and stop deforestation.</st></pre>
			<p><st c="43767">At this point, judging which version is better is a little subjective, but we will cover a more objective approach in </st><a href="B22475_09.xhtml#_idTextAnchor184"><em class="italic"><st c="43886">Chapter 9</st></em></a><st c="43895"> when we talk about RAG evaluation. </st><st c="43931">In the meantime, let’s just look at a few things that stand out. </st><st c="43996">Our hybrid search version seems to have broader coverage of the </st><st c="44060">different initiatives.</st></p>
			<p><st c="44082">This is the hybrid </st><st c="44102">search approach:</st></p>
			<pre class="source-code"><st c="44118">
Google's environmental initiatives include partnering with suppliers to reduce energy consumption and GHG emissions, engaging with suppliers to report and manage emissions, empowering individuals to take action through sustainability features in products, working together with partners and customers to reduce carbon emissions, operating sustainably at their campuses, focusing on net-zero carbon energy, water stewardship, circular economy practices, and supporting various environmental projects and initiatives such as the iMasons Climate Accord, ReFED, and The Nature Conservancy.</st></pre>
			<p><st c="44704">This is the </st><a id="_idIndexMarker454"/><st c="44717">dense </st><st c="44723">search approach:</st></p>
			<pre class="source-code"><st c="44739">
Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, and promoting a circular economy.</st></pre>
			<p><st c="44992">You might say the dense search approach focuses on more precise details, but whether that is a good or bad thing is subjective. </st><st c="45121">For example, you do not see anything about the one billion people goal in the hybrid search, but you see it here in the </st><st c="45241">dense search:</st></p>
			<pre class="source-code"><st c="45254">
They have reached a goal to help 1 billion people make more sustainable choices through their products and aim to collectively reduce 1 gigaton of carbon equivalent emissions annually by 2030.</st></pre>
			<p><st c="45447">The hybrid search took a more general approach, saying </st><st c="45503">the following:</st></p>
			<pre class="source-code"><st c="45517">
They also work on sustainable consumption of public goods and engage with coalitions and sustainability initiatives to promote environmental sustainability.</st></pre>
			<p><st c="45674">You can run this code with other questions and see how they compare across the different </st><st c="45764">search approaches.</st></p>
			<p><st c="45782">Okay, we did a lot of work to set up this function, but now we are going to look at what LangChain is offering and replace our </st><st c="45910">function completely.</st></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor168"/><st c="45930">Code lab 8.3 – Hybrid search with LangChain’s EnsembleRetriever to replace our custom function</st></h1>
			<p><st c="46025">The file you need to access from the GitHub repository is </st><st c="46084">titled </st><code><st c="46091">CHAPTER8-3_HYBRID-ENSEMBLE.ipynb</st></code><st c="46123">.</st></p>
			<p><st c="46124">We </st><a id="_idIndexMarker455"/><st c="46128">continue this code from the last lab starting with the </st><code><st c="46183">CHAPTER8-2_HYBRID-CUSTOM.ipynb</st></code><st c="46213"> file. </st><st c="46220">The complete code for this code lab is </st><code><st c="46259">CHAPTER8-3_HYBRID-ENSEMBLE.ipynb</st></code><st c="46291">. First, we need to import the retriever from LangChain; add this to </st><st c="46360">your imports:</st></p>
			<pre class="source-code"><st c="46373">
from langchain.retrievers import EnsembleRetriever</st></pre>
			<p><st c="46424">This adds </st><code><st c="46435">EnsembleRetriever</st></code><st c="46452"> from LangChain to be used as a third retriever that combines the other two retrievers. </st><st c="46540">Note that previously, in </st><em class="italic"><st c="46565">Code lab 8.2</st></em><st c="46577">, we added </st><code><st c="46588">k=10</st></code><st c="46592"> to each of the two retrievers to make sure we got enough responses to be similar to the </st><st c="46681">other response.</st></p>
			<p><st c="46696">In the past, we just had one set of documents that we defined as </st><code><st c="46762">documents</st></code><st c="46771">, but here we want to change the name of those documents to </st><code><st c="46831">dense_documents</st></code><st c="46846">, and then add a second set of documents </st><st c="46887">called </st><code><st c="46894">sparse_documents</st></code><st c="46910">:</st></p>
			<pre class="source-code"><st c="46912">
dense_documents = [Document(page_content=text,
    metadata={"id": str(i), "source": "dense"}) for i,
    text in enumerate(splits)]
sparse_documents = [Document(page_content=text,
    metadata={"id": str(i), "source": "sparse"}) for i,
    text in enumerate(splits)]</st></pre>
			<p><st c="47164">This allowed me to tag the dense documents with the </st><code><st c="47217">"dense"</st></code><st c="47224"> source and the sparse documents with the </st><code><st c="47266">"sparse"</st></code><st c="47274"> source in their metadata. </st><st c="47301">We pass this through to the final results and can use it to show the source for each document. </st><st c="47396">This is not as effective as the approach we used in our custom function though, because when the content comes from both sources, it does not indicate both. </st><st c="47553">This highlights an advantage of creating our </st><st c="47598">own function.</st></p>
			<p><st c="47611">We then want to add our new type of retriever, </st><code><st c="47659">EnsembleRetriever</st></code><st c="47676">, which we will add to the bottom of the cell where we define the other </st><st c="47748">two retrievers:</st></p>
			<pre class="source-code"><st c="47763">
ensemble_retriever = EnsembleRetriever(retrievers=[
    dense_retriever, sparse_retriever], weights=[0.5, 0.5],
    c=0)</st></pre>
			<p><code><st c="47876">ensemble_retriever</st></code><st c="47895"> takes both retrievers, weights for how to emphasize them, and a </st><code><st c="47960">c</st></code><st c="47961"> value. </st><st c="47969">The </st><code><st c="47973">c</st></code><st c="47974"> value is described as a constant added to the rank, controlling the balance </st><a id="_idIndexMarker456"/><st c="48051">between the importance of high-ranked items and the consideration given to lower-ranked items. </st><st c="48146">The default is </st><code><st c="48161">60</st></code><st c="48163">, but I set it to </st><code><st c="48181">0</st></code><st c="48182">. We don’t have a </st><code><st c="48200">c</st></code><st c="48201"> parameter in our function, so that would make it difficult to compare results! </st><st c="48281">But that can be a handy parameter if you want more IDs to float up from </st><st c="48353">the bottom.</st></p>
			<p><st c="48364">You can remove our </st><code><st c="48384">hybrid_search</st></code><st c="48397"> function altogether. </st><st c="48419">Delete the entire cell that starts with </st><st c="48459">this code:</st></p>
			<pre class="source-code"><st c="48469">
def hybrid_search(query, k=10, dense_weight=0.5,
    sparse_weight=0.5):</st></pre>
			<p><st c="48538">Next, we update the </st><code><st c="48559">"context"</st></code><st c="48568"> input from </st><code><st c="48580">rag_chain_with_source</st></code><st c="48601"> with the </st><st c="48611">new retriever:</st></p>
			<pre class="source-code"><st c="48625">
rag_chain_with_source = RunnableParallel(
    {"context": ensemble_retriever,
     "question": RunnablePassthrough()}
).assign(answer=rag_chain_from_docs)</st></pre>
			<p><st c="48771">Now our code for output has to change because we no longer have all that metadata we were able to add with the </st><st c="48883">custom function:</st></p>
			<pre class="source-code"><st c="48899">
user_query = "What are Google's environmental initiatives?"
</st><st c="48960">result = rag_chain_with_source.invoke(user_query)
relevance_score = result['answer']['relevance_score']
final_answer = result['answer']['final_answer']
retrieved_docs = result['context']
print(f"Original Question: {user_query}\n")
print(f"Relevance Score: {relevance_score}\n")
print(f"Final Answer:\n{final_answer}\n\n")
print("Retrieved Documents:")
for i, doc in enumerate(retrieved_docs, start=1):
    print(f"Document {i}: Document ID: {doc.metadata['id']}
        source: {doc.metadata['source']}")
    print(f"Content:\n{doc.page_content}\n")</st></pre>
			<p><st c="49493">The </st><a id="_idIndexMarker457"/><st c="49498">output looks </st><st c="49511">like this:</st></p>
			<pre class="source-code"><st c="49521">
Original Question: What are Google's environmental initiatives?
</st><st c="49586">Relevance Score: 5
Final Answer:
Google's environmental initiatives include being a founding member of the iMasons Climate Accord, providing funding for the ReFED Catalytic Grant Fund to address food waste, supporting projects with The Nature Conservancy for reforestation and deforestation prevention, engaging with suppliers to reduce energy consumption and emissions, auditing suppliers for environmental compliance, addressing climate-related risks, advocating for sustainable consumption of public goods, engaging with coalitions like the RE-Source Platform, and working on improving data center efficiency.
</st><st c="50199">Retrieved Documents:
</st><strong class="bold"><st c="50220">Document 1: Document ID: 344 source: dense</st></strong><st c="50262">
Content:
iMasons Climate AccordGoogle is a founding member and part…
</st><strong class="bold"><st c="50332">Document 2: Document ID: 150 source: sparse</st></strong><st c="50375">
Content:
sustainability, and we're partnering with them to develop decarbonization roadmaps…
</st><strong class="bold"><st c="50469">Document 3: Document ID: 309 source: dense</st></strong><st c="50511">
Content:
that enable us to ensure that those we partner with are responsible environmental stewards…</st></pre>
			<p><st c="50612">The result is</st><a id="_idIndexMarker458"/><st c="50626"> almost exactly the same as our function, but with the dense search results winning any ties in the order (in our function, the sparse result is winning), which is pretty minor, but something you can easily address by changing the weights. </st><st c="50866">Remember that </st><code><st c="50880">c</st></code><st c="50881"> value though? </st><st c="50896">If you change that, you see big changes in the results. </st><st c="50952">With more time, we should go back and add a </st><code><st c="50996">c</st></code><st c="50997"> value to our function, but </st><st c="51025">I digress!</st></p>
			<p><st c="51035">Building our own function certainly gave us more flexibility and allowed us to see and change the inner workings of the function. </st><st c="51166">With the LangChain </st><code><st c="51185">EnsembleRetriever</st></code><st c="51202">, we cannot change any steps in the search or ranking to better fit our needs and we have little quirks such as the </st><code><st c="51318">"source"</st></code><st c="51326"> metadata issue where we don’t know whether or when it is coming from both sources. </st><st c="51410">It is difficult to judge what is a better approach from this small example. </st><st c="51486">The reality is that everything you do will need consideration and you will have to decide for yourself what works in </st><st c="51603">your situation.</st></p>
			<p><st c="51618">If hybrid search is important, you may want to consider a vector database or vector search service that gives you more features and flexibility in defining your hybrid search. </st><st c="51795">LangChain provides weights that allow you to emphasize one of the search mechanisms over the other, but as of right now, you can only use the built-in ranking mechanism in the RRF. </st><st c="51976">Weaviate, for example, lets you pick from two different ranking algorithms. </st><st c="52052">This is yet another consideration to take into account when making decisions on what infrastructure to use in your </st><st c="52167">RAG pipeline.</st></p>
			<p><st c="52180">Next, let’s talk about the algorithms that use </st><st c="52228">these distances.</st></p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor169"/><st c="52244">Semantic search algorithms</st></h1>
			<p><st c="52271">We have discussed the concept of a semantic search in depth. </st><st c="52333">Our next step is to walk through the different approaches we can take to conduct a semantic search. </st><st c="52433">These are the actual search algorithms that use things such as the distance metrics we’ve already discussed (Euclidean distance, dot product, and cosine similarity) to conduct their search of dense embeddings. </st><st c="52643">We start with </st><strong class="bold"><st c="52657">k-nearest </st></strong><strong class="bold"><st c="52667">neighbors</st></strong><st c="52676"> (</st><strong class="bold"><st c="52678">k-NN</st></strong><st c="52682">).</st></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor170"/><st c="52685">k-NN</st></h2>
			<p><st c="52690">One way to</st><a id="_idIndexMarker459"/><st c="52701"> find </st><a id="_idIndexMarker460"/><st c="52707">similar vectors is through brute force. </st><st c="52747">With brute force, you find the distances between the query and all the data vectors. </st><st c="52832">Then, you sort the distances from closest to furthest, returning a</st><a id="_idIndexMarker461"/><st c="52898"> certain number of results. </st><st c="52926">You can cut off the results based on a threshold, or you can define a set number to return, such as </st><code><st c="53026">5</st></code><st c="53027">. The set number is called </st><code><st c="53054">k</st></code><st c="53055">, so you would say </st><code><st c="53074">k=5</st></code><st c="53077">. This is known in classical machine learning as the k-NN algorithm. </st><st c="53146">This is a straightforward algorithm, but its performance degrades as the dataset grows. </st><st c="53234">The increase in computational cost for this algorithm is linear based on the amount of data you are querying. </st><st c="53344">The time complexity is represented by </st><code><st c="53382">O(n * d)</st></code><st c="53390">, where </st><code><st c="53398">n</st></code><st c="53399"> is the number of instances in the training dataset and </st><code><st c="53455">d</st></code><st c="53456"> is the dimensionality of the data. </st><st c="53492">This means that if your data doubles, the query time will double. </st><st c="53558">For large datasets that reach into the millions or even billions of data points, brute force comparison between every pair of items can become </st><st c="53701">computationally infeasible.</st></p>
			<p><st c="53728">If you have a relatively small dataset, it may be worth considering k-NN, as it is considered to be more accurate than the next approach we discuss. </st><st c="53878">What constitutes </st><em class="italic"><st c="53895">small</st></em><st c="53900"> can be dependent on your data and embedding dimensions, but I have used k-NN successfully for projects with 25,000 to 30,000 embeddings and 256 dimensions. </st><st c="54057">I’ve seen a 2–6% improvement in the retrieval evaluation metrics we will talk about in </st><a href="B22475_09.xhtml#_idTextAnchor184"><em class="italic"><st c="54144">Chapter 9</st></em></a><st c="54153">, which is significant enough for me to offset the small increase in </st><st c="54222">computation cost.</st></p>
			<p><st c="54239">But what about all the distance metrics we just discussed; where do those come in with k-NN? </st><st c="54333">k-NN can use any of those distance metrics to determine the similarity between the query vector and the vectors in the dataset. </st><st c="54461">The most common distance metric used in k-NN is Euclidean distance. </st><st c="54529">Other distance metrics, such as Manhattan distance (also known as city block distance) or cosine similarity, can also be used, depending on the nature of the data and the problem at hand. </st><st c="54717">The choice of distance metric can significantly impact the performance of the k-NN algorithm. </st><st c="54811">Once the distances between the query vector and all the vectors in the dataset are calculated, k-NN sorts the distances and</st><a id="_idIndexMarker462"/><st c="54934"> selects the </st><code><st c="54947">k</st></code><st c="54948"> nearest neighbors based on the chosen </st><st c="54987">distance metric.</st></p>
			<p><st c="55003">If you find your dataset has outgrown k-NN, there are many other algorithms that allow us to find the nearest vectors in a more efficient way. </st><st c="55147">In general, we call this </st><strong class="bold"><st c="55172">Approximate Nearest Neighbors</st></strong><st c="55201"> (</st><strong class="bold"><st c="55203">ANN</st></strong><st c="55206">), which we will </st><st c="55224">discuss next.</st></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor171"/><st c="55237">ANN</st></h2>
			<p><st c="55241">ANN is a </st><a id="_idIndexMarker463"/><st c="55251">family of</st><a id="_idIndexMarker464"/><st c="55260"> algorithms designed to address the scalability limitations of k-NN while still providing satisfactory results. </st><st c="55372">ANN algorithms aim to find the most similar vectors to a query vector in a more efficient manner, sacrificing some accuracy for </st><st c="55500">improved performance.</st></p>
			<p><st c="55521">Compared to k-NN, which performs an exhaustive search by calculating the distances between the query vector and all data vectors, ANN algorithms employ various techniques to reduce the search space and speed up the retrieval process. </st><st c="55756">These techniques include indexing, partitioning, and approximation methods that allow ANN algorithms to focus on a subset of the data points that are likely to be the </st><st c="55923">nearest neighbors.</st></p>
			<p><st c="55941">One key difference between k-NN and ANN is the trade-off between accuracy and efficiency. </st><st c="56032">While k-NN guarantees finding the exact </st><code><st c="56072">k</st></code><st c="56073"> nearest neighbors, it becomes computationally expensive as the dataset grows. </st><st c="56152">On the other hand, ANN algorithms prioritize efficiency by approximating the nearest neighbors, accepting the possibility of missing some of the true nearest neighbors in exchange for faster </st><st c="56343">retrieval times.</st></p>
			<p><st c="56359">ANN algorithms often</st><a id="_idIndexMarker465"/><st c="56380"> leverage </st><strong class="bold"><st c="56390">indexing structures</st></strong><st c="56409"> such as </st><strong class="bold"><st c="56418">hierarchical trees</st></strong><st c="56436"> (e.g., KD-trees, Ball trees), </st><strong class="bold"><st c="56467">hashing techniques</st></strong><st c="56485"> (e.g., </st><strong class="bold"><st c="56493">Locality-Sensitive Hashing</st></strong><st c="56519"> (</st><strong class="bold"><st c="56521">LSH</st></strong><st c="56524">)), or </st><strong class="bold"><st c="56532">graph-based methods</st></strong><st c="56551"> (e.g., </st><strong class="bold"><st c="56559">Hierarchical Navigable Small World</st></strong><st c="56593"> (</st><strong class="bold"><st c="56595">HNSW</st></strong><st c="56599">)) to </st><a id="_idIndexMarker466"/><st c="56606">organize</st><a id="_idIndexMarker467"/><st c="56614"> the data </st><a id="_idIndexMarker468"/><st c="56624">points in a way that facilitates efficient</st><a id="_idIndexMarker469"/><st c="56666"> search. </st><st c="56675">These indexing structures allow ANN algorithms to quickly narrow down the search space and identify candidate neighbors without exhaustively comparing the query vector to every data point. </st><st c="56864">We will talk in more depth about indexing approaches for ANN in the </st><st c="56932">next section.</st></p>
			<p><st c="56945">The time complexity of ANN algorithms varies depending on the specific algorithm and indexing technique used. </st><st c="57056">However, in general, ANN algorithms aim to achieve sublinear search times, meaning that the query time grows more slowly than the size of the dataset. </st><st c="57207">This makes ANN algorithms more suitable for large-scale datasets where the computational cost of k-NN </st><st c="57309">becomes prohibitive.</st></p>
			<p><st c="57329">Again, what</st><a id="_idIndexMarker470"/><st c="57341"> about those distance metrics? </st><st c="57372">Well, like k-NN, ANN algorithms rely on distance metrics to measure </st><a id="_idIndexMarker471"/><st c="57440">the similarity between the query vector and the vectors in the dataset. </st><st c="57512">The choice of distance metric depends on the nature of the data and the problem at hand. </st><st c="57601">Common distance metrics used in ANN include Euclidean distance, Manhattan distance, and cosine similarity. </st><st c="57708">However, unlike k-NN, which calculates distances between the query vector and all data vectors, ANN algorithms employ indexing structures and approximation techniques to reduce the number of distance calculations. </st><st c="57922">These techniques allow ANN algorithms to quickly identify a subset of candidate neighbors that are likely to be close to the query vector. </st><st c="58061">The distance metric is then applied to this subset to determine the approximate nearest neighbors, rather than computing distances for the entire dataset. </st><st c="58216">By minimizing the number of distance calculations, ANN algorithms can significantly speed up the retrieval process while still providing </st><st c="58353">satisfactory results.</st></p>
			<p><st c="58374">It’s important to note that the choice between k-NN and ANN depends on the specific requirements of the application. </st><st c="58492">If exact nearest neighbors are critical and the dataset is relatively small, k-NN may still be a viable option. </st><st c="58604">However, when dealing with massive datasets or when near real-time retrieval is required, ANN algorithms provide a practical solution by striking a balance between accuracy </st><st c="58777">and efficiency.</st></p>
			<p><st c="58792">In summary, ANN algorithms can offer a more scalable and efficient alternative to k-NN for finding similar vectors in large datasets. </st><st c="58927">By employing indexing techniques and approximation methods, ANN algorithms can significantly reduce the search space and retrieval times, making them suitable for applications that require fast and scalable </st><st c="59134">similarity search.</st></p>
			<p><st c="59152">While it is important to understand what ANN is, it is just as important to know that the real benefit is in all the ways you can enhance it. </st><st c="59295">Let’s review some of those </st><st c="59322">techniques next.</st></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor172"/><st c="59338">Enhancing search with indexing techniques</st></h1>
			<p><st c="59380">ANN and k-NN search </st><a id="_idIndexMarker472"/><st c="59401">are fundamental solutions in computer science and machine learning, with applications in various domains such as image retrieval, recommendation systems, and similarity search. </st><st c="59578">While search algorithms play a crucial role in ANN and k-NN, indexing techniques and data structures are equally important for enhancing the efficiency and performance of </st><st c="59749">these algorithms.</st></p>
			<p><st c="59766">These </st><a id="_idIndexMarker473"/><st c="59773">indexing techniques are used to optimize the search process by reducing the number of vectors that need to be compared during the search. </st><st c="59911">They help in quickly identifying a smaller subset of candidate vectors that are likely to be similar to the query vector. </st><st c="60033">The search algorithms (such as k-NN, ANN, or other similarity search algorithms) can then operate on this reduced set of candidate vectors to find the actual nearest neighbors or </st><st c="60212">similar vectors.</st></p>
			<p><st c="60228">All these techniques aim to improve the efficiency and scalability of similarity search by reducing the search space and enabling faster retrieval of relevant vectors. </st><st c="60397">However, each has its own advantages and trade-offs in terms of indexing time, search time, memory usage, and accuracy. </st><st c="60517">The choice of technique depends on the specific requirements of the application, such as the dimensionality of the vectors, the desired level of accuracy, and the available </st><st c="60690">computational resources.</st></p>
			<p><st c="60714">In practice, these techniques can be used independently or in combination to achieve the best performance for a given vector search task. </st><st c="60853">Some libraries and frameworks, such as Facebook AI Similarity Search (FAISS) and pgvector, provide implementations of multiple indexing techniques, including PQ (product quantization), HNSW, and LSH, allowing users to choose the most suitable technique for their specific </st><st c="61125">use case.</st></p>
			<p><st c="61134">Before we dive in, let’s review where we are so far. </st><st c="61188">There are distance/similarity metrics (e.g., cosine similarity, Euclidean distance, and dot product) used by the search algorithms. </st><st c="61320">These search algorithms include k-NN, ANN, and others. </st><st c="61375">The search algorithms can use indexing techniques such as LSH, KD-trees, Ball trees, PQ, and HNSW to improve their efficiency </st><st c="61501">and scalability.</st></p>
			<p><st c="61517">Okay, are we all caught up? </st><st c="61546">Great! </st><st c="61553">Let’s talk more about several indexing techniques that complement search algorithms and improve the overall efficiency of </st><st c="61675">ANN search:</st></p>
			<ul>
				<li><strong class="bold"><st c="61686">LSH</st></strong><st c="61690">: LSH is an </st><a id="_idIndexMarker474"/><st c="61703">indexing technique that maps similar </st><a id="_idIndexMarker475"/><st c="61740">vectors to the same hash buckets with high probability. </st><st c="61796">The goal of LSH is to quickly identify potential candidate vectors for similarity search by reducing the search space. </st><st c="61915">It achieves this by dividing the</st><a id="_idIndexMarker476"/><st c="61947"> vector </st><a id="_idIndexMarker477"/><st c="61955">space into regions using hash functions, where similar items are more likely to be hashed to the </st><st c="62052">same bucket.</st><p class="list-inset"><st c="62064">LSH offers a </st><a id="_idIndexMarker478"/><st c="62078">trade-off between accuracy and efficiency. </st><st c="62121">By using LSH as a preprocessing step, the set of vectors that need to be examined by the search algorithm can be significantly narrowed down. </st><st c="62263">This reduces the computational overhead and improves the overall </st><st c="62328">search performance.</st></p></li>
				<li><strong class="bold"><st c="62347">Tree-based indexing</st></strong><st c="62367">: Tree-based indexing</st><a id="_idIndexMarker479"/><st c="62389"> techniques</st><a id="_idIndexMarker480"/><st c="62400"> organize vectors into hierarchical structures based on their spatial properties. </st><st c="62482">Two popular tree-based indexing techniques </st><a id="_idIndexMarker481"/><st c="62525">are </st><strong class="bold"><st c="62529">KD-trees</st></strong><st c="62537"> and </st><strong class="bold"><st c="62542">Ball trees</st></strong><st c="62552">.</st><p class="list-inset"><st c="62553">KD-trees</st><a id="_idIndexMarker482"/><st c="62562"> are binary space partitioning</st><a id="_idIndexMarker483"/><st c="62592"> trees used for organizing points in a </st><em class="italic"><st c="62631">k</st></em><st c="62632">-dimensional space. </st><st c="62652">They recursively divide the space into subregions based on the dimensions of the vectors. </st><st c="62742">During the search process, KD-trees enable efficient nearest neighbor search by pruning irrelevant branches of </st><st c="62853">the tree.</st></p><p class="list-inset"><st c="62862">Ball trees, on</st><a id="_idIndexMarker484"/><st c="62877"> the other hand, partition the data points</st><a id="_idIndexMarker485"/><st c="62919"> into nested hyperspheres. </st><st c="62946">Each node in the tree represents a hypersphere that encapsulates a subset of the data points. </st><st c="63040">Ball trees are particularly effective for nearest-neighbor search in </st><st c="63109">high-dimensional spaces.</st></p><p class="list-inset"><st c="63133">Both KD-trees and Ball trees provide a way to efficiently navigate through possible candidates and speed up the </st><st c="63246">search process.</st></p></li>
				<li><strong class="bold"><st c="63261">PQ</st></strong><st c="63264">: PQ is a</st><a id="_idIndexMarker486"/><st c="63274"> compression and indexing technique that quantizes</st><a id="_idIndexMarker487"/><st c="63324"> vectors into a set of sub-vectors and represents them using code books. </st><st c="63397">Do you remember the earlier discussion about quantizing vectors? </st><st c="63462">We use those same concepts here. </st><st c="63495">PQ allows for compact</st><a id="_idIndexMarker488"/><st c="63516"> storage and efficient distance computation by approximating the distances between the query vector and the </st><st c="63624">quantized vectors.</st><p class="list-inset"><st c="63642">PQ is particularly</st><a id="_idIndexMarker489"/><st c="63661"> effective for high-dimensional vectors and has been widely used in applications like image retrieval and recommendation systems. </st><st c="63791">By compressing the vectors and approximating distances, PQ reduces the memory footprint and computational cost of </st><st c="63905">similarity search.</st></p></li>
				<li><strong class="bold"><st c="63923">HNSW</st></strong><st c="63928">: HNSW is a</st><a id="_idIndexMarker490"/><st c="63940"> graph-based indexing technique that </st><a id="_idIndexMarker491"/><st c="63977">builds a hierarchical structure of interconnected nodes to enable fast approximate nearest neighbor search. </st><st c="64085">It creates a multi-layer graph where each layer represents a different level of abstraction, allowing for efficient traversal and retrieval of approximate </st><st c="64240">nearest neighbors.</st><p class="list-inset"><st c="64258">HNSW is highly </st><a id="_idIndexMarker492"/><st c="64274">scalable and solves the runtime complexity issues of brute-force KNN search. </st><st c="64351">It is offered by the most advanced vector databases and has gained popularity due to its high performance and scalability, especially for </st><st c="64489">high-dimensional data.</st></p><p class="list-inset"><st c="64511">The NSW part of HNSW works by finding vectors that are well-positioned among many other vectors (closeness-wise) relative to other vectors. </st><st c="64652">These vectors become the starting points for the search. </st><st c="64709">The number of connections counted among nodes can be defined, allowing for the selection of the best-positioned nodes to connect to a large number of </st><st c="64859">other nodes.</st></p></li>
			</ul>
			<p><st c="64871">During a query, the </st><a id="_idIndexMarker493"/><st c="64892">search algorithm starts with a random entry node and moves toward the nearest neighbor to the query vector. </st><st c="65000">For each node that gets closer, the distance from the user query node to the current node is recalculated, and the next node that gets the closest among the current node’s network connections is selected. </st><st c="65205">This process traverses across nodes, skipping large parts of the data, making it </st><st c="65286">significantly faster.</st></p>
			<p><st c="65307">The hierarchical (H) part of HNSW adds several layers of navigable small worlds on top of each other. </st><st c="65410">This can be imagined as traveling to a place by taking a plane to the nearest airport, then catching a train to a town, and finally searching within a much smaller set of </st><em class="italic"><st c="65581">node</st></em><st c="65585"> locations to find the </st><st c="65608">desired location.</st></p>
			<p class="callout-heading"><st c="65625">Fun fact</st></p>
			<p class="callout"><st c="65634">HNSW is inspired by the observed phenomenon in human social networks where everyone is closely connected, such</st><a id="_idIndexMarker494"/><st c="65745"> as the </st><strong class="bold"><st c="65753">six degrees of separation</st></strong><st c="65778"> concept. </st><em class="italic"><st c="65788">Six degrees of separation</st></em><st c="65813"> says that any two individuals are on average separated by six acquaintance links. </st><st c="65896">This concept was originally inspired by a 1929 short story by Frigyes Karinthy that described a group of people playing a game of trying to connect any person in the world to themselves by a chain of five others. </st><st c="66109">This chain of connections can be made to connect any two people in the world, in theory, by a maximum of six steps. </st><st c="66225">It is also known </st><a id="_idIndexMarker495"/><st c="66242">as the </st><strong class="bold"><st c="66249">six </st></strong><strong class="bold"><st c="66253">handshakes rule</st></strong><st c="66268">.</st></p>
			<p><st c="66269">All of these </st><a id="_idIndexMarker496"/><st c="66283">indexing techniques play a vital role in enhancing the efficiency and performance of ANN search algorithms. </st><st c="66391">LSH, tree-based indexing, PQ, and HNSW are some of the prominent indexing techniques </st><a id="_idIndexMarker497"/><st c="66476">used in conjunction with search algorithms. </st><st c="66520">By leveraging these indexing techniques, the search space can be reduced, irrelevant candidates can be pruned, and the overall search process can be accelerated. </st><st c="66682">Indexing techniques provide a way to organize and structure the data, enabling efficient retrieval and similarity search in </st><st c="66806">high-dimensional spaces.</st></p>
			<p><st c="66830">Now that we have added the indexing techniques to our repertoire, we still have another important aspect to talk about before we can start actually implementing these capabilities. </st><st c="67012">ANN and k-NN are not services you can just sign up for; they are search algorithm approaches that services and software packages use. </st><st c="67146">So, next, we need to learn what those packages are so that you can actually put them to use. </st><st c="67239">Let’s talk about </st><st c="67256">vector search!</st></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor173"/><st c="67270">Vector search options</st></h1>
			<p><st c="67292">In basic terms, vector search </st><a id="_idIndexMarker498"/><st c="67323">is the process of finding the vectors most similar to the query vector within the vector store. </st><st c="67419">The ability to quickly identify relevant vectors is crucial for the system’s overall performance, as it determines which pieces of information will be used by the LLM for generating responses. </st><st c="67612">This component bridges the gap between the raw user query and the data-rich inputs needed for high-quality generation. </st><st c="67731">There are numerous offerings and numerous types of offerings in the marketplace that you can use to conduct your vector search. </st><st c="67859">We have talked a lot so far about the components and concepts that make a good vector search. </st><st c="67953">You can apply that knowledge to selecting the best vector search option for your specific project needs. </st><st c="68058">Services are evolving quickly with new start-ups emerging every day, so it is worth your effort to do some deep research before deciding on an option. </st><st c="68209">In the next few subsections, we will look at the few options that can give you an idea of the breadth of what </st><st c="68319">is available.</st></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor174"/><st c="68332">pgvector</st></h2>
			<p><st c="68341">pgvector is an </st><a id="_idIndexMarker499"/><st c="68357">open source vector similarity search </st><a id="_idIndexMarker500"/><st c="68394">extension for PostgreSQL, a popular relational database management system. </st><st c="68469">It allows you to store and search vectors directly within PostgreSQL, leveraging its robust features and ecosystem. </st><st c="68585">pgvector supports various distance metrics and indexing algorithms, including L2 distance and cosine distance. </st><st c="68696">pgvector is one of the few services that supports both exact k-NN search and approximate k-NN search using ANN algorithms. </st><st c="68819">Indexing options </st><a id="_idIndexMarker501"/><st c="68836">include </st><strong class="bold"><st c="68844">Inverted File Index</st></strong><st c="68863"> (</st><strong class="bold"><st c="68865">IVF</st></strong><st c="68868">) and HNSW to accelerate the search process. </st><st c="68914">pgvector can be used to perform a k-NN search by specifying the desired number of nearest neighbors and choosing between exact or approximate search. </st><st c="69064">We’ve discussed HNSW thoroughly, but IVF is an indexing technique commonly used in combination with vector stores. </st><st c="69179">IVF is designed to efficiently identify a subset of vectors that are likely to be similar to a given query vector, reducing the number of distance calculations required during the search process. </st><st c="69375">IVF can be used in combination with HNSW, improving efficiencies and speed even further. </st><st c="69464">pgvector integrates seamlessly with existing PostgreSQL tools and libraries, making it easy to incorporate vector search into applications that already use PostgreSQL. </st><st c="69632">pgvector is a good fit if you are already using PostgreSQL and want to add vector search capabilities without introducing a separate system. </st><st c="69773">It benefits from the reliability, scalability, and transaction support </st><st c="69844">of PostgreSQL.</st></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/><st c="69858">Elasticsearch</st></h2>
			<p><st c="69872">Elasticsearch</st><a id="_idIndexMarker502"/><st c="69886"> is a popular open source search and analytics</st><a id="_idIndexMarker503"/><st c="69932"> engine that supports vector similarity search. </st><st c="69980">It is widely adopted and has a large ecosystem of plugins and integrations. </st><st c="70056">Elasticsearch uses ANN algorithms, particularly HNSW, for efficient vector similarity search. </st><st c="70150">It does not explicitly use k-NN, but the similarity search functionality can be used to find the </st><em class="italic"><st c="70247">k</st></em><st c="70248">-nearest neighbors. </st><st c="70268">Elasticsearch offers advanced search capabilities, including full-text search, aggregations, and geospatial search, as well as a distributed architecture that allows for high scalability and fault tolerance. </st><st c="70476">It integrates well with LangChain and offers robust scalability, distributed architecture, and a wide range of features. </st><st c="70597">Elasticsearch is a good fit if you already have experience with it or need its advanced search and analytics capabilities. </st><st c="70720">However, it may require more setup and configuration compared to some other </st><st c="70796">vector stores.</st></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor176"/><st c="70810">FAISS</st></h2>
			<p><strong class="bold"><st c="70816">FAISS</st></strong><st c="70822"> is a</st><a id="_idIndexMarker504"/><st c="70827"> library for efficient similarity search and clustering of </st><a id="_idIndexMarker505"/><st c="70886">dense vectors, developed by Facebook. </st><st c="70924">It is known for its exceptional performance and ability to handle billion-scale vector datasets. </st><st c="71021">FAISS heavily relies on ANN algorithms for efficient similarity search offering a wide range of ANN indexing and search algorithms, including IVF, PQ, and HNSW. </st><st c="71182">FAISS can be used to perform a k-NN search by retrieving the </st><em class="italic"><st c="71243">k</st></em><st c="71244"> most similar vectors to a query vector. </st><st c="71285">FAISS offers a wide range of indexing and search algorithms, including quantization-based methods for compact vector representation, and provides GPU support for accelerated similarity search. </st><st c="71478">It can be used as a vector store and integrates with LangChain. </st><st c="71542">FAISS is a good choice if you have high-performance requirements and are comfortable working with a lower-level library. </st><st c="71663">However, it may require more manual setup and management compared to </st><st c="71732">managed services.</st></p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor177"/><st c="71749">Google Vertex AI Vector Search</st></h2>
			<p><st c="71780">Google Vertex AI Vector Search</st><a id="_idIndexMarker506"/><st c="71811"> is a fully</st><a id="_idIndexMarker507"/><st c="71822"> managed vector similarity search service offered by GCP that includes both vector storage and search capabilities. </st><st c="71938">Google Vertex AI Vector Search </st><a id="_idIndexMarker508"/><st c="71969">uses ANN algorithms under the hood to enable fast and scalable vector similarity search. </st><st c="72058">The specific ANN algorithms used are not disclosed, but it likely employs state-of-the-art techniques based on Google ScaNN (Scalable, Channel-Aware Nearest Neighbors). </st><st c="72227">It can be used to perform a k-NN search by specifying the desired number of nearest neighbors. </st><st c="72322">When you use Vertex AI Vector Search, the vectors are stored within the managed service itself. </st><st c="72418">It </st><a id="_idIndexMarker509"/><st c="72421">integrates seamlessly with other Google Cloud services, such as BigQuery and Dataflow, enabling efficient data processing pipelines. </st><st c="72554">Vertex AI Vector Search offers features such as online updates, allowing you to incrementally add or delete vectors without rebuilding the entire index. </st><st c="72707">It integrates with LangChain and provides scalability, high availability, and easy integration with other Google Cloud services. </st><st c="72836">If you are already using Google Cloud and want a managed solution with minimal setup, Vertex AI Vector Search is a good option. </st><st c="72964">However, it may be more expensive compared to </st><st c="73010">self-hosted solutions.</st></p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/><st c="73032">Azure AI Search</st></h2>
			<p><st c="73048">Azure AI Search is a</st><a id="_idIndexMarker510"/><st c="73069"> fully managed search service provided </st><a id="_idIndexMarker511"/><st c="73108">by Microsoft Azure. </st><st c="73128">It supports vector similarity search alongside traditional keyword-based search. </st><st c="73209">Azure AI Search utilizes ANN algorithms for efficient vector similarity search. </st><st c="73289">The exact ANN algorithms used are not specified, but it leverages advanced indexing techniques to enable fast retrieval of similar vectors. </st><st c="73429">It can be used to perform a k-NN search by querying for the </st><em class="italic"><st c="73489">k</st></em><st c="73490"> nearest neighbors. </st><st c="73510">Azure AI Search offers features such as synonyms, autocomplete, and faceted navigation. </st><st c="73598">It integrates with Azure Machine Learning for seamless deployment of machine learning models. </st><st c="73692">Azure AI Search is a good choice if you are already using Azure services and want a managed solution with advanced search capabilities. </st><st c="73828">However, it may have a steeper learning curve compared to some </st><st c="73891">other options.</st></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor179"/><st c="73905">Approximate Nearest Neighbors Oh Yeah</st></h2>
			<p><strong class="bold"><st c="73943">Approximate Nearest Neighbors Oh Yeah</st></strong><st c="73981"> (</st><strong class="bold"><st c="73983">ANNOY</st></strong><st c="73988">) is an open source library developed </st><a id="_idIndexMarker512"/><st c="74027">by Spotify </st><a id="_idIndexMarker513"/><st c="74038">for ANN search. </st><st c="74054">It is known for its fast indexing and querying speed, as well as its ability to handle high-dimensional vectors efficiently. </st><st c="74179">It uses a combination of random projections and binary space partitioning to build a forest of trees that enables a fast similarity search. </st><st c="74319">ANNOY can be used to perform a k-NN search by retrieving the </st><em class="italic"><st c="74380">k</st></em><st c="74381"> approximate nearest neighbors. </st><st c="74413">ANNOY </st><a id="_idIndexMarker514"/><st c="74419">uses a combination of random projections and binary space partitioning to build a forest of trees that enables a fast similarity search. </st><st c="74556">It has a simple and intuitive API, making it easy to integrate into existing projects. </st><st c="74643">ANNOY is a good fit if you prioritize speed and have a smaller dataset. </st><st c="74715">However, it may not scale as well as some other options for extremely </st><st c="74785">large datasets.</st></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/><st c="74800">Pinecone</st></h2>
			<p><st c="74809">Pinecone is a</st><a id="_idIndexMarker515"/><st c="74823"> fully managed vector database designed</st><a id="_idIndexMarker516"/><st c="74862"> specifically for machine learning applications. </st><st c="74911">It offers high-performance vector similarity search and supports both dense and sparse vector representations. </st><st c="75022">Pinecone employs ANN algorithms to achieve its high-performance vector similarity search. </st><st c="75112">It supports various ANN indexing algorithms, including HNSW, to enable efficient retrieval of similar vectors. </st><st c="75223">Pinecone can be used to perform a k-NN search by querying for the </st><em class="italic"><st c="75289">k</st></em><st c="75290">-nearest neighbors. </st><st c="75310">Pinecone provides features such as real-time updates, horizontal scaling, and multi-region replication for high availability. </st><st c="75436">It has a simple API and integrates seamlessly with various machine learning frameworks and libraries. </st><st c="75538">Pinecone is a good choice if you want a dedicated vector database with a focus on machine learning use cases. </st><st c="75648">However, it may be more expensive compared to some open source or </st><st c="75714">self-hosted solutions.</st></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/><st c="75736">Weaviate</st></h2>
			<p><st c="75745">Weaviate is an</st><a id="_idIndexMarker517"/><st c="75760"> open source vector search engine that enables </st><a id="_idIndexMarker518"/><st c="75807">efficient similarity search and data exploration. </st><st c="75857">It supports multiple vector indexing algorithms, including HNSW, and offers a GraphQL-based API for easy integration. </st><st c="75975">Weaviate utilizes ANN algorithms, particularly HNSW, for efficient vector similarity search. </st><st c="76068">It leverages the NSW structure of HNSW to enable fast retrieval of similar vectors. </st><st c="76152">Weaviate can be used to perform a k-NN search by specifying the desired number of nearest neighbors. </st><st c="76253">Weaviate provides features such as schema management, data validation, and real-time updates. </st><st c="76347">It can be self-hosted or used as a managed service. </st><st c="76399">Weaviate is a good fit if you prefer an open source solution with a focus on data exploration and graph-like queries. </st><st c="76517">However, it may require more setup and configuration compared to fully </st><st c="76588">managed services.</st></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor182"/><st c="76605">Chroma</st></h2>
			<p><st c="76612">This is what</st><a id="_idIndexMarker519"/><st c="76625"> you have been using so far for vector search in </st><a id="_idIndexMarker520"/><st c="76674">most of the code in this book. </st><st c="76705">Chroma is an open source embedded vector database designed for easy integration with existing tools and frameworks. </st><st c="76821">Chroma supports ANN algorithms, including HNSW, for fast and efficient vector similarity search. </st><st c="76918">It can be used to perform a k-NN search by retrieving the </st><em class="italic"><st c="76976">k</st></em><st c="76977">-nearest neighbors to a query vector. </st><st c="77015">Chroma provides a simple and intuitive Python API for storing and searching vectors, making it particularly convenient for machine learning and data science workflows. </st><st c="77183">That is the primary reason we selected it to showcase in the code in this book. </st><st c="77263">Chroma supports various indexing algorithms, including HNSW, and offers features such as dynamic filtering and metadata storage. </st><st c="77392">It can be used as an in-memory database or persist data to disk for longer-term storage. </st><st c="77481">Chroma is a good choice if you want a lightweight and easy-to-use vector database that can be embedded directly into your Python applications. </st><st c="77624">However, it may not have the same level of scalability and advanced features as some of the more mature vector </st><st c="77735">search solutions.</st></p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor183"/><st c="77752">Summary</st></h1>
			<p><st c="77760">In this chapter, we covered a wide range of topics related to similarity searching with vectors, a crucial component of RAG systems. </st><st c="77894">We explored the concept of a vector space, discussed the differences between semantic and keyword searches, and covered various distance metrics used to compare the similarity between embeddings, providing code examples to demonstrate </st><st c="78129">their calculation.</st></p>
			<p><st c="78147">We reviewed code that implemented hybrid search using the BM25 algorithm for sparse search and a dense retriever for semantic search, showcasing how to combine and rerank the results. </st><st c="78332">We also discussed semantic search algorithms, focusing on k-NN and ANN, and covered indexing techniques that enhance the efficiency of ANN search, such as LSH, tree-based indexing, PQ, </st><st c="78517">and HNSW.</st></p>
			<p><st c="78526">Finally, we provided an overview of several vector search options available in the market, discussing their key features, strengths, and considerations to help you make an informed decision when selecting a vector search solution for your specific </st><st c="78775">project needs.</st></p>
			<p><st c="78789">In the next chapter, we will take a deep look at ways to visualize and evaluate your </st><st c="78875">RAG pipeline.</st></p>
		</div>
	<div></body></html>