- en: 2\. Analyzing Documents and Text with Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: This chapter describes the use of Amazon Comprehend to summarize text documents
    and create Lambda functions to analyze the texts. You will learn how to develop
    services by applying the serverless computing paradigm, and use Amazon Comprehend
    to examine texts to determine their primary language. You will extract information
    such as entities (people or places), key phrases (noun phrases that are indicative
    of the content), emotional sentiments, and topics from a set of documents.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will able to set up a Lambda function to process
    and analyze imported text using Comprehend and extract structured information
    from scanned paper documents using Amazon Textract.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since 2005, when Amazon formally launched its **Elastic Compute Cloud** (**EC2**)
    web service, cloud computing has grown from a developer service to mission-critical
    infrastructure. The spectrum of applications is broad—most highly scalable consumer
    platforms such as Netflix are based on AWS, and so are many pharmaceuticals and
    genomics, as well as organizations such as the BBC and The Weather Channel, BMW,
    and Canon. As of January 2020, there are about 143 distinct AWS services spanning
    25 categories, from compute and storage to quantum technologies, robotics, and
    machine learning. In this book, we will cover a few of them, as shown in the following diagram:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Amazon AI services covered'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Amazon AI services covered'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![a](img/B16061_02_Inline_image1.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: '**S3** is the versatile object store that we use to store the inputs to our
    AI services as well as the outputs from those services. You have been working
    with S3 since *Chapter 1*, *An Introduction to AWS*.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![b](img/B16061_02_Inline_image2.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: '**Lambda** is the glue service that makes serverless computing possible. You
    will use Lambda later in this chapter to analyze text using Comprehend.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![c](img/B16061_02_Inline_image3.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: '**API Gateway** is a delivery service that can enable you to create microservices
    that can be accessed by various clients, such as web, mobile, and server applications,
    via internet protocols such as HTTP, WebSocket, and REST. API Gateway gives you
    the ability to expose your microservices in a secure and scalable way. In the
    age of microservices and the "API-first" approach, the greatest challenge is the
    creation, publishing, monitoring, and maintenance of API endpoints. Almost all
    AWS services are APIs and use the API Gateway infrastructure.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's machine learning services, the main focus of our book, are a set of
    16 services as of January 2020\. They are also called AI services, and currently,
    the terms are interchangeable. Let's take a quick look at the ones we are interested
    in.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![d](img/B16061_02_Inline_image4.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: '**Comprehend**, the topic of this chapter, is a very versatile text analytics
    service. It performs a variety of tasks—keyphrase extraction, sentiment analysis
    (positive, negative, neutral, or mixed), syntax analysis, entity recognition,
    medical **Named Entity Recognition** (**NER**), language detection, and topic
    modeling. You will see this in action later in this chapter.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![e](img/B16061_02_Inline_image5.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: '**Lex** is a platform for building conversational AI, bots, or intelligent
    assistants. Conversational AI capabilities such as **automatic speech recognition**
    (**ASR**) and **natural language understanding** (**NLU**) are built into the
    Lex framework. Lex provides a very intuitive object model consisting of bots,
    utterances, slots, and sessions, as well as integration with Amazon Lambda, thus
    enabling you to develop interesting, intelligent bots in a serverless environment.
    We will see more of Lex in *Chapter 4*, *Conversational Artificial Intelligence*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![f](img/B16061_02_Inline_image6.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: '**Personalize** is a very useful service that allows you to personalize your
    bots. For example, incorporating personalized recommendations/content delivery,
    personalized searching based on previous interactions, or even personalized notifications
    and marketing based on user behavior! While we will not be using Amazon Personalize
    in this book, we wanted to bring your attention to services closely related to
    the ones covered in this book. That way, you can add extremely rich features as
    you expand the power of your bots and NLP services.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![g](img/B16061_02_Inline_image7.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: '**Polly** is a text-to-speech service using **neural text-to-speech** (**NTTS**)
    technologies. It is very flexible and powerful, offering two styles: a newscaster
    reading style and a normal conversational style. The voice need not be monotone—Amazon
    Polly supports **Speech Synthesis Markup Language** (**SSML**), which enables
    you to adjust the speaking style, volume, speech rate, pitch, phrasing, emphasis,
    intonation, and other characteristics.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '![h](img/B16061_02_Inline_image8.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: '**Textract**, as the name implies, extracts text from documents. It is an **optical
    character recognition** (**OCR**) solution that is suitable for process automation.
    It can extract key-value pairs or tables from documents such as tax forms, legal
    documents, medical forms, bank forms, patent registration, and so forth.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![i](img/B16061_02_Inline_image9.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: '**Transcribe** is a speech-to-text **Automatic Speech Recognition** (**ASR**)
    service and is very versatile; for example, it can recognize multiple speakers
    and you can filter out words. It is very useful in medical transcription, for
    time-stamped subtitle generation, and for transcribing customer interactions.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![j](img/B16061_02_Inline_image10.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: '**Translate** is another very useful service that''s able to translate more
    than 50 languages in a scalable, real-time fashion.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![k](img/B16061_02_Inline_image11.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: '**Rekognition**, of course, is a visual analysis and image detection service
    capable of a variety of tasks, such as facial recognition, video analysis, object
    detection, and recognizing text in images. *Chapter 6*, *Computer Vision and Image
    Processing* is dedicated to Amazon Rekognition.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![l](img/B16061_02_Inline_image12.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Unlike the AI services we have looked at so far in this chapter, **Amazon Connect**
    is a very feature-rich contact center application. It consists of an omnichannel
    cloud contact center with high-quality audio, web/mobile secure chat, and a web-based
    contact control panel. The Contact Lens for Amazon Connect is a set of Contact
    center analytics services that adds capabilities such as full-text search and
    sentiment analysis, with forthcoming features such as theme detection and custom
    vocabulary. The integration with Amazon Lex for chatbots is an interesting capability
    where we can leverage the flexibility of Lex to create intelligent and useful
    bots.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![m](img/B16061_02_Inline_image13.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: '**Amazon Alexa**, of course, is a platform for a conversational interface as
    well as a set of hardware devices such as smart speakers that leverage the Alexa
    service to become smart assistants.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The reason for including customer engagement platforms such as Connect and Alexa
    is to show the wider possibilities of the work we are doing in this book. While
    we will not be directly showing how to develop bots for an Amazon Connect or Amazon
    Alexa-based bot **voice user interface** (**VUI**), we want to open your mind
    to the possibility of an omnichannel customer experience across different integration
    points—web, mobile, smart speakers, and so forth.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the services cover a wide variety of layers, from the storage
    and infrastructure layer to the AI services layer, and finally extending to the
    UX.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Computing
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless computing is a relatively new architecture that takes a different
    spin on the cloud application architecture. Let's start with a traditional on-premise
    server-based architecture.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Usually, a traditional application architecture starts with a set of computer
    hardware, a host operating system, virtualization, containers, and an application
    stack consisting of libraries and frameworks tied together by networking and storage.
    On top of all this, we write business logic. In essence, to maintain a business
    capability, we have to maintain the server hardware, operating system patches,
    updates, library updates, and so forth. We also have to worry about scalability,
    fault tolerance, and security at the least.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: With cloud computing, the application architecture is free of computer hardware
    as well as having elasticity. We still have to maintain the OS, libraries, patches,
    and so on. This where serverless computing comes in—in the words of Amazon, serverless
    computing "shifts more of your operational responsibilities to AWS."
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Serverless computing improves upon cloud computing, eliminating infrastructure
    management, starting from provisioning to scaling up and down, depending on the
    load, as well as the patching and maintenance of the whole runtime stack. As Amazon
    depicts it, serverless computing definitely "reduces cost and increases agility
    and innovation" as well as enabling automated high availability, if designed properly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: An O'Reilly report defines serverless computing as "an architectural approach
    to software solutions that relies on small independent functions running on transient
    servers in an elastic runtime environment." So, there are servers—serverless is
    not the right term, but in some sense, the servers are transparent, managed by
    Amazon during the execution of a Lambda function, which is usually in milliseconds.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Lambda and Function as a Service
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Essentially, serverless computing is enabled by functions, more precisely, **Function
    as a Service** (**FaaS**). Amazon Lambda is the prime example of an enabling platform
    for serverless computing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: You write the business logic as a set of Lambda functions that are event-driven,
    stateless, fault-tolerant, and autoscaling. A Lambda function has an upstream
    side and a downstream side—it responds to upstream events; the runtime processor
    executes the embedded code and the results are sent to downstream destinations.
    The upstream events could be generated by something put into a queue or something
    that is dropped into an S3 bucket or a **Simple Notification Service** (**SNS**)
    message. And the downstream can be S3 buckets, queues, DynamoDB, and so forth.
    The runtime supports multiple languages, such as Python, Go, Java, Ruby, Node.js,
    and .NET.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: A Lambda function is much more granular than a microservice—you can think of
    it as a nano service. It is charged on a 100 ms basis and will time out after
    15 minutes. The payload size is 6 MB. That gives you an estimate of the size of
    a Lambda function. Also, as you have noticed, there are no charges when a Lambda
    function is idling – that means we can scale down to zero. And you can implement
    data parallelism easily—trigger a Lambda function for each row of data. As one
    Lambda function can trigger another Lambda function, you can even do task parallelism.
    Of course, all of this requires careful architecture, but it's worth the effort.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's serverless platform covers compute, storage, networking, orchestration,
    API proxy, analytics, and developer tooling. We will look at some of these components—Lambda
    for compute, S3 for storage, API Gateway for networking.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Computing as an Approach
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Industry analysts and technologists consider serverless computing as an approach
    and a set of principles. Amazon Lambda is not serverless computing but an enabler
    of the approach. The serverless computing architecture does reduce what you have
    to build—some of the traditional code that we write now manifests as a function
    chaining pipeline, the configuration of events, triggers, and attributes of Lambda
    functions. The essential business logic does need to be written, and that will
    reside inside the Lambda functions. As a result, there is a very well-defined
    separation between the platform and the business code, and that is the value of
    serverless computing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Comprehend
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon Comprehend is a text analytics service. It has a broad spectrum of capabilities.
    Amazon Comprehend can extract key phrases and entities. It can do language detection
    and topic modeling. It can also perform sentiment analysis as well as syntax analysis.
    Amazon Comprehend is multilingual. Some of the applications of Amazon Comprehend
    include:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the main themes and topics of various unstructured text items
    such as support tickets, social media posts, customer feedback, customer complaints,
    and business documents such as contracts and medical records.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge management by categorizing business documents such as internal procedures,
    white papers, notes and descriptions, media posts, and emails.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brand monitoring—effectively responding to social media posts, reviews, and
    other user-generated content from various channels. Respond faster by prioritizing
    the content as well as routing the content to the appropriate person or process.
    To prioritize and respond faster, businesses need to analyze the content for language,
    topics, and the entities mentioned in the media – all of which are capabilities
    of Amazon Comprehend.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important capability of Comprehend is the fact that underneath the hood,
    it improves models by monitoring errors and training AI models with new and improved
    data.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, you can fine-tune models with your domain-specific data, thus increasing
    the accuracy to fit your application while leveraging the general capability of
    the AI models.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One interesting application of Comprehend is to extract information from business
    documents such as contract numbers, terms of contracts, various codes, and even
    the dosage of medication
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An interesting end-to-end use case is to use Amazon Comprehend to analyze a
    collection of text documents and organize the articles by topic, identify the
    most frequently mentioned features, and group articles by subject matter, to enable
    personalized recommendations for website visitors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Amazon Comprehend search flow'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Amazon Comprehend search flow'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Comprehend Medical** is a feature-rich service for analyzing patient
    health records, doctor''s notes, and reports from clinical trials as well as links
    to medical ontologies. It can even figure out medication dosages, test results,
    and treatment information that can be used for analysis by healthcare professionals:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon Comprehend Medical** 是一个功能丰富的服务，用于分析患者健康记录、医生笔记和临床试验报告，以及医学本体学的链接。它甚至可以确定用于分析的药物剂量、测试结果和治疗信息：'
- en: '![Figure 2.3: Amazon Comprehend Medical flow'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：Amazon Comprehend Medical 流程'
- en: '](img/B16061_02_03.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16061_02_03.jpg)'
- en: 'Figure 2.3: Amazon Comprehend Medical flow'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：Amazon Comprehend Medical 流程
- en: The Amazon Comprehend service continually learns from new data from Amazon product
    descriptions and consumer reviews, and thus, it perpetually improves its ability
    to understand a variety of topics from government, health, media, education, advertising,
    and so on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Comprehend 服务不断从 Amazon 产品描述和消费者评论的新数据中学习，因此，它不断改进其理解来自政府、健康、媒体、教育、广告等各种主题的能力。
- en: In *Chapter 1*, *An Introduction to AWS*, you learned how to use Amazon Comprehend
    to extract insights by using **Natural Language Processing** **(NLP)** from the
    contents of documents. In this chapter, we will dig deeper and you will learn
    how to use the Amazon Comprehend API to produce insights by recognizing the language,
    entities, key phrases, sentiments, and topics in a document. This will allow you
    to understand deep learning-based NLP to build more complex applications, which
    we will cover further.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第一章*，*AWS 简介* 中，你学习了如何使用 Amazon Comprehend 通过文档内容中的 **自然语言处理** **(NLP**)
    提取洞察。在本章中，我们将深入探讨，你将学习如何使用 Amazon Comprehend API 通过识别文档中的语言、实体、关键词、情感和主题来产生洞察。这将使你能够理解基于深度学习的
    NLP，以构建更复杂的应用程序，我们将在后续内容中介绍。
- en: In the second part of this chapter, you will learn about AWS Lambda, and how
    to integrate this service with Amazon Comprehend. You will also integrate a database
    to provide the foundation to build scalable NLP processing applications.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，你将了解 AWS Lambda，以及如何将此服务与 Amazon Comprehend 集成。你还将集成数据库，为构建可扩展的 NLP
    处理应用程序提供基础。
- en: What Is an NLP Service?
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 NLP 服务？
- en: Amazon Comprehend is an NLP service. The overall goal of an NLP service is to
    make machines understand our spoken and written language. Virtual assistants,
    such as Alexa or Siri, use NLP to produce insights from input data. The input
    data is structured by a language, which has a unique grammar, syntax, and vocabulary.
    Thus, processing text data requires identifying the language first and applying
    subsequent rules to identify the document's information. NLP's general task is
    to capture this information as a numeral representation. This general task is
    split into specific tasks, such as identifying languages, entities, key phrases,
    emotional sentiments, and topics.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Comprehend 是一个 NLP 服务。NLP 服务的总体目标是使机器理解我们的口语和书面语言。虚拟助手，如 Alexa 或 Siri，使用
    NLP 从输入数据中产生洞察。输入数据由一种语言结构化，该语言具有独特的语法、句法和词汇。因此，处理文本数据需要首先识别语言，然后应用后续规则来识别文档的信息。NLP
    的通用任务是捕获这些信息作为数值表示。这个通用任务被分解为具体任务，例如识别语言、实体、关键词、情感和主题。
- en: '![Figure 2.4: Amazon Comprehend data flow'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4：Amazon Comprehend 数据流'
- en: '](img/B16061_02_04.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16061_02_04.jpg)'
- en: 'Figure 2.4: Amazon Comprehend data flow'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：Amazon Comprehend 数据流
- en: As we discussed earlier, Amazon Comprehend uses pre-trained models to perform
    document analysis tasks. This is very good because it enables a business to develop
    capabilities without going through an exhaustive AI model training effort. And
    Amazon keeps up with the latest developments in ML and AI, constantly retraining
    the models—so the models get better without any work from users. Also, there are
    capabilities for fine-tuning the models by training them with your domain-specific content.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，Amazon Comprehend 使用预训练的模型来执行文档分析任务。这非常好，因为它使企业能够在不进行耗时的 AI 模型训练努力的情况下开发能力。而且
    Amazon 不断跟进 ML 和 AI 的最新发展，不断重新训练模型——因此，模型在没有用户任何工作的情况下变得更好。此外，还有通过使用你的特定领域内容来微调模型的能力。
- en: Using Amazon Comprehend to Inspect Text and Determine the Primary Language
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon Comprehend 检查文本并确定主要语言
- en: Amazon Comprehend is used for searching and examining texts and then gathering
    insights from a variety of topics (health, media, telecom, education, government,
    and so on) and languages in the text data format. Thus, the first step to analyze
    text data and utilize more complex features (such as topic, entity, and sentiment
    analysis) is to determine the dominant language. Determining the dominant language
    ensures the accuracy of more in-depth analysis. To examine the text in order to
    determine the primary language, there are two operations (`DetectDominantLanguage`
    and `BatchDetectDominantLanguage`).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Both operations expect the text in the UTF-8 format with a length of at least
    20 characters and a maximum of 5,000 bytes. If you are sending a list, it should
    not contain more than 25 items.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'The response includes what language was identified using a two-letter code.
    The following table shows the language codes for different languages:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Check out [https://docs.aws.amazon.com/comprehend/latest/dg/how-languages.html](https://docs.aws.amazon.com/comprehend/latest/dg/how-languages.html)
    for an updated list of the supported languages.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Amazon Comprehend''s supported languages'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_05.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.5: Amazon Comprehend''s supported languages'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three ways to invoke dominant language detection. The result is the
    code for the dominant language in the content and a confidence score determined
    by the Comprehend algorithms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '`DetectDominantLanguage` will return the dominant language in a single document.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BatchDetectDominantLanguage` works on a set of documents and will return a
    list of the dominant language in each of the documents.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While both of the preceding APIs work in synchronous mode, that is, you send
    the content to the API and it will return the results, `StartDominantLanguageDetectionJob`
    works on a collection of jobs asynchronously. This API is well suited to large
    jobs that take more time.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.6: Dominant language score confidence output'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_06.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Dominant language score confidence output'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.01: Detecting the Dominant Language in a Text Document Using the
    Command-Line Interface'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will learn how to detect the dominant language in a text
    using Comprehend''s `DetectDominantLanguage` function. The following steps describe
    how to detect the dominant language:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The source code for the Jupyter notebook is available via GitHub in the repository
    at [https://packt.live/2O4cw0V](https://packt.live/2O4cw0V).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The files for this chapter are located in the `Chapter02` folder in the GitHub
    repository [https://packt.live/31TIzbU](https://packt.live/31TIzbU). As we mentioned
    in *Chapter 1*, *An Introduction to AWS*, you should have downloaded the GitHub
    files into a local subdirectory.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we have downloaded the files in the `Documents/aws-book/The-Applied-AI-and-Natural-Language-Processing-with-AWS`
    directory.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For configuration instructions, refer the section titled *Pre checkup* on GitHub:
    [https://packt.live/2O4cw0V](https://packt.live/2O4cw0V).'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before we begin, the `boto3` library must be installed. On a fresh Jupyter
    Notebook cell, type in the following command to install it:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let''s go ahead and import Boto3\. Boto3 is nothing but the AWS SDK for
    Python. ([https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)):'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, import the JSON module to serialize the JSON ([https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)):'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Instantiate a new Comprehend client:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we assign English and Spanish strings to be analyzed by Comprehend:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we print a string to indicate the respective variable that our script
    is about to execute:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Lastly, call Comprehend's `detect_dominant_language` method with the `english_string`
    and `spanish_string` variables ([https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html](https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html)).
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`json.dumps()` writes the JSON data to a Python string in the terminal:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Save the notebook.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press *Shift* + *Enter* to run the two notebook cells. Executing the cells
    will produce the following output (see the following screenshot):![Figure 2.7:
    Detecting the dominant language output – English and Spanish'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_07.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.7: Detecting the dominant language output – English and Spanish'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the `english_text` string is identified as English (with the `en`
    language code) with a ~0.99 confidence score.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Also as expected, the `spanish_text` string is identified as Spanish (with the
    `es` language code) with a ~0.99 confidence score.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.02: Detecting the Dominant Language in Multiple Documents by Using
    the CLI'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will learn how to use Comprehend''s `DetectDominantLanguage`
    operation for multiple documents. The following steps describe how to detect the
    dominant language:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/2Z8Vbu4](https://packt.live/2Z8Vbu4).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On a fresh empty cell, import the AWS SDK for Python (boto3:[https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)):'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, import the JSON module to serialize the JSON ([https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)):'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Instantiate a new Comprehend client:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, assign a list of English and Spanish strings to be analyzed by Comprehend:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Lastly, we call Comprehend''s `batch_detect_dominant_language` method with
    the `english_string_list` and `spanish_string_list` variables ([https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html](https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html)).
    Then, `json.dumps()` writes the JSON data to a Python string to the terminal:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Save the notebook.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press *Shift* + *Enter* to run the two notebook cells. Executing the cells
    will produce the following output (see the following partial screenshot—the output
    is too long to fit; you can see the full output in the notebook):![Figure 2.8:
    Detecting the dominant language (multiple documents) output—English'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_08.jpg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.8: Detecting the dominant language (multiple documents) output—English'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The important concepts to remember are that Comprehend has the ability to detect
    different languages and can take text input as a single string or in a batch format
    as a list of strings.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In this topic, we reviewed how Comprehend's `DetectDominantLanguage` method
    is structured, and how to pass in both strings and a list of strings. Next, we
    will extract entities, phrases, and sentiments from a set of documents.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Information from a Set of Documents
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a business level, knowing if and why a customer is angry or happy when they
    contact a virtual assistant is extremely important, to retain the customer. At
    an NLP level, this requires more information to be extracted and a more complex
    algorithm. The additional information to extract and quantify is `entities`, `key
    phrases`, `emotional sentiment`, and `topics`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Named Entities—AWS SDK for Python (boto3)
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An entity is a broader concept—it is something that has an identity of its own.
    An entity can be a person or a place, a company name or an organization; it can
    also be a number (say quantity, price, number of days) or a date, a title, a policy
    number, or a medical code. For example, in the text "Martin lives at 27 Broadway
    St.", **Martin** might be detected as a **PERSON**, while **27 Broadway St** might
    be detected as a **LOCATION**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Entities also have a score to indicate the confidence level that the entity
    type was detected correctly. The following table shows a complete list of entity
    types and descriptions:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: AWS Comprehend entity types and descriptions'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_09.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: AWS Comprehend entity types and descriptions'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three ways to invoke the detection of entities:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '`DetectEntities` will return the entities in a single document.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BatchDetectEntities` works on a set of documents and will return a list of
    the entities in each of the documents.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While both the preceding APIs work in synchronous mode, that is, you send the
    content to the API and it will return the results, `StartEntitiesDetectionJob`
    works on a collection of jobs asynchronously. This API is well suited to large
    jobs that take more time.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DetectEntities – Input and Output
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`DetectEntities` takes a `LanguageCode` and a string of text as an input and
    then provides the following information about each entity within the input text:
    `BeginOffset`, `EndOffset`, `Score`, `Text`, and `Type`. The following table shows
    a complete list of AWS Comprehend `DetectEntities`, types, and descriptions:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: AWS Comprehend entity types and descriptions'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_10.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.10: AWS Comprehend entity types and descriptions'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.03: Determining the Named Entities in a Document (the DetectEntities
    method)'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will determine the named entities in a document. For this,
    we will use Amazon Comprehend''s `DetectEntities` operation. The following are
    the steps for detecting named entities:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/2ADssUI](https://packt.live/2ADssUI).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the AWS SDK for Python (boto3: [https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html))
    by using the following command:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, import the `JSON` module to serialize `JSON` from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)
    by using the following command:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, instantiate a new Comprehend client:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, after instantiating a new Comprehend client, provide the `English` text
    to analyze:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, `json.dumps()` writes JSON data to a Python string:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Press *Shift* + *Enter* to run the two notebook cells. The output of the preceding
    code is shown in the following screenshot:![Figure 2.11: AWS Comprehend DetectEntities
    output'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_11.jpg)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.11: AWS Comprehend DetectEntities output'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'The confidence scores were both ~0.99, as the inputs were simple examples.
    As expected, `Seattle` was detected as a `LOCATION`, and `Thursday` was detected
    as a `DATE`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: AWS Comprehend BeginOffset and EndOffset review'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_12.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: AWS Comprehend BeginOffset and EndOffset review'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.04: Detecting Entities in a Set of Documents (Text Files)'
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will determine the named entities in multiple documents.
    For this, we will use Amazon Comprehend''s `DetectEntities` operation. The following
    are the steps for detecting the named entities from a set of documents:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/31UCuMs](https://packt.live/31UCuMs).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the AWS SDK for Python (boto3: [https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html))
    by using the following command:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, import the `JSON` module to serialize `JSON` from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)
    by using the following command:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We also need to do some file operations to iterate through the documents. Import
    the `glob` module to find text files ending `.txt` from [https://docs.python.org/3.6/library/glob.html](https://docs.python.org/3.6/library/glob.html)
    by using the following command:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We also need the `os` library. Import the `os` module from [https://docs.python.org/3.6/library/os.html](https://docs.python.org/3.6/library/os.html)
    by using the following command:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, instantiate a new Comprehend client:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s get a list of all the documents (assumes in Jupyter notebook you navigated
    to `Chapter02/Exercise02.04/` directory and the opened the notebook `Exercise2.04.ipynb`):'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we can iterate through the documents and detect the entities in the documents.
    We will be calling `detect_entities` on each of the documents. As before, we will
    also use `json.dumps()` to write the JSON data to a Python string:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Press *Shift* + *Enter* to run the two notebook cells. The output of the preceding
    code is shown in the following screenshot. It is a long output—we are showing
    the output for one file. You will see the entities listed for all the files in
    the `/reviews__pos/*.txt` subdirectory:![Figure 2.13: DetectEntities output'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_13.jpg)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.13: DetectEntities output'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we extended entity detection to a set of documents, calling
    Amazon Comprehend's `DetectEntities` recursively.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Key Phrases
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key phrase for AWS is analogous to a noun phrase, which represents an actual
    thing. In English, when we put together different words that represent one concrete
    idea, we call it a noun phrase.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: For example, **A fast machine** is a noun phrase because it consists of **A**,
    the article; **fast**, an adjective; and **machine**, which is a noun. AWS looks
    for appropriate word combinations and gives scores that indicate the confidence
    that a string is a noun phrase.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.05: Detecting Key Phrases'
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will detect key phrases. To do so, we will use Amazon
    Comprehend''s `DetectKeyPhrase` operation. The following are the steps for detecting
    key phrases:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/2Z75cI4](https://packt.live/2Z75cI4).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the AWS SDK for Python (boto3: [http://boto3.readthedocs.io/en/latest/](http://boto3.readthedocs.io/en/latest/))
    by using the following command:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, import the JSON module to serialize the JSON from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html
    ) by using the following command:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, instantiate a new Comprehend client by using the following code:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, provide the `English` text to analyze using the following code:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Run the code by executing the cells with *Shift* + *Enter*. You will see the
    following output:![Figure 2.14: AWS Comprehend DetectKeyPhrase output'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_14.jpg)'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.14: AWS Comprehend DetectKeyPhrase output'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Sentiments
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon Comprehend has the capability to detect sentiments, usually used for
    social media posts, blog posts, reviews, emails, and other user-generated content.
    Amazon Comprehend can determine the four shades of sentiment polarity: positive,
    negative, neutral, and mixed. Mixed sentiment is interesting as it can differentiate
    between different aspects; for example, a user might like your website but not
    be thrilled about the price of a product.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.06: Conducting Sentiment Analysis'
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will carry out sentiment analysis. To do so, we will use
    Amazon Comprehend''s `DetectSentiment` operation. The following are the steps
    for detecting sentiment:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/3ebVNU1](https://packt.live/3ebVNU1).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `AWS SDK` for Python (boto3) from [http://boto3.readthedocs.io/en/latest/](http://boto3.readthedocs.io/en/latest/)
    by using the following command:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, import the `JSON` module to serialize JSON from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)
    by using the following command:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, instantiate a new Comprehend client, using the following code:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, provide a text string to analyze, using the following code:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Run the code by executing the cells with *Shift* + *Enter*. The output is as
    follows:![Figure 2.15: AWS Comprehend—DetectSentiment output'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_15.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.15: AWS Comprehend—DetectSentiment output'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we saw how easy it is to perform sentiment analysis using
    AWS Comprehend. `DetectSentiment` correctly predicted the sentiment of the statement
    *Today is my birthday, I am so happy* as positive.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a Lambda Function and Analyzing Imported Text Using Comprehend
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used Amazon Comprehend to do various NLP tasks, such as detecting entities
    and key phrases and carrying out sentiment analysis.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Comprehend and AWS Lambda for responsive NLP
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this topic, we will be integrating AWS Lambda functions with Comprehend,
    which provides a more powerful, scalable infrastructure. You can use AWS Lambda
    to run your code in response to events, such as changes to data in an Amazon S3
    bucket.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Executing code in response to events provides a real-world solution for developing
    scalable software architecture. Overall, this increases our data pipeline and
    provides the ability to handle more complex big data volumes and NLP operations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: What Is AWS Lambda?
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS Lambda is a compute service that runs code without provisioning or managing
    servers. AWS Lambda executes code only when needed, and scales automatically.
    AWS Lambda runs your code on a high-availability compute infrastructure, which
    performs the administration of the compute service. More specifically, AWS Lambda
    performs the following: server and operating system maintenance, capacity provisioning
    and automatic scaling, code monitoring, and logging.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the goal of AWS Lambda is to make short, simple, modular code segments
    that you can tie together into a larger processing infrastructure.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: What Does AWS Lambda Do?
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lambda allows users to run small segments of code (Java, Node, or Python) to
    complete a specific task. These specific tasks can be storing and then executing
    changes to your AWS setup, or responding to events in S3 (we will explore the
    latter later in this topic). Before Lambda, you would typically need a separate
    EC2 server to run your entire code; however, Lambda allows small segments of code
    to run without the need for EC2.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Lambda Function Anatomy
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS Lambda provides two options for implementing Python code. First, you can
    upload a complete Python code file. Second, you can use the Lambda function editor
    entirely inline, which means that you can enter and modify the code directly,
    without having to upload any files to AWS. The code that you enter will be executed
    when the Lambda function is invoked. The second option will allow for easier testing,
    so we will use it.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the structure of a Lambda function:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'When you create a function (for example, `s3_trigger`), AWS creates a folder
    named the same, with a Python file named `Lambda_function.py` within the folder.
    This file contains a stub for the `Lambda_handler` function, which is the entry
    point of our Lambda function. The entry point takes two parameters as arguments:
    The `event` argument and the `context` argument.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `event` argument provides the value of the payload, which is sent to the
    function from the `calling` process. It typically takes the form of a Python `dict`
    type, although it could also be one of `list`, `str`, `int`, `float`, or `NoneType`.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `context` argument is of the type `LambdaContext` and contains runtime information.
    You will be using this parameter for an exercise in a later section. The return
    value of the function can be any type that is JSON-serializable. This value gets
    returned to the calling application, after serializing.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will incorporate Lambda, S3, and Amazon Comprehend, to automatically perform
    document analysis when a text document is uploaded to S3\. The architecture of
    a Lambda function is as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16: Architecture diagram'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_16.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.16: Architecture diagram'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.07: Setting Up a Lambda Function for S3'
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will integrate the following AWS services: S3, Lambda,
    and Amazon Comprehend. To perform this exercise, the architecture should be recollected.
    Upload a file (`test_s3trigger_configured.txt`) to S3 and view the results of
    Comprehend''s analysis. The following are the steps for setting up a Lambda function:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating the S3 bucket**'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: You should have an AWS account and have completed the exercises and activities
    in *Chapter 1*, *An Introduction to AWS*.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, navigate to the Amazon S3 service, [https://console.aws.amazon.com/s3/](https://console.aws.amazon.com/s3/),
    and click `Create bucket`:![Figure 2.17: S3 Bucket creation for the Lambda trigger'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_17.jpg)'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.17: S3 Bucket creation for the Lambda trigger'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For `Bucket name`, type `aws-ml-s3-trigger`, and then click `Create`:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Bucket names in AWS have to be unique, otherwise you will get an error "`Bucket
    name already exists`". One easy way to get a unique name is to append the bucket
    name with today's date plus the time, for instance, YYYYMMDDHHMM. While writing
    this chapter, I created the bucket `aws-ml-s3-trigger-202001181023` .
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18: Creating an S3 bucket'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_18.jpg)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.18: Creating an S3 bucket'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Your bucket will be created, and you will be redirected to the bucket list
    in the `S3 buckets` screen as shown:![Figure 2.19: S3 Bucket list screen'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_19.jpg)'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.19: S3 Bucket list screen'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, navigate to Amazon Lambda, under `Services`, and click `Lambda` under `Compute`:![Figure
    2.20: Services | Compute | Lambda'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_20.jpg)'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.20: Services | Compute | Lambda'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will see the Lambda console, as shown here:![Figure 2.21: Lambda console'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_21.jpg)'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.21: Lambda console'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the Lambda console, click `Create function`:![Figure 2.22: AWS Lambda Create
    function button'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_22.jpg)'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.22: AWS Lambda Create function button'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose `Author from scratch` from the options. For `Name`, type `s3_trigger`:![Figure
    2.23: AWS Lambda—Creating a function with the Author from scratch option'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_23.jpg)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.23: AWS Lambda—Creating a function with the Author from scratch option'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the runtime options, choose `Python 3.6` from the list:![Figure 2.24: AWS
    Lambda—Python 3.6 selection'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_24.jpg)'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.24: AWS Lambda—Python 3.6 selection'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Choose or create an execution role` and choose `Create new role from
    AWS policy template(s)` and enter the name `s3TriggerRole` in the `Role name`
    field:![Figure 2.25: AWS Lambda Create Role template'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_25.jpg)'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.25: AWS Lambda Create Role template'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click the dropdown in `Policy templates` and select `Amazon S3 object read-only
    permissions`. You will see AWS Lambda Policy template dropdown box, as shown here:![Figure
    2.26: AWS Lambda Policy templates dropdown box'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_26.jpg)'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.26: AWS Lambda Policy templates dropdown box'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, click the `Create function` button to create the Lambda function in AWS.
    The final AWS Lambda Create function screen looks as follows:![Figure 2.27: AWS
    Lambda—Create a function screen'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_27.jpg)'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.27: AWS Lambda—Create a function screen'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will see the Lambda function designer. There is lot of information displayed.
    Let''s focus on the essentials for this exercise:![Figure 2.28: AWS Lambda—function
    designer'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_28.jpg)'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.28: AWS Lambda—function designer'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Add trigger`, and from the drop-down menu, select `S3`:![Figure 2.29:
    Trigger configuration drop-down menu'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_29.jpg)'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.29: Trigger configuration drop-down menu'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Take a quick look at the options and select `Add`:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The bucket name should be the S3 trigger bucket you created (in my case, it
    was `aws-ml-s3-trigger-202001181023`); in the `Event type` section, `All object
    create events` must be selected in the dropdown and `Enable Trigger` should be
    checked, as shown here:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You might get the error "`An error occurred when creating the trigger: Configurations
    overlap. Configurations on the same bucket cannot share a common event type`."
    This would happen if you created a function and deleted it. The easiest way is
    to delete the event via `Services | Storage/S3 | Click the bucket | Properties
    | Events` and deleting the Lambda event. Make sure you click the `Save` button
    after deleting the event.'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.30: Amazon S3 Trigger configuration'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_30.jpg)'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.30: Amazon S3 Trigger configuration'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will see S3 on the Lambda `Designer` screen:'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.31: Lambda function designer with S3'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_31.jpg)'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.31: Lambda function designer with S3'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Again, choose `Add trigger` and choose `CloudWatch/Events/EventBridge`:![Figure
    2.32: Adding the trigger configuration'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_32.jpg)'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.32: Adding the trigger configuration'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then click the box next to `Rule`:![Figure 2.33: Add trigger – creating a new
    rule'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_33.jpg)'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.33: Add trigger – creating a new rule'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select `Create a new rule`. The following screen will be displayed. Type `s3_trigger_CWRule`
    for the rule name.![Figure 2.34: Add Trigger—New Rule Configuration'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_34.jpg)'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.34: Add Trigger—New Rule Configuration'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose `Event pattern` in `Rule type`. Then select `Simple Storage Service
    (S3)` from the dropdown and `All events` and click `Add`:![Figure 2.35: Adding
    an S3 rule type'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_35.jpg)'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.35: Adding an S3 rule type'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s explore the interface a bit more so that you can get comfortable navigating
    through different pages. Click `Functions` in the top-left corner:![Figure 2.36:
    Top navigation bar to navigate back to functions'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_36.jpg)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.36: Top navigation bar to navigate back to functions'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `s3_trigger` to go back to the function you are working on:![Figure 2.37:
    Selecting the lambda function to work on'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_37.jpg)'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.37: Selecting the lambda function to work on'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, scroll down the screen to the `Function code` section. The default code
    will be the same as, or similar to, the following:![Figure 2.38: AWS Lambda—the
    default lambda_function screen'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_38.jpg)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.38: AWS Lambda—the default lambda_function screen'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can enter and edit our code entirely within the `lambda_function` screen
    (as long as `Code entry type` is set to `Edit code inline`, which is the default
    value in the drop-down menu).
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For this step, you may either follow along and type in the code or obtain it
    from the source code folder at [https://packt.live/2O6WsLW](https://packt.live/2O6WsLW).
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we import the **AWS SDK** for Python (boto3: [http://boto3.readthedocs.io/en/latest/](http://boto3.readthedocs.io/en/latest/)):'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, import the JSON module to serialize the JSON ([https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)):'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, create a function that takes two parameters—`event` and `context`:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, create the `s3` client object:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Add an `if` event to check whether an event occurs.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, replace `<input Bucket name>` with the bucket you created (`aws-ml-s3-trigger-202001181023`,
    in are example):'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, access the first index of the `Records` event to obtain the text file
    object:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, assign the `filename` text to a variable and print the filename:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, create the file object by getting the bucket and key:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Assign the text to the `body_str_obj` variable:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create the `comprehend` variable:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The next three lines of code call the respective Comprehend functions to detect
    the sentiment, entities, and key phrases from the text document. Then, the output
    is printed to the console:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The final statement returns the `''Hello from Lambda''` string, like so:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, click the `Save` button:![Figure 2.39: AWS Lambda – save screen'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_39.jpg)'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.39: AWS Lambda – save screen'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: From this exercise, the `s3_trigger` function has access to S3, but not Amazon
    Comprehend. We need to attach a policy to the `s3_trigger` function to allow it
    to access Amazon Comprehend to execute the text analysis functions (`detect_sentiment`,
    `detect_entities`, and `detect_key_phrases`).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.08: Assigning Policies to S3_trigger to Access Comprehend'
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will attach the policies to the `S3_trigger` function
    to allow it to access Comprehend. The steps for completion for assigning the policies
    are as follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Amazon Management Console, click `Services` at the top left:![Figure
    2.40: AWS Services from the AWS Management Console'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_40.jpg)'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.40: AWS Services from the AWS Management Console'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to the `Identity and Access Management` dashboard in the `Security,
    Identity, & Compliance` section. You can also type `IAM` and select it from the
    dropdown:![Figure 2.41: IAM dashboard'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_41.jpg)'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.41: IAM dashboard'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, once you get to the IAM dashboard, click `Roles`:![Figure 2.42: Left-hand
    side of the IAM dashboard'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_42.jpg)'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.42: Left-hand side of the IAM dashboard'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, the screen will be populated with the role list. Click `s3TriggerRole`
    in the role list:![Figure 2.43: Role list—selecting s3TriggerRole'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_43.jpg)'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.43: Role list—selecting s3TriggerRole'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `s3TriggerRole` option will be enabled. Then, click `Attach policies`:![Figure
    2.44: Permissions tab for s3TriggerRole'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_44.jpg)'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.44: Permissions tab for s3TriggerRole'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Type `Comprehend` to filter the policies. Then, click the checkbox next to
    `ComprehendFullAccess`:![Figure 2.45: ComprehendFullAccess policy selection'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_45.jpg)'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.45: ComprehendFullAccess policy selection'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you have selected the checkbox, click `Attach policy` (located in the
    lower right-hand corner of the screen):![Figure 2.46: Attaching the selected policies'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_46.jpg)'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.46: Attaching the selected policies'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will be redirected to the `s3TriggerRole` screen, and you will receive
    the following message:![Figure 2.47: Successfully attached policies message'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_47.jpg)'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.47: Successfully attached policies message'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have successfully attached the policies to the `S3_trigger` function
    thus allowing it to access Comprehend.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.01: Integrating Lambda with Amazon Comprehend to Perform Text Analysis'
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we will integrate the Lambda functions with Comprehend to
    perform text analysis (`detect_sentiment`, `detect_entities`, and `detect_key_phrases`)
    when a document is uploaded to S3.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you are creating a chatbot. You have identified a business topic
    and the corresponding text documents, with content that will allow the chatbot
    to make your business successful. Your next step is to integrate the Lambda functions
    with Comprehend, for sentiment, key phrases, and entities. To ensure that this
    happens correctly, you will need to have `test_s3trigger_configured.txt`.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: The `test_s3trigger_configured.txt` file can be found on GitHub at link [https://packt.live/3gAxqku](https://packt.live/3gAxqku).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you execute `s3_trigger`, consider the output, based on the following
    aspects of the text: sentiment (positive, negative, or neutral), entities (quantity,
    person, place, and so on), and key phrases:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: First, navigate to the `S3_trigger` Lambda function.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `test_s3trigger_configured.txt` to the S3 bucket, to verify the Lambda `S3_trigger`
    function.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, upload the file into the bucket and monitor the file.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click `View logs` in `CloudWatch` by using the log stream.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, expand the output in a text format.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following will be the output:'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Sentiment_response` -> Classified as 60.0% likely to be positive'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Sentiment_response`:'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`entity_response` -> Classified as 70.5% likely to be a quantity'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`entity_response`:'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`key_phases_response` -> Classified as 89.9% likely "a test file" and 98.5%
    likely "the s3 trigger" are the key phrases:'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`key_phases_response`:'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 279.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Amazon Textract
  id: totrans-422
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another interesting NLP Amazon service is Textract. Essentially, Textract can
    extract information from documents, usually business documents such as tax forms,
    legal documents, medical forms, bank forms, patent registrations, and so forth.
    It is an **optical character recognition (OCR**) solution for scanning structured
    documents, suitable for **robotic process automation** (**RPA**). Textract is
    a relatively new service—previewed in November 2018 and generally available in
    May 2019.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of Textract is that it understands documents and can extract tables
    and/or key-value pairs suitable for downstream processing. A lot of business processes,
    such as health insurance processing, tax preparation, loan application processing,
    monitoring and evaluation of existing loans, compliance evaluation, and engineering
    evaluations take in these documents, usually processing them manually to extract
    information and then start digital processes. Using Amazon Textract, the manual
    intake of various documents can be automated, resulting in a faster turnaround
    when approving loans, accelerated processing of health claims, or approving an
    engineering design quickly, thus achieving good business value.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.09: Extracting Tax Information Using Amazon Textract'
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will take a page of a sample tax return document from
    documentcloud.org ([https://www.documentcloud.org/documents/3462212-Sample-2016-Tax-Return.html](https://www.documentcloud.org/documents/3462212-Sample-2016-Tax-Return.html))
    and see how much information Textract can extract:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: The sample document (page 1 of US Tax form 1040) is available at [https://packt.live/2O5e1Mn](https://packt.live/2O5e1Mn).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we will use the Textract interface directly. This is very
    useful to try out and to see how a document is amenable to OCR.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, go to the Textract dashboard by selecting `Services | Machine Learning
    | Amazon Textract`. There are lots of interesting details on that page. Take the
    time to read through the materials:![Figure 2.48: Amazon Textract dashboard'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_48.jpg)'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.48: Amazon Textract dashboard'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Try Amazon Textract`. A very simple utilitarian page appears:![Figure
    2.49: Amazon Textract Analyze document'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_49.jpg)'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.49: Amazon Textract Analyze document'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Upload document` and upload the `Sample-2016-Tax-Return.jpeg` file.
    The service thinks for a minute and shows very informative tabs and the information
    it has extracted:![Figure 2.50: Amazon Textract Analyze document screen with the
    sample tax form'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_50.jpg)'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.50: Amazon Textract Analyze document screen with the sample tax form'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The raw text is interesting, but we are looking for more value for our automation pipeline.
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the `Forms` tab and you will see a very interesting page—it can get the
    value as well as the key. For example, line 7 is extracted as `7 Wages, salaries,
    tips, etc. Attach Form(s) W-2 7` and a value of `93,500`. Now, a downstream loan
    processing application can get the value as well as the context and act on it.
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can click other fields on the image on the left-hand side and see the extracted
    entry on the right-hand side.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can download the results as JSON, CSV, table, and text formats. As expected,
    `keyvalues.csv` has the line 7 we saw earlier as the key and `93,500` as the value:'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.51: Amazon Textract Analyze document screen with the sample tax
    document form'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_51.jpg)'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.51: Amazon Textract Analyze document screen with the sample tax document
    form'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can see the extracted fields in a table format (with the keys as the caption
    and the value in the grey box under the captions) as shown below:![Figure 2.52:
    Amazon Textract Analyze document screen with the sample tax document Forms tab
    showing the key value'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_52.jpg)'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.52: Amazon Textract Analyze document screen with the sample tax document
    Forms tab showing the key value'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `Tables` tab is also interesting. Textract was able to extract two tables—the
    top and the bottom portion—but was not able to extract the middle one:![Figure
    2.53: Amazon Textract Analyze document screen'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: with the sample tax form showing Tables (form)
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_53.jpg)'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.53: Amazon Textract Analyze document screen with the sample tax form
    showing Tables (form)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can see the extracted fields in a table format by clicking the `Tables`
    tab:![Figure 2.54: Amazon Textract Analyze document screen'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: with the sample tax form showing Tables (extracted)
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_54.jpg)'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.54: Amazon Textract Analyze document screen with the sample tax form
    showing Tables (extracted)'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a feel for what Textract can do, another useful exercise would
    be to develop a loan processing pipeline using Lambda. When page 1 of US Tax 1040
    is dropped into an S3 bucket as a JPEG file, trigger a Lambda that takes the file
    and invokes Textract and stores the key-value file as a CSV in another bucket.
    If you feel adventurous, you can develop another Lambda downstream of Textract
    that gets triggered when the output file is created, and it can either alert a
    loan officer via SMS or a queue or even a mobile app alert.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with high-level concepts around Amazon AI services
    and serverless computing. On a conceptual level, you learned about serverless
    computing as well as the various AI services available on the AWS platform.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the culmination of these independent functions provides the foundation
    for building complex machine learning-based NLP applications (for example, Siri,
    Alexa, and so on). Knowing how and why the individual functions operate will allow
    you to build your own AWS-based NLP applications.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: Then, we dived into the details of Amazon Comprehend—how Comprehend's `DetectDominantLanguage`
    method is structured, and how to pass in both strings and a list of strings. You
    learned how to extract entities, sentiments, key phrases, and topics, which provide
    the data for complex NLP. This allows Amazon Comprehend to become more efficient
    by automating text analysis upon a text document that's been uploaded to S3.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入探讨了 Amazon Comprehend 的细节——Comprehend 的 `DetectDominantLanguage` 方法是如何构建的，以及如何传入字符串和字符串列表。你学习了如何提取实体、情感、关键词和主题，这些为复杂的
    NLP 提供了数据。这使得 Amazon Comprehend 通过在已上传到 S3 的文本文档上自动进行文本分析而变得更加高效。
- en: You also learned how to use Amazon Textract to extract structured information
    (tables and key-value pairs) out of scanned documents as a prelude to process automation.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了如何使用 Amazon Textract 从扫描的文档中提取结构化信息（表格和键值对），作为处理自动化的前奏。
- en: In the next chapter, we will explore topic modeling and perform theme extraction.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨主题建模并执行主题提取。
