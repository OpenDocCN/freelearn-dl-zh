- en: 2\. Analyzing Documents and Text with Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter describes the use of Amazon Comprehend to summarize text documents
    and create Lambda functions to analyze the texts. You will learn how to develop
    services by applying the serverless computing paradigm, and use Amazon Comprehend
    to examine texts to determine their primary language. You will extract information
    such as entities (people or places), key phrases (noun phrases that are indicative
    of the content), emotional sentiments, and topics from a set of documents.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will able to set up a Lambda function to process
    and analyze imported text using Comprehend and extract structured information
    from scanned paper documents using Amazon Textract.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since 2005, when Amazon formally launched its **Elastic Compute Cloud** (**EC2**)
    web service, cloud computing has grown from a developer service to mission-critical
    infrastructure. The spectrum of applications is broad—most highly scalable consumer
    platforms such as Netflix are based on AWS, and so are many pharmaceuticals and
    genomics, as well as organizations such as the BBC and The Weather Channel, BMW,
    and Canon. As of January 2020, there are about 143 distinct AWS services spanning
    25 categories, from compute and storage to quantum technologies, robotics, and
    machine learning. In this book, we will cover a few of them, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Amazon AI services covered'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Amazon AI services covered'
  prefs: []
  type: TYPE_NORMAL
- en: '![a](img/B16061_02_Inline_image1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**S3** is the versatile object store that we use to store the inputs to our
    AI services as well as the outputs from those services. You have been working
    with S3 since *Chapter 1*, *An Introduction to AWS*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![b](img/B16061_02_Inline_image2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Lambda** is the glue service that makes serverless computing possible. You
    will use Lambda later in this chapter to analyze text using Comprehend.'
  prefs: []
  type: TYPE_NORMAL
- en: '![c](img/B16061_02_Inline_image3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**API Gateway** is a delivery service that can enable you to create microservices
    that can be accessed by various clients, such as web, mobile, and server applications,
    via internet protocols such as HTTP, WebSocket, and REST. API Gateway gives you
    the ability to expose your microservices in a secure and scalable way. In the
    age of microservices and the "API-first" approach, the greatest challenge is the
    creation, publishing, monitoring, and maintenance of API endpoints. Almost all
    AWS services are APIs and use the API Gateway infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's machine learning services, the main focus of our book, are a set of
    16 services as of January 2020\. They are also called AI services, and currently,
    the terms are interchangeable. Let's take a quick look at the ones we are interested
    in.
  prefs: []
  type: TYPE_NORMAL
- en: '![d](img/B16061_02_Inline_image4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Comprehend**, the topic of this chapter, is a very versatile text analytics
    service. It performs a variety of tasks—keyphrase extraction, sentiment analysis
    (positive, negative, neutral, or mixed), syntax analysis, entity recognition,
    medical **Named Entity Recognition** (**NER**), language detection, and topic
    modeling. You will see this in action later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![e](img/B16061_02_Inline_image5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Lex** is a platform for building conversational AI, bots, or intelligent
    assistants. Conversational AI capabilities such as **automatic speech recognition**
    (**ASR**) and **natural language understanding** (**NLU**) are built into the
    Lex framework. Lex provides a very intuitive object model consisting of bots,
    utterances, slots, and sessions, as well as integration with Amazon Lambda, thus
    enabling you to develop interesting, intelligent bots in a serverless environment.
    We will see more of Lex in *Chapter 4*, *Conversational Artificial Intelligence*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![f](img/B16061_02_Inline_image6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Personalize** is a very useful service that allows you to personalize your
    bots. For example, incorporating personalized recommendations/content delivery,
    personalized searching based on previous interactions, or even personalized notifications
    and marketing based on user behavior! While we will not be using Amazon Personalize
    in this book, we wanted to bring your attention to services closely related to
    the ones covered in this book. That way, you can add extremely rich features as
    you expand the power of your bots and NLP services.'
  prefs: []
  type: TYPE_NORMAL
- en: '![g](img/B16061_02_Inline_image7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Polly** is a text-to-speech service using **neural text-to-speech** (**NTTS**)
    technologies. It is very flexible and powerful, offering two styles: a newscaster
    reading style and a normal conversational style. The voice need not be monotone—Amazon
    Polly supports **Speech Synthesis Markup Language** (**SSML**), which enables
    you to adjust the speaking style, volume, speech rate, pitch, phrasing, emphasis,
    intonation, and other characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: '![h](img/B16061_02_Inline_image8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Textract**, as the name implies, extracts text from documents. It is an **optical
    character recognition** (**OCR**) solution that is suitable for process automation.
    It can extract key-value pairs or tables from documents such as tax forms, legal
    documents, medical forms, bank forms, patent registration, and so forth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![i](img/B16061_02_Inline_image9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Transcribe** is a speech-to-text **Automatic Speech Recognition** (**ASR**)
    service and is very versatile; for example, it can recognize multiple speakers
    and you can filter out words. It is very useful in medical transcription, for
    time-stamped subtitle generation, and for transcribing customer interactions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![j](img/B16061_02_Inline_image10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Translate** is another very useful service that''s able to translate more
    than 50 languages in a scalable, real-time fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: '![k](img/B16061_02_Inline_image11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Rekognition**, of course, is a visual analysis and image detection service
    capable of a variety of tasks, such as facial recognition, video analysis, object
    detection, and recognizing text in images. *Chapter 6*, *Computer Vision and Image
    Processing* is dedicated to Amazon Rekognition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![l](img/B16061_02_Inline_image12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Unlike the AI services we have looked at so far in this chapter, **Amazon Connect**
    is a very feature-rich contact center application. It consists of an omnichannel
    cloud contact center with high-quality audio, web/mobile secure chat, and a web-based
    contact control panel. The Contact Lens for Amazon Connect is a set of Contact
    center analytics services that adds capabilities such as full-text search and
    sentiment analysis, with forthcoming features such as theme detection and custom
    vocabulary. The integration with Amazon Lex for chatbots is an interesting capability
    where we can leverage the flexibility of Lex to create intelligent and useful
    bots.
  prefs: []
  type: TYPE_NORMAL
- en: '![m](img/B16061_02_Inline_image13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Amazon Alexa**, of course, is a platform for a conversational interface as
    well as a set of hardware devices such as smart speakers that leverage the Alexa
    service to become smart assistants.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason for including customer engagement platforms such as Connect and Alexa
    is to show the wider possibilities of the work we are doing in this book. While
    we will not be directly showing how to develop bots for an Amazon Connect or Amazon
    Alexa-based bot **voice user interface** (**VUI**), we want to open your mind
    to the possibility of an omnichannel customer experience across different integration
    points—web, mobile, smart speakers, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the services cover a wide variety of layers, from the storage
    and infrastructure layer to the AI services layer, and finally extending to the
    UX.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless computing is a relatively new architecture that takes a different
    spin on the cloud application architecture. Let's start with a traditional on-premise
    server-based architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, a traditional application architecture starts with a set of computer
    hardware, a host operating system, virtualization, containers, and an application
    stack consisting of libraries and frameworks tied together by networking and storage.
    On top of all this, we write business logic. In essence, to maintain a business
    capability, we have to maintain the server hardware, operating system patches,
    updates, library updates, and so forth. We also have to worry about scalability,
    fault tolerance, and security at the least.
  prefs: []
  type: TYPE_NORMAL
- en: With cloud computing, the application architecture is free of computer hardware
    as well as having elasticity. We still have to maintain the OS, libraries, patches,
    and so on. This where serverless computing comes in—in the words of Amazon, serverless
    computing "shifts more of your operational responsibilities to AWS."
  prefs: []
  type: TYPE_NORMAL
- en: Serverless computing improves upon cloud computing, eliminating infrastructure
    management, starting from provisioning to scaling up and down, depending on the
    load, as well as the patching and maintenance of the whole runtime stack. As Amazon
    depicts it, serverless computing definitely "reduces cost and increases agility
    and innovation" as well as enabling automated high availability, if designed properly.
  prefs: []
  type: TYPE_NORMAL
- en: An O'Reilly report defines serverless computing as "an architectural approach
    to software solutions that relies on small independent functions running on transient
    servers in an elastic runtime environment." So, there are servers—serverless is
    not the right term, but in some sense, the servers are transparent, managed by
    Amazon during the execution of a Lambda function, which is usually in milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Lambda and Function as a Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Essentially, serverless computing is enabled by functions, more precisely, **Function
    as a Service** (**FaaS**). Amazon Lambda is the prime example of an enabling platform
    for serverless computing.
  prefs: []
  type: TYPE_NORMAL
- en: You write the business logic as a set of Lambda functions that are event-driven,
    stateless, fault-tolerant, and autoscaling. A Lambda function has an upstream
    side and a downstream side—it responds to upstream events; the runtime processor
    executes the embedded code and the results are sent to downstream destinations.
    The upstream events could be generated by something put into a queue or something
    that is dropped into an S3 bucket or a **Simple Notification Service** (**SNS**)
    message. And the downstream can be S3 buckets, queues, DynamoDB, and so forth.
    The runtime supports multiple languages, such as Python, Go, Java, Ruby, Node.js,
    and .NET.
  prefs: []
  type: TYPE_NORMAL
- en: A Lambda function is much more granular than a microservice—you can think of
    it as a nano service. It is charged on a 100 ms basis and will time out after
    15 minutes. The payload size is 6 MB. That gives you an estimate of the size of
    a Lambda function. Also, as you have noticed, there are no charges when a Lambda
    function is idling – that means we can scale down to zero. And you can implement
    data parallelism easily—trigger a Lambda function for each row of data. As one
    Lambda function can trigger another Lambda function, you can even do task parallelism.
    Of course, all of this requires careful architecture, but it's worth the effort.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon's serverless platform covers compute, storage, networking, orchestration,
    API proxy, analytics, and developer tooling. We will look at some of these components—Lambda
    for compute, S3 for storage, API Gateway for networking.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Computing as an Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Industry analysts and technologists consider serverless computing as an approach
    and a set of principles. Amazon Lambda is not serverless computing but an enabler
    of the approach. The serverless computing architecture does reduce what you have
    to build—some of the traditional code that we write now manifests as a function
    chaining pipeline, the configuration of events, triggers, and attributes of Lambda
    functions. The essential business logic does need to be written, and that will
    reside inside the Lambda functions. As a result, there is a very well-defined
    separation between the platform and the business code, and that is the value of
    serverless computing.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon Comprehend is a text analytics service. It has a broad spectrum of capabilities.
    Amazon Comprehend can extract key phrases and entities. It can do language detection
    and topic modeling. It can also perform sentiment analysis as well as syntax analysis.
    Amazon Comprehend is multilingual. Some of the applications of Amazon Comprehend
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the main themes and topics of various unstructured text items
    such as support tickets, social media posts, customer feedback, customer complaints,
    and business documents such as contracts and medical records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge management by categorizing business documents such as internal procedures,
    white papers, notes and descriptions, media posts, and emails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brand monitoring—effectively responding to social media posts, reviews, and
    other user-generated content from various channels. Respond faster by prioritizing
    the content as well as routing the content to the appropriate person or process.
    To prioritize and respond faster, businesses need to analyze the content for language,
    topics, and the entities mentioned in the media – all of which are capabilities
    of Amazon Comprehend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important capability of Comprehend is the fact that underneath the hood,
    it improves models by monitoring errors and training AI models with new and improved
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, you can fine-tune models with your domain-specific data, thus increasing
    the accuracy to fit your application while leveraging the general capability of
    the AI models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One interesting application of Comprehend is to extract information from business
    documents such as contract numbers, terms of contracts, various codes, and even
    the dosage of medication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An interesting end-to-end use case is to use Amazon Comprehend to analyze a
    collection of text documents and organize the articles by topic, identify the
    most frequently mentioned features, and group articles by subject matter, to enable
    personalized recommendations for website visitors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Amazon Comprehend search flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Amazon Comprehend search flow'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Comprehend Medical** is a feature-rich service for analyzing patient
    health records, doctor''s notes, and reports from clinical trials as well as links
    to medical ontologies. It can even figure out medication dosages, test results,
    and treatment information that can be used for analysis by healthcare professionals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Amazon Comprehend Medical flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.3: Amazon Comprehend Medical flow'
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon Comprehend service continually learns from new data from Amazon product
    descriptions and consumer reviews, and thus, it perpetually improves its ability
    to understand a variety of topics from government, health, media, education, advertising,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 1*, *An Introduction to AWS*, you learned how to use Amazon Comprehend
    to extract insights by using **Natural Language Processing** **(NLP)** from the
    contents of documents. In this chapter, we will dig deeper and you will learn
    how to use the Amazon Comprehend API to produce insights by recognizing the language,
    entities, key phrases, sentiments, and topics in a document. This will allow you
    to understand deep learning-based NLP to build more complex applications, which
    we will cover further.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of this chapter, you will learn about AWS Lambda, and how
    to integrate this service with Amazon Comprehend. You will also integrate a database
    to provide the foundation to build scalable NLP processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: What Is an NLP Service?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Comprehend is an NLP service. The overall goal of an NLP service is to
    make machines understand our spoken and written language. Virtual assistants,
    such as Alexa or Siri, use NLP to produce insights from input data. The input
    data is structured by a language, which has a unique grammar, syntax, and vocabulary.
    Thus, processing text data requires identifying the language first and applying
    subsequent rules to identify the document's information. NLP's general task is
    to capture this information as a numeral representation. This general task is
    split into specific tasks, such as identifying languages, entities, key phrases,
    emotional sentiments, and topics.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: Amazon Comprehend data flow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.4: Amazon Comprehend data flow'
  prefs: []
  type: TYPE_NORMAL
- en: As we discussed earlier, Amazon Comprehend uses pre-trained models to perform
    document analysis tasks. This is very good because it enables a business to develop
    capabilities without going through an exhaustive AI model training effort. And
    Amazon keeps up with the latest developments in ML and AI, constantly retraining
    the models—so the models get better without any work from users. Also, there are
    capabilities for fine-tuning the models by training them with your domain-specific content.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Comprehend to Inspect Text and Determine the Primary Language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Comprehend is used for searching and examining texts and then gathering
    insights from a variety of topics (health, media, telecom, education, government,
    and so on) and languages in the text data format. Thus, the first step to analyze
    text data and utilize more complex features (such as topic, entity, and sentiment
    analysis) is to determine the dominant language. Determining the dominant language
    ensures the accuracy of more in-depth analysis. To examine the text in order to
    determine the primary language, there are two operations (`DetectDominantLanguage`
    and `BatchDetectDominantLanguage`).
  prefs: []
  type: TYPE_NORMAL
- en: Both operations expect the text in the UTF-8 format with a length of at least
    20 characters and a maximum of 5,000 bytes. If you are sending a list, it should
    not contain more than 25 items.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response includes what language was identified using a two-letter code.
    The following table shows the language codes for different languages:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Check out [https://docs.aws.amazon.com/comprehend/latest/dg/how-languages.html](https://docs.aws.amazon.com/comprehend/latest/dg/how-languages.html)
    for an updated list of the supported languages.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Amazon Comprehend''s supported languages'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.5: Amazon Comprehend''s supported languages'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three ways to invoke dominant language detection. The result is the
    code for the dominant language in the content and a confidence score determined
    by the Comprehend algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DetectDominantLanguage` will return the dominant language in a single document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BatchDetectDominantLanguage` works on a set of documents and will return a
    list of the dominant language in each of the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While both of the preceding APIs work in synchronous mode, that is, you send
    the content to the API and it will return the results, `StartDominantLanguageDetectionJob`
    works on a collection of jobs asynchronously. This API is well suited to large
    jobs that take more time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.6: Dominant language score confidence output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Dominant language score confidence output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.01: Detecting the Dominant Language in a Text Document Using the
    Command-Line Interface'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will learn how to detect the dominant language in a text
    using Comprehend''s `DetectDominantLanguage` function. The following steps describe
    how to detect the dominant language:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The source code for the Jupyter notebook is available via GitHub in the repository
    at [https://packt.live/2O4cw0V](https://packt.live/2O4cw0V).
  prefs: []
  type: TYPE_NORMAL
- en: The files for this chapter are located in the `Chapter02` folder in the GitHub
    repository [https://packt.live/31TIzbU](https://packt.live/31TIzbU). As we mentioned
    in *Chapter 1*, *An Introduction to AWS*, you should have downloaded the GitHub
    files into a local subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, we have downloaded the files in the `Documents/aws-book/The-Applied-AI-and-Natural-Language-Processing-with-AWS`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For configuration instructions, refer the section titled *Pre checkup* on GitHub:
    [https://packt.live/2O4cw0V](https://packt.live/2O4cw0V).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before we begin, the `boto3` library must be installed. On a fresh Jupyter
    Notebook cell, type in the following command to install it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s go ahead and import Boto3\. Boto3 is nothing but the AWS SDK for
    Python. ([https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, import the JSON module to serialize the JSON ([https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a new Comprehend client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we assign English and Spanish strings to be analyzed by Comprehend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we print a string to indicate the respective variable that our script
    is about to execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Lastly, call Comprehend's `detect_dominant_language` method with the `english_string`
    and `spanish_string` variables ([https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html](https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`json.dumps()` writes the JSON data to a Python string in the terminal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press *Shift* + *Enter* to run the two notebook cells. Executing the cells
    will produce the following output (see the following screenshot):![Figure 2.7:
    Detecting the dominant language output – English and Spanish'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.7: Detecting the dominant language output – English and Spanish'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the `english_text` string is identified as English (with the `en`
    language code) with a ~0.99 confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: Also as expected, the `spanish_text` string is identified as Spanish (with the
    `es` language code) with a ~0.99 confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.02: Detecting the Dominant Language in Multiple Documents by Using
    the CLI'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will learn how to use Comprehend''s `DetectDominantLanguage`
    operation for multiple documents. The following steps describe how to detect the
    dominant language:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/2Z8Vbu4](https://packt.live/2Z8Vbu4).
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On a fresh empty cell, import the AWS SDK for Python (boto3:[https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, import the JSON module to serialize the JSON ([https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a new Comprehend client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, assign a list of English and Spanish strings to be analyzed by Comprehend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, we call Comprehend''s `batch_detect_dominant_language` method with
    the `english_string_list` and `spanish_string_list` variables ([https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html](https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectDominantLanguage.html)).
    Then, `json.dumps()` writes the JSON data to a Python string to the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press *Shift* + *Enter* to run the two notebook cells. Executing the cells
    will produce the following output (see the following partial screenshot—the output
    is too long to fit; you can see the full output in the notebook):![Figure 2.8:
    Detecting the dominant language (multiple documents) output—English'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.8: Detecting the dominant language (multiple documents) output—English'
  prefs: []
  type: TYPE_NORMAL
- en: The important concepts to remember are that Comprehend has the ability to detect
    different languages and can take text input as a single string or in a batch format
    as a list of strings.
  prefs: []
  type: TYPE_NORMAL
- en: In this topic, we reviewed how Comprehend's `DetectDominantLanguage` method
    is structured, and how to pass in both strings and a list of strings. Next, we
    will extract entities, phrases, and sentiments from a set of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Information from a Set of Documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At a business level, knowing if and why a customer is angry or happy when they
    contact a virtual assistant is extremely important, to retain the customer. At
    an NLP level, this requires more information to be extracted and a more complex
    algorithm. The additional information to extract and quantify is `entities`, `key
    phrases`, `emotional sentiment`, and `topics`.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Named Entities—AWS SDK for Python (boto3)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An entity is a broader concept—it is something that has an identity of its own.
    An entity can be a person or a place, a company name or an organization; it can
    also be a number (say quantity, price, number of days) or a date, a title, a policy
    number, or a medical code. For example, in the text "Martin lives at 27 Broadway
    St.", **Martin** might be detected as a **PERSON**, while **27 Broadway St** might
    be detected as a **LOCATION**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Entities also have a score to indicate the confidence level that the entity
    type was detected correctly. The following table shows a complete list of entity
    types and descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: AWS Comprehend entity types and descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: AWS Comprehend entity types and descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three ways to invoke the detection of entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DetectEntities` will return the entities in a single document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BatchDetectEntities` works on a set of documents and will return a list of
    the entities in each of the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While both the preceding APIs work in synchronous mode, that is, you send the
    content to the API and it will return the results, `StartEntitiesDetectionJob`
    works on a collection of jobs asynchronously. This API is well suited to large
    jobs that take more time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DetectEntities – Input and Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`DetectEntities` takes a `LanguageCode` and a string of text as an input and
    then provides the following information about each entity within the input text:
    `BeginOffset`, `EndOffset`, `Score`, `Text`, and `Type`. The following table shows
    a complete list of AWS Comprehend `DetectEntities`, types, and descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: AWS Comprehend entity types and descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.10: AWS Comprehend entity types and descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.03: Determining the Named Entities in a Document (the DetectEntities
    method)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will determine the named entities in a document. For this,
    we will use Amazon Comprehend''s `DetectEntities` operation. The following are
    the steps for detecting named entities:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/2ADssUI](https://packt.live/2ADssUI).
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the AWS SDK for Python (boto3: [https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html))
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, import the `JSON` module to serialize `JSON` from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, instantiate a new Comprehend client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, after instantiating a new Comprehend client, provide the `English` text
    to analyze:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, `json.dumps()` writes JSON data to a Python string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Press *Shift* + *Enter* to run the two notebook cells. The output of the preceding
    code is shown in the following screenshot:![Figure 2.11: AWS Comprehend DetectEntities
    output'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.11: AWS Comprehend DetectEntities output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The confidence scores were both ~0.99, as the inputs were simple examples.
    As expected, `Seattle` was detected as a `LOCATION`, and `Thursday` was detected
    as a `DATE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: AWS Comprehend BeginOffset and EndOffset review'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: AWS Comprehend BeginOffset and EndOffset review'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.04: Detecting Entities in a Set of Documents (Text Files)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will determine the named entities in multiple documents.
    For this, we will use Amazon Comprehend''s `DetectEntities` operation. The following
    are the steps for detecting the named entities from a set of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/31UCuMs](https://packt.live/31UCuMs).
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the AWS SDK for Python (boto3: [https://boto3.amazonaws.com/v1/documentation/api/latest/index.html](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html))
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, import the `JSON` module to serialize `JSON` from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need to do some file operations to iterate through the documents. Import
    the `glob` module to find text files ending `.txt` from [https://docs.python.org/3.6/library/glob.html](https://docs.python.org/3.6/library/glob.html)
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We also need the `os` library. Import the `os` module from [https://docs.python.org/3.6/library/os.html](https://docs.python.org/3.6/library/os.html)
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, instantiate a new Comprehend client:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s get a list of all the documents (assumes in Jupyter notebook you navigated
    to `Chapter02/Exercise02.04/` directory and the opened the notebook `Exercise2.04.ipynb`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can iterate through the documents and detect the entities in the documents.
    We will be calling `detect_entities` on each of the documents. As before, we will
    also use `json.dumps()` to write the JSON data to a Python string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Press *Shift* + *Enter* to run the two notebook cells. The output of the preceding
    code is shown in the following screenshot. It is a long output—we are showing
    the output for one file. You will see the entities listed for all the files in
    the `/reviews__pos/*.txt` subdirectory:![Figure 2.13: DetectEntities output'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.13: DetectEntities output'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we extended entity detection to a set of documents, calling
    Amazon Comprehend's `DetectEntities` recursively.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Key Phrases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key phrase for AWS is analogous to a noun phrase, which represents an actual
    thing. In English, when we put together different words that represent one concrete
    idea, we call it a noun phrase.
  prefs: []
  type: TYPE_NORMAL
- en: For example, **A fast machine** is a noun phrase because it consists of **A**,
    the article; **fast**, an adjective; and **machine**, which is a noun. AWS looks
    for appropriate word combinations and gives scores that indicate the confidence
    that a string is a noun phrase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.05: Detecting Key Phrases'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will detect key phrases. To do so, we will use Amazon
    Comprehend''s `DetectKeyPhrase` operation. The following are the steps for detecting
    key phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/2Z75cI4](https://packt.live/2Z75cI4).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the AWS SDK for Python (boto3: [http://boto3.readthedocs.io/en/latest/](http://boto3.readthedocs.io/en/latest/))
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, import the JSON module to serialize the JSON from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html
    ) by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, instantiate a new Comprehend client by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, provide the `English` text to analyze using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the code by executing the cells with *Shift* + *Enter*. You will see the
    following output:![Figure 2.14: AWS Comprehend DetectKeyPhrase output'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.14: AWS Comprehend DetectKeyPhrase output'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Sentiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon Comprehend has the capability to detect sentiments, usually used for
    social media posts, blog posts, reviews, emails, and other user-generated content.
    Amazon Comprehend can determine the four shades of sentiment polarity: positive,
    negative, neutral, and mixed. Mixed sentiment is interesting as it can differentiate
    between different aspects; for example, a user might like your website but not
    be thrilled about the price of a product.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.06: Conducting Sentiment Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will carry out sentiment analysis. To do so, we will use
    Amazon Comprehend''s `DetectSentiment` operation. The following are the steps
    for detecting sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The *Pre checkup instructions* and the source code for this exercise are available
    via GitHub in the repository at [https://packt.live/3ebVNU1](https://packt.live/3ebVNU1).
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `AWS SDK` for Python (boto3) from [http://boto3.readthedocs.io/en/latest/](http://boto3.readthedocs.io/en/latest/)
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, import the `JSON` module to serialize JSON from [https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)
    by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, instantiate a new Comprehend client, using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, provide a text string to analyze, using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the code by executing the cells with *Shift* + *Enter*. The output is as
    follows:![Figure 2.15: AWS Comprehend—DetectSentiment output'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.15: AWS Comprehend—DetectSentiment output'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we saw how easy it is to perform sentiment analysis using
    AWS Comprehend. `DetectSentiment` correctly predicted the sentiment of the statement
    *Today is my birthday, I am so happy* as positive.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a Lambda Function and Analyzing Imported Text Using Comprehend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have used Amazon Comprehend to do various NLP tasks, such as detecting entities
    and key phrases and carrying out sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Comprehend and AWS Lambda for responsive NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this topic, we will be integrating AWS Lambda functions with Comprehend,
    which provides a more powerful, scalable infrastructure. You can use AWS Lambda
    to run your code in response to events, such as changes to data in an Amazon S3
    bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Executing code in response to events provides a real-world solution for developing
    scalable software architecture. Overall, this increases our data pipeline and
    provides the ability to handle more complex big data volumes and NLP operations.
  prefs: []
  type: TYPE_NORMAL
- en: What Is AWS Lambda?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AWS Lambda is a compute service that runs code without provisioning or managing
    servers. AWS Lambda executes code only when needed, and scales automatically.
    AWS Lambda runs your code on a high-availability compute infrastructure, which
    performs the administration of the compute service. More specifically, AWS Lambda
    performs the following: server and operating system maintenance, capacity provisioning
    and automatic scaling, code monitoring, and logging.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the goal of AWS Lambda is to make short, simple, modular code segments
    that you can tie together into a larger processing infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: What Does AWS Lambda Do?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lambda allows users to run small segments of code (Java, Node, or Python) to
    complete a specific task. These specific tasks can be storing and then executing
    changes to your AWS setup, or responding to events in S3 (we will explore the
    latter later in this topic). Before Lambda, you would typically need a separate
    EC2 server to run your entire code; however, Lambda allows small segments of code
    to run without the need for EC2.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda Function Anatomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS Lambda provides two options for implementing Python code. First, you can
    upload a complete Python code file. Second, you can use the Lambda function editor
    entirely inline, which means that you can enter and modify the code directly,
    without having to upload any files to AWS. The code that you enter will be executed
    when the Lambda function is invoked. The second option will allow for easier testing,
    so we will use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the structure of a Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you create a function (for example, `s3_trigger`), AWS creates a folder
    named the same, with a Python file named `Lambda_function.py` within the folder.
    This file contains a stub for the `Lambda_handler` function, which is the entry
    point of our Lambda function. The entry point takes two parameters as arguments:
    The `event` argument and the `context` argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `event` argument provides the value of the payload, which is sent to the
    function from the `calling` process. It typically takes the form of a Python `dict`
    type, although it could also be one of `list`, `str`, `int`, `float`, or `NoneType`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `context` argument is of the type `LambdaContext` and contains runtime information.
    You will be using this parameter for an exercise in a later section. The return
    value of the function can be any type that is JSON-serializable. This value gets
    returned to the calling application, after serializing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will incorporate Lambda, S3, and Amazon Comprehend, to automatically perform
    document analysis when a text document is uploaded to S3\. The architecture of
    a Lambda function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16: Architecture diagram'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16061_02_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.16: Architecture diagram'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.07: Setting Up a Lambda Function for S3'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will integrate the following AWS services: S3, Lambda,
    and Amazon Comprehend. To perform this exercise, the architecture should be recollected.
    Upload a file (`test_s3trigger_configured.txt`) to S3 and view the results of
    Comprehend''s analysis. The following are the steps for setting up a Lambda function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating the S3 bucket**'
  prefs: []
  type: TYPE_NORMAL
- en: You should have an AWS account and have completed the exercises and activities
    in *Chapter 1*, *An Introduction to AWS*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, navigate to the Amazon S3 service, [https://console.aws.amazon.com/s3/](https://console.aws.amazon.com/s3/),
    and click `Create bucket`:![Figure 2.17: S3 Bucket creation for the Lambda trigger'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.17: S3 Bucket creation for the Lambda trigger'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For `Bucket name`, type `aws-ml-s3-trigger`, and then click `Create`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Bucket names in AWS have to be unique, otherwise you will get an error "`Bucket
    name already exists`". One easy way to get a unique name is to append the bucket
    name with today's date plus the time, for instance, YYYYMMDDHHMM. While writing
    this chapter, I created the bucket `aws-ml-s3-trigger-202001181023` .
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18: Creating an S3 bucket'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.18: Creating an S3 bucket'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Your bucket will be created, and you will be redirected to the bucket list
    in the `S3 buckets` screen as shown:![Figure 2.19: S3 Bucket list screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.19: S3 Bucket list screen'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, navigate to Amazon Lambda, under `Services`, and click `Lambda` under `Compute`:![Figure
    2.20: Services | Compute | Lambda'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.20: Services | Compute | Lambda'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will see the Lambda console, as shown here:![Figure 2.21: Lambda console'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.21: Lambda console'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the Lambda console, click `Create function`:![Figure 2.22: AWS Lambda Create
    function button'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.22: AWS Lambda Create function button'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose `Author from scratch` from the options. For `Name`, type `s3_trigger`:![Figure
    2.23: AWS Lambda—Creating a function with the Author from scratch option'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.23: AWS Lambda—Creating a function with the Author from scratch option'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the runtime options, choose `Python 3.6` from the list:![Figure 2.24: AWS
    Lambda—Python 3.6 selection'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.24: AWS Lambda—Python 3.6 selection'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Choose or create an execution role` and choose `Create new role from
    AWS policy template(s)` and enter the name `s3TriggerRole` in the `Role name`
    field:![Figure 2.25: AWS Lambda Create Role template'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.25: AWS Lambda Create Role template'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click the dropdown in `Policy templates` and select `Amazon S3 object read-only
    permissions`. You will see AWS Lambda Policy template dropdown box, as shown here:![Figure
    2.26: AWS Lambda Policy templates dropdown box'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.26: AWS Lambda Policy templates dropdown box'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, click the `Create function` button to create the Lambda function in AWS.
    The final AWS Lambda Create function screen looks as follows:![Figure 2.27: AWS
    Lambda—Create a function screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.27: AWS Lambda—Create a function screen'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will see the Lambda function designer. There is lot of information displayed.
    Let''s focus on the essentials for this exercise:![Figure 2.28: AWS Lambda—function
    designer'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.28: AWS Lambda—function designer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Add trigger`, and from the drop-down menu, select `S3`:![Figure 2.29:
    Trigger configuration drop-down menu'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.29: Trigger configuration drop-down menu'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Take a quick look at the options and select `Add`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The bucket name should be the S3 trigger bucket you created (in my case, it
    was `aws-ml-s3-trigger-202001181023`); in the `Event type` section, `All object
    create events` must be selected in the dropdown and `Enable Trigger` should be
    checked, as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You might get the error "`An error occurred when creating the trigger: Configurations
    overlap. Configurations on the same bucket cannot share a common event type`."
    This would happen if you created a function and deleted it. The easiest way is
    to delete the event via `Services | Storage/S3 | Click the bucket | Properties
    | Events` and deleting the Lambda event. Make sure you click the `Save` button
    after deleting the event.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.30: Amazon S3 Trigger configuration'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.30: Amazon S3 Trigger configuration'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will see S3 on the Lambda `Designer` screen:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.31: Lambda function designer with S3'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.31: Lambda function designer with S3'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Again, choose `Add trigger` and choose `CloudWatch/Events/EventBridge`:![Figure
    2.32: Adding the trigger configuration'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.32: Adding the trigger configuration'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then click the box next to `Rule`:![Figure 2.33: Add trigger – creating a new
    rule'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.33: Add trigger – creating a new rule'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Select `Create a new rule`. The following screen will be displayed. Type `s3_trigger_CWRule`
    for the rule name.![Figure 2.34: Add Trigger—New Rule Configuration'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.34: Add Trigger—New Rule Configuration'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Choose `Event pattern` in `Rule type`. Then select `Simple Storage Service
    (S3)` from the dropdown and `All events` and click `Add`:![Figure 2.35: Adding
    an S3 rule type'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.35: Adding an S3 rule type'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s explore the interface a bit more so that you can get comfortable navigating
    through different pages. Click `Functions` in the top-left corner:![Figure 2.36:
    Top navigation bar to navigate back to functions'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.36: Top navigation bar to navigate back to functions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `s3_trigger` to go back to the function you are working on:![Figure 2.37:
    Selecting the lambda function to work on'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.37: Selecting the lambda function to work on'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, scroll down the screen to the `Function code` section. The default code
    will be the same as, or similar to, the following:![Figure 2.38: AWS Lambda—the
    default lambda_function screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.38: AWS Lambda—the default lambda_function screen'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can enter and edit our code entirely within the `lambda_function` screen
    (as long as `Code entry type` is set to `Edit code inline`, which is the default
    value in the drop-down menu).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For this step, you may either follow along and type in the code or obtain it
    from the source code folder at [https://packt.live/2O6WsLW](https://packt.live/2O6WsLW).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we import the **AWS SDK** for Python (boto3: [http://boto3.readthedocs.io/en/latest/](http://boto3.readthedocs.io/en/latest/)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, import the JSON module to serialize the JSON ([https://docs.python.org/3.6/library/json.html](https://docs.python.org/3.6/library/json.html)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a function that takes two parameters—`event` and `context`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create the `s3` client object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Add an `if` event to check whether an event occurs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, replace `<input Bucket name>` with the bucket you created (`aws-ml-s3-trigger-202001181023`,
    in are example):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, access the first index of the `Records` event to obtain the text file
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, assign the `filename` text to a variable and print the filename:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create the file object by getting the bucket and key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign the text to the `body_str_obj` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `comprehend` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next three lines of code call the respective Comprehend functions to detect
    the sentiment, entities, and key phrases from the text document. Then, the output
    is printed to the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final statement returns the `''Hello from Lambda''` string, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, click the `Save` button:![Figure 2.39: AWS Lambda – save screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.39: AWS Lambda – save screen'
  prefs: []
  type: TYPE_NORMAL
- en: From this exercise, the `s3_trigger` function has access to S3, but not Amazon
    Comprehend. We need to attach a policy to the `s3_trigger` function to allow it
    to access Amazon Comprehend to execute the text analysis functions (`detect_sentiment`,
    `detect_entities`, and `detect_key_phrases`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.08: Assigning Policies to S3_trigger to Access Comprehend'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will attach the policies to the `S3_trigger` function
    to allow it to access Comprehend. The steps for completion for assigning the policies
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Amazon Management Console, click `Services` at the top left:![Figure
    2.40: AWS Services from the AWS Management Console'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.40: AWS Services from the AWS Management Console'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to the `Identity and Access Management` dashboard in the `Security,
    Identity, & Compliance` section. You can also type `IAM` and select it from the
    dropdown:![Figure 2.41: IAM dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.41: IAM dashboard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, once you get to the IAM dashboard, click `Roles`:![Figure 2.42: Left-hand
    side of the IAM dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_42.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.42: Left-hand side of the IAM dashboard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, the screen will be populated with the role list. Click `s3TriggerRole`
    in the role list:![Figure 2.43: Role list—selecting s3TriggerRole'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.43: Role list—selecting s3TriggerRole'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `s3TriggerRole` option will be enabled. Then, click `Attach policies`:![Figure
    2.44: Permissions tab for s3TriggerRole'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_44.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.44: Permissions tab for s3TriggerRole'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Type `Comprehend` to filter the policies. Then, click the checkbox next to
    `ComprehendFullAccess`:![Figure 2.45: ComprehendFullAccess policy selection'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.45: ComprehendFullAccess policy selection'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you have selected the checkbox, click `Attach policy` (located in the
    lower right-hand corner of the screen):![Figure 2.46: Attaching the selected policies'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_46.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.46: Attaching the selected policies'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will be redirected to the `s3TriggerRole` screen, and you will receive
    the following message:![Figure 2.47: Successfully attached policies message'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_47.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.47: Successfully attached policies message'
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have successfully attached the policies to the `S3_trigger` function
    thus allowing it to access Comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.01: Integrating Lambda with Amazon Comprehend to Perform Text Analysis'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we will integrate the Lambda functions with Comprehend to
    perform text analysis (`detect_sentiment`, `detect_entities`, and `detect_key_phrases`)
    when a document is uploaded to S3.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you are creating a chatbot. You have identified a business topic
    and the corresponding text documents, with content that will allow the chatbot
    to make your business successful. Your next step is to integrate the Lambda functions
    with Comprehend, for sentiment, key phrases, and entities. To ensure that this
    happens correctly, you will need to have `test_s3trigger_configured.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `test_s3trigger_configured.txt` file can be found on GitHub at link [https://packt.live/3gAxqku](https://packt.live/3gAxqku).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you execute `s3_trigger`, consider the output, based on the following
    aspects of the text: sentiment (positive, negative, or neutral), entities (quantity,
    person, place, and so on), and key phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: First, navigate to the `S3_trigger` Lambda function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `test_s3trigger_configured.txt` to the S3 bucket, to verify the Lambda `S3_trigger`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, upload the file into the bucket and monitor the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click `View logs` in `CloudWatch` by using the log stream.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, expand the output in a text format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following will be the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Sentiment_response` -> Classified as 60.0% likely to be positive'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Sentiment_response`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`entity_response` -> Classified as 70.5% likely to be a quantity'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`entity_response`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`key_phases_response` -> Classified as 89.9% likely "a test file" and 98.5%
    likely "the s3 trigger" are the key phrases:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`key_phases_response`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 279.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Amazon Textract
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another interesting NLP Amazon service is Textract. Essentially, Textract can
    extract information from documents, usually business documents such as tax forms,
    legal documents, medical forms, bank forms, patent registrations, and so forth.
    It is an **optical character recognition (OCR**) solution for scanning structured
    documents, suitable for **robotic process automation** (**RPA**). Textract is
    a relatively new service—previewed in November 2018 and generally available in
    May 2019.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of Textract is that it understands documents and can extract tables
    and/or key-value pairs suitable for downstream processing. A lot of business processes,
    such as health insurance processing, tax preparation, loan application processing,
    monitoring and evaluation of existing loans, compliance evaluation, and engineering
    evaluations take in these documents, usually processing them manually to extract
    information and then start digital processes. Using Amazon Textract, the manual
    intake of various documents can be automated, resulting in a faster turnaround
    when approving loans, accelerated processing of health claims, or approving an
    engineering design quickly, thus achieving good business value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.09: Extracting Tax Information Using Amazon Textract'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will take a page of a sample tax return document from
    documentcloud.org ([https://www.documentcloud.org/documents/3462212-Sample-2016-Tax-Return.html](https://www.documentcloud.org/documents/3462212-Sample-2016-Tax-Return.html))
    and see how much information Textract can extract:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The sample document (page 1 of US Tax form 1040) is available at [https://packt.live/2O5e1Mn](https://packt.live/2O5e1Mn).
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we will use the Textract interface directly. This is very
    useful to try out and to see how a document is amenable to OCR.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, go to the Textract dashboard by selecting `Services | Machine Learning
    | Amazon Textract`. There are lots of interesting details on that page. Take the
    time to read through the materials:![Figure 2.48: Amazon Textract dashboard'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_48.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.48: Amazon Textract dashboard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Try Amazon Textract`. A very simple utilitarian page appears:![Figure
    2.49: Amazon Textract Analyze document'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_49.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.49: Amazon Textract Analyze document'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Click `Upload document` and upload the `Sample-2016-Tax-Return.jpeg` file.
    The service thinks for a minute and shows very informative tabs and the information
    it has extracted:![Figure 2.50: Amazon Textract Analyze document screen with the
    sample tax form'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_50.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.50: Amazon Textract Analyze document screen with the sample tax form'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The raw text is interesting, but we are looking for more value for our automation pipeline.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click the `Forms` tab and you will see a very interesting page—it can get the
    value as well as the key. For example, line 7 is extracted as `7 Wages, salaries,
    tips, etc. Attach Form(s) W-2 7` and a value of `93,500`. Now, a downstream loan
    processing application can get the value as well as the context and act on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can click other fields on the image on the left-hand side and see the extracted
    entry on the right-hand side.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can download the results as JSON, CSV, table, and text formats. As expected,
    `keyvalues.csv` has the line 7 we saw earlier as the key and `93,500` as the value:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.51: Amazon Textract Analyze document screen with the sample tax
    document form'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_51.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.51: Amazon Textract Analyze document screen with the sample tax document
    form'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can see the extracted fields in a table format (with the keys as the caption
    and the value in the grey box under the captions) as shown below:![Figure 2.52:
    Amazon Textract Analyze document screen with the sample tax document Forms tab
    showing the key value'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16061_02_52.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.52: Amazon Textract Analyze document screen with the sample tax document
    Forms tab showing the key value'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `Tables` tab is also interesting. Textract was able to extract two tables—the
    top and the bottom portion—but was not able to extract the middle one:![Figure
    2.53: Amazon Textract Analyze document screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: with the sample tax form showing Tables (form)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_53.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.53: Amazon Textract Analyze document screen with the sample tax form
    showing Tables (form)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can see the extracted fields in a table format by clicking the `Tables`
    tab:![Figure 2.54: Amazon Textract Analyze document screen'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: with the sample tax form showing Tables (extracted)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16061_02_54.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.54: Amazon Textract Analyze document screen with the sample tax form
    showing Tables (extracted)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a feel for what Textract can do, another useful exercise would
    be to develop a loan processing pipeline using Lambda. When page 1 of US Tax 1040
    is dropped into an S3 bucket as a JPEG file, trigger a Lambda that takes the file
    and invokes Textract and stores the key-value file as a CSV in another bucket.
    If you feel adventurous, you can develop another Lambda downstream of Textract
    that gets triggered when the output file is created, and it can either alert a
    loan officer via SMS or a queue or even a mobile app alert.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with high-level concepts around Amazon AI services
    and serverless computing. On a conceptual level, you learned about serverless
    computing as well as the various AI services available on the AWS platform.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the culmination of these independent functions provides the foundation
    for building complex machine learning-based NLP applications (for example, Siri,
    Alexa, and so on). Knowing how and why the individual functions operate will allow
    you to build your own AWS-based NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we dived into the details of Amazon Comprehend—how Comprehend's `DetectDominantLanguage`
    method is structured, and how to pass in both strings and a list of strings. You
    learned how to extract entities, sentiments, key phrases, and topics, which provide
    the data for complex NLP. This allows Amazon Comprehend to become more efficient
    by automating text analysis upon a text document that's been uploaded to S3.
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how to use Amazon Textract to extract structured information
    (tables and key-value pairs) out of scanned documents as a prelude to process automation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore topic modeling and perform theme extraction.
  prefs: []
  type: TYPE_NORMAL
