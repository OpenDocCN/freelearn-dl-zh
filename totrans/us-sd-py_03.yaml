- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating Images Using Stable Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will start using common Stable Diffusion functionalities
    by leveraging the Hugging Face Diffusers package ([https://github.com/huggingface/diffusers](https://github.com/huggingface/diffusers))
    and open-source packages. As we mentioned in [*Chapter 1*](B21263_01.xhtml#_idTextAnchor015),
    *Introduction to Stable Diffusion*, Hugging Face Diffusers is currently the most
    widely used Python implementation of Stable Diffusion. As we explore image generation,
    we will walk through the common terminologies used.
  prefs: []
  type: TYPE_NORMAL
- en: Assume you have all the packages and dependencies installed; if you see an error
    message saying no GPU is found or CUDA is required, refer to [*Chapter 2*](B21263_02.xhtml#_idTextAnchor037)
    to set up the environment to run Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, I aim to familiarize you with Stable Diffusion by using the
    Diffusers package from Hugging Face. We will dig into the internals of Stable
    Diffusion in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to log in to Hugging Face with Hugging Face tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating an image using Stable Diffusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a generation seed to reproduce an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Stable Diffusion scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swapping or changing a Stable Diffusion model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a guidance scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start.
  prefs: []
  type: TYPE_NORMAL
- en: Logging in to Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may use the `login()` function in the `huggingface_hub` library like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In doing so, you are authenticating with the Hugging Face Hub. This allows you
    to download pre-trained diffusion models that are hosted on the Hub. Without logging
    in, you may not be able to download these models using the model ID, such as `runwayml/stable-diffusion-v1-5`.
  prefs: []
  type: TYPE_NORMAL
- en: When you run the preceding code, you are providing your Hugging Face token.
    You may wonder about the steps to *access* the token, but don’t worry. The token
    input dialog will provide links and information to *access* the token.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have logged in, you can download pre-trained diffusion models by
    using the `from_pretrained()` function in the Diffusers package. For example,
    the following code will download the `stable-diffusion-v1-5` model from the Hugging
    Face Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that I am using `to("cuda:0")` instead of `to("cuda")`
    because in the case of multiple-GPU scenarios, you can change the CUDA index to
    tell Diffusers to use a specified GPU. For instance, you can use `to("cuda:1")`
    to use the second CUDA-enabled GPU to generate Stable Diffusion images.
  prefs: []
  type: TYPE_NORMAL
- en: After downloading the model, it is time to generate an image using Stable Diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the Stable Diffusion model loaded up to the GPU, let’s generate
    an image. `text2img_pipe` holds the pipeline object; all we need to provide is
    a `prompt` string, using natural language to describe the image we want to generate,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to change the prompt to anything else that comes to your mind when
    you are reading this, for example, `high resolution, a photograph of a cat running
    on the surface of Mars` or `4k, high quality image of a cat driving a plane`.
    It is amazing how Stable Diffusion can generate images according to a description
    in purely natural language.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the preceding code without changing it, you may see an image like
    this showing up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: An image of an astronaut riding a horse](img/B21263_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: An image of an astronaut riding a horse'
  prefs: []
  type: TYPE_NORMAL
- en: I said *you may see an image like this* because there is a 99.99% chance you
    will not see the same image; instead, you will see an image with a similar look
    and feel. To make the generation consistent, we will need another parameter, called
    `generator`.
  prefs: []
  type: TYPE_NORMAL
- en: Generation seed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Stable Diffusion, a seed is a random number that is used to initialize the
    generation process. The seed is used to create a noise tensor, which is then used
    by the diffusion model to generate an image. The same seed together with the same
    prompt and settings will generally produce the same image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generation seed is needed for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reproducibility**: By using the same seed, you can consistently generate
    the same image with identical settings and prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: You can discover diverse image variations by altering the
    seed number. This often leads to the emergence of novel and intriguing images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When a seed number is not provided, the Diffusers package automatically generates
    a random number for each image creation process. However, you have the option
    to specify your preferred seed number, as demonstrated in the following Python
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we use `torch` to create a `torch.Generator` object with
    a manual seed provided. We specifically use this generator for image generation.
    By doing this, we can reproduce the same image repeatedly.
  prefs: []
  type: TYPE_NORMAL
- en: The generation seed is one method to control Stable Diffusion image generation.
    Next, let's explore the scheduler for further customization.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After discussing the generation seed, let’s now delve into another essential
    aspect of Stable Diffusion image generation: the sampling scheduler.'
  prefs: []
  type: TYPE_NORMAL
- en: The original Diffusion models have demonstrated impressive results in generating
    images. However, one drawback is the slow reverse-denoising process, which typically
    requires 1,000 steps to transform a random noise data space into a coherent image
    (specifically, latent data space, a concept we will explore further in [*Chapter
    4*](B21263_04.xhtml#_idTextAnchor081)). This lengthy process can be burdensome.
  prefs: []
  type: TYPE_NORMAL
- en: 'To shorten the image generation process, several solutions have been brought
    out by researchers. The idea is simple: instead of denoising 1,000 steps, what
    if we could take a sample and only perform the key steps on that sample? And this
    idea works. Samplers or schedulers enable the Diffusion model to generate an image
    in a mere 20 steps!'
  prefs: []
  type: TYPE_NORMAL
- en: In the Hugging Face Diffusers package, these helpful components are referred
    to as **schedulers**. However, you may also encounter the term **sampler** in
    other resources. You may take a look at the Diffusers *Schedulers* [2] page for
    the latest supported schedulers.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the Diffusers package uses `PNDMScheduler`. We can find it by running
    this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code will return an object like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: At first glance, the `PNDMScheduler` object’s fields might seem complex and
    unfamiliar. However, as you delve deeper into the internals of the Stable Diffusion
    model in *Chapters 4* and *5*, these fields will become more familiar and comprehensible.
    The learning journey ahead promises to unravel the intricacies of the Stable Diffusion
    model and shed light on the purpose and significance of each field within the
    `PNDMScheduler` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many list schedulers can generate images in as few as 20 to 50 steps. Based
    on my experience, the `Euler` scheduler is one of the top choices. Let’s apply
    the `Euler` scheduler to generate an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can customize the number of denoising steps by using the `num_inference_steps`
    parameter. A higher step count generally leads to better image quality. Here,
    we set the scheduling steps to `20` and compared the results of the default `PNDMScheduler`
    and `EulerDiscreteScheduler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the difference between the two schedulers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Left: Euler scheduler with 20 steps; right: PNDMScheduler with
    20 steps](img/B21263_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Left: Euler scheduler with 20 steps; right: PNDMScheduler with
    20 steps'
  prefs: []
  type: TYPE_NORMAL
- en: In this comparison, the Euler scheduler correctly generates an image with all
    four horse legs, while the PNDM scheduler provides more detail but misses one
    horse leg. These schedulers perform remarkably well, reducing the entire image
    generation process from 1,000 steps to just 20 steps, making it feasible to run
    Stable Diffusion on home computers.
  prefs: []
  type: TYPE_NORMAL
- en: Note that each scheduler has advantages and disadvantages. You may need to try
    out the schedulers to find out which one fits the best.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the process of replacing the original Stable Diffusion model
    with a community-contributed, fine-tuned alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Changing a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing this chapter, there are numerous models available, fine-tuned
    based on the V1.5 Stable Diffusion model, contributed by the thriving user community.
    If the model file is hosted on Hugging Face, you can easily switch to a different
    model by changing its identifier, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, you can also use a `ckpt/safetensors` model downloaded from civitai.com
    ([http://civitai.com](http://civitai.com)). Here, we demonstrate loading the `deliberate-v2`
    model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The primary difference when loading a model from a local file lies in the use
    of the `from_single_file` function instead of `from_pretrained`. A `ckpt` model
    file can be loaded up using the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B21263_06.xhtml#_idTextAnchor117) of this book, we will focus
    exclusively on model loading, covering both Hugging Face and local storage methods.
    By experimenting with various models, you can discover improvements, unique artistic
    styles, or better compatibility for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: We have touched on the generation seed, scheduler, and model usage. Another
    parameter that plays a key role is `guidance_scale`. Let’s take a look at it next.
  prefs: []
  type: TYPE_NORMAL
- en: Guidance scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Guidance scale or **Classifier-Free Guidance** (**CFG**) is a parameter that
    controls the adherence of the generated image to the text prompt. A higher guidance
    scale will force the image to be more aligned with the prompt, while a lower guidance
    scale will give more space for Stable Diffusion to decide what to put into the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample of applying a different guidance scale while keeping other
    parameters the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 3**.3* provides a side-by-side comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Left: guidance_scale = 3; middle: guidance_scale = 7; right:
    guidance_scale = 10](img/B21263_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Left: guidance_scale = 3; middle: guidance_scale = 7; right: guidance_scale
    = 10'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, besides prompt adherence, we can notice that a high guidance scale
    setting has the following effects:'
  prefs: []
  type: TYPE_NORMAL
- en: Increases the color saturation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increases the contrast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May lead to a blurred image if set too high
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `guidance_scale` parameter is typically set between `7` and `8.5`. A value
    of `7.5` is a good default value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored the essentials of using Stable Diffusion through
    the Hugging Face Diffusers package. We accomplished the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Logged in to Hugging Face to enable automatic model downloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generated images deterministically using the generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilized the scheduler for efficient image creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusted the guidance scale for desired image qualities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With just a few lines of code, we successfully created images, demonstrating
    the remarkable capabilities of the Diffusers package. This chapter did not cover
    every feature and option; keep in mind that the package is continually evolving,
    with new functions and enhancements regularly added.
  prefs: []
  type: TYPE_NORMAL
- en: For those eager to unlock the full potential of the Diffusers package, I encourage
    you to explore its source code. Dive into the inner workings, uncover hidden gems,
    and build a Stable Diffusion pipeline from scratch. A rewarding journey awaits!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the next chapter, we will delve into the internals of the package and learn
    how to construct a custom Stable Diffusion pipeline tailored to your unique needs
    and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*High-Resolution Image Synthesis with Latent Diffusion* *Models*: [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hugging Face Diffusers schedulers: [https://huggingface.co/docs/diffusers/api/schedulers/overview](https://huggingface.co/docs/diffusers/api/schedulers/overview)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
