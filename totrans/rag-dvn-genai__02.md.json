["```py\n!pip install beautifulsoup4==4.12.3\n!pip install requests==2.31.0 \n```", "```py\n#Google Drive option to store API Keys\n#Store your key in a file and read it(you can type it directly in the # #notebook but it will be visible for somebody next to you)\nfrom google.colab import drive\ndrive.mount('/content/drive') \n```", "```py\nimport subprocess\nurl = \"https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/commons/grequests.py\"\noutput_file = \"grequests.py\"\n# Prepare the curl command using the private token\ncurl_command = [\n    \"curl\",\n    \"-o\", output_file,\n    url\n]\n# Execute the curl command\ntry:\n    subprocess.run(curl_command, check=True)\n    print(\"Download successful.\")\nexcept subprocess.CalledProcessError:\n    print(\"Failed to download the file.\") \n```", "```py\nimport subprocess\nimport os\n# add a private token after the filename if necessary\ndef download(directory, filename):\n    # The base URL of the image files in the GitHub repository\n    base_url = 'https://raw.githubusercontent.com/Denis2054/RAG-Driven-Generative-AI/main/'\n    # Complete URL for the file\n    file_url = f\"{base_url}{directory}/{filename}\"\n    # Use curl to download the file, including an Authorization header for the private token\n    try:\n        # Prepare the curl command with the Authorization header\n        #curl_command = f'curl -H \"Authorization: token {private_token}\" -o {filename} {file_url}'\n        curl_command = f'curl -H -o {filename} {file_url}'\n        # Execute the curl command\n        subprocess.run(curl_command, check=True, shell=True)\n        print(f\"Downloaded '{filename}' successfully.\")\n    except subprocess.CalledProcessError:\n        print(f\"Failed to download '{filename}'. Check the URL, your internet connection, and if the token is correct and has appropriate permissions.\") \n```", "```py\n!pip install deeplake==3.9.18\n!pip install openai==1.40.3 \n```", "```py\n# For Google Colab and Activeloop(Deeplake library)\n#This line writes the string \"nameserver 8.8.8.8\" to the file. This is specifying that the DNS server the system\n#should use is at the IP address 8.8.8.8, which is one of Google's Public DNS servers.\nwith open('/etc/resolv.conf', 'w') as file:\n   file.write(\"nameserver 8.8.8.8\") \n```", "```py\n#Retrieving and setting OpenAI API key\nf = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\nAPI_KEY=f.readline().strip()\nf.close()\n#The OpenAI API key\nimport os\nimport openai\nos.environ['OPENAI_API_KEY'] =API_KEY\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\") \n```", "```py\n#Retrieving and setting Activeloop API token\nf = open(\"drive/MyDrive/files/activeloop.txt\", \"r\")\nAPI_token=f.readline().strip()\nf.close()\nACTIVELOOP_TOKEN=API_token\nos.environ['ACTIVELOOP_TOKEN'] =ACTIVELOOP_TOKEN \n```", "```py\nimport requests\nfrom bs4 import BeautifulSoup\nimport re \n```", "```py\n# URLs of the Wikipedia articles\nurls = [\n    \"https://en.wikipedia.org/wiki/Space_exploration\",\n    \"https://en.wikipedia.org/wiki/Apollo_program\",\n    \"https://en.wikipedia.org/wiki/Hubble_Space_Telescope\",\n    \"https://en.wikipedia.org/wiki/Mars_over\",\n    \"https://en.wikipedia.org/wiki/International_Space_Station\",\n    \"https://en.wikipedia.org/wiki/SpaceX\",\n    \"https://en.wikipedia.org/wiki/Juno_(spacecraft)\",\n    \"https://en.wikipedia.org/wiki/Voyager_program\",\n    \"https://en.wikipedia.org/wiki/Galileo_(spacecraft)\",\n    \"https://en.wikipedia.org/wiki/Kepler_Space_Telescope\"\n] \n```", "```py\ndef clean_text(content):\n    # Remove references that usually appear as [1], [2], etc.\n    content = re.sub(r'\\[\\d+\\]', '', content)\n    return content \n```", "```py\ndef fetch_and_clean(url):\n    # Fetch the content of the URL\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    # Find the main content of the article, ignoring side boxes and headers\n    content = soup.find('div', {'class': 'mw-parser-output'})\n    # Remove the bibliography section, which generally follows a header like \"References\", \"Bibliography\"\n    for section_title in ['References', 'Bibliography', 'External links', 'See also']:\n        section = content.find('span', id=section_title)\n        if section:\n            # Remove all content from this section to the end of the document\n            for sib in section.parent.find_next_siblings():\n                sib.decompose()\n            section.parent.decompose()\n    # Extract and clean the text\n    text = content.get_text(separator=' ', strip=True)\n    text = clean_text(text)\n    return text \n```", "```py\n# File to write the clean text\nwith open('llm.txt', 'w', encoding='utf-8') as file:\n    for url in urls:\n        clean_article_text = fetch_and_clean(url)\n        file.write(clean_article_text + '\\n')\nprint(\"Content written to llm.txt\") \n```", "```py\nContent written to llm.txt \n```", "```py\n# Open the file and read the first 20 lines\nwith open('llm.txt', 'r', encoding='utf-8') as file:\n    lines = file.readlines()\n    # Print the first 20 lines\n    for line in lines[:20]:\n        print(line.strip()) \n```", "```py\nExploration of space, planets, and moons \"Space Exploration\" redirects here. For the company, see SpaceX . For broader coverage of this topic, see Exploration . Buzz Aldrin taking a core sample of the Moon during the Apollo 11 mission… \n```", "```py\nfrom grequests import download\nsource_text = \"llm.txt\"\ndirectory = \"Chapter02\"\nfilename = \"llm.txt\"\ndownload(directory, filename) \n```", "```py\n# Open the file and read the first 20 lines\nwith open('llm.txt', 'r', encoding='utf-8') as file:\n    lines = file.readlines()\n    # Print the first 20 lines\n    for line in lines[:20]:\n        print(line.strip()) \n```", "```py\nExploration of space, planets, and moons \"Space Exploration\" redirects here. \n```", "```py\nwith open(source_text, 'r') as f:\n    text = f.read()\nCHUNK_SIZE = 1000\nchunked_text = [text[i:i+CHUNK_SIZE] for i in range(0,len(text), CHUNK_SIZE)] \n```", "```py\nvector_store_path = \"hub://denis76/space_exploration_v1\" \n```", "```py\nfrom deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\nimport deeplake.util\ntry:\n    # Attempt to load the vector store\n    vector_store = VectorStore(path=vector_store_path)\n    print(\"Vector store exists\")\nexcept FileNotFoundError:\n    print(\"Vector store does not exist. You can create it.\")\n    # Code to create the vector store goes here\n    create_vector_store=True \n```", "```py\nYour Deep Lake dataset has been successfully created!\nVector store exists \n```", "```py\ndef embedding_function(texts, model=\"text-embedding-3-small\"):\n   if isinstance(texts, str):\n       texts = [texts]\n   texts = [t.replace(\"\\n\", \" \") for t in texts]\n   return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data] \n```", "```py\nadd_to_vector_store=True\nif add_to_vector_store == True:\n    with open(source_text, 'r') as f:\n        text = f.read()\n        CHUNK_SIZE = 1000\n        chunked_text = [text[i:i+1000] for i in range(0, len(text), CHUNK_SIZE)]\nvector_store.add(text = chunked_text,\n              embedding_function = embedding_function,\n              embedding_data = chunked_text,\n              metadata = [{\"source\": source_text}]*len(chunked_text)) \n```", "```py\nCreating 839 embeddings in 2 batches of size 500:: 100%|██████████| 2/2 [01:44<00:00, 52.04s/it]\nDataset(path='hub://denis76/space_exploration_v1', tensors=['text', 'metadata', 'embedding', 'id'])\n  tensor      htype       shape      dtype  compression\n  -------    -------     -------    -------  -------\n   text       text      (839, 1)      str     None  \n metadata     json      (839, 1)      str     None  \n embedding  embedding  (839, 1536)  float32   None  \n    id        text      (839, 1)      str     None \n```", "```py\n# Print the summary of the Vector Store\nprint(vector_store.summary()) \n```", "```py\nds = deeplake.load(vector_store_path) \n```", "```py\nThis dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/denis76/space_exploration_v1\nhub://denis76/space_exploration_v1 loaded successfully. \n```", "```py\n#Estimates the size in bytes of the dataset.\nds_size=ds.size_approx() \n```", "```py\n# Convert bytes to megabytes and limit to 5 decimal places\nds_size_mb = ds_size / 1048576\nprint(f\"Dataset size in megabytes: {ds_size_mb:.5f} MB\")\n# Convert bytes to gigabytes and limit to 5 decimal places\nds_size_gb = ds_size / 1073741824\nprint(f\"Dataset size in gigabytes: {ds_size_gb:.5f} GB\") \n```", "```py\nDataset size in megabytes: 55.31311 MB\nDataset size in gigabytes: 0.05402 GB \n```", "```py\nvector_store_path = \"hub://denis76/space_exploration_v1\" \n```", "```py\nfrom deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\nimport deeplake.util\nds = deeplake.load(vector_store_path) \n```", "```py\nvector_store = VectorStore(path=vector_store_path) \n```", "```py\nDeep Lake Dataset in hub://denis76/space_exploration_v1 already exists, loading from the storage \n```", "```py\ndef embedding_function(texts, model=\"text-embedding-3-small\"):\n   if isinstance(texts, str):\n       texts = [texts]\n   texts = [t.replace(\"\\n\", \" \") for t in texts]\n   return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data] \n```", "```py\ndef get_user_prompt():\n    # Request user input for the search prompt\n    return input(\"Enter your search query: \")\n# Get the user's search query\n#user_prompt = get_user_prompt()\nuser_prompt=\"Tell me about space exploration on the Moon and Mars.\" \n```", "```py\nsearch_results = vector_store.search(embedding_data=user_prompt, embedding_function=embedding_function) \n```", "```py\nprint(user_prompt) \n```", "```py\n# Function to wrap text to a specified width\ndef wrap_text(text, width=80):\n    lines = []\n    while len(text) > width:\n        split_index = text.rfind(' ', 0, width)\n        if split_index == -1:\n            split_index = width\n        lines.append(text[:split_index])\n        text = text[split_index:].strip()\n    lines.append(text)\n    return '\\n'.join(lines) \n```", "```py\nimport textwrap\n# Assuming the search results are ordered with the top result first\ntop_score = search_results['score'][0]\ntop_text = search_results['text'][0].strip()\ntop_metadata = search_results['metadata'][0]['source']\n# Print the top search result\nprint(\"Top Search Result:\")\nprint(f\"Score: {top_score}\")\nprint(f\"Source: {top_metadata}\")\nprint(\"Text:\")\nprint(wrap_text(top_text)) \n```", "```py\nTop Search Result:\nScore: 0.6016581654548645\nSource: llm.txt\nText:\nExploration of space, planets, and moons \"Space Exploration\" redirects here.\nFor the company, see SpaceX . For broader coverage of this topic, see\nExploration . Buzz Aldrin taking a core sample of the Moon during the Apollo 11 mission Self-portrait of Curiosity rover on Mars 's surface Part of a series on… \n```", "```py\naugmented_input=user_prompt+\" \"+top_text\nprint(augmented_input) \n```", "```py\nTell me about space exploration on the Moon and Mars. Exploration of space, planets … \n```", "```py\nfrom openai import OpenAI\nclient = OpenAI()\nimport time\ngpt_model = \"gpt-4o\" \nstart_time = time.time()  # Start timing before the request \n```", "```py\ndef call_gpt4_with_full_text(itext):\n    # Join all lines to form a single string\n    text_input = '\\n'.join(itext)\n    prompt = f\"Please summarize or elaborate on the following content:\\n{text_input}\"\n    try:\n        response = client.chat.completions.create(\n            model=gpt_model,\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a space exploration expert.\"},\n                {\"role\": \"assistant\", \"content\": \"You can read the input and answer in detail.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            temperature=0.1  # Fine-tune parameters as needed\n        )\n        return response.choices[0].message.content.strip()\n    except Exception as e:\n        return str(e) \n```", "```py\ngpt4_response = call_gpt4_with_full_text(augmented_input)\nresponse_time = time.time() - start_time  # Measure response time\nprint(f\"Response Time: {response_time:.2f} seconds\")  # Print response time\nprint(gpt_model, \"Response:\", gpt4_response) \n```", "```py\nResponse Time: 8.44 seconds\ngpt-4o Response: Space exploration on the Moon and Mars has been a significant focus of human spaceflight and robotic missions. Here's a detailed summary… \n```", "```py\nimport textwrap\nimport re\nfrom IPython.display import display, Markdown, HTML\nimport markdown\ndef print_formatted_response(response):\n    # Check for markdown by looking for patterns like headers, bold, lists, etc.\n    markdown_patterns = [\n        r\"^#+\\s\",           # Headers\n        r\"^\\*+\",            # Bullet points\n        r\"\\*\\*\",            # Bold\n        r\"_\",               # Italics\n        r\"\\[.+\\]\\(.+\\)\",    # Links\n        r\"-\\s\",             # Dashes used for lists\n        r\"\\`\\`\\`\"           # Code blocks\n    ]\n    # If any pattern matches, assume the response is in markdown\n    if any(re.search(pattern, response, re.MULTILINE) for pattern in markdown_patterns):\n        # Markdown detected, convert to HTML for nicer display\n        html_output = markdown.markdown(response)\n        display(HTML(html_output))  # Use display(HTML()) to render HTML in Colab\n    else:\n        # No markdown detected, wrap and print as plain text\n        wrapper = textwrap.TextWrapper(width=80)\n        wrapped_text = wrapper.fill(text=response)\n        print(\"Text Response:\")\n        print(\"--------------------\")\n        print(wrapped_text)\n        print(\"--------------------\\n\")\nprint_formatted_response(gpt4_response) \n```", "```py\nMoon Exploration\n    Historical Missions:\n    1\\. Apollo Missions: NASA's Apollo program, particularly Apollo 11, marked the first manned Moon landing in 1969\\. Astronauts like Buzz Aldrin collected core samples and conducted experiments.\n    2\\. Lunar Missions: Various missions have been conducted to explore the Moon, including robotic landers and orbiters from different countries.\nScientific Goals:\n    3\\. Geological Studies: Understanding the Moon's composition, structure, and history.\n    4\\. Resource Utilization: Investigating the potential for mining resources like Helium-3 and water ice.\n    Future Plans:\n    1\\. Artemis Program: NASA's initiative to return humans to the Moon and establish a sustainable presence by the late 2020s.\n    2\\. International Collaboration: Partnerships with other space agencies and private companies to build lunar bases and conduct scientific research.\nMars Exploration\n    Robotic Missions:\n    1\\. Rovers: NASA's rovers like Curiosity and Perseverance have been exploring Mars' surface, analyzing soil and rock samples, and searching for signs of past life.\n    2\\. Orbiters: Various orbiters have been mapping Mars' surface and studying its atmosphere… \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef calculate_cosine_similarity(text1, text2):\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform([text1, text2])\n    similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])\n    return similarity[0][0] \n```", "```py\nsimilarity_score = calculate_cosine_similarity(user_prompt, gpt4_response)\nprint(f\"Cosine Similarity Score: {similarity_score:.3f}\") \n```", "```py\nCosine Similarity Score: 0.396 \n```", "```py\n# Example usage with your existing functions\nsimilarity_score = calculate_cosine_similarity(augmented_input, gpt4_response)\nprint(f\"Cosine Similarity Score: {similarity_score:.3f}\") \n```", "```py\nCosine Similarity Score: 0.857 \n```", "```py\n!pip install sentence-transformers \n```", "```py\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\ndef calculate_cosine_similarity_with_embeddings(text1, text2):\n    embeddings1 = model.encode(text1)\n    embeddings2 = model.encode(text2)\n    similarity = cosine_similarity([embeddings1], [embeddings2])\n    return similarity[0][0] \n```", "```py\nsimilarity_score = calculate_cosine_similarity_with_embeddings(augmented_input, gpt4_response)\nprint(f\"Cosine Similarity Score: {similarity_score:.3f}\") \n```", "```py\nCosine Similarity Score: 0.739 \n```"]