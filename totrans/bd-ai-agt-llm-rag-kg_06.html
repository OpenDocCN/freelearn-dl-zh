<html><head></head><body>
<div id="_idContainer180">
<h1 class="chapter-number" id="_idParaDest-91"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.2.1">Advanced RAG Techniques for Information Retrieval and Augmentation</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we discussed RAG and how this paradigm has evolved to solve some shortcomings of LLMs. </span><span class="koboSpan" id="kobo.3.2">However, even naïve RAG (the basic form of this paradigm) is not without its challenges and problems. </span><span class="koboSpan" id="kobo.3.3">Naïve RAG consists of a few simple components: an embedder, a vector database for retrieval, and an LLM for generation. </span><span class="koboSpan" id="kobo.3.4">As mentioned in the previous chapter, naïve RAG involves a collection of text being embedded in a database; once a query from a user arrives, text chunks that are relevant to the query are searched for and provided to the LLM to generate a response. </span><span class="koboSpan" id="kobo.3.5">These components allow us to respond effectively to user queries; but as we shall see, we can add additional components to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">the system.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will see how in advanced RAG, we can modify or improve the various steps in the pipeline (data ingestion, indexing, retrieval, and generation). </span><span class="koboSpan" id="kobo.5.2">This solves some of the problems of naïve RAG and gives us more control over the whole process. </span><span class="koboSpan" id="kobo.5.3">We will later see how the demand for more flexibility led to a further step forward (modular RAG). </span><span class="koboSpan" id="kobo.5.4">We will also discuss important aspects of RAG, especially when the system (a RAG base product) is being produced. </span><span class="koboSpan" id="kobo.5.5">For example, we will discuss the challenges when we have a large amount of data or users. </span><span class="koboSpan" id="kobo.5.6">Also, since these systems may contain sensitive data, we will discuss both robustness and privacy. </span><span class="koboSpan" id="kobo.5.7">Finally, although RAG is a popular system today, it is still relatively new. </span><span class="koboSpan" id="kobo.5.8">So, there are still unanswered questions and exciting prospects for </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">its future.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we’ll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Discussing naïve </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">RAG issues</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Exploring advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">RAG pipelines</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Modular RAG and integration with </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">other systems</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Implementing an advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">RAG pipeline</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Understanding the scalability and performance </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">of RAG</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.19.1">Open questions</span></span></li>
</ul>
<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.20.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.21.1">Most of the code in this chapter can be run on a CPU, but it is preferable for it to be run on a GPU. </span><span class="koboSpan" id="kobo.21.2">The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, LangChain, </span><strong class="source-inline"><span class="koboSpan" id="kobo.22.1">chromadb</span></strong><span class="koboSpan" id="kobo.23.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">sentence-transformer</span></strong><span class="koboSpan" id="kobo.25.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">faiss-cpu</span></strong><span class="koboSpan" id="kobo.27.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">The code for this chapter can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr6"><span class="No-Break"><span class="koboSpan" id="kobo.31.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr6</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.32.1">.</span></span></p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.33.1">Discussing naïve RAG issues</span></h1>
<p><span class="koboSpan" id="kobo.34.1">In</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.35.1"> the previous chapter, we introduced RAG in its basic version (called naïve RAG). </span><span class="koboSpan" id="kobo.35.2">Although the basic version of RAG has gone a long way in solving some of the most pressing problems of LLMs, several issues remain. </span><span class="koboSpan" id="kobo.35.3">For industrial applications, in particular (as well as medical, legal, and financial), naïve RAG is not enough, and we need a more sophisticated pipeline. </span><span class="koboSpan" id="kobo.35.4">We will now explore the problems associated with naïve RAG, each of which is associated with a specific step in the pipeline (query handling, retrieval, </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">and generation).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer155">
<span class="koboSpan" id="kobo.37.1"><img alt="Figure 6.1 – Summary of naïve RAG issues and identifying different steps in the pipeline where the issues can arise" src="image/B21257_06_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.38.1">Figure 6.1 – Summary of naïve RAG issues and identifying different steps in the pipeline where the issues can arise</span></p>
<p><span class="koboSpan" id="kobo.39.1">Let’s </span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.40.1">discuss these issues </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">in detail:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.42.1">Retrieval challenges</span></strong><span class="koboSpan" id="kobo.43.1">: The phase of retrieval struggles with precision (retrieved chunks are misaligned) and recall (finding all relevant chunks). </span><span class="koboSpan" id="kobo.43.2">In addition, the knowledge base could be outdated. </span><span class="koboSpan" id="kobo.43.3">This could lead to either hallucinations or, depending on the prompt used, a response such as, “Sorry, I do not know the answer” or “The context does not allow the query to be answered.” </span><span class="koboSpan" id="kobo.43.4">This can also be derived from poor database indexing or the documents being of different types (PDF, HTML, text, and so on) and being treated incorrectly (chunking for all file types is </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">an example).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.45.1">Missed top-rank documents</span></strong><span class="koboSpan" id="kobo.46.1">: Documents essential to answering the query may not be at the top of the list. </span><span class="koboSpan" id="kobo.46.2">By selecting the top </span><em class="italic"><span class="koboSpan" id="kobo.47.1">k</span></em><span class="koboSpan" id="kobo.48.1"> documents, we might select top chunks that are less relevant (or do not contain the answer) and not return the really relevant chunks to the LLM. </span><span class="koboSpan" id="kobo.48.2">The semantic representation capability of the embedding model may be weak (i.e., we chose an ineffective model because it was too small or not suitable for the domain of </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">our documents).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.50.1">Relevant information not in context</span></strong><span class="koboSpan" id="kobo.51.1">: Documents </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.52.1">with the answer are found but there are too many to fit in the LLM’s context. </span><span class="koboSpan" id="kobo.52.2">For example, the response might need several chunks, and these are too many for the context length of </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">the model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Failed extraction</span></strong><span class="koboSpan" id="kobo.55.1">: The right context might be returned to the LLM, but it might not extract the right answer. </span><span class="koboSpan" id="kobo.55.2">Usually, this happens when there is too much noise or conflicting information in the context. </span><span class="koboSpan" id="kobo.55.3">The model might generate hallucinations despite having the answer in the prompt (</span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">contextual hallucinations).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.57.1">Answer in the wrong format</span></strong><span class="koboSpan" id="kobo.58.1">: There may be additional specifics in the query. </span><span class="koboSpan" id="kobo.58.2">For example, we may want an LLM to generate bullet points or report the information in a table. </span><span class="koboSpan" id="kobo.58.3">The LLM may ignore </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">this information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.60.1">Incorrect specificity</span></strong><span class="koboSpan" id="kobo.61.1">: The generated answer is not specific enough or too specific with respect to the user’s needs. </span><span class="koboSpan" id="kobo.61.2">This is generally a problem associated with how the system is designed and what its purpose is. </span><span class="koboSpan" id="kobo.61.3">Our RAG may be part of a product designed for students and must give clear and comprehensive answers on a topic. </span><span class="koboSpan" id="kobo.61.4">The model, on the other hand, might answer vaguely or too technically for a student. </span><span class="koboSpan" id="kobo.61.5">Typically, this is a problem when the query (or instructions) is not </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">clear enough.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.63.1">Augmentation hurdles or information redundancy</span></strong><span class="koboSpan" id="kobo.64.1">: Our database may contain information from different corpora, and many of the documents may contain redundant information or be in different styles and tones. </span><span class="koboSpan" id="kobo.64.2">The LLM could then generate repetition and/or create hallucinations. </span><span class="koboSpan" id="kobo.64.3">Also, the answer may not be good quality because the model fails to integrate the information from the </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">various chunks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Incomplete answers</span></strong><span class="koboSpan" id="kobo.67.1">: These are answers that are not wrong but are incomplete (this can result from either not finding all the necessary information or errors on the part of the LLM in using the context). </span><span class="koboSpan" id="kobo.67.2">Sometimes, it can also be a problem of the query being too complex (“Summarize items A, B, and C”), and so it might also be better to modify </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">the query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.69.1">Lack of flexibility</span></strong><span class="koboSpan" id="kobo.70.1">: This when the system is not flexible; it does not currently allow efficient updating, and we cannot incorporate feedback from users, past interactions, and so on. </span><span class="koboSpan" id="kobo.70.2">The system does not allow us to handle certain files that are abundant in our corpus (for example, our system does not </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">allow Excel).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">Scalability and overall performance</span></strong><span class="koboSpan" id="kobo.73.1">: In this case, our system may be too slow to conduct an embedding, generate a response, and so on. </span><span class="koboSpan" id="kobo.73.2">Alternatively, we cannot handle embedding multiple documents per second, or we have performance issues that are specific to our product or domain. </span><span class="koboSpan" id="kobo.73.3">System security is a sore point, especially </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.74.1">if we have </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">sensitive data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.76.1">Now that we understand the issues with naïve RAG, let’s understand how advanced RAG helps us tackle </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">these issues.</span></span></p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.78.1">Exploring the advanced RAG pipeline</span></h1>
<p><span class="koboSpan" id="kobo.79.1">Advanced RAG introduces a </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.80.1">number of specific improvements to try to address the issues highlighted in naïve RAG. </span><span class="koboSpan" id="kobo.80.2">Advanced RAG, in other words, modifies the various components of RAG to try to optimize the RAG paradigm. </span><span class="koboSpan" id="kobo.80.3">These various modifications occur at the different steps of RAG: </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">pre-retrieval</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.82.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.83.1">post-retrieval</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">In the </span><strong class="bold"><span class="koboSpan" id="kobo.86.1">pre-retrieval process</span></strong><span class="koboSpan" id="kobo.87.1">, the purpose is to optimize indexing and querying. </span><span class="koboSpan" id="kobo.87.2">For example, </span><strong class="bold"><span class="koboSpan" id="kobo.88.1">adding metadata</span></strong><span class="koboSpan" id="kobo.89.1"> enables more granular searching, and we provide more content to the LLM to generate text. </span><span class="koboSpan" id="kobo.89.2">Metadata can succinctly contain information that would otherwise be dispersed throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">the document.</span></span></p>
<p><span class="koboSpan" id="kobo.91.1">In naïve RAG, we divide the document into different chunks and find the relevant chunks for each document. </span><span class="koboSpan" id="kobo.91.2">This approach has </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">two limitations:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.93.1">When we have many documents, it impacts latency time </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">and performance</span></span></li>
<li><span class="koboSpan" id="kobo.95.1">When the documents are large, we may not be able to easily find the </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">relevant chunks</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.97.1">In naïve RAG, there is only one level (all chunks are equivalent even if they are derived from different documents). </span><span class="koboSpan" id="kobo.97.2">In general, though, for many corpora, there is a hierarchy, and it might be beneficial to </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">use it.</span></span></p>
<p><span class="koboSpan" id="kobo.99.1">To address these limitations, advanced RAG introduces several enhancements designed to improve both retrieval and generation. </span><span class="koboSpan" id="kobo.99.2">In the following subsections, we will explore </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">some techniques</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.101.1">Hierarchical indexing</span></h2>
<p><span class="koboSpan" id="kobo.102.1">For a document consisting of several </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.103.1">chapters, we could first find the chapters of interest and from there search for the various sections of interest. </span><span class="koboSpan" id="kobo.103.2">Since the chapters may </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.104.1">be of considerable size (rich in noise), embedding may not best represent their contextual significance. </span><span class="koboSpan" id="kobo.104.2">The solution is to use summaries and metadata. </span><span class="koboSpan" id="kobo.104.3">In a </span><strong class="bold"><span class="koboSpan" id="kobo.105.1">hierarchical index</span></strong><span class="koboSpan" id="kobo.106.1">, you create summaries at each hierarchical level (which can be considered abstracts). </span><span class="koboSpan" id="kobo.106.2">At the first level, we have summaries that highlight only the key points in large document segments. </span><span class="koboSpan" id="kobo.106.3">In the lower levels, the granularity will increase, and these abstracts will be closer and closer to only the relevant section of data. </span><span class="koboSpan" id="kobo.106.4">Next, we will conduct the embedding of these abstracts. </span><span class="koboSpan" id="kobo.106.5">At inference time, we will calculate the similarity with these summary embeddings. </span><span class="koboSpan" id="kobo.106.6">Of course, this means either we manually write the summaries or we use an LLM to conduct summarization. </span><span class="koboSpan" id="kobo.106.7">Then, using the associated metadata, we can find the chunks that match the summary and provide it to </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<span class="koboSpan" id="kobo.108.1"><img alt="Figure 6.2 – Hierarchical indexing" src="image/B21257_06_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.109.1">Figure 6.2 – Hierarchical indexing</span></p>
<p><span class="koboSpan" id="kobo.110.1">As seen in the preceding figure, the corpus is divided into documents; we then obtain a summary of each document and embed it (in naïve RAG, we were dividing into chunks and embedding the chunks). </span><span class="koboSpan" id="kobo.110.2">In the next step, we embed the summary of a lower hierarchical level of the documents (chapter, sections, heading, and subheadings) until we reach the chunk level. </span><span class="koboSpan" id="kobo.110.3">At inference time, a similarity search is conducted on the summaries to retrieve the chunks we are </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">interested in.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">There are some variations to this approach. </span><span class="koboSpan" id="kobo.112.2">For more control, we can choose a split approach for each file type (HTML, PDF, and GitHub repository). </span><span class="koboSpan" id="kobo.112.3">In this way, we can make the summary data type-specific and embed the summary, which works as a kind of </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">text normalization.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">When we have documents </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.115.1">that are too long for our LLM summarizer, we can use </span><strong class="bold"><span class="koboSpan" id="kobo.116.1">map and reduce</span></strong><span class="koboSpan" id="kobo.117.1">, where we first conduct a summarization of various parts of the document, then collate these summaries and get a single summary. </span><span class="koboSpan" id="kobo.117.2">If the documents are too encyclopedic (i.e., deal with too many topics), there is a risk of semantic noise impacting retrieval. </span><span class="koboSpan" id="kobo.117.3">To solve this, we can have multiple summaries per document (e.g., one summary per 10K tokens or every 10 pages </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">of document).</span></span></p>
<p><span class="koboSpan" id="kobo.119.1">Hierarchical indexing improves the contextual understanding of the document (because it respects its hierarchy and captures the relationships between various sections, such as chapters, headings, and subheadings). </span><span class="koboSpan" id="kobo.119.2">This approach allows greater accuracy in finding the results, and they are </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.120.1">more relevant. </span><span class="koboSpan" id="kobo.120.2">On the other hand, this approach comes at a cost both during the pre-retrieval stage and in inference. </span><span class="koboSpan" id="kobo.120.3">Too many levels and you risk having a combinatorial explosion, that is, a rapid growth in complexity due to the exponential increase in possible combinations, with a huge </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">latency cost.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer157">
<span class="koboSpan" id="kobo.122.1"><img alt="Figure 6.3 – Hierarchical indexing variation" src="image/B21257_06_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.123.1">Figure 6.3 – Hierarchical indexing variation</span></p>
<p><span class="koboSpan" id="kobo.124.1">In the preceding figure, we can see these hierarchical </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">index variations:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.126.1">A</span></em><span class="koboSpan" id="kobo.127.1">: Different handling for each document type to better represent </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">their structure</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.129.1">B</span></em><span class="koboSpan" id="kobo.130.1">: Map and reduce to handle too-long documents (intermediate summaries are created and then used to create the final </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">document summary)</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.132.1">C</span></em><span class="koboSpan" id="kobo.133.1">: Multi-summary</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.134.1"> for each document when documents are </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.135.1">discussing too </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">many topics</span></span></li>
</ul>
<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.137.1">Hypothetical questions and HyDE</span></h2>
<p><span class="koboSpan" id="kobo.138.1">Another modification of the </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.139.1">naïve RAG pipeline is to try to make chunks and possible questions more semantically similar. </span><span class="koboSpan" id="kobo.139.2">By having an idea of who our users are, we can imagine the kind of use they will get out of our system (for a chatbot, most queries will be questions, so we can tailor the system toward these kinds of queries). </span><strong class="bold"><span class="koboSpan" id="kobo.140.1">Hypothetical questions</span></strong><span class="koboSpan" id="kobo.141.1"> is a type of strategy in which we</span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.142.1"> use an LLM to generate one (or more) hypothetical question(s) for each chunk. </span><span class="koboSpan" id="kobo.142.2">These hypothetical questions are then transformed into vectors (embedding), and these vectors are used to do a similarity search when there is a query from a user. </span><span class="koboSpan" id="kobo.142.3">Of course, once we have identified the hypothetical questions most similar to our real query, we find the chunks (thanks to the metadata) and provide them to the model. </span><span class="koboSpan" id="kobo.142.4">We can generate either a single query or multiple queries for each chunk (this increases the accuracy as well as the computational cost). </span><span class="koboSpan" id="kobo.142.5">In this case, we are not using the vector representation of chunks (we do not conduct embeddings of chunks but hypothetical questions). </span><span class="koboSpan" id="kobo.142.6">Also, we do not necessarily have to save the hypothetical questions, just their vectors (the important thing is that we can map them back to </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">the chunks).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.144.1">Hypothetical Document Embeddings</span></strong><span class="koboSpan" id="kobo.145.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.146.1">HyDE</span></strong><span class="koboSpan" id="kobo.147.1">) instead tries to convert the user answers to better match the chunks. </span><span class="koboSpan" id="kobo.147.2">Given a </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.148.1">query, we </span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.149.1">create hypothetical answers to it. </span><span class="koboSpan" id="kobo.149.2">After that, we conduct embeddings of these generated answers and carry out a similarity search to find the chunks of interest. </span><span class="koboSpan" id="kobo.149.3">These generated answers should be most semantically similar to the user’s query, allowing us to be able to find better chunks. </span><span class="koboSpan" id="kobo.149.4">In some variants, we create five different generated answers and conduct the average of their embedding vectors before conducting the similarity search. </span><span class="koboSpan" id="kobo.149.5">This approach can help when we have a low recall metric in the retrieval step or when the documents (or queries) come from a specific domain that is different from the retrieval domain. </span><span class="koboSpan" id="kobo.149.6">In fact, embedding models generalize poorly to knowledge domains that they have not seen. </span><span class="koboSpan" id="kobo.149.7">An interesting little note is that when an LLM generates these hypothetical answers, it does not know the exact answer (that is not even the purpose of the approach) but is able to capture relevant patterns in the question. </span><span class="koboSpan" id="kobo.149.8">We can then use these captured patterns to retrieve </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">the chunks.</span></span></p>
<p><span class="koboSpan" id="kobo.151.1">Let’s look in detail at the difference between the two approaches. </span><span class="koboSpan" id="kobo.151.2">With the hypothetical questions approach, we generate hypothetical questions and use the embedding of these hypothetical </span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.152.1">questions to then find the chunks of interest. </span><span class="koboSpan" id="kobo.152.2">With HyDE, we generate </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.153.1">hypothetical answers to our query and then use the embedding of these answers to find the chunks </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">of</span></span><span class="No-Break"><a id="_idIndexMarker599"/></span><span class="No-Break"><span class="koboSpan" id="kobo.155.1"> interest.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer158">
<span class="koboSpan" id="kobo.156.1"><img alt="Figure 6.4 – Hypothetical questions and HyDE approaches" src="image/B21257_06_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.157.1">Figure 6.4 – Hypothetical questions and HyDE approaches</span></p>
<p><span class="koboSpan" id="kobo.158.1">We can then look in detail at the</span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.159.1"> differences between the two approaches, imagining that we have a hypothetical user question (“What are the potential side effects of </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">using acetaminophen?”):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.161.1">Pre-retrieval phase</span></strong><span class="koboSpan" id="kobo.162.1">: During this phase, we have to create our drug embedding database. </span><span class="koboSpan" id="kobo.162.2">We reduce our documents (sections of a drug’s safety report) into chunks. </span><span class="koboSpan" id="kobo.162.3">In the hypothetical questions approach, for each chunk, hypothetical questions are generated using an LLM (for example, “What are the side effects of this drug?” </span><span class="koboSpan" id="kobo.162.4">or “Are there any adverse reactions mentioned?”). </span><span class="koboSpan" id="kobo.162.5">Each of these hypothetical questions is then embedded into a vector space (a database of vectors for these questions). </span><span class="koboSpan" id="kobo.162.6">At this stage, HyDE is equal to classic RAG; no variation </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">is conducted.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.164.1">Query phase</span></strong><span class="koboSpan" id="kobo.165.1">: In the hypothetical questions approach, when a user submits the query, it is embedded and matched against the embedded hypothetical questions. </span><span class="koboSpan" id="kobo.165.2">The system looks for the hypothetical questions that are most similar to the user’s question (in this case, it might be, “What are the side effects of this drug?”). </span><span class="koboSpan" id="kobo.165.3">At this point, the chunks from which these hypothetical questions were generated are identified (we use metadata). </span><span class="koboSpan" id="kobo.165.4">These chunks are provided in context for generation. </span><span class="koboSpan" id="kobo.165.5">In HyDE, when the user query arrives, an LLM generates hypothetical answers (for example, “Paracetamol may cause side effects such as nausea, liver damage, and rashes” or “Potential adverse reactions include dizziness and </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">gastrointestinal discomfort”).</span></span><p class="list-inset"><span class="koboSpan" id="kobo.167.1">Note that these answers are generated using LLM knowledge without retrieval. </span><span class="koboSpan" id="kobo.167.2">At this point, we conduct the embedding of these hypothetical answers (we use an embedding model), then conduct the embedding of the query and try to match it with the embedded hypothetical answers. </span><span class="koboSpan" id="kobo.167.3">For example, “Paracetamol may cause side effects such as nausea, liver damage, and rashes” is the one closest to the user query. </span><span class="koboSpan" id="kobo.167.4">We then search for the chunks closest to these hypothetical answers and provide the LLM to generate </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">the context.</span></span></p></li>
</ul>
<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.169.1">Context enrichment</span></h2>
<p><span class="koboSpan" id="kobo.170.1">Another </span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.171.1">technique is </span><strong class="bold"><span class="koboSpan" id="kobo.172.1">context enrichment</span></strong><span class="koboSpan" id="kobo.173.1">, in </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.174.1">which we find smaller chunks (greater granularity for better search quality) and then add surrounding context. </span><strong class="bold"><span class="koboSpan" id="kobo.175.1">Sentence window retrieval</span></strong><span class="koboSpan" id="kobo.176.1"> is one such technique in which each sentence in a document is </span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.177.1">embedded separately (the embedded textual unit is smaller and therefore more granular). </span><span class="koboSpan" id="kobo.177.2">This allows us to have higher precision in finding answers, though we risk losing context for LLM reasoning (and thus worse generation). </span><span class="koboSpan" id="kobo.177.3">To solve this, we expand our context window. </span><span class="koboSpan" id="kobo.177.4">Having found a sentence </span><em class="italic"><span class="koboSpan" id="kobo.178.1">x</span></em><span class="koboSpan" id="kobo.179.1">, we take </span><em class="italic"><span class="koboSpan" id="kobo.180.1">k</span></em><span class="koboSpan" id="kobo.181.1"> sentences that surround it in the document (sentences that are before and after our sentence </span><em class="italic"><span class="koboSpan" id="kobo.182.1">x</span></em><span class="koboSpan" id="kobo.183.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">the document).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.185.1">Parent document retriever</span></strong><span class="koboSpan" id="kobo.186.1"> is a</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.187.1"> similar technique that tries to find a balance between searching on small chunks and providing context with larger chunks. </span><span class="koboSpan" id="kobo.187.2">The documents are divided into small child chunks, but we preserve the hierarchy of their parent documents. </span><span class="koboSpan" id="kobo.187.3">In this case, we conduct embedding of small chunks that directly address the specifics of a query (ensuring larger chunks’ relevance). </span><span class="koboSpan" id="kobo.187.4">But then we find the larger parent documents (to which the found chunks belong) and provide them to the LLM for generation (more contextual information and depth). </span><span class="koboSpan" id="kobo.187.5">To avoid retrieving too many parent documents, once the top </span><em class="italic"><span class="koboSpan" id="kobo.188.1">k</span></em><span class="koboSpan" id="kobo.189.1"> chunks are found, if more than </span><em class="italic"><span class="koboSpan" id="kobo.190.1">n</span></em><span class="koboSpan" id="kobo.191.1"> chunks belong to a parent document, we add this document to the </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">LLM context.</span></span></p>
<p><span class="koboSpan" id="kobo.193.1">These approaches are </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.194.1">depicted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">following </span></span><span class="No-Break"><a id="_idIndexMarker606"/></span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">figure:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.197.1">Once a chunk is found, we expand the selection with the previous and </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">next chunks.</span></span></li>
<li><span class="koboSpan" id="kobo.199.1">We conduct embedding of small chunks and find the top </span><em class="italic"><span class="koboSpan" id="kobo.200.1">k</span></em><span class="koboSpan" id="kobo.201.1"> chunks; if most chunks (greater than a parameter </span><em class="italic"><span class="koboSpan" id="kobo.202.1">n</span></em><span class="koboSpan" id="kobo.203.1">) are derived from a document, we provide the LLM with the document as </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">the context.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer159">
<span class="koboSpan" id="kobo.205.1"><img alt="Figure 6.5 – Context enrichment approaches" src="image/B21257_06_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.206.1">Figure 6.5 – Context enrichment approaches</span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.207.1">Query transformation</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.208.1">Query transformation</span></strong><span class="koboSpan" id="kobo.209.1"> is a</span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.210.1"> family of techniques that leverages an LLM to improve</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.211.1"> retrieval. </span><span class="koboSpan" id="kobo.211.2">If a query is too complex, it can be decomposed into a series of queries. </span><span class="koboSpan" id="kobo.211.3">In fact, we may not find a chunk that responds to the query, but more easily find chunks that respond to each subquery (e.g., “Who was the inventor of the telegraph and the telephone?” </span><span class="koboSpan" id="kobo.211.4">is best broken down into two independent queries). </span><strong class="bold"><span class="koboSpan" id="kobo.212.1">Step-back prompting</span></strong><span class="koboSpan" id="kobo.213.1"> uses an </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.214.1">LLM to generate a more general query that can match a high-level context. </span><span class="koboSpan" id="kobo.214.2">It stems from the idea that when a human being is faced with a difficult task, they take a step back and do abstractions to get to the high-level principles. </span><span class="koboSpan" id="kobo.214.3">In this case, we use the embedding of this high-level query and the user’s query, and both found contexts are provided to the LLM for generation. </span><strong class="bold"><span class="koboSpan" id="kobo.215.1">Query rewriting</span></strong><span class="koboSpan" id="kobo.216.1">, on</span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.217.1"> the other hand, reformulates the initial query with an LLM to make </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">retrieval easier.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<span class="koboSpan" id="kobo.219.1"><img alt="Figure 6.6 – Three examples of query transformation" src="image/B21257_06_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.220.1">Figure 6.6 – Three examples of query transformation</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.221.1">Query expansion</span></strong><span class="koboSpan" id="kobo.222.1"> is a </span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.223.1">technique similar to query rewriting. </span><span class="koboSpan" id="kobo.223.2">Underlying it is the idea that adding terms to the query can allow it to find relevant documents that do not have lexical overlap with the query (and thus improve retrieval recall). </span><span class="koboSpan" id="kobo.223.3">Again, we use an LLM to modify the query. </span><span class="koboSpan" id="kobo.223.4">There are two </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">main possibilities:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.225.1">Ask an LLM to generate an answer to the query, after which the generated answer and the query are embedded and used </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">for retrieval.</span></span></li>
<li><span class="koboSpan" id="kobo.227.1">Generate several queries similar to the original query (usually a prefixed number </span><em class="italic"><span class="koboSpan" id="kobo.228.1">n</span></em><span class="koboSpan" id="kobo.229.1">). </span><span class="koboSpan" id="kobo.229.2">This </span><em class="italic"><span class="koboSpan" id="kobo.230.1">n</span></em><span class="koboSpan" id="kobo.231.1"> set of queries is then vectorized and used </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">for search.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.233.1">This</span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.234.1"> approach usually improves retrieval because it helps disambiguate the</span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.235.1"> query and find documents that otherwise would not be found; it also helps the system better compile the query. </span><span class="koboSpan" id="kobo.235.2">On the other hand, though, it also leads to finding irrelevant documents, so it pays to combine it with post-processing techniques for </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">finding documents.</span></span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.237.1">Keyword-based search and hybrid search</span></h2>
<p><span class="koboSpan" id="kobo.238.1">Another way to improve </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.239.1">search is to focus not only on contextual information but also on keywords. </span><strong class="bold"><span class="koboSpan" id="kobo.240.1">Keyword-based search</span></strong><span class="koboSpan" id="kobo.241.1"> is a </span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.242.1">search by an exact match of certain keywords. </span><span class="koboSpan" id="kobo.242.2">This type of search is beneficial for specific terms (such as product or company names or specific industry jargon). </span><span class="koboSpan" id="kobo.242.3">However, it is sensitive to typos and synonyms and does not capture context. </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">Vector or semantic search</span></strong><span class="koboSpan" id="kobo.244.1">, on the contrary, finds the semantic meaning of a</span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.245.1"> query but does</span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.246.1"> not find exact terms or keywords (which is sometimes essential for some queries, especially in some domains such as marketing). </span><strong class="bold"><span class="koboSpan" id="kobo.247.1">Hybrid search</span></strong><span class="koboSpan" id="kobo.248.1"> takes</span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.249.1"> the best of both </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.250.1">worlds by combining a model for keyword search and </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">vectorial search.</span></span></p>
<p><span class="koboSpan" id="kobo.252.1">The most </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.253.1">commonly used model for keyword search is </span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.254.1">BM25 (which we discussed in the previous chapter), which generates sparse</span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.255.1"> embeddings. </span><span class="koboSpan" id="kobo.255.2">BM25 then allows us to identify documents that contain specific terms in the query. </span><span class="koboSpan" id="kobo.255.3">So, we create two embeddings: a sparse embedding with BM25 and a dense embedding with a transformer. </span><span class="koboSpan" id="kobo.255.4">To select the best chunks, you generally try to balance the impact of your different types of searches. </span><span class="koboSpan" id="kobo.255.5">The final score is a weighted combination (you use an alpha hyperparameter) of the </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">two scores:</span></span></p>
<p class="Basic-Paragraph"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>h</mi><mi>y</mi><mi>b</mi><mi>r</mi><mi>i</mi><mi>d</mi></mrow></msub><mo>=</mo><mfenced close=")" open="("><mrow><mn>1</mn><mo>−</mo><mi mathvariant="normal">α</mi></mrow></mfenced><mo>∙</mo><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi></mrow></msub><mo>+</mo><mi mathvariant="normal">α</mi><mo>∙</mo><msub><mrow><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>d</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>e</mi></mrow></msub></mrow></mrow></math></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.257.1">α</span></em><span class="koboSpan" id="kobo.258.1"> has a value between 0 and 1 (0 means pure vectorial search, while 1 means only keyword search). </span><span class="koboSpan" id="kobo.258.2">Typically, the value of </span><em class="italic"><span class="koboSpan" id="kobo.259.1">α</span></em><span class="koboSpan" id="kobo.260.1"> is 0.4 or 0.5 (other articles even </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">suggest 0.3).</span></span></p>
<p><span class="koboSpan" id="kobo.262.1">As a practical example, we can imagine an e-commerce platform with a vast product catalog containing millions of items across categories such as electronics, fashion, and home appliances. </span><span class="koboSpan" id="kobo.262.2">Users search for products with different types of queries, which may include </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.264.1">Specific terms such as a brand or product name (e.g., “</span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">iPhone 16”)</span></span></li>
<li><span class="koboSpan" id="kobo.266.1">A general description (e.g., “Medium-price phone with a </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">good camera”)</span></span></li>
<li><span class="koboSpan" id="kobo.268.1">Queries that contain mixed elements (e.g., “iPhone with cost less </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">than $500”)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.270.1">A pure keyword-based search ( such as the BM25 algorithm) would struggle with vague or purely descriptive descriptions, while a vector-based semantic search might miss exact matches for a product. </span><span class="koboSpan" id="kobo.270.2">Hybrid search combines the best of both. </span><span class="koboSpan" id="kobo.270.3">BM25 prioritizes exact matches, such as matches of “iPhone,” allowing us to find specific items using keywords. </span><span class="koboSpan" id="kobo.270.4">Semantic search allows us to capture the semantic meaning of phrases such as “phone with a good camera.” </span><span class="koboSpan" id="kobo.270.5">Hybrid search is a great solution for all three of the previously </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">mentioned cases.</span></span></p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.272.1">Query routing</span></h2>
<p><span class="koboSpan" id="kobo.273.1">So far, we have assumed that once a</span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.274.1"> query arrives, it is used for a search within the vector database. </span><span class="koboSpan" id="kobo.274.2">In</span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.275.1"> reality, we may want to conduct the search differently or control the flow within the system. </span><span class="koboSpan" id="kobo.275.2">For example, the system should be able to interact with different types of databases (vector, SQL, and proprietary databases), different sources, or different types of modalities (image, text, and sound). </span><span class="koboSpan" id="kobo.275.3">Some queries do not, then, need to be searched with RAG; the parametric memory of the model might suffice (we will discuss this in more depth in the </span><em class="italic"><span class="koboSpan" id="kobo.276.1">Open questions and future perspectives</span></em><span class="koboSpan" id="kobo.277.1"> section). </span><span class="koboSpan" id="kobo.277.2">Query routing thus allows control over how the system should respond to the query. </span><span class="koboSpan" id="kobo.277.3">You can imagine it as being a series of if/else causes, though instead of being hardcoded, we have a router (usually an LLM) that makes a decision whenever a query arrives. </span><span class="koboSpan" id="kobo.277.4">Obviously, this means that we have a nondeterministic system, and it will not always make the right decision, although it can have a major positive impact </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">on performance.</span></span></p>
<p><span class="koboSpan" id="kobo.279.1">The router can be a set of logical rules or a neural model. </span><span class="koboSpan" id="kobo.279.2">Some options for a router are </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.281.1">Logical routers</span></strong><span class="koboSpan" id="kobo.282.1">: A set of </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.283.1">logical rules that can be if/else clauses (e.g., if the query is an image, it searches the image database; otherwise, it searches the text database). </span><span class="koboSpan" id="kobo.283.2">Logical routers don’t understand the query, but they are very fast </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">and deterministic.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.285.1">Keyword routers</span></strong><span class="koboSpan" id="kobo.286.1">: A slightly</span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.287.1"> more sophisticated alternative in which we try to select a route by matching keywords between the query and a list of options. </span><span class="koboSpan" id="kobo.287.2">This search can be done with a sparse encoder, a specialized package, or even </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">an LLM.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.289.1">Zero-shot classification router</span></strong><span class="koboSpan" id="kobo.290.1">: Zero-shot classification is a task in which an LLM is asked to classify an item</span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.291.1"> with a set of labels without being specified and trained for it. </span><span class="koboSpan" id="kobo.291.2">Each query is given to an LLM that must assign a route label from those in </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">a list.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.293.1">LLM function calling router</span></strong><span class="koboSpan" id="kobo.294.1">: The</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.295.1"> different routes are described as functions (with a specific description) and the model must decide where to direct the queries by selecting the function (in this approach, we leverage its </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">decision-making ability).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.297.1">Semantic router</span></strong><span class="koboSpan" id="kobo.298.1">: In this </span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.299.1">approach, we use a semantic search to decide on the best route. </span><span class="koboSpan" id="kobo.299.2">In short, we have a list of example queries and the associated routes. </span><span class="koboSpan" id="kobo.299.3">These are then embedded and saved as vectors in a database. </span><span class="koboSpan" id="kobo.299.4">When a query arrives, we conduct a similarity search with the other queries in our database. </span><span class="koboSpan" id="kobo.299.5">We then select the option associated with the query with the best </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">similarity match.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer161">
<span class="koboSpan" id="kobo.301.1"><img alt="Figure 6.7 – Query routing" src="image/B21257_06_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.302.1">Figure 6.7 – Query routing</span></p>
<p><span class="koboSpan" id="kobo.303.1">Once we have found the context, we need to integrate it with the query and provide it to the LLM for generation. </span><span class="koboSpan" id="kobo.303.2">There are several strategies to improve this process, usually called </span><strong class="bold"><span class="koboSpan" id="kobo.304.1">post-retrieval strategies</span></strong><span class="koboSpan" id="kobo.305.1">. </span><span class="koboSpan" id="kobo.305.2">After the </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.306.1">vector search, retrieval returns the top </span><em class="italic"><span class="koboSpan" id="kobo.307.1">k</span></em><span class="koboSpan" id="kobo.308.1"> documents (an arbitrary cutoff that is determined in advance). </span><span class="koboSpan" id="kobo.308.2">This can lead to the loss of relevant information. </span><span class="koboSpan" id="kobo.308.3">The simplest solution is to increase the value of the top </span><em class="italic"><span class="koboSpan" id="kobo.309.1">k</span></em><span class="koboSpan" id="kobo.310.1"> chunks. </span><span class="koboSpan" id="kobo.310.2">Obviously, we cannot return all retrieved chunks, both because they</span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.311.1"> would not fit into the context length of the model and because the </span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.312.1">LLM would then have problems with handling all this information (efficient use of a long </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">context length).</span></span></p>
<p><span class="koboSpan" id="kobo.314.1">We can imagine a company offering different services across different domains, such as banking, insurance, and finance. </span><span class="koboSpan" id="kobo.314.2">Customers interact with a chatbot to seek assistance with banking services (account details, transactions, and so on), insurance services (policy details, claims, etc.), and financial services (suggestions, investments, etc.). </span><span class="koboSpan" id="kobo.314.3">Each domain is different. </span><span class="koboSpan" id="kobo.314.4">Due to regulations and privacy issues, we want to prevent a chatbot from searching for details for a customer of another service. </span><span class="koboSpan" id="kobo.314.5">Also, searching all databases for every query is inefficient and leads to more latency and </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">irrelevant results.</span></span></p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.316.1">Reranking</span></h2>
<p><span class="koboSpan" id="kobo.317.1">One proposed solution to this dilemma </span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.318.1">is to maximize document retrieval (increase the top </span><em class="italic"><span class="koboSpan" id="kobo.319.1">k</span></em><span class="koboSpan" id="kobo.320.1"> retrieved results and thus increase the retrieval recall metric) but at the same time maximize the LLM recall (by minimizing the number of documents supplied to the LLM). </span><span class="koboSpan" id="kobo.320.2">This strategy is called </span><strong class="bold"><span class="koboSpan" id="kobo.321.1">reranking</span></strong><span class="koboSpan" id="kobo.322.1">. </span><span class="koboSpan" id="kobo.322.2">Reranking </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.323.1">consists of </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">two steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.325.1">First, we conduct a classical retrieval and find a large number </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">of chunks.</span></span></li>
<li><span class="koboSpan" id="kobo.327.1">Next, we use a reranker (a second model) to reorder the chunks and then select the top </span><em class="italic"><span class="koboSpan" id="kobo.328.1">k</span></em><span class="koboSpan" id="kobo.329.1"> chunks to provide to </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">the LLM.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.331.1">The reranker improves the quality of chunks returned to the LLM and reduces hallucinations in the system. </span><span class="koboSpan" id="kobo.331.2">In addition, reranking considers contrasting information (related to the query) and then considers chunks in context with the query. </span><span class="koboSpan" id="kobo.331.3">There are several types of rerankers, each with its own limitations </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">and advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.333.1">Cross-encoders</span></strong><span class="koboSpan" id="kobo.334.1">: These are </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.335.1">transformers (such as BGE) that take two textual sequences (the query and the various chunks one at a time) as input and return the similarity between 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">and 1.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.337.1">Multi-vector rerankers</span></strong><span class="koboSpan" id="kobo.338.1">: These are</span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.339.1"> still transformers (such as ColBERT) and require less computation than cross-encoders (the interaction between the two sequences is late-stage). </span><span class="koboSpan" id="kobo.339.2">The principle is similar; given two sequences, they return a similarity between 0 and 1. </span><span class="koboSpan" id="kobo.339.3">There are improved versions with a large context length, such </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">as jina-colbert-v1-en.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.341.1">LLMs for reranking</span></strong><span class="koboSpan" id="kobo.342.1">: LLMs can also </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.343.1">be used as rerankers. </span><span class="koboSpan" id="kobo.343.2">Several strategies are used to improve the ranking capabilities of </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">an LLM:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.345.1">Pointwise methods</span></strong><span class="koboSpan" id="kobo.346.1"> are used </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.347.1">to calculate the relevance of a query and a single document (also referred to as zero-shot </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">document reranking).</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.349.1">Pairwise methods</span></strong><span class="koboSpan" id="kobo.350.1"> consist </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.351.1">of providing an LLM with both the query and two documents and asking it to choose which one is </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">more relevant.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.353.1">Listwise methods</span></strong><span class="koboSpan" id="kobo.354.1">, on the</span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.355.1"> other hand, propose to provide a query and a list of documents to the LLM and instruct it to produce as output a ranked list. </span><span class="koboSpan" id="kobo.355.2">Models such as GPT are usually used, with the risk of high computational or </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">economic costs.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.357.1">Fine-tuned LLMs</span></strong><span class="koboSpan" id="kobo.358.1">: This is a </span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.359.1">class of models that is specifically for ranking tasks. </span><span class="koboSpan" id="kobo.359.2">Although LLMs are generalist models, they do not have specific training for ranking and therefore cannot accurately measure query-document relevance. </span><span class="koboSpan" id="kobo.359.3">Fine-tuning allows them to improve their capability. </span><span class="koboSpan" id="kobo.359.4">Generally, there are two types of models used: encoder-decoder transformers (RankT5) or decoder-only transformers (e.g., derivatives of Llama </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">and GPT).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.361.1">All these </span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.362.1">approaches have an impact on both performance (retrieval quality) and </span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.363.1">cost (computational cost, system latency, and potential system cost). </span><span class="koboSpan" id="kobo.363.2">Generally, multi-vectors are those with lower computational cost and discrete performance. </span><span class="koboSpan" id="kobo.363.3">LLM-based methods may have the best performance but have high computational costs. </span><span class="koboSpan" id="kobo.363.4">In general, reranking has a positive impact on the system, which is why it is often a component of </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">the pipeline.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer162">
<span class="koboSpan" id="kobo.365.1"><img alt="Figure 6.8 – Reranking approach. Chunks highlighted in red are the chunks relevant to the query" src="image/B21257_06_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.366.1">Figure 6.8 – Reranking approach. </span><span class="koboSpan" id="kobo.366.2">Chunks highlighted in red are the chunks relevant to the query</span></p>
<p><span class="koboSpan" id="kobo.367.1">Alternatively, there are </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.368.1">other </span><strong class="bold"><span class="koboSpan" id="kobo.369.1">post-processing techniques</span></strong><span class="koboSpan" id="kobo.370.1">. </span><span class="koboSpan" id="kobo.370.2">For example, it is possible to filter out chunks if the similarity achieved is below a certain score threshold, if they do not include certain keywords, if a certain value is not present in the metadata associated with the chunks, if the chunks are older than a certain date, and many other possibilities. </span><span class="koboSpan" id="kobo.370.3">An additional strategy is that once we have found chunks, starting from the </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.371.1">embedding vectors, we conduct </span><strong class="bold"><span class="koboSpan" id="kobo.372.1">k-nearest neighbors</span></strong><span class="koboSpan" id="kobo.373.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.374.1">kNN</span></strong><span class="koboSpan" id="kobo.375.1">) research. </span><span class="koboSpan" id="kobo.375.2">In other words, we add other chunks that are neighbors in the latent space of those found (this strategy can be done before or </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">after reranking).</span></span></p>
<p><span class="koboSpan" id="kobo.377.1">In addition, once the chunks are selected to be provided in context to the LLM, we can alter their order. </span><span class="koboSpan" id="kobo.377.2">As shown in the following figure, a study published in 2023 shows that the best performance for an LLM is when the important information is placed at the beginning or end of the input context length (performance drops if the information is in the middle of the context length, especially if it is </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">very long):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<span class="koboSpan" id="kobo.379.1"><img alt="Figure 6.9 – Changing the location of relevant information impacts the performance of an LLM (https://arxiv.org/abs/2307.03172)" src="image/B21257_06_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.380.1">Figure 6.9 – Changing the location of relevant information impacts the performance of an LLM (</span><a href="https://arxiv.org/abs/2307.03172"><span class="koboSpan" id="kobo.381.1">https://arxiv.org/abs/2307.03172</span></a><span class="koboSpan" id="kobo.382.1">)</span></p>
<p><span class="koboSpan" id="kobo.383.1">That is why </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.384.1">it has been proposed to </span><strong class="bold"><span class="koboSpan" id="kobo.385.1">reorder the chunks</span></strong><span class="koboSpan" id="kobo.386.1">. </span><span class="koboSpan" id="kobo.386.2">They can be placed in </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.387.1">order of relevance, but also in alternating patterns (chunks with an even index are placed at the beginning of the list and chunks with an odd index at the end). </span><span class="koboSpan" id="kobo.387.2">The alternating pattern is used especially when using wide top </span><em class="italic"><span class="koboSpan" id="kobo.388.1">k</span></em><span class="koboSpan" id="kobo.389.1"> chunks, so the most relevant chunks are placed at the beginning and end (while the less relevant ones are in the middle of the </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">context length).</span></span></p>
<p><span class="koboSpan" id="kobo.391.1">You can notice that reranking improves the performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">the system:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<span class="koboSpan" id="kobo.393.1"><img alt="Figure 6.10 – Reranking improves the performance in question-answering (https://arxiv.org/pdf/2409.07691)" src="image/B21257_06_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.394.1">Figure 6.10 – Reranking improves the performance in question-answering (</span><a href="https://arxiv.org/pdf/2409.07691"><span class="koboSpan" id="kobo.395.1">https://arxiv.org/pdf/2409.07691</span></a><span class="koboSpan" id="kobo.396.1">)</span></p>
<p><span class="koboSpan" id="kobo.397.1">In addition to reranking, several complementary techniques can be applied after the retrieval stage to further refine the information passed to the LLM. </span><span class="koboSpan" id="kobo.397.2">These include methods for improving citation accuracy, managing chat history, compressing context, and optimizing prompt formulation. </span><span class="koboSpan" id="kobo.397.3">Let's have a look at some </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">of them.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.399.1">Reference citations</span></strong><span class="koboSpan" id="kobo.400.1"> is not</span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.401.1"> really a technique for system improvement, but it is highly recommended as a component of a RAG system. </span><span class="koboSpan" id="kobo.401.2">Especially if we are using</span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.402.1"> different sources to compose our query response, it is good to keep track </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.403.1">of which sources were used (e.g., the documents that the LLM used). </span><span class="koboSpan" id="kobo.403.2">We can simply safeguard the sources that were used for generation (which documents the chunks correspond to). </span><span class="koboSpan" id="kobo.403.3">Another possibility is to mention in the prompt for the LLM the sources used. </span><span class="koboSpan" id="kobo.403.4">A more sophisticated technique is fuzzy citation query engine. </span><span class="koboSpan" id="kobo.403.5">Fuzzy matching is a string search to match the generated response to the found chunks (a technique that is based on dividing the words in the chunk into n-grams and then conducting </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">a TF-IDF).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.405.1">ChatEngine</span></strong><span class="koboSpan" id="kobo.406.1"> is another </span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.407.1">extension of RAG. </span><span class="koboSpan" id="kobo.407.2">Conducting fine-tuning of the model is complex, but at the same time, we want the LLM to remember previous interactions with the user. </span><span class="koboSpan" id="kobo.407.3">RAG makes it easy to do this, so we can save previous dialogues with users. </span><span class="koboSpan" id="kobo.407.4">A simple technique is to include the previous chat in the prompt. </span><span class="koboSpan" id="kobo.407.5">Alternatively, we can conduct embedding of the chats and find the highlights. </span><span class="koboSpan" id="kobo.407.6">Another technique is to try to capture the context of the user dialogue (chat logic). </span><span class="koboSpan" id="kobo.407.7">Since the discussion can wind through several messages, one solution to avoid a prompt that may exceed the context </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.408.1">length is </span><strong class="bold"><span class="koboSpan" id="kobo.409.1">prompt compression</span></strong><span class="koboSpan" id="kobo.410.1">. </span><span class="koboSpan" id="kobo.410.2">We reduce the prompt length by reducing the previous interaction with </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">the user.</span></span></p>
<p><span class="koboSpan" id="kobo.412.1">In general, </span><strong class="bold"><span class="koboSpan" id="kobo.413.1">contextual compression</span></strong><span class="koboSpan" id="kobo.414.1"> is a </span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.415.1">concept that helps the LLM during generation. </span><span class="koboSpan" id="kobo.415.2">It also saves computational (or economic, if using a model via an API) resources. </span><span class="koboSpan" id="kobo.415.3">Once the documents are found, we can compress the context,  with the aim of retaining only the relevant information. </span><span class="koboSpan" id="kobo.415.4">In fact, the context often also contains information irrelevant to the query, or even repetitions. </span><span class="koboSpan" id="kobo.415.5">Additionally, most of the words in a sentence could be predicted directly from the context and are not needed to provide the information to the LLM during generation. </span><span class="koboSpan" id="kobo.415.6">There are several strategies to reduce the prompt provided to </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">the LLM:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.417.1">Context filtering</span></strong><span class="koboSpan" id="kobo.418.1">: In</span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.419.1"> information theory, tokens with low entropy are easily predictable and thus contain redundant information (provide less relevant information to the LLM and have little impact on its understanding of the context). </span><span class="koboSpan" id="kobo.419.2">We therefore use an LLM that assigns an information value to each lexical unit (how much it expects to see that token or sentence in context). </span><span class="koboSpan" id="kobo.419.3">We conduct a ranking in descending order and keep only those tokens that are in the first p-th percentile (we decide this </span><em class="italic"><span class="koboSpan" id="kobo.420.1">a priori</span></em><span class="koboSpan" id="kobo.421.1">, or it can </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">be context-dependent).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.423.1">LongLLMLingua</span></strong><span class="koboSpan" id="kobo.424.1">: This is </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.425.1">another approach based on information entropy and using information from both context and query (question aware). </span><span class="koboSpan" id="kobo.425.2">The approach conducts dynamic compression and reordering of documents to make generation </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">more efficient.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.427.1">Autocompressors</span></strong><span class="koboSpan" id="kobo.428.1">: This uses a</span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.429.1"> kind of fine-tuning of the system and summary vectors. </span><span class="koboSpan" id="kobo.429.2">The idea behind it is that a long text can be summarized in a small vector representation (summary vectors). </span><span class="koboSpan" id="kobo.429.3">These vectors can be used as soft prompts to give context to the model. </span><span class="koboSpan" id="kobo.429.4">The process relies on keeping the LLM’s weights frozen while introducing trainable tokens into the prompt. </span><span class="koboSpan" id="kobo.429.5">These tokens are learned during training, enabling the system to be optimized end-to-end without modifying the model’s core parameters. </span><span class="koboSpan" id="kobo.429.6">During generation, these vectors are joined, and the model is then context-aware. </span><span class="koboSpan" id="kobo.429.7">Already trained models exist, </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">as follows:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer165">
<span class="koboSpan" id="kobo.431.1"><img alt="Figure 6.11 – A) Context compression and filtering. B) Autocompressor. (Adapted from https://arxiv.org/abs/2305.14788)" src="image/B21257_06_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.432.1">Figure 6.11 – A) Context compression and filtering. </span><span class="koboSpan" id="kobo.432.2">B) Autocompressor. </span><span class="koboSpan" id="kobo.432.3">(Adapted from </span><a href="https://arxiv.org/abs/2305.14788"><span class="koboSpan" id="kobo.433.1">https://arxiv.org/abs/2305.14788</span></a><span class="koboSpan" id="kobo.434.1">)</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.435.1">Prompt engineering</span></strong><span class="koboSpan" id="kobo.436.1"> is another </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.437.1">solution to improve generation. </span><span class="koboSpan" id="kobo.437.2">Some suggestions are common to any interaction with an LLM. </span><span class="koboSpan" id="kobo.437.3">Thus, principles such as providing clear (“Reply using the context”) and unambiguous (“If the answer is not in the context, write I do not know”) instructions apply to RAG. </span><span class="koboSpan" id="kobo.437.4">There may, however, be specific directions or even examples for designing the best possible prompt for our system. </span><span class="koboSpan" id="kobo.437.5">Other instructions may be specific to how we want the output (for example, as a list, in HTML, and so on). </span><span class="koboSpan" id="kobo.437.6">There are also libraries for creating prompts for RAG that follow a </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">specific format.</span></span></p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.439.1">Response optimization</span></h2>
<p><span class="koboSpan" id="kobo.440.1">The last step in a pipeline before </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.441.1">conducting the final response is to</span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.442.1"> improve the response from the user. </span><span class="koboSpan" id="kobo.442.2">One strategy is that of </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.443.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.444.1">response synthesizer</span></strong><span class="koboSpan" id="kobo.445.1">. </span><span class="koboSpan" id="kobo.445.2">The basic strategy is to concatenate the prompt, context, and query and provide it to the LLM for generation. </span><span class="koboSpan" id="kobo.445.3">More sophisticated strategies involve more calls from the LLM. </span><span class="koboSpan" id="kobo.445.4">There are several alternatives to </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">this idea:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.447.1">Iteratively refine the response using one chunk at a time. </span><span class="koboSpan" id="kobo.447.2">The previous response and a subsequent chunk are sent to the model to improve the response with the </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">new information.</span></span></li>
<li><span class="koboSpan" id="kobo.449.1">Generate several responses with different chunks, then concatenate them all together and generate a </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">summary response.</span></span></li>
<li><span class="koboSpan" id="kobo.451.1">Hierarchical summarization starts with the responses generated for each different context and recursively combines them until we arrive at a single response. </span><span class="koboSpan" id="kobo.451.2">While this approach enhances the quality of both summaries and generated answers, it requires significantly more LLM calls, making it costly in terms of both computational resources and </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">financial expense.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.453.1">An</span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.454.1"> interesting development is the possibility of </span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.455.1">using RAG as a component of an agent system. </span><span class="koboSpan" id="kobo.455.2">As we introduced in </span><a href="B21257_04.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.456.1">Chapter 4</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.457.1">,</span></em><span class="koboSpan" id="kobo.458.1"> RAG can act as the memory of the system. </span><span class="koboSpan" id="kobo.458.2">RAG can be combined with </span><strong class="bold"><span class="koboSpan" id="kobo.459.1">agents</span></strong><span class="koboSpan" id="kobo.460.1">. </span><span class="koboSpan" id="kobo.460.2">An LLM is</span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.461.1"> capable of reasoning that can be merged with RAG and call-up tools or connect to sites when a query requires additional steps. </span><span class="koboSpan" id="kobo.461.2">An agent can also handle different components (retrieve chat history, conduct query routing, connect to APIs, and execute code). </span><span class="koboSpan" id="kobo.461.3">A complex RAG pipeline can have several components that are not the best fit for every situation, and an LLM can decide which are the best components </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">to use.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer166">
<span class="koboSpan" id="kobo.463.1"><img alt="Figure 6.12 – Different elements in a pipeline of advanced RAG" src="image/B21257_06_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.464.1">Figure 6.12 – Different elements in a pipeline of advanced RAG</span></p>
<p><span class="koboSpan" id="kobo.465.1">So far, we have assumed that a pipeline should be executed only once. </span><span class="koboSpan" id="kobo.465.2">The standard practice is we conduct retrieval once and then generate. </span><span class="koboSpan" id="kobo.465.3">This approach, though, can be insufficient for complex problems that require multi-step reasoning. </span><span class="koboSpan" id="kobo.465.4">There are three possibilities in </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">this case:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.467.1">Iterative retrieval</span></strong><span class="koboSpan" id="kobo.468.1">: In this case, the</span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.469.1"> retrieval is conducted multiple times. </span><span class="koboSpan" id="kobo.469.2">Given a query, we conduct the retrieval, we generate the result, and then the result is judged by an LLM. </span><span class="koboSpan" id="kobo.469.3">Depending on the judgment, we repeat the process up to </span><em class="italic"><span class="koboSpan" id="kobo.470.1">n</span></em><span class="koboSpan" id="kobo.471.1"> times. </span><span class="koboSpan" id="kobo.471.2">This process improves the robustness of the answers after each iteration, but it can also lead to the accumulation of </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">irrelevant information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.473.1">Recursive retrieval</span></strong><span class="koboSpan" id="kobo.474.1">: This</span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.475.1"> system was developed to increase the depth and relevance of search results. </span><span class="koboSpan" id="kobo.475.2">It is similar to the previous one, but at each iteration, the query is refined in response to previous search results. </span><span class="koboSpan" id="kobo.475.3">The purpose is to find the most relevant information by exploiting a feedback loop. </span><span class="koboSpan" id="kobo.475.4">Many of these approaches </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.476.1">exploit </span><strong class="bold"><span class="koboSpan" id="kobo.477.1">chain-of-thought</span></strong><span class="koboSpan" id="kobo.478.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.479.1">CoT</span></strong><span class="koboSpan" id="kobo.480.1">) to guide the retrieval process. </span><span class="koboSpan" id="kobo.480.2">In this case, the system then breaks down the query into a series of intermediate steps that it must solve. </span><span class="koboSpan" id="kobo.480.3">This approach is advantageous when the query is not particularly clear or when the information sought is highly specialized or requires careful consideration of </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">nuanced details.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.482.1">Adaptive retrieval</span></strong><span class="koboSpan" id="kobo.483.1">: In this </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.484.1">case, the LLM actively determines when to search and whether the retrieved content is optimal. </span><span class="koboSpan" id="kobo.484.2">The LLM judges not only the retrieval step but also its own operation. </span><span class="koboSpan" id="kobo.484.3">The LLM can decide when to respond, when to search, or whether additional tools are needed. </span><span class="koboSpan" id="kobo.484.4">This approach is often used not only when searching on the RAG but also when conducting web searches. </span><span class="koboSpan" id="kobo.484.5">Flare (an adaptative approach to RAG) analyzes confidence during the generation process and makes a decision when the confidence falls below a certain threshold. </span><span class="koboSpan" id="kobo.484.6">Self-RAG, on the other hand, introduces </span><strong class="bold"><span class="koboSpan" id="kobo.485.1">reflection tokens</span></strong><span class="koboSpan" id="kobo.486.1"> to </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.487.1">monitor the process and force an introspection of </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">the LLM.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer167">
<span class="koboSpan" id="kobo.489.1"><img alt="Figure 6.13 – Augmentation of RAG pipelines (https://arxiv.org/pdf/2312.10997)" src="image/B21257_06_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.490.1">Figure 6.13 – Augmentation of RAG pipelines (</span><a href="https://arxiv.org/pdf/2312.10997"><span class="koboSpan" id="kobo.491.1">https://arxiv.org/pdf/2312.10997</span></a><span class="koboSpan" id="kobo.492.1">)</span></p>
<p><span class="koboSpan" id="kobo.493.1">To better understand how advanced</span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.494.1"> RAG techniques address known limitations, </span><em class="italic"><span class="koboSpan" id="kobo.495.1">Table 6.1</span></em><span class="koboSpan" id="kobo.496.1"> presents the mapping between key problems and the most effective solutions proposed in </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">recent research</span></span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">.</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.499.1">Problem </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.500.1">to Solve</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.501.1">Solution</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.502.1">Issues in naïve RAG: Latency and performance degradation with many or </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.503.1">large documents</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.504.1">Use hierarchical indexing: Summarize large sections, create multi-level embeddings, use metadata, and implement variations such as map-reduce for long documents or multi-summary for </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">diverse topics.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.506.1">Flat hierarchy limits relevance when the corpus contains an </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.507.1">inherent structure</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.508.1">Apply hierarchical indexing: Respect the document’s structure (chapters, headings, and subheadings), and retrieve context based on hierarchical summaries </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">and embeddings.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.510.1">Low retrieval accuracy and domain-specific </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.511.1">generalization challenges</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.512.1">Generate and embed hypothetical questions for each chunk (Hypothetical Qs). </span><span class="koboSpan" id="kobo.512.2">Use HyDE: generate hypothetical answers to match query semantics, embed them, and retrieve </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">relevant chunks.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.514.1">Loss of context in </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.515.1">granular chunking</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.516.1">Use context enrichment: Expand retrieved chunks with surrounding context using sentence windows or retrieve parent documents to </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">broaden context.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.518.1">Complex queries and low recall from </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.519.1">initial retrieval</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.520.1">Apply query transformation: Decompose complex queries into subqueries, use step-back prompting or query expansion. </span><span class="koboSpan" id="kobo.520.2">Embed transformed queries for </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">improved retrieval.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.522.1">Context mismatch for specific terms </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.523.1">or keywords</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.524.1">Use hybrid search: Combine keyword-based (e.g., BM25) and vector-based retrieval using </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">weighted scoring.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.526.1">Inefficiency in managing diverse </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.527.1">query types</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.528.1">Implement</span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.529.1"> query routing: Use logical rules, keyword-based or semantic classifiers, zero-shot models, or LLM-based routers to direct queries to the </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">appropriate backends.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.531.1">Loss of relevant chunks due to arbitrary </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.532.1">top-k cutoff</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.533.1">Apply reranking: Use cross-encoders, multi-vector rerankers, or LLM-based (pointwise, pairwise, or listwise) reranking to reorder </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">retrieved chunks.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.535.1">Loss of information or efficiency in </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.536.1">LLM context</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.537.1">Use context compression: Filter low-entropy tokens, compress or reorder chunks dynamically (e.g., LongLLMLingua), or apply summary vectors </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">and autocompressors.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.539.1">Inefficient </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.540.1">response generation</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.541.1">Optimize responses: Use iterative refinement, hierarchical summarization, or multi-step response synthesis. </span><span class="koboSpan" id="kobo.541.2">Improve prompt quality </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">and specificity.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.543.1">Memory limitations in </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.544.1">dialogue systems</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.545.1">Use ChatEngine techniques: Save and embed past conversations, compress user dialogue, and merge chat history with </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">current queries.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.547.1">Need for complex reasoning or dynamic </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.548.1">query adaptation</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.549.1">Adopt adaptive and multi-step retrieval: Use recursive, iterative approaches with feedback loops and self-reflection (e.g., </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">Flare, Self-RAG).</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.551.1">Lack of source tracking in </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.552.1">generated responses</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.553.1">Include citations: Use fuzzy citation matching, metadata tagging, or embed source references </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">in prompts.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.555.1">Need for pipeline customization based on query complexity </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.556.1">or modality</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.557.1">Augment</span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.558.1"> RAG pipelines: Combine with agents for reasoning, tool use, and decision-making. </span><span class="koboSpan" id="kobo.558.2">Apply adaptive and recursive retrieval loops for </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">complex queries.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.560.1">Table 6.1 – Problems and solutions in RAG</span></p>
<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.561.1">Modular RAG and its integration with other systems</span></h1>
<p><span class="koboSpan" id="kobo.562.1">Modular RAG is </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.563.1">a further advancement; it can be considered as an extension of advanced RAG but focused on adaptability and versatility. </span><span class="koboSpan" id="kobo.563.2">In this sense, the modular system means it has separate components that can be used either sequentially or </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">in parallel.</span></span></p>
<p><span class="koboSpan" id="kobo.565.1">The pipeline itself is remodeled, with alternating search and generation. </span><span class="koboSpan" id="kobo.565.2">In general, modular RAG involves optimizing the system toward performance and adapting to different tasks. </span><span class="koboSpan" id="kobo.565.3">Modular RAG introduces modules for this that are specialized. </span><span class="koboSpan" id="kobo.565.4">Some examples of the modules that are included are </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.567.1">Search module</span></strong><span class="koboSpan" id="kobo.568.1">: This</span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.569.1"> module is responsible for finding relevant information about a query. </span><span class="koboSpan" id="kobo.569.2">It allows </span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.570.1">searching through search engines, databases, and </span><strong class="bold"><span class="koboSpan" id="kobo.571.1">knowledge graphs</span></strong><span class="koboSpan" id="kobo.572.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.573.1">KGs</span></strong><span class="koboSpan" id="kobo.574.1">). </span><span class="koboSpan" id="kobo.574.2">It can also use sophisticated search algorithms, use machine learning, and </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">execute code.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.576.1">Memory module</span></strong><span class="koboSpan" id="kobo.577.1">: This </span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.578.1">module serves to store relevant information during the search process. </span><span class="koboSpan" id="kobo.578.2">In addition, the system can retrieve context that was </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">previously searched.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.580.1">Routing module</span></strong><span class="koboSpan" id="kobo.581.1">: This </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.582.1">module tries to identify the best path for a query, where it can either search for different information in different databases or decompose </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">the query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.584.1">Generation module</span></strong><span class="koboSpan" id="kobo.585.1">: Different</span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.586.1"> queries may require a different type of generation, such as summarization, paraphrasing, and context expansion. </span><span class="koboSpan" id="kobo.586.2">The focus of this module is on improving the quality and relevance of </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">the output.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.588.1">Task-adaptable module</span></strong><span class="koboSpan" id="kobo.589.1">: This</span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.590.1"> module allows dynamic adaptation to tasks that are requested from the system. </span><span class="koboSpan" id="kobo.590.2">In this way, the system dynamically adjusts retrieval, processing, </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">and generation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.592.1">Validation module</span></strong><span class="koboSpan" id="kobo.593.1">: This</span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.594.1"> module evaluates retrieved responses and context. </span><span class="koboSpan" id="kobo.594.2">The system can identify errors, biases, and inconsistencies. </span><span class="koboSpan" id="kobo.594.3">The process becomes iterative, in which the system can improve </span><span class="No-Break"><span class="koboSpan" id="kobo.595.1">its </span></span><span class="No-Break"><a id="_idIndexMarker680"/></span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">responses.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer168">
<span class="koboSpan" id="kobo.597.1"><img alt="Figure 6.14 – Three different paradigms of RAG (https://arxiv.org/pdf/2312.10997)" src="image/B21257_06_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.598.1">Figure 6.14 – Three different paradigms of RAG (</span><a href="https://arxiv.org/pdf/2312.10997"><span class="koboSpan" id="kobo.599.1">https://arxiv.org/pdf/2312.10997</span></a><span class="koboSpan" id="kobo.600.1">)</span></p>
<p><span class="koboSpan" id="kobo.601.1">Modular RAG </span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.602.1">offers the advantage of adaptability because these modules can be replaced or reconfigured as needed. </span><span class="koboSpan" id="kobo.602.2">The flow between different modules can be finely tuned, allowing an additional level of flexibility. </span><span class="koboSpan" id="kobo.602.3">Furthermore, if naïve and advanced RAG are characterized by a “retrieve and read” mechanism, modular RAG allows “retrieve, read, and rewrite.” </span><span class="koboSpan" id="kobo.602.4">In fact, through the ability to evaluate and provide feedback, the system can refine the response to </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">the query.</span></span></p>
<p><span class="koboSpan" id="kobo.604.1">As this new paradigm spread, interesting alternatives were experimented with, such as integrating information coming from the parametric memory of the LLM. </span><span class="koboSpan" id="kobo.604.2">In this case, the model is asked to generate a response before retrieval (recite and answer). </span><strong class="bold"><span class="koboSpan" id="kobo.605.1">Demonstrate-search-predict</span></strong><span class="koboSpan" id="kobo.606.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.607.1">DSP</span></strong><span class="koboSpan" id="kobo.608.1">) shows</span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.609.1"> how you can have different interactions between the LLM and RAG to solve complex queries (or knowledge-intensive tasks). </span><span class="koboSpan" id="kobo.609.2">DSP shows how a modular RAG allows for robust and flexible pipelines at the same</span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.610.1"> time. </span><strong class="bold"><span class="koboSpan" id="kobo.611.1">Self-reflective retrieval-augmented generation</span></strong><span class="koboSpan" id="kobo.612.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.613.1">Self-RAG</span></strong><span class="koboSpan" id="kobo.614.1">), on the other hand, introduces an element of criticism into the system. </span><span class="koboSpan" id="kobo.614.2">The LLM reflects on what it generates, critiquing its output in terms of factuality and overall quality. </span><span class="koboSpan" id="kobo.614.3">Another alternative is to use interleaved CoT generation and retrieval. </span><span class="koboSpan" id="kobo.614.4">These approaches usually work best when we have issues that </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">require reasoning.</span></span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.616.1">Training and training-free approaches</span></h2>
<p><span class="koboSpan" id="kobo.617.1">RAG approaches fall into two groups: training-free and training-based. </span><span class="koboSpan" id="kobo.617.2">Naïve RAG approaches are </span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.618.1">generally considered training-free. </span><strong class="bold"><span class="koboSpan" id="kobo.619.1">Training-free</span></strong><span class="koboSpan" id="kobo.620.1"> means that the two main components of the system (the embedder and LLM) are kept frozen from the beginning. </span><span class="koboSpan" id="kobo.620.2">This is possible because they are two components that are pre-trained and therefore have already acquired capabilities that allow us to </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">use them.</span></span></p>
<p><span class="koboSpan" id="kobo.622.1">Alternatively, we can have three </span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.623.1">types of </span><strong class="bold"><span class="koboSpan" id="kobo.624.1">training-based approaches</span></strong><span class="koboSpan" id="kobo.625.1">: independent training, sequential training, and </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">joint training.</span></span></p>
<p><span class="koboSpan" id="kobo.627.1">In </span><strong class="bold"><span class="koboSpan" id="kobo.628.1">independent training</span></strong><span class="koboSpan" id="kobo.629.1">, both the </span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.630.1">retriever and LLMs are</span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.631.1"> trained separately in totally independent processes (there is no interaction during training). </span><span class="koboSpan" id="kobo.631.2">In this case, we have separate fine-tuning of the various components of the system. </span><span class="koboSpan" id="kobo.631.3">This approach is useful when we want to adapt our system to a specific domain (legal, financial, or medical, for example). </span><span class="koboSpan" id="kobo.631.4">Compared to a training-free approach, this type of training improves the capabilities of the system for the domain of our application. </span><span class="koboSpan" id="kobo.631.5">LLMs can also be fine-tuned to make better use of </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">the context.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.633.1">Sequential training</span></strong><span class="koboSpan" id="kobo.634.1">, on the other</span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.635.1">RAG:sequential training”  hand, assumes that we use </span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.636.1">these two components sequentially, so it is better to find a form of training that increases the synergy between these components. </span><span class="koboSpan" id="kobo.636.2">The components can first be trained independently, following which they are trained sequentially. </span><span class="koboSpan" id="kobo.636.3">One of the components is kept frozen while the other undergoes additional training. </span><span class="koboSpan" id="kobo.636.4">Depending on what the order of training is, we can have two classes, retriever-first </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">or LLM-first:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.638.1">Retriever-first</span></strong><span class="koboSpan" id="kobo.639.1">: In this class, the</span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.640.1"> trainer’s training is conducted and then it is kept frozen. </span><span class="koboSpan" id="kobo.640.2">Then, the LLM is trained to understand how to use the knowledge in the retriever context. </span><span class="koboSpan" id="kobo.640.3">For example, we conduct the fine-tuning of our retriever independently and then we conduct fine-tuning of the LLM using the retrieved chunks. </span><span class="koboSpan" id="kobo.640.4">The LLM receives the retriever chunks during its fine-tuning and learns how best to use this context </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">for generation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.642.1">LLM-first</span></strong><span class="koboSpan" id="kobo.643.1">: This is a bit more complex, but it uses the supervision of an LLM to train the retriever. </span><span class="koboSpan" id="kobo.643.2">An</span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.644.1"> LLM is usually a much more capable model than the retriever because it has many more parameters and has been trained on many more tokens, thus making it a good supervisor. </span><span class="koboSpan" id="kobo.644.2">In a sense, this approach can be seen as a kind of knowledge distillation in which we take advantage of the greater knowledge of a larger model to train a </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">smaller model.</span></span></li>
</ul>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.646.1">Training Approach</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.647.1">Domains/Applications</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.648.1">Reasoning</span></strong></span></p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.649.1">Retriever-first</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.650.1">Search engines (general </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.651.1">or domain-specific)</span></strong></span></p>
<p><span class="koboSpan" id="kobo.652.1">For example, legal document search, medical literature search, or e-commerce </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">product search</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.654.1">Focuses on retrieving the most relevant documents quickly and accurately. </span><span class="koboSpan" id="kobo.654.2">Essential for systems where domain-specific precision is critical, and the retriever must handle vast, structured, or </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">semi-structured corpora.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.656.1">Enterprise </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.657.1">knowledge management</span></strong></span></p>
<p><span class="koboSpan" id="kobo.658.1">For example, internal corporate documentation, FAQs, or </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">CRM systems</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.660.1">Emphasizes retrieving the right documents efficiently from proprietary databases, where the quality of retrieval has a more significant impact than the quality </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">of generation.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.662.1">Scientific </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.663.1">research repositories</span></strong></span></p>
<p><span class="koboSpan" id="kobo.664.1">For example, PubMed, arXiv, </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">or patents</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.666.1">Ensures precise and recall-optimized retrieval in highly technical or specialized fields where high-quality retrieval is essential for downstream tasks such as summarization or </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">report generation.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.668.1">Regulatory and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.669.1">compliance systems</span></strong></span></p>
<p><span class="koboSpan" id="kobo.670.1">For example, financial compliance checks, or legal case </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">law databases</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.672.1">In domains where accuracy and compliance are critical, the retriever must reliably surface the most relevant content while minimizing irrelevant or </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">low-confidence retrievals.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.674.1">LLM-first</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.675.1">Conversational agents</span></strong></span></p>
<p><span class="koboSpan" id="kobo.676.1">For example, customer support </span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.677.1">chatbots or </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">personal assistants</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.679.1">Relies heavily on the generative capabilities of the LLM to provide nuanced, conversational responses. </span><span class="koboSpan" id="kobo.679.2">Retrieval is secondary as the LLM interprets and integrates </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">retrieved content.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.681.1">Creative applications</span></strong></span></p>
<p><span class="koboSpan" id="kobo.682.1">For example, content writing, storytelling, </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">or brainstorming</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.684.1">The LLM’s ability to create, synthesize, and infer from retrieved data is paramount. </span><span class="koboSpan" id="kobo.684.2">Retrieval supports generation by providing a broader context rather than being the focal point </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">of optimization.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.686.1">Complex </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.687.1">reasoning tasks</span></strong></span></p>
<p><span class="koboSpan" id="kobo.688.1">For example, multi-step problem-solving or </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">decision-making systems</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.690.1">The LLM’s role as a reasoner outweighs retrieval precision, as the focus is on the ability to process, relate, and infer knowledge. </span><span class="koboSpan" id="kobo.690.2">Retrieval primarily ensures access to supplementary information </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">for reasoning.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.692.1">Educational tools</span></strong></span></p>
<p><span class="koboSpan" id="kobo.693.1">For example, learning assistants or</span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.694.1"> personalized </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">tutoring systems</span></span></p>
</td>
<td class="No-Table-Style" colspan="2">
<p><span class="koboSpan" id="kobo.696.1">The LLM’s ability to adapt and generate instructional content tailored to the user’s context is more critical than precise retrieval. </span><span class="koboSpan" id="kobo.696.2">Retrieval serves as a secondary mechanism to ensure the completeness </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">of information.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.698.1">Table 6.2 – Training approaches</span></p>
<p><span class="koboSpan" id="kobo.699.1">According to this article (Izacard, </span><a href="https://arxiv.org/abs/2012.04584"><span class="koboSpan" id="kobo.700.1">https://arxiv.org/abs/2012.04584</span></a><span class="koboSpan" id="kobo.701.1">), attention activation values in the LLM are a good proxy for defining the relevance of a document, so they can be used to provide a label (a kind of guide) to the retriever on how good the search results are. </span><span class="koboSpan" id="kobo.701.2">Hence, the retriever is trained with a metric based on attention in the LLM. </span><span class="koboSpan" id="kobo.701.3">For a less expensive approach, a small LLM can be used to generate the label to then train the retriever. </span><span class="koboSpan" id="kobo.701.4">There are then variations in these approaches, but all are based on the principle that once we have fine-tuned the LLM, we want to align </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">the retriever.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.703.1">Joint methods</span></strong><span class="koboSpan" id="kobo.704.1">, on the</span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.705.1"> other hand, represent end-to-end training</span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.706.1"> of the system. </span><span class="koboSpan" id="kobo.706.2">In other words, both the retriever and the generator are aligned at the same time (simultaneously). </span><span class="koboSpan" id="kobo.706.3">The idea is that we want the system to simultaneously improve both its ability to find knowledge and its ability to use this knowledge for generation. </span><span class="koboSpan" id="kobo.706.4">The advantage is that we have a </span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.707.1">synergistic effect </span><a id="_idIndexMarker697"/><span class="No-Break"><span class="koboSpan" id="kobo.708.1">during training.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer169">
<span class="koboSpan" id="kobo.709.1"><img alt="Figure 6.15 – Different training methods in RAG (https://arxiv.org/pdf/2405.06211)" src="image/B21257_06_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.710.1">Figure 6.15 – Different training methods in RAG (</span><a href="https://arxiv.org/pdf/2405.06211"><span class="koboSpan" id="kobo.711.1">https://arxiv.org/pdf/2405.06211</span></a><span class="koboSpan" id="kobo.712.1">)</span></p>
<p><span class="koboSpan" id="kobo.713.1">Now that we know the different modifications that we can apply to our RAG, let’s try them in the </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">next section.</span></span></p>
<h1 id="_idParaDest-106"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.715.1">Implementing an advanced RAG pipeline</span></h1>
<p><span class="koboSpan" id="kobo.716.1">In this section, we will </span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.717.1">describe how an advanced RAG pipeline can be implemented. </span><span class="koboSpan" id="kobo.717.2">In this pipeline, we use a more advanced version of naïve RAG, including some add-ons to improve it. </span><span class="koboSpan" id="kobo.717.3">This shows us how the starting basis is a classic RAG pipeline (embedding, retrieval, and generation) but more sophisticated components are inserted. </span><span class="koboSpan" id="kobo.717.4">In this pipeline, we have used the </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">following add-ons:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.719.1">Reranker</span></strong><span class="koboSpan" id="kobo.720.1">: This allows us </span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.721.1">to sort the context found during the retrieval step. </span><span class="koboSpan" id="kobo.721.2">This is one of the most widely used elements in advanced RAG because it has been seen to significantly </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">improve results.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.723.1">Query transformation</span></strong><span class="koboSpan" id="kobo.724.1">: In</span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.725.1"> this case, we are using a simple query transformation. </span><span class="koboSpan" id="kobo.725.2">This is because we want to try to broaden our retrieval range, since some relevant documents may </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">be missed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.727.1">Query routing</span></strong><span class="koboSpan" id="kobo.728.1">: This </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.729.1">prevents us from treating all queries the same and allows us to establish rules for more </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">efficient retrieval.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.731.1">Hybrid search</span></strong><span class="koboSpan" id="kobo.732.1">: With this, we</span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.733.1"> combine the power of keyword-based search with </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">semantic search.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.735.1">Summarization</span></strong><span class="koboSpan" id="kobo.736.1">: With this, we try</span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.737.1"> to eliminate redundant information from our </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">retrieved context.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.739.1">Of course, we </span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.740.1">could add other components, but generally, these are the most commonly used and give an overview of what components we can add to </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">naïve RAG.</span></span></p>
<p><span class="koboSpan" id="kobo.742.1">We can see in the following figure how our pipeline </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">is modified:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer170">
<span class="koboSpan" id="kobo.744.1"><img alt="Figure 6.16 – Pipeline of advanced RAG" src="image/B21257_06_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.745.1">Figure 6.16 – Pipeline of advanced RAG</span></p>
<p><span class="koboSpan" id="kobo.746.1">The complete code can be found in the repository; here, we will just see the highlights. </span><span class="koboSpan" id="kobo.746.2">In this code snippet, we are defining a function to represent the query transformation. </span><span class="koboSpan" id="kobo.746.3">In this case, we are developing only a small modification of the query (searching for other related</span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.747.1"> terms in </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">our query):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.749.1">
def advanced_query_transformation(query):
    """
    Transforms the input query by adding synonyms, extensions, or modifying the structure
    for better search performance.
</span><span class="koboSpan" id="kobo.749.2">    Args:
        query (str): The original query.
</span><span class="koboSpan" id="kobo.749.3">    Returns:
        str: The transformed query with added synonyms or related terms.
</span><span class="koboSpan" id="kobo.749.4">    """
    expanded_query = query + " OR related_term"
    return expanded_query</span></pre> <p><span class="koboSpan" id="kobo.750.1">Next, we perform query routing. </span><span class="koboSpan" id="kobo.750.2">Query routing enforces a simple rule: if specific keywords are present in the query, a keyword-based search is performed; otherwise, a semantic (embedding-based) search is used. </span><span class="koboSpan" id="kobo.750.3">In some cases, we may want to first retrieve only documents that contain certain keywords—such as references to a specific product—and then narrow the results further using </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">semantic search:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.752.1">
def advanced_query_routing(query):
    """
    Determines the retrieval method based on the presence of specific keywords in the query.
</span><span class="koboSpan" id="kobo.752.2">    Args:
        query (str): The user's query.
</span><span class="koboSpan" id="kobo.752.3">    Returns:
        str: 'textual' if the query requires text-based retrieval, 'vector' otherwise.
</span><span class="koboSpan" id="kobo.752.4">    """
    if "specific_keyword" in query:
        return "textual"
    else:
        return "vector"</span></pre> <p><span class="koboSpan" id="kobo.753.1">Next, we perform a hybrid</span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.754.1"> search, which allows us to use search based on semantic and keyword content. </span><span class="koboSpan" id="kobo.754.2">This is one of the most widely used components in RAG pipelines today. </span><span class="koboSpan" id="kobo.754.3">When chunking is used, sometimes documents relevant to a query can only be found because they contain a keyword (e.g., the name of a product, a person, and so on). </span><span class="koboSpan" id="kobo.754.4">Obviously, not all chunks that contain a keyword are relevant documents (especially for queries where we are more interested in a semantic concept). </span><span class="koboSpan" id="kobo.754.5">With hybrid search, we can balance the two types of search, choosing how many chunks to take from one or the other type </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">of search:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.756.1">
def fusion_retrieval(query, top_k=5):
    """
    Retrieves the top_k most relevant documents using a combination of vector-based
    and textual retrieval methods.
</span><span class="koboSpan" id="kobo.756.2">    Args:
        query (str): The search query.
</span><span class="koboSpan" id="kobo.756.3">        top_k (int): The number of top documents to retrieve.
</span><span class="koboSpan" id="kobo.756.4">    Returns:
        list: A list of combined results from both vector and textual retrieval methods.
</span><span class="koboSpan" id="kobo.756.5">    """
    query_embedding = sentence_model.encode(query).tolist()
    vector_results = collection.query(query_embeddings=[query_embedding], n_results=min(top_k, len(documents)))
    es_body = {
        "size": top_k,  # Move size into body
        "query": {
            "match": {
                "content": query
            }
        }
    }
    es_results = es.search(index=index_name, body=es_body)
    es_documents = [hit["_source"]["content"] for hit in es_results['hits']['hits']]
    combined_results = vector_results['documents'][0] + es_documents
    return combined_results</span></pre> <p><span class="koboSpan" id="kobo.757.1">As mentioned, the </span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.758.1">reranker is one of the most frequently used elements; it is a transformer that is used to reorder the context. </span><span class="koboSpan" id="kobo.758.2">If we have found 10 chunks, we reorder the found chunks and usually take a subset of them. </span><span class="koboSpan" id="kobo.758.3">Sometimes, semantic search can find the most relevant chunks again, but these may then be found further down the order. </span><span class="koboSpan" id="kobo.758.4">The reranker ensures that these chunks are then actually placed </span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.759.1">in the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">the LLM:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.761.1">
def rerank_documents(query, documents):
    """
    Reranks the retrieved documents based on their relevance to the query using a pre-trained
    BERT model.
</span><span class="koboSpan" id="kobo.761.2">    Args:
        query (str): The user's query.
</span><span class="koboSpan" id="kobo.761.3">        documents (list): A list of documents retrieved from the search.
</span><span class="koboSpan" id="kobo.761.4">    Returns:
        list: A list of reranked documents, sorted by relevance.
</span><span class="koboSpan" id="kobo.761.5">    """
    inputs = [rerank_tokenizer.encode_plus(query, doc, return_tensors='pt', truncation=True, padding=True) for doc in documents]
    scores = []
    for input in inputs:
        outputs = rerank_model(**input)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=1)
        positive_class_probability = probabilities[:, 1].item()
        scores.append(positive_class_probability)
    ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked_docs]</span></pre> <p><span class="koboSpan" id="kobo.762.1">As mentioned earlier, context can also contain information that is redundant. </span><span class="koboSpan" id="kobo.762.2">LLMs are sensitive to noise, so reducing this noise can help generation. </span><span class="koboSpan" id="kobo.762.3">In this case, we use an LLM to summarize the </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.763.1">found context (of course, we set a limit to avoid losing too </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">much information):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.765.1">
def select_and_compress_context(documents):
    """
    Summarizes the content of the retrieved documents to create a compressed context.
</span><span class="koboSpan" id="kobo.765.2">    Args:
        documents (list): A list of documents to summarize.
</span><span class="koboSpan" id="kobo.765.3">    Returns:
        list: A list of summarized texts for each document.
</span><span class="koboSpan" id="kobo.765.4">    """
    summarized_context = []
    for doc in documents:
        input_length = len(doc.split())
        max_length = min(100, input_length)  than 100
        summary = summarizer(doc, max_length=max_length, min_length=5, do_sample=False)[0]['summary_text']
        summarized_context.append(summary)
    return summarized_context</span></pre> <p><span class="koboSpan" id="kobo.766.1">Once defined, we </span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.767.1">just need to assemble them into a single pipeline. </span><span class="koboSpan" id="kobo.767.2">Once that’s done, we can use our RAG pipeline. </span><span class="koboSpan" id="kobo.767.3">Check the code in the repository and play around with the code. </span><span class="koboSpan" id="kobo.767.4">Once you have a RAG pipeline that works, the next natural step is deployment. </span><span class="koboSpan" id="kobo.767.5">In the next section, we will discuss potential challenges to </span><span class="No-Break"><span class="koboSpan" id="kobo.768.1">the deployment.</span></span></p>
<h1 id="_idParaDest-107"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.769.1">Understanding the scalability and performance of RAG</span></h1>
<p><span class="koboSpan" id="kobo.770.1">In this section, we will mainly describe challenges that are related to the commissioning of a RAG system or that may emerge with the scaling of the system. </span><span class="koboSpan" id="kobo.770.2">The main advantage of RAG over an LLM is that it can be scaled without conducting additional training. </span><span class="koboSpan" id="kobo.770.3">The purpose and requirements of development and production are mainly different. </span><span class="koboSpan" id="kobo.770.4">LLMs and RAG pose new challenges, especially when you want to take a system into production. </span><span class="koboSpan" id="kobo.770.5">Productionizing means taking a complex system such as RAG from a prototype to a stable, operational environment. </span><span class="koboSpan" id="kobo.770.6">This can be extremely complex when you have to manage different users who may be connected remotely. </span><span class="koboSpan" id="kobo.770.7">While in development, accuracy might be the most important metric, while in production, special care must be taken to balance performance </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">and cost.</span></span></p>
<p><span class="koboSpan" id="kobo.772.1">Large organizations, in particular, may already have big data stored and may therefore want to use RAG with it. </span><span class="koboSpan" id="kobo.772.2">Big data can be a significant challenge for a RAG system, especially considering the volume, velocity, and variety of</span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.773.1"> data. </span><strong class="bold"><span class="koboSpan" id="kobo.774.1">Scalability</span></strong><span class="koboSpan" id="kobo.775.1"> is a critical concern when discussing big data; the same principle applies </span><span class="No-Break"><span class="koboSpan" id="kobo.776.1">to RAG.</span></span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.777.1">Data scalability, storage, and preprocessing</span></h2>
<p><span class="koboSpan" id="kobo.778.1">So far, we have talked about how to find information. </span><span class="koboSpan" id="kobo.778.2">We have assumed that the data is in textual form. </span><span class="koboSpan" id="kobo.778.3">The data structure of the text is an important parameter, and putting it into production can be problematic. </span><span class="koboSpan" id="kobo.778.4">So, our system may have to integrate </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.780.1">Unstructured data</span></strong><span class="koboSpan" id="kobo.781.1">: Text is the</span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.782.1"> most commonly used data type present in a corpus. </span><span class="koboSpan" id="kobo.782.2">It can have different origins: encyclopedic (from Wikipedia), domain-specific (scientific, medical, or financial), industry-specific (reports or standard documents), downloaded from the internet, or user chat. </span><span class="koboSpan" id="kobo.782.3">It can thus be generated by humans but also include data generated by automated systems or by LLMs themselves (previous interactions with users). </span><span class="koboSpan" id="kobo.782.4">In addition, it can be multi-language, and the system may have to conduct a cross-language search. </span><span class="koboSpan" id="kobo.782.5">Today, there are both LLMs that have been trained with different languages and multi-lingual embedders (specifically designed for multi-lingual capabilities). </span><span class="koboSpan" id="kobo.782.6">There </span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.783.1">are also other types of unstructured data, such as image and video. </span><span class="koboSpan" id="kobo.783.2">We will discuss multimodal RAG in a little more detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">next section.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.785.1">Semi-structured data</span></strong><span class="koboSpan" id="kobo.786.1">: Generally, this </span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.787.1">means data that contains a mixture of textual and table information (such as PDFs). </span><span class="koboSpan" id="kobo.787.2">Other examples of semi-structured data are JSON, XML, and HTML. </span><span class="koboSpan" id="kobo.787.3">These types of data are often complex to use with RAG. </span><span class="koboSpan" id="kobo.787.4">There are usually file-specific pipelines (chunking, metadata storing, and so on) because they can create problems for the system. </span><span class="koboSpan" id="kobo.787.5">In the case of PDF, chunking can separate tables into multiple chunks, making retrieval inefficient. </span><span class="koboSpan" id="kobo.787.6">In addition, tables make similarity search more complicated. </span><span class="koboSpan" id="kobo.787.7">An alternative is to extract the tables and turn them into text or insert them into compatible databases (such as SQL). </span><span class="koboSpan" id="kobo.787.8">Since the available methods are not yet optimal, there is still intense research in </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">the field.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.789.1">Structured data</span></strong><span class="koboSpan" id="kobo.790.1">: Structured data is </span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.791.1">data that is in a standardized format that can be accessed efficiently by both humans and software. </span><span class="koboSpan" id="kobo.791.2">Structured data generally has some special features: defined attributes (same attributes for all data values as in a table), relational attributes (tables have common values that tie different datasets together; for example, in a customer dataset, there are IDs that allow users and their purchases to be found), quantitative data (data is optimized for mathematical analysis), and storage (data is stored in a particular format and with precise rules). </span><span class="koboSpan" id="kobo.791.3">Examples of structured data are Excel files, SQL databases, web form results, point-of-sale data, and product directories. </span><span class="koboSpan" id="kobo.791.4">Another example of structured data is KGs, which we will discuss in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">next chapter.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.793.1">These factors must be taken into account. </span><span class="koboSpan" id="kobo.793.2">For example, if we are designing a system that needs to search for compliance documents in various regions and in different languages, we need a RAG that can conduct cross-lingual retrieval. </span><span class="koboSpan" id="kobo.793.3">If our organization has primarily one type of data (PDF or SQL databases), it is important to take this into account and optimize the system to search for this type of data. </span><span class="koboSpan" id="kobo.793.4">There are specific alternatives to improve the capabilities of RAGs with structured data. </span><span class="koboSpan" id="kobo.793.5">One example, chain-of-table, is a method that integrates CoT prompting with table transformations. </span><span class="koboSpan" id="kobo.793.6">In a step-by-step process with an LLM and a set of predefined operations, it extracts and modifies tables. </span><span class="koboSpan" id="kobo.793.7">This approach is designed for handling complex tables, and it exploits step-by-step reasoning and step-by-step tabular operations to accomplish this. </span><span class="koboSpan" id="kobo.793.8">This approach is useful if we have complex SQL databases or large amounts of data frames as data sources. </span><span class="koboSpan" id="kobo.793.9">Then, there are more sophisticated alternatives that combine symbolic reasoning and textual reasoning. </span><span class="koboSpan" id="kobo.793.10">Mix self-consistency is a dedicated approach to tabular data understanding that uses textual and symbolic reasoning with self-consistency, thus creating multi-paths of reasoning and then aggregating with self-consistency. </span><span class="koboSpan" id="kobo.793.11">For semi-structured data such as PDFs and JHTML, there are dedicated packages that allow us to extract information from them or to </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">parse data.</span></span></p>
<p><span class="koboSpan" id="kobo.795.1">It is not only the type of data that impacts RAG performance but also the amount of data itself. </span><span class="koboSpan" id="kobo.795.2">As the volume of data increases, so does the difficulty in finding relevant information. </span><span class="koboSpan" id="kobo.795.3">Likewise, it is likely to increase the latency of </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">the system.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.797.1">Data storage</span></strong><span class="koboSpan" id="kobo.798.1"> is one</span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.799.1"> of the focal points to be addressed before bringing the system into production. </span><span class="koboSpan" id="kobo.799.2">Distributed storage systems (an infrastructure that divides data into several physical servers or data centers) can be a solution for large volumes of data. </span><span class="koboSpan" id="kobo.799.3">This has the advantage of increasing system speed and reducing the risk of data loss, but risks increasing costs and management complexity. </span><span class="koboSpan" id="kobo.799.4">When you have different types of data, it can be advantageous to use a structure called a data lake. </span><span class="koboSpan" id="kobo.799.5">A </span><strong class="bold"><span class="koboSpan" id="kobo.800.1">data lake</span></strong><span class="koboSpan" id="kobo.801.1"> is </span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.802.1">a centralized repository that is designed for the storage and processing of structured, semi-structured, and unstructured data. </span><span class="koboSpan" id="kobo.802.2">The advantage of the data lake is that it is a scalable and flexible structure for ingesting, processing, and storing data of different types. </span><span class="koboSpan" id="kobo.802.3">The data lake is advantageous for RAG because it allows more data context to be maintained than other data structures. </span><span class="koboSpan" id="kobo.802.4">On the other hand, data lakes require more expertise to be functional. </span><span class="koboSpan" id="kobo.802.5">Alternatives may be partitioning data into smaller, more manageable partitions (based on geography, topic, time, and so on), which allows more efficient retrieval. </span><span class="koboSpan" id="kobo.802.6">In the case of numerous requests, frequently accessed data caching can be conducted to avoid repetition. </span><span class="koboSpan" id="kobo.802.7">These strategies can be used in the case of big data storage </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">and access.</span></span></p>
<p><span class="koboSpan" id="kobo.804.1">Another important aspect is building</span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.805.1"> solid pipelines for </span><strong class="bold"><span class="koboSpan" id="kobo.806.1">data preprocessing and cleaning</span></strong><span class="koboSpan" id="kobo.807.1">. </span><span class="koboSpan" id="kobo.807.2">In the development stage, it is common to work with well-polished datasets, but in production, this is not the case. </span><span class="koboSpan" id="kobo.807.3">Especially in big data, it is essential to make sure that there are no inconsistencies or that the system can handle missing or incomplete data. </span><span class="koboSpan" id="kobo.807.4">In a big data environment, data comes from many sources and not all of them are good quality. </span><span class="koboSpan" id="kobo.807.5">Therefore, imputation techniques (KNN or others) can be used to fill in missing data. </span><span class="koboSpan" id="kobo.807.6">Other additions that can improve the process are techniques to eliminate noisy or erroneous data, such as outlier detection algorithms, normalization </span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.808.1">techniques, and regular expression techniques to eliminate erroneous </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">data points.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.810.1">Data deduplication</span></strong><span class="koboSpan" id="kobo.811.1"> is another</span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.812.1"> important aspect when working with LLMs. </span><span class="koboSpan" id="kobo.812.2">Duplicate data harms the training of LLMs and is also detrimental when found during the generation process (risking outputs that are inaccurate, biased, or of poor quality). </span><span class="koboSpan" id="kobo.812.3">As the volume of data increases, data duplication is a risk that increases linearly. </span><span class="koboSpan" id="kobo.812.4">There are techniques such as fuzzy matching and hash-based deduplication that can be used to eliminate duplicate elements. </span><span class="koboSpan" id="kobo.812.5">In general, a pipeline should be created to control the quality and governance of the data in the system (data quality monitoring). </span><span class="koboSpan" id="kobo.812.6">These pipelines should include rules and tracking systems to be able to identify problematic data and its origin. </span><span class="koboSpan" id="kobo.812.7">Although these pipelines are essential, pipelines that are too complex to maintain or slow down the system too much should </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">be avoided.</span></span></p>
<p><span class="koboSpan" id="kobo.814.1">Once we have decided on our data storage infrastructure, we need to make sure we have efficient </span><strong class="bold"><span class="koboSpan" id="kobo.815.1">data indexing and retrieval</span></strong><span class="koboSpan" id="kobo.816.1">. </span><span class="koboSpan" id="kobo.816.2">There are indexing methods that are specialized for big data, such as Apache </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.817.1">Lucene or Elasticsearch. </span><span class="koboSpan" id="kobo.817.2">Also, the most used data can be cached, or the retrieval process can be distributed to create a parallel infrastructure and reduce bottlenecks when there are multiple users. </span><span class="koboSpan" id="kobo.817.3">Given the complexity of some of these techniques, it is always best to test and conduct benchmarks before putting them </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">into production.</span></span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.819.1">Parallel processing</span></h2>
<p><span class="koboSpan" id="kobo.820.1">Especially for applications with a large number of</span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.821.1"> users, </span><strong class="bold"><span class="koboSpan" id="kobo.822.1">parallel processing</span></strong><span class="koboSpan" id="kobo.823.1"> can significantly increase system scalability. </span><span class="koboSpan" id="kobo.823.2">This obviously requires a good cloud infrastructure with well-organized clusters. </span><span class="koboSpan" id="kobo.823.3">Applying parallel processing to RAG significantly decreases system latency even when there are large datasets. </span><span class="koboSpan" id="kobo.823.4">Apache Spark and Dask are among the most widely used solutions for implementing parallel computing with RAG. </span><span class="koboSpan" id="kobo.823.5">As we have seen, parallel computing can be implemented at various stages of the RAG pipeline: storage, retrieval, and generation. </span><span class="koboSpan" id="kobo.823.6">During storage, the various nodes can be used to implement the entire data preprocessing pipeline, that is, preprocessing, indexing, and chunking of part of the dataset (up to embedding). </span><span class="koboSpan" id="kobo.823.7">Although it seems less intuitive, during retrieval, the dataset can be divided among various nodes, with each node responsible for finding information from a particular dataset shard. </span><span class="koboSpan" id="kobo.823.8">In this way, we reduce the computational burden on each node and make the retrieval </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">process parallel.</span></span></p>
<p><span class="koboSpan" id="kobo.825.1">Similarly, generation</span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.826.1"> can be made parallel. </span><span class="koboSpan" id="kobo.826.2">In fact, LLMs are computationally intensive but are transformer-based. </span><span class="koboSpan" id="kobo.826.3">The transformer was designed with both parallelization of training and inference in mind. </span><span class="koboSpan" id="kobo.826.4">There are techniques that allow parallelization in the case of long sequences or large batches of data. </span><span class="koboSpan" id="kobo.826.5">Later, more sophisticated techniques, such as tensor parallelism, model parallelism, and specialized frameworks, were developed. </span><span class="koboSpan" id="kobo.826.6">Paralleling the system, however, has inherent challenges and the risk of emerging errors. </span><span class="koboSpan" id="kobo.826.7">For these reasons, it is important to monitor the system during use and implement fault-tolerance mechanisms (such as checkpoints), advanced scheduling (such as dynamic task assignment), and other </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">potential solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.828.1">RAG is a resource-intensive process (or at least some of the steps are), so it is good practice to implement techniques that dynamically allocate resources and monitor the workloads of the various processes. </span><span class="koboSpan" id="kobo.828.2">Also, it is recommended to use a modular approach that separates the various components, such as data ingestion, storage, retrieval, and generation. </span><span class="koboSpan" id="kobo.828.3">In any case, it is advisable to have a process that monitors not only performance in terms of accuracy but also memory usage, costs, network usage, and </span><span class="No-Break"><span class="koboSpan" id="kobo.829.1">so on.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<span class="koboSpan" id="kobo.830.1"><img alt="Figure 6.17 – Big data solutions for RAG scalability" src="image/B21257_06_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.831.1">Figure 6.17 – Big data solutions for RAG scalability</span></p>
<p><span class="koboSpan" id="kobo.832.1">We have talked generally about RAG. </span><span class="koboSpan" id="kobo.832.2">As we saw earlier, though, RAG today can be composed of several components. </span><span class="koboSpan" id="kobo.832.3">With advanced RAG and modular RAG, we saw how this system can be rapidly extended with additional components that impact both the accuracy of the system and its computational and latency costs. </span><span class="koboSpan" id="kobo.832.4">Thus, there are many alternatives for our system, and it is difficult to choose which components are most important. </span><span class="koboSpan" id="kobo.832.5">To date, there are a few benchmark studies that have conducted a rigorous analysis of both performance and computational costs. </span><span class="koboSpan" id="kobo.832.6">In a recent study (Wang, 2024), the authors analyzed</span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.833.1"> the potential best components and gave guidance on which elements to use. </span><span class="koboSpan" id="kobo.833.2">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.834.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.835.1">.18</span></em><span class="koboSpan" id="kobo.836.1">, the components marked in blue are those, according to the authors of the study, that give the best performance, while those in bold are </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">optional components.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer172">
<span class="koboSpan" id="kobo.838.1"><img alt="Figure 6.18 – Contribution of each component for an optimal RAG (https://arxiv.org/pdf/2407.01219)" src="image/B21257_06_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.839.1">Figure 6.18 – Contribution of each component for an optimal RAG (</span><a href="https://arxiv.org/pdf/2407.01219"><span class="koboSpan" id="kobo.840.1">https://arxiv.org/pdf/2407.01219</span></a><span class="koboSpan" id="kobo.841.1">)</span></p>
<p><span class="koboSpan" id="kobo.842.1">For example, the addition of some components improves system accuracy with a noticeable increase in latency. </span><span class="koboSpan" id="kobo.842.2">HyDE achieves the highest performance score but seems to have a significant computational cost. </span><span class="koboSpan" id="kobo.842.3">In this case, the performance improvement does not justify this increased latency. </span><span class="koboSpan" id="kobo.842.4">Other components increase the computational cost, but their absence results in an appreciable drop in performance (this is the case with reranking). </span><span class="koboSpan" id="kobo.842.5">Summarization modules help the model achieve optimal accuracy; their cost can be justified if latency is not problematic. </span><span class="koboSpan" id="kobo.842.6">Although it is virtually impossible to test all components in a systematic search, some guidelines can be provided. </span><span class="koboSpan" id="kobo.842.7">The best performance is achieved with the query classification module, HyDE, the reranking module, context repacking, and summarization. </span><span class="koboSpan" id="kobo.842.8">If this is too expensive computationally or in terms of latency, however, it is better to avoid techniques such as HyDE and stick to the other </span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.843.1">modules (perhaps choosing less expensive alternatives, for example, a reranker with fewer parameters). </span><span class="koboSpan" id="kobo.843.2">This is summarized in the following table comparing individual modules and techniques in terms of performance and </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">computational efficiency:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer173">
<span class="koboSpan" id="kobo.845.1"><img alt="Figure 6.19 – Impact of single modules and techniques on accuracy and latency (https://arxiv.org/pdf/2407.01219)" src="image/B21257_06_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.846.1">Figure 6.19 – Impact of single modules and techniques on accuracy and latency (</span><a href="https://arxiv.org/pdf/2407.01219"><span class="koboSpan" id="kobo.847.1">https://arxiv.org/pdf/2407.01219</span></a><span class="koboSpan" id="kobo.848.1">)</span></p>
<p><span class="koboSpan" id="kobo.849.1">In addition, there are also parallelization strategies specifically designed for RAG. </span><span class="koboSpan" id="kobo.849.2">LlamaIndex offers a parallel pipeline for data ingestion and processing. </span><span class="koboSpan" id="kobo.849.3">In addition, to increase the robustness of the system, there are systems to prevent errors. </span><span class="koboSpan" id="kobo.849.4">For example, when using a model, you may encounter runtime errors (especially if you use external APIs such as OpenAI or Anthropic). </span><span class="koboSpan" id="kobo.849.5">In these cases, it pays to have fallback models. </span><span class="koboSpan" id="kobo.849.6">An </span><strong class="bold"><span class="koboSpan" id="kobo.850.1">LLM router</span></strong><span class="koboSpan" id="kobo.851.1"> is a </span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.852.1">system that allows you to route queries to different LLMs. </span><span class="koboSpan" id="kobo.852.2">Typically, there is a predictor model to intelligently decide which LLM is best suited for a given prompt (taking into account potential accuracy or factors such as cost). </span><span class="koboSpan" id="kobo.852.3">These routers can be used either as closed source models or to route queries to different external </span><span class="No-Break"><span class="koboSpan" id="kobo.853.1">LLM APIs.</span></span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.854.1">Security and privacy</span></h2>
<p><span class="koboSpan" id="kobo.855.1">An important aspect</span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.856.1"> to consider when a system goes into production is the </span><strong class="bold"><span class="koboSpan" id="kobo.857.1">security and privacy</span></strong><span class="koboSpan" id="kobo.858.1"> of the system. </span><span class="koboSpan" id="kobo.858.2">RAG can handle an enormous amount of sensitive and confidential data; breaching the system can lead to devastating consequences for an organization (regulatory fines, lawsuits, reputational damages, and so on). </span><span class="koboSpan" id="kobo.858.3">One of the main solutions is data encryption. </span><span class="koboSpan" id="kobo.858.4">Some algorithms and protocols are widely used in the industry and can also be applied to RAG (e.g., AES-256 and TLS/SSL). </span><span class="koboSpan" id="kobo.858.5">Similarly, it is important to implement internal policies to safeguard keys and change them frequently. </span><span class="koboSpan" id="kobo.858.6">In addition, a system of credentials and privileges must be implemented to ensure </span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.859.1">controlled access by users. </span><span class="koboSpan" id="kobo.859.2">It is good practice today to use methods such as </span><strong class="bold"><span class="koboSpan" id="kobo.860.1">multi-factor authentication</span></strong><span class="koboSpan" id="kobo.861.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.862.1">MFA</span></strong><span class="koboSpan" id="kobo.863.1">), strong password rules, and policies for access from multiple devices. </span><span class="koboSpan" id="kobo.863.2">Again, an important part of this is continuous monitoring of potential breakage, incident reporting, and policies if they occur. </span><span class="koboSpan" id="kobo.863.3">Before deployment, it is essential to conduct testing of the system and its robustness to identify </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">potential vulnerabilities.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.865.1">Privacy</span></strong><span class="koboSpan" id="kobo.866.1"> is a crucial and increasingly sensitive topic today. </span><span class="koboSpan" id="kobo.866.2">It is important that the system complies with key </span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.867.1">regulations such as the </span><strong class="bold"><span class="koboSpan" id="kobo.868.1">General Data Protection Regulation</span></strong><span class="koboSpan" id="kobo.869.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.870.1">GDPR</span></strong><span class="koboSpan" id="kobo.871.1">) and the </span><strong class="bold"><span class="koboSpan" id="kobo.872.1">California Consumer Privacy Act</span></strong><span class="koboSpan" id="kobo.873.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.874.1">CCPA</span></strong><span class="koboSpan" id="kobo.875.1">). </span><span class="koboSpan" id="kobo.875.2">Especially </span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.876.1">when handling large amounts of personal data, violations of these regulations expose an organization to hefty fines. </span><span class="koboSpan" id="kobo.876.2">To avoid penalties, it is a good idea to implement robust data governance, tracking practices, and data management. </span><span class="koboSpan" id="kobo.876.3">There are also techniques that can be used to improve system privacy, such as differential privacy and secure multi-party computation. </span><span class="koboSpan" id="kobo.876.4">In addition, incidents should be tracked and there should be policies for handling problems and </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">resolving them.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<span class="koboSpan" id="kobo.878.1"><img alt="Figure 6.20 – The RAG system and potential risks (https://arxiv.org/pdf/2402.16893)" src="image/B21257_06_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.879.1">Figure 6.20 – The RAG system and potential risks (</span><a href="https://arxiv.org/pdf/2402.16893"><span class="koboSpan" id="kobo.880.1">https://arxiv.org/pdf/2402.16893</span></a><span class="koboSpan" id="kobo.881.1">)</span></p>
<p><span class="koboSpan" id="kobo.882.1">Then, there are </span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.883.1">several security problems today that are specific to RAG systems. </span><span class="koboSpan" id="kobo.883.2">For example, vectors might look like simple numbers but in fact can be converted back into text. </span><span class="koboSpan" id="kobo.883.3">The embedding process can be seen as lossy, but that doesn’t mean it can’t be decoded into the original text. </span><span class="koboSpan" id="kobo.883.4">In theory, embedding vectors should only maintain the semantic meaning of the original text, thus protecting sensitive data. </span><span class="koboSpan" id="kobo.883.5">In fact, in some studies, they have been able to recover more than 70% of the words in the original text. </span><span class="koboSpan" id="kobo.883.6">Moreover, extremely sophisticated techniques are not necessary. </span><span class="koboSpan" id="kobo.883.7">In what are </span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.884.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.885.1">embedding inversion attacks</span></strong><span class="koboSpan" id="kobo.886.1">, you acquire the vectors and then decode them into the original text. </span><span class="koboSpan" id="kobo.886.2">In other words, contrary to popular belief, you can reconstruct text from vectors, and so these vectors should be protected as well. </span><span class="koboSpan" id="kobo.886.3">In addition, any system that includes an LLM is susceptible</span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.887.1"> to </span><strong class="bold"><span class="koboSpan" id="kobo.888.1">prompt injection attacks</span></strong><span class="koboSpan" id="kobo.889.1">. </span><span class="koboSpan" id="kobo.889.2">This is a type of attack in what looks like a legitimate prompt where malicious instructions are added. </span><span class="koboSpan" id="kobo.889.3">This could be to prompt the model to leak information. </span><span class="koboSpan" id="kobo.889.4">Prompt injection is one of the greatest risks to models, and often, new methods are described in the literature, so all previous precautions quickly become obsolete. </span><span class="koboSpan" id="kobo.889.5">In addition, particular prompts can induce outputs that are not expected by RAG. </span><span class="koboSpan" id="kobo.889.6">Adversarial prefixes are prefixes added to what is a prompt for RAG and can induce the generation of hallucinations and factual </span><span class="No-Break"><span class="koboSpan" id="kobo.890.1">incorrect outputs.</span></span></p>
<p><span class="koboSpan" id="kobo.891.1">Another type of attack</span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.892.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.893.1">poisoning RAG</span></strong><span class="koboSpan" id="kobo.894.1">, in which an attempt is made to enter erroneous data that will then be used by the LLM to generate skewed outputs. </span><span class="koboSpan" id="kobo.894.2">For example, to generate misinformation, we can craft target text that when injected will cause the system to generate a desired output. </span><span class="koboSpan" id="kobo.894.3">In the example in the figure, we inject text to poison RAG to influence the answer to </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">a question.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<span class="koboSpan" id="kobo.896.1"><img alt="Figure 6.21 – Overview of poisoned RAG (https://arxiv.org/pdf/2402.07867)" src="image/B21257_06_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.897.1">Figure 6.21 – Overview of poisoned RAG (</span><a href="https://arxiv.org/pdf/2402.07867"><span class="koboSpan" id="kobo.898.1">https://arxiv.org/pdf/2402.07867</span></a><span class="koboSpan" id="kobo.899.1">)</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.900.1">Membership inference attacks</span></strong><span class="koboSpan" id="kobo.901.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.902.1">MIAs</span></strong><span class="koboSpan" id="kobo.903.1">) are </span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.904.1">another type of attack in which an attempt is made to</span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.905.1"> infer whether certain data is present within a dataset. </span><span class="koboSpan" id="kobo.905.2">If a sample resides in the RAG dataset, it will probably be found for a particular query and inserted into the context of an LLM. </span><span class="koboSpan" id="kobo.905.3">With an MIA, we can know if a piece of data is present in the system and then try to extract it with prompt injection (e.g., by making LLM output the retrieved </span><span class="No-Break"><span class="koboSpan" id="kobo.906.1">context ).</span></span></p>
<p><span class="koboSpan" id="kobo.907.1">That is why there are specific solutions for the RAG (or for LLMs in general). </span><span class="koboSpan" id="kobo.907.2">One example is </span><strong class="bold"><span class="koboSpan" id="kobo.908.1">NeMo Guardrails</span></strong><span class="koboSpan" id="kobo.909.1">, which</span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.910.1"> is an open source toolkit developed by NVIDIA to add programmable rails to LLM-based applications. </span><span class="koboSpan" id="kobo.910.2">These rails provide a mechanism to control the LLM output of a model (so we act directly at the generation level). </span><span class="koboSpan" id="kobo.910.3">In this way, we can provide constraints (not engaging in harmful topics, following a path during dialog, not responding to certain requests, using a certain language, and so on). </span><span class="koboSpan" id="kobo.910.4">The advantage of this approach over other embedded techniques (such as model alignment at training) is that it happens at runtime and we do not have to conduct additional training for the model. </span><span class="koboSpan" id="kobo.910.5">This approach is also model agnostic and, generally, these rails are interpretable (during alignment, we should analyze the dataset used for training). </span><span class="koboSpan" id="kobo.910.6">NeMo Guardrails implements user-defined programmable rails via an interpretable language (called Colang) that allows us to define behavior rules </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">for LLMs.</span></span></p>
<p><span class="koboSpan" id="kobo.912.1">With this toolkit, we can use different types of guardrails: input rails (reject input, conduct further processing, or modify the input, to avoid leakage of sensitive information), output rails (refuse to produce outputs in case of problematic content), retrieval rails (reject chunks and thus do not put them in the context for LLM, or alter present chunks), or dialog rails (decide whether to perform an action, use the LLM for a next step, or use a </span><a id="_idIndexMarker738"/><span class="No-Break"><span class="koboSpan" id="kobo.913.1">default response).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<span class="koboSpan" id="kobo.914.1"><img alt="Figure 6.22 – Programmable versus embedded rails for LLMs (https://arxiv.org/abs/2310.10501)" src="image/B21257_06_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.915.1">Figure 6.22 – Programmable versus embedded rails for LLMs (</span><a href="https://arxiv.org/abs/2310.10501"><span class="koboSpan" id="kobo.916.1">https://arxiv.org/abs/2310.10501</span></a><span class="koboSpan" id="kobo.917.1">)</span></p>
<p><span class="koboSpan" id="kobo.918.1">Llama Guard, on the other hand, is a system designed to examine input (via prompt classification) and output (via response classification) and judge whether the text is safe or unsafe. </span><span class="koboSpan" id="kobo.918.2">This approach then uses Llama 2 for classification and then uses a specifically adapted LLM as </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">the judge.</span></span></p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.920.1">Open questions and future perspectives</span></h1>
<p><span class="koboSpan" id="kobo.921.1">Although there have been significant advances in RAG technology, there are still challenges. </span><span class="koboSpan" id="kobo.921.2">In this section, we will discuss these challenges </span><span class="No-Break"><span class="koboSpan" id="kobo.922.1">and prospects.</span></span></p>
<p><span class="koboSpan" id="kobo.923.1">Recently, there has been wide interest and discussion about the expansion of the context length of LLMs. </span><span class="koboSpan" id="kobo.923.2">Today, most of the best-performing LLMs have a context length of more than 100K tokens (some up to over 1 million). </span><span class="koboSpan" id="kobo.923.3">This capability means that a model has the capacity for long document question-answering (in other words, the ability to insert long documents such as books within a single prompt). </span><span class="koboSpan" id="kobo.923.4">Many small user cases can be covered by a context length of 1 to 10 million tokens. </span><span class="koboSpan" id="kobo.923.5">The</span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.924.1"> advantage of a </span><strong class="bold"><span class="koboSpan" id="kobo.925.1">long-context LLM </span></strong><span class="koboSpan" id="kobo.926.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.927.1">LC-LLM</span></strong><span class="koboSpan" id="kobo.928.1">) is that it can then conduct interleaved retrieval and generation of the information in the prompt and conduct one-shot reasoning over the entire document. </span><span class="koboSpan" id="kobo.928.2">Especially for summarization tasks, the LC-LLM has a competitive advantage because it can conduct a scan of the whole document and relate information present at the top and bottom of the document. </span><span class="koboSpan" id="kobo.928.3">For some, LC-LLM means that the RAG is doomed </span><span class="No-Break"><span class="koboSpan" id="kobo.929.1">to disappear.</span></span></p>
<p><span class="koboSpan" id="kobo.930.1">In reality, the LC-LLM does not compete with RAG, and RAG is not doomed to disappear in the short term. </span><span class="koboSpan" id="kobo.930.2">The LC-LLM does not use the whole framework efficiently. </span><span class="koboSpan" id="kobo.930.3">In particular, the information in the middle of the context is attended much less efficiently. </span><span class="koboSpan" id="kobo.930.4">Similarly, reasoning is impacted by irrelevant information, and a long prompt inevitably provides an unnecessary amount of detail to answer a query. </span><span class="koboSpan" id="kobo.930.5">The LC-LLM hallucinates much more than RAG, and the latter allows for reference checking (which documents were used, thus making the retrieval and reasoning process observable and transparent). </span><span class="koboSpan" id="kobo.930.6">The LC-LLM also has difficulty with structured data (which is most data in many industries) and has a fairly considerable cost (latency increases significantly with a long prompt and also the cost per query). </span><span class="koboSpan" id="kobo.930.7">Finally, 1 million tokens are not a lot when considering the amount of data that even a small organization has (so retrieval is </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">always necessary).</span></span></p>
<p><span class="koboSpan" id="kobo.932.1">The LC-LLM opens up exciting possibilities for developers. </span><span class="koboSpan" id="kobo.932.2">First, it means that a precise chunking strategy will be necessary much less frequently. </span><span class="koboSpan" id="kobo.932.3">Chunks can be much larger (up to a document per chunk or at least a group of pages). </span><span class="koboSpan" id="kobo.932.4">This will mean less need to balance granularity and performance. </span><span class="koboSpan" id="kobo.932.5">Second, less prompt engineering will be needed. </span><span class="koboSpan" id="kobo.932.6">Especially for reasoning tasks, some questions can be answered with the information in one chunk, but others require deep analysis among several sections or multiple documents. </span><span class="koboSpan" id="kobo.932.7">Instead of a complex CoT, it is possible to answer these questions with a single prompt. </span><span class="koboSpan" id="kobo.932.8">Third, summarization is easier with the LC-LLM, so it can be conducted with a single retrieval. </span><span class="koboSpan" id="kobo.932.9">Finally, the LC-LLM allows for better customization and interaction with the user. </span><span class="koboSpan" id="kobo.932.10">In such a long prompt, it will be possible to upload the entire chat with the user. </span><span class="koboSpan" id="kobo.932.11">There are still some open challenges, though, especially in retrieving documents for </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">the LC-LLM.</span></span></p>
<p><span class="koboSpan" id="kobo.934.1">Similarly, there are</span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.935.1"> no embedding models today that can handle similar context lengths (currently, the maximum context length of an embedder is 32K). </span><span class="koboSpan" id="kobo.935.2">Therefore, even with an LC-LLM, the chunks cannot be larger than 32K. </span><span class="koboSpan" id="kobo.935.3">The LC-LLM is still expensive in terms of performance and can seriously impact the scalability of the system. </span><span class="koboSpan" id="kobo.935.4">In any case, there are already potential RAG variations being studied that take the LC-LLM into account – for example, adapting small-to-big retrieval in which you find the necessary chunks and then send the entire document associated with the LC-LLM, or conduct routing of a query to pipeline whole-document retrieval (such as whole-document summarization tasks) or to find chunks (specific questions or multi-part questions that require chunks of different documents). </span><span class="koboSpan" id="kobo.935.5">Many companies work with KV caching, which is an approach in which you store the activations from the key and query from an attention layer (so you don’t have to recompute the entire activations for a sequence during generation). </span><span class="koboSpan" id="kobo.935.6">So, it has been proposed that RAG could also be used to find </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">the cache</span></span></p>
<p><span class="koboSpan" id="kobo.937.1">We can see these possible evolutions visually in the </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<span class="koboSpan" id="kobo.939.1"><img alt="Figure 6.23 – Possible evolution of RAG with the LC-LLM" src="image/B21257_06_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.940.1">Figure 6.23 – Possible evolution of RAG with the LC-LLM</span></p>
<ul>
<li><span class="koboSpan" id="kobo.941.1">A. </span><span class="koboSpan" id="kobo.941.2">Retrieving first the chunks and then the </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">associated documents</span></span></li>
<li><span class="koboSpan" id="kobo.943.1">B. </span><span class="koboSpan" id="kobo.943.2">Router deciding whether it is necessary to retrieve small chunks or </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">whole documents</span></span></li>
<li><span class="koboSpan" id="kobo.945.1">C. </span><span class="koboSpan" id="kobo.945.2">Retrieving the document and then KV caching them for </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">the LC-LLM</span></span></li>
</ul>
<p><strong class="bold"><span class="koboSpan" id="kobo.947.1">Multimodal RAG</span></strong><span class="koboSpan" id="kobo.948.1"> is an </span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.949.1">exciting prospect and challenge that has been discussed. </span><span class="koboSpan" id="kobo.949.2">Most organizations have not only textual data but also extensive amounts of data in other modalities (images, audio, video, and so on). </span><span class="koboSpan" id="kobo.949.3">In addition, many files may contain more than one modality (for example, a book that contains not only text but also images). </span><span class="koboSpan" id="kobo.949.4">Searching for multimodal data can be of particular interest in different contexts and different applications. </span><span class="koboSpan" id="kobo.949.5">On the other hand, multimodal RAG is complicated by the fact that each modality has its own challenges. </span><span class="koboSpan" id="kobo.949.6">There are some alternatives to how we can achieve multimodal RAG. </span><span class="koboSpan" id="kobo.949.7">We will see three </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">possible strategies:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.951.1">Embed all modalities into the same vector space</span></strong><span class="koboSpan" id="kobo.952.1">: We previously saw the case of CLIP in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.953.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.954.1"> (a model trained by contrastive learning to achieve unique embedding for images and text), which allowed us to search both images and text. </span><span class="koboSpan" id="kobo.954.2">We can use a model such as CLIP to conduct embedding of all modalities (in this case, images and text, but other cross-modal models exist). </span><span class="koboSpan" id="kobo.954.3">We can then find both images and text and use a multimodal model for generation (for example, we can use BLIP2 or BLIP3 as a vision language model). </span><span class="koboSpan" id="kobo.954.4">A multimodal model can conduct reasoning about both images and text. </span><span class="koboSpan" id="kobo.954.5">This approach has the advantage that we only need to change the embedding model to our system. </span><span class="koboSpan" id="kobo.954.6">In addition, a multimodal model can conduct reasoning by exploiting the information in both the image and the text. </span><span class="koboSpan" id="kobo.954.7">For example, if we have a PDF with tables, we can find the chunk of interest and the associated graphs. </span><span class="koboSpan" id="kobo.954.8">The model can use the information contained in both modalities to be able to answer the query more effectively. </span><span class="koboSpan" id="kobo.954.9">The disadvantage is that CLIP is an expensive model, and </span><strong class="bold"><span class="koboSpan" id="kobo.955.1">multimodal LLMs</span></strong><span class="koboSpan" id="kobo.956.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.957.1">MMLLMs</span></strong><span class="koboSpan" id="kobo.958.1">) are </span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.959.1">more expensive than text-only LLMs. </span><span class="koboSpan" id="kobo.959.2">Also, we need to be sure that our embedding model is capable of capturing all the nuances of images </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">and text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.961.1">Single-grounded modality</span></strong><span class="koboSpan" id="kobo.962.1">: Another option is to transform all modes into the primary mode (which can be different depending on the focus of the application). </span><span class="koboSpan" id="kobo.962.2">For example, we extract text from the PDF and create text descriptions for each of the images along with metadata (for audio, we can use a transcript). </span><span class="koboSpan" id="kobo.962.3">In some variants, we keep the images in storage. </span><span class="koboSpan" id="kobo.962.4">During retrieval, we find the text again (so we use a classic embedding model and a database that contains only vectors obtained from text). </span><span class="koboSpan" id="kobo.962.5">We can then use an LLM or MMLLM (if we want to add the images obtained by retrieving metadata or description) during the</span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.963.1"> generation phase. </span><span class="koboSpan" id="kobo.963.2">Again, the main advantage is that we do not have to train any new type of model, but it can be expensive as an approach, and we lose some nuances from </span><span class="No-Break"><span class="koboSpan" id="kobo.964.1">the image.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.965.1">Separate retrieval for each modality</span></strong><span class="koboSpan" id="kobo.966.1">: In this case, each modality is embedded separately. </span><span class="koboSpan" id="kobo.966.2">For example, if we have three modalities, we will have three separate models (audio-text-aligned model, image-text-aligned model, and text embedder) and three separate databases (audio, images, and text). </span><span class="koboSpan" id="kobo.966.3">When the query arrives, we encode for each mode (so audio, images, and text). </span><span class="koboSpan" id="kobo.966.4">So, in this case, we have done three retrievals and may have found different elements, so it pays to have a rerank step (to efficiently combine the results). </span><span class="koboSpan" id="kobo.966.5">Obviously, we need a dedicated multimodal rerank that can allow us to retrieve the most relevant chunks. </span><span class="koboSpan" id="kobo.966.6">It simplifies the organization because we have dedicated models for each mode (a model that works well for all modes is difficult to obtain) but it increases the complexity of the system. </span><span class="koboSpan" id="kobo.966.7">Similarly, while a classical reranker has to reorder </span><em class="italic"><span class="koboSpan" id="kobo.967.1">n</span></em><span class="koboSpan" id="kobo.968.1"> chunks, a multimodal reranker has the complexity of reordering </span><em class="italic"><span class="koboSpan" id="kobo.969.1">m</span></em><span class="koboSpan" id="kobo.970.1"> x </span><em class="italic"><span class="koboSpan" id="kobo.971.1">n</span></em><span class="koboSpan" id="kobo.972.1"> chunks (where </span><em class="italic"><span class="koboSpan" id="kobo.973.1">m</span></em><span class="koboSpan" id="kobo.974.1"> is the number </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">of modes).</span></span><p class="list-inset"><span class="koboSpan" id="kobo.976.1">Finally, once the multimodal chunks have been obtained, there may be alternatives; for example, we can use an MMLM to generate a response, and then this response needs to be integrated into the context for a final LLM. </span><span class="koboSpan" id="kobo.976.2">As we saw earlier, our RAG pipeline can be more sophisticated than naïve RAG. </span><span class="koboSpan" id="kobo.976.3">We can then combine all the </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.977.1">elements we saw earlier into a </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">single system.</span></span></p></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer178">
<span class="koboSpan" id="kobo.979.1"><img alt="Figure 6.24 – Three potential approaches to multimodal RAG" src="image/B21257_06_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.980.1">Figure 6.24 – Three potential approaches to multimodal RAG</span></p>
<p><span class="koboSpan" id="kobo.981.1">Although </span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.982.1">RAG efficiently mitigates hallucinations, they can happen. </span><span class="koboSpan" id="kobo.982.2">We have previously discussed hallucinations as a plague of LLMs. </span><span class="koboSpan" id="kobo.982.3">In this section, we will mainly discuss hallucinations in RAG. </span><span class="koboSpan" id="kobo.982.4">One of the most peculiar cases</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.983.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.984.1">contextual hallucinations</span></strong><span class="koboSpan" id="kobo.985.1">, in which the correct facts are provided in the context, but the LLM still generates the wrong output. </span><span class="koboSpan" id="kobo.985.2">Although the model provides the correct information, it produces a wrong answer (this often occurs in tasks such as summarization or document-based questions). </span><span class="koboSpan" id="kobo.985.3">This occurs because the LLM has its own prior knowledge, and it is wrong to assume that the model does not use this internal knowledge. </span><span class="koboSpan" id="kobo.985.4">Furthermore, the model is instruction-tuned or otherwise aligned, so it implicitly makes a decision on whether to use the context or ignore it and use its knowledge to answer the user’s question. </span><span class="koboSpan" id="kobo.985.5">In some cases, this might even be useful, since it could happen that we have found the wrong or misleading context. </span><span class="koboSpan" id="kobo.985.6">In general, for many closed source models, we do not know what they were trained on, though we can monitor their confidence in an answer. </span><span class="koboSpan" id="kobo.985.7">Given a question </span><em class="italic"><span class="koboSpan" id="kobo.986.1">x</span></em><span class="koboSpan" id="kobo.987.1">, the model will respond with an answer </span><em class="italic"><span class="koboSpan" id="kobo.988.1">x</span></em><span class="koboSpan" id="kobo.989.1">. </span><span class="koboSpan" id="kobo.989.2">Depending on its knowledge, this will have a confidence </span><em class="italic"><span class="koboSpan" id="kobo.990.1">c</span></em><span class="koboSpan" id="kobo.991.1"> (which is based on the probability associated with the tokens generated by the model). </span><span class="koboSpan" id="kobo.991.2">Basically, the more confident a model is in its answer, the less prone it will be to changing its answer if the context suggests differently. </span><span class="koboSpan" id="kobo.991.3">An interesting finding is that if the correct answer is slightly different from the LLM’s knowledge, the LLM is likely to change its answer. </span><span class="koboSpan" id="kobo.991.4">In case of a large divergence, the LLM will choose its own answer. </span><span class="koboSpan" id="kobo.991.5">For example, to the question, “What is the maximum dosage of drug x?” </span><span class="koboSpan" id="kobo.991.6">the model may have seen 20 µg in its training. </span><span class="koboSpan" id="kobo.991.7">If the context suggests 30, the LLM will provide 30 as the output; if the context suggests 100, the LLM will state 20. </span><span class="koboSpan" id="kobo.991.8">Larger LLMs are generally more confident and prefer their answer, while smaller models are more willing to use context. </span><span class="koboSpan" id="kobo.991.9">Finally, this behavior can be altered with prompt engineering. </span><span class="koboSpan" id="kobo.991.10">Stricter prompts will force the model to use context, while weaker prompts will push the model to use its </span><span class="No-Break"><span class="koboSpan" id="kobo.992.1">prior knowledge.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<span class="koboSpan" id="kobo.993.1"><img alt="Figure 6.25 – Example of a standard prompt in comparison with a loose or strict prompt (https://arxiv.org/pdf/2404.10198)" src="image/B21257_06_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.994.1">Figure 6.25 – Example of a standard prompt in comparison with a loose or strict prompt (</span><a href="https://arxiv.org/pdf/2404.10198"><span class="koboSpan" id="kobo.995.1">https://arxiv.org/pdf/2404.10198</span></a><span class="koboSpan" id="kobo.996.1">)</span></p>
<p><span class="koboSpan" id="kobo.997.1">Other factors also help reduce hallucinations </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">in RAG:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.999.1">Data quality</span></strong><span class="koboSpan" id="kobo.1000.1">: Data quality has a big</span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.1001.1"> impact on system quality </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">in general.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1003.1">Contextual awareness</span></strong><span class="koboSpan" id="kobo.1004.1">: The LLM may not best understand the user’s intent, or the found context may not be the right one. </span><span class="koboSpan" id="kobo.1004.2">Query rewriting and other components of advanced RAG might be </span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1">the solution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1006.1">Negative rejection</span></strong><span class="koboSpan" id="kobo.1007.1">: When retrieval fails to find the appropriate context for the query, the model attempts to respond anyway, thereby generating hallucinations or incorrect answers. </span><span class="koboSpan" id="kobo.1007.2">This is often the fault of a poorly written query, so it can be improved with components that modify the query (such as HyDE). </span><span class="koboSpan" id="kobo.1007.3">Alternatively, stricter prompts force the LLM to respond only if there </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">is context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1009.1">Reasoning abilities</span></strong><span class="koboSpan" id="kobo.1010.1">: Some queries may require reasoning or are too complex. </span><span class="koboSpan" id="kobo.1010.2">The reasoning limit of the system depends on the LLM; RAG is for finding the context to answer </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">the query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1012.1">Domain mismatch</span></strong><span class="koboSpan" id="kobo.1013.1">: A generalist model will have difficulty with domains that are too technical. </span><span class="koboSpan" id="kobo.1013.2">Fine-tuning the embedder and LLM can be </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">a solution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1015.1">Objective mismatch</span></strong><span class="koboSpan" id="kobo.1016.1">: The goals of the embedder and LLM are not aligned, so today there are systems that try to optimize end-to-end retrieval and generation. </span><span class="koboSpan" id="kobo.1016.2">This can be a solution for complex queries or </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">specialized domains.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1018.1">There are other exciting perspectives. </span><span class="koboSpan" id="kobo.1018.2">For example, there is some work on using reinforcement learning to improve the ability of RAG to respond to complex queries. </span><span class="koboSpan" id="kobo.1018.3">Other research deals with integrating graph research; we will discuss this in more detail in the next chapter. </span><span class="koboSpan" id="kobo.1018.4">In addition, we have assumed so far that the database is static, but in the age of the internet, there is a discussion on how to integrate the internet into RAG (e.g., conducting a hybrid search in an organization’s protected data and also finding context through an internet search). </span><span class="koboSpan" id="kobo.1018.5">This opens up exciting but complex questions, such as whether or not to conduct database updates, how to filter out irrelevant search engine results, and security issues. </span><span class="koboSpan" id="kobo.1018.6">In addition, there are more and more specialized applications of RAG, where the authors focus on creating systems optimized for their field of application (e.g., RAG for math, medicine, biology, and so on). </span><span class="koboSpan" id="kobo.1018.7">All this shows active research into RAG and interest in </span><span class="No-Break"><span class="koboSpan" id="kobo.1019.1">its application.</span></span></p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.1020.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1021.1">In this chapter, we initially discussed what the problems of naïve RAG are. </span><span class="koboSpan" id="kobo.1021.2">This allowed us to see a number of add-ons that can be used to solve the sore points of naïve RAG. </span><span class="koboSpan" id="kobo.1021.3">Using these add-ons is the basis of what is now called the advanced RAG paradigm. </span><span class="koboSpan" id="kobo.1021.4">Over time, the community then moved toward a more flexible and modular structure that is now called </span><span class="No-Break"><span class="koboSpan" id="kobo.1022.1">modular RAG.</span></span></p>
<p><span class="koboSpan" id="kobo.1023.1">We then saw how to scale this structure in the presence of big data. </span><span class="koboSpan" id="kobo.1023.2">Like any LLM-based application, there are computational and cost challenges when you have to take the system from a development environment to a production environment. </span><span class="koboSpan" id="kobo.1023.3">In addition, both LLMs and RAGs can have security and privacy risks. </span><span class="koboSpan" id="kobo.1023.4">These are important points, especially when these products are open to the public. </span><span class="koboSpan" id="kobo.1023.5">Today, there is an increasing focus on compliance and more and more regulations are </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">being considered.</span></span></p>
<p><span class="koboSpan" id="kobo.1025.1">Finally, we saw that some issues remain open, such as the relationship with long-context LLMs or the multimodal extension of these models. </span><span class="koboSpan" id="kobo.1025.2">In addition, there is a delicate balance between retrieval and generation, and we explored potential solutions in case of problems. </span><span class="koboSpan" id="kobo.1025.3">Recently, there has been active research into integration with KGs. </span><span class="koboSpan" id="kobo.1025.4">GraphRAG is often discussed today; in the next chapter, we will discuss what a KG is and the relationship between graphs </span><span class="No-Break"><span class="koboSpan" id="kobo.1026.1">and RAG.</span></span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.1027.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.1028.1">LlamaIndex, </span><em class="italic"><span class="koboSpan" id="kobo.1029.1">Node Postprocessor </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1030.1">Modules</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">: </span></span><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/"><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/</span></span></a></li>
<li><span class="koboSpan" id="kobo.1033.1">Nelson, </span><em class="italic"><span class="koboSpan" id="kobo.1034.1">Lost in the Middle: How Language Models Use Long Contexts</span></em><span class="koboSpan" id="kobo.1035.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">2023: </span></span><a href="https://arxiv.org/abs/2307.03172"><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">https://arxiv.org/abs/2307.03172</span></span></a></li>
<li><span class="koboSpan" id="kobo.1038.1">Jerry Liu, </span><em class="italic"><span class="koboSpan" id="kobo.1039.1">Unifying LLM-powered QA Techniques with Routing Abstractions</span></em><span class="koboSpan" id="kobo.1040.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">2023: </span></span><a href="https://betterprogramming.pub/unifying-llm-powered-qa-techniques-with-routing-abstractions-438e2499a0d0"><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">https://betterprogramming.pub/unifying-llm-powered-qa-techniques-with-routing-abstractions-438e2499a0d0</span></span></a></li>
<li><span class="koboSpan" id="kobo.1043.1">Chevalier, </span><em class="italic"><span class="koboSpan" id="kobo.1044.1">Adapting Language Models to Compress Contexts</span></em><span class="koboSpan" id="kobo.1045.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">2023: </span></span><a href="https://arxiv.org/abs/2305.14788"><span class="No-Break"><span class="koboSpan" id="kobo.1047.1">https://arxiv.org/abs/2305.14788</span></span></a></li>
<li><span class="koboSpan" id="kobo.1048.1">Li, </span><em class="italic"><span class="koboSpan" id="kobo.1049.1">Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering</span></em><span class="koboSpan" id="kobo.1050.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">2023: </span></span><a href="https://arxiv.org/abs/2304.12102"><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">https://arxiv.org/abs/2304.12102</span></span></a></li>
<li><span class="koboSpan" id="kobo.1053.1">Izacard, </span><em class="italic"><span class="koboSpan" id="kobo.1054.1">Distilling Knowledge from Reader to Retriever for Question Answering</span></em><span class="koboSpan" id="kobo.1055.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">2020: </span></span><a href="https://arxiv.org/abs/2012.04584"><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">https://arxiv.org/abs/2012.04584</span></span></a></li>
<li><span class="koboSpan" id="kobo.1058.1">Wang, </span><em class="italic"><span class="koboSpan" id="kobo.1059.1">Searching for Best Practices in Retrieval-Augmented Generation</span></em><span class="koboSpan" id="kobo.1060.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">2024: </span></span><a href="https://arxiv.org/pdf/2407.01219"><span class="No-Break"><span class="koboSpan" id="kobo.1062.1">https://arxiv.org/pdf/2407.01219</span></span></a></li>
<li><span class="koboSpan" id="kobo.1063.1">Li, </span><em class="italic"><span class="koboSpan" id="kobo.1064.1">Retrieval Augmented Generation or Long-Context LLMs? </span><span class="koboSpan" id="kobo.1064.2">A Comprehensive Study and Hybrid Approach</span></em><span class="koboSpan" id="kobo.1065.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">2024: </span></span><a href="https://www.arxiv.org/abs/2407.16833"><span class="No-Break"><span class="koboSpan" id="kobo.1067.1">https://www.arxiv.org/abs/2407.16833</span></span></a></li>
<li><span class="koboSpan" id="kobo.1068.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.1069.1">RAG is Dead, Long Live RAG</span></em><span class="koboSpan" id="kobo.1070.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">2024: </span></span><a href="https://levelup.gitconnected.com/rag-is-dead-long-live-rag-c607e1799199"><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">https://levelup.gitconnected.com/rag-is-dead-long-live-rag-c607e1799199</span></span></a></li>
<li><span class="koboSpan" id="kobo.1073.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.1074.1">War and Peace: A Conflictual Love Between the LLM and RAG</span></em><span class="koboSpan" id="kobo.1075.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1076.1">2024: </span></span><a href="https://ai.plainenglish.io/war-and-peace-a-conflictual-love-between-the-llm-and-rag-78428a5776fb"><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">https://ai.plainenglish.io/war-and-peace-a-conflictual-love-between-the-llm-and-rag-78428a5776fb</span></span></a></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1078.1">jinaai/jina-colbert-v2: </span></span><a href="https://huggingface.co/jinaai/jina-colbert-v2"><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">https://huggingface.co/jinaai/jina-colbert-v2</span></span></a></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1080.1">mix_self_consistency</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">: </span></span><a href="https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/tables/mix_self_consistency/mix_self_consistency.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.1082.1">https://github.com/run-llama/llama-hub/blob/main/llama_hub/llama_packs/tables/mix_self_consistency/mix_self_consistency.ipynb</span></span></a></li>
<li><span class="koboSpan" id="kobo.1083.1">Zeng, </span><em class="italic"><span class="koboSpan" id="kobo.1084.1">The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)</span></em><span class="koboSpan" id="kobo.1085.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">2024: </span></span><a href="https://arxiv.org/abs/2402.16893"><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">https://arxiv.org/abs/2402.16893</span></span></a></li>
<li><span class="koboSpan" id="kobo.1088.1">Xue, </span><em class="italic"><span class="koboSpan" id="kobo.1089.1">BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models</span></em><span class="koboSpan" id="kobo.1090.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">2024: </span></span><a href="https://arxiv.org/abs/2406.00083"><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">https://arxiv.org/abs/2406.00083</span></span></a></li>
<li><span class="koboSpan" id="kobo.1093.1">Chen, </span><em class="italic"><span class="koboSpan" id="kobo.1094.1">Controlling Risk of Retrieval-augmented Generation: A Counterfactual Prompting Framework</span></em><span class="koboSpan" id="kobo.1095.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">2024: </span></span><a href="https://arxiv.org/abs/2409.16146"><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">https://arxiv.org/abs/2409.16146</span></span></a></li>
<li><span class="koboSpan" id="kobo.1098.1">Zhang, </span><em class="italic"><span class="koboSpan" id="kobo.1099.1">HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models</span></em><span class="koboSpan" id="kobo.1100.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1101.1">2024: </span></span><a href="https://arxiv.org/abs/2410.22832"><span class="No-Break"><span class="koboSpan" id="kobo.1102.1">https://arxiv.org/abs/2410.22832</span></span></a></li>
<li><span class="koboSpan" id="kobo.1103.1">Xian, </span><em class="italic"><span class="koboSpan" id="kobo.1104.1">On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains</span></em><span class="koboSpan" id="kobo.1105.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">2024: </span></span><a href="https://arxiv.org/abs/2409.17275v1"><span class="No-Break"><span class="koboSpan" id="kobo.1107.1">https://arxiv.org/abs/2409.17275v1</span></span></a></li>
</ul>
</div>
</body></html>