- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover **topic modeling**, or the classification of
    topics present in a corpus of text. Topic modeling is a very useful technique
    that can give us an idea about which topics appear in a document set. For example,
    topic modeling is used for trend discovery on social media. Also, in many cases,
    it is useful to do topic modeling as part of the preliminary data analysis of
    a dataset to understand which topics appear in it.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different algorithms available to do this. All of them try to
    find similarities between different texts and put them into several clusters.
    These different clusters indicate different topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will learn how to create and use topic models via various techniques with
    the BBC news dataset in this chapter. This dataset has news that falls within
    the following topics: politics, sport, business, tech, and entertainment. Thus,
    we know that in each case, we need to have five topic clusters. This is not going
    to be the case in real-life scenarios, and you will need to estimate the number
    of topic clusters. A good reference on how to do this is *The Hundred-Page Machine
    Learning Book* by Andriy Burkov (p. 112).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: LDA topic modeling with **gensim**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Community detection clustering with SBERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-Means topic modeling with BERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling using BERTopic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using contextualized topic models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will work with the same BBC dataset that we worked with
    in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106). The dataset is located in
    the book GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv)'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also available through Hugging Face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/datasets/SetFit/bbc-news](https://huggingface.co/datasets/SetFit/bbc-news)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is used in this book with permission from the researchers. The
    original paper associated with this dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Derek Greene and Pádraig Cunningham. “Practical Solutions to the Problem of
    Diagonal Dominance in Kernel Document Clustering”, in Proc. 23rd International
    Conference on Machine Learning (ICML’06), 2006.
  prefs: []
  type: TYPE_NORMAL
- en: All rights, including copyright, in the text content of the original articles
    are owned by the BBC.
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure to download all the Python notebooks in the `util` folder on
    GitHub into the `util` folder on your computer. The directory structure on your
    computer should mirror the setup in the GitHub repository. We will be accessing
    files in this directory in several recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: LDA topic modeling with gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation** (**LDA**) is one of the oldest algorithms for
    topic modeling. It is a statistical generative model that calculates the probabilities
    of different words. In general, LDA is a good choice of model for longer texts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use one of the main topic modeling algorithms, LDA, to create a topic
    model for the BBC news texts. We know that the BBC news dataset has five topics:
    tech, politics, business, entertainment, and sport. Thus, we will use five as
    the expected number of clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the `gensim` package, which is part of the poetry environment.
    You can also install the `requirements.txt` file to get the package.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv)
    and should be downloaded to the `data` folder.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An LDA model requires the data to be clean. This means that stopwords and other
    unnecessary tokens need to be removed from the text. This includes digits and
    punctuation. If this step is skipped, topics that center around stopwords, digits,
    or punctuation might appear.
  prefs: []
  type: TYPE_NORMAL
- en: We will load the data, clean it, and preprocess it using the `simple_preprocess`
    function, which is available through the `gensim` package. Then we will create
    the LDA model. Any topic model requires the engineer to estimate the number of
    topics in advance. We will use five, as we know that there are five topics present
    in the data. For more information on how to estimate the number of topics, please
    see the introductory section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the stopwords and the BBC news data, then print the resulting dataframe.
    Here, we use the standard stopword list from NLTK. As we saw in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106),
    in the *Clustering sentences using K-Means: unsupervised text classification*
    recipe, the **said** word is also considered a stopword in this dataset, so we
    must manually add it to the list.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The BBC news dataframe output
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we will create the **clean_text** function. This function removes
    extra whitespace in the first line and digits in the second line from text. It
    then uses the **simple_preprocess** function from **gensim**. The **simple_preprocess**
    function splits the text into a list of tokens, lowercases them, and removes tokens
    that are too long or too short. We then remove stopwords from the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now apply the function to the data. The text column now contains a
    list of words that are in lowercase and no stopwords:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – The processed BBC news output
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use the `gensim.corpora.Dictionary` class to create a mapping
    from a word to its integer ID. This is necessary to then create a bag-of-words
    representation of the text. Then, using this mapping, we create the corpus as
    a bag of words. To learn more about the bag-of-words concept, please see [*Chapter
    3*](B18411_03.xhtml#_idTextAnchor067), *Putting Documents into a Bag of Words*.
    In this recipe, instead of using `sklean`’s `CountVectorizer` class, we will use
    the classes provided by the `gensim` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we will initialize and train the LDA model. We will pass in the
    preprocessed and vectorized data (**corpus**), the word-to-ID mapping (**id_dict**),
    the number of topics, which we initialized to five, the chunk size, and the number
    of passes. The chunk size determines the number of documents used in each training
    chunk, and the number of passes specifies the number of passes through the corpus
    during training. You can experiment with these hyperparameters to see whether
    they improve the model. The parameters used here, 100 documents per chunk and
    20 passes, were chosen experimentally to produce a good model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will vary. Our output looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Our LDA output
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s save the model and apply it to novel input:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the **save_model** function. To save the model, we need the model itself,
    the path where we want to save it, the vectorizer (**id_dict**), the path where
    we want to save the vectorizer, the corpus, and the path where the corpus will
    be saved. The function will save these three components to their corresponding
    paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the model, the word-to-ID dictionary, and the vectorized corpus by using
    the function we just defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the saved model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a new example for testing. This example is on the topic of sports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will preprocess the text using the same **clean_text** function,
    then convert it into a bag-of-words vector and run it through the model. The prediction
    is a list of tuples, where the first element in each tuple is the number of the
    topic and the second element is the probability that this text belongs to this
    particular topic. In this example, we can see that topic 1 is the most probable,
    which is sport, and is the correct identification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will vary. Our results look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Community detection clustering with SBERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the community detection algorithm included with
    the **SentenceTransformers** (**SBERT**) package. SBERT will allow us to easily
    encode sentences using the BERT model. See the *Using BERT and OpenAI embeddings
    instead of word embeddings* recipe in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067)
    for a more detailed explanation of how to use the sentence transformers.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm is frequently used to find **communities** in social media but
    can also be used for topic modeling. The advantage of this algorithm is that it
    is very fast. It works best on shorter texts, such as texts found on social media.
    It also only discovers the main topics in the document dataset, as opposed to
    LDA, which clusters all available text. One use of the community detection algorithm
    is finding duplicate posts on social media.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the SBERT package in this recipe. It is included in the poetry environment.
    You can also install it together with other packages by installing the `requirements.txt`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will transform the text using the BERT sentence transformers model and then
    use the community detection clustering algorithm on the resulting embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Do the necessary imports. Here, you might need to download the stopwords corpus
    from NLTK. Please see the *Removing stopwords* recipe in [*Chapter 1*](B18411_01.xhtml#_idTextAnchor013)
    for detailed instructions on how to do this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the **language** **utilities** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the BBC data and print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result will look like *Figure 6**.1*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the model and create the embeddings. See the *Using BERT and OpenAI embeddings
    instead of word embeddings* recipe in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067)
    for more information on sentence embeddings. The community detection algorithm
    requires the embeddings to be in the form of tensors; hence, we must set **convert_to_tensor**
    to **True**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will create the clusters. We will specify the threshold for
    similarity to be **0.7** on a scale from 0 to 1\. This will make sure that the
    resulting communities are very similar to each other. The minimum community size
    is 10; that means that a minimum of 10 news articles are required to form a cluster.
    If we want larger, more general clusters, we should use a larger number for the
    minimum community size. A more granular clustering should use a smaller number.
    Any cluster with fewer members will not appear in the output. The result is a
    list of lists, where each inner list represents a cluster and lists the row IDs
    of cluster members in the original dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will vary and might look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will define a function that prints the most common words by cluster.
    We will take the clusters created by the community detection algorithm and the
    original dataframe. For each cluster, we will first select the sentences that
    represent it and then get the most frequent words by using the **get_most_frequent_words**
    function, which we defined in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106),
    in the *Clustering sentences using K-Means: unsupervised text classification*
    recipe. This function is also located in the **lang_utils** notebook that we ran
    in the second step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the function on the model output (**truncated**). We can see that
    there are many more specific clusters than just the five topics in the original
    BBC dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: K-Means topic modeling with BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the K-Means algorithm to do unsupervised topic classification,
    using the BERT embeddings to encode the data. This recipe shares many commonalities
    with the *Clustering sentences using K-Means – unsupervised text classification*
    recipe in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106).
  prefs: []
  type: TYPE_NORMAL
- en: The K-Means algorithm is used to find similar clusters with any kind of data
    and is an easy way to see trends in the data. It is frequently used while performing
    preliminary data analysis to quickly check the different types of data that appear
    in a dataset. We can use it with text data and encode the data using a sentence
    transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the `sklearn.cluster.KMeans` object to do the unsupervised
    clustering, as well as using HuggingFace `sentence transformers`. Both packages
    are part of the poetry environment.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the recipe, we will load the BBC dataset and encode it by using the sentence
    transformers package. We will then use the K-Means clustering algorithm to create
    five clusters. After that, we will test the model on the test set to see how well
    it would perform on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the language utilities file. This will allow us to reuse the **print_most_common_words_by_cluster**
    function in this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read and print the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result should look like the one in *Figure 6**.1*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we will split the data into **training** and **testing**. We
    limit the testing size to 10% of the whole dataset. The length of the training
    set is 2002 and the length of the test set is 223:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will assign the list of texts to the **documents** variable. We will
    then read in the **all-MiniLM-L6-v2** model to use for the sentence embeddings
    and encode the text data. Next, we will initialize a KMeans model with five clusters,
    setting the **n_init** parameter to **auto**, which determines the number of times
    the algorithm is run. We will also set the **init** parameter to **k-means++**.
    This parameter ensures faster convergence of the algorithm. We will then train
    the initialized model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the most common words by topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will vary; our results look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that the mapping of topics is as follows: 0 is tech, 1 is sport,
    2 is entertainment, 3 is politics, and 4 is business.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can now use the test data to see how well the model performs on unseen data.
    First, we must create a prediction column in the test dataframe and populate it
    with the cluster number for each of the test inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will vary; this is our output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – The result of running K-Means on the test dataframe
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will create a mapping between the cluster number and the topic name,
    which we discovered manually by looking at the most frequent words for each cluster.
    We will then create a column with the predicted topic name for every text in the
    test set by using the mapping and the **prediction** column we created in the
    previous step. Now, we can compare the predictions of the model with the true
    value of the data. We will use the **classification_report** function from **sklearn**
    to get the corresponding statistics. Finally, we will print out the classification
    report for the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The scores are very high – almost perfect. Most of this speaks to the quality
    of the sentence embedding model that we used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a new example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the prediction for the new example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Cluster number 1 corresponds to sport, which is the correct classification.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling using BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the BERTopic package that provides many different
    and versatile tools for topic modeling and visualization. It is especially useful
    if you would like to do different visualizations of the topic clusters created.
    This topic modeling algorithm uses BERT embeddings to encode the data, hence the
    “BERT” in the name. You can learn more about the algorithm and its constituent
    parts at [https://maartengr.github.io/BERTopic/algorithm/algorithm.html](https://maartengr.github.io/BERTopic/algorithm/algorithm.html).
  prefs: []
  type: TYPE_NORMAL
- en: The BERTopic package, by default, uses the HDBSCAN algorithm to create clusters
    from the data in an unsupervised fashion. You can learn more about how the HDBSCAN
    algorithm works at [https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html).
    However, it is also possible to customize the inner workings of a BERTopic object
    to use other algorithms. It is also possible to substitute other custom components
    into its pipeline. In this recipe, we will use the default settings, and you can
    experiment with other components.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting topics are of very high quality. There might be several reasons
    for this. One of them is the result of using BERT embeddings, which we saw in
    [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106), to positively impact the classification
    results
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the BERTopic package to create topic models for the BBC dataset.
    The package is included in the poetry environment and is also part of the `requirements.txt`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will load the BBC dataset and preprocess it again. The preprocessing
    step will involve tokenizing the data and removing stopwords. Then we will create
    the topic model using BERTopic and inspect the results. We will also test the
    topic model on unseen data and use `classification_report` to see the accuracy
    statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the language utilities file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define and amend the stopwords, then read in the BBC data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will preprocess the data. We will first tokenize it using
    the `word_tokenize` method from NLTK as shown in the *Dividing sentences into
    words – tokenization* recipe in [*Chapter 1*](B18411_01.xhtml#_idTextAnchor013).
    Then remove stopwords and finally put the text back together into a string. We
    must do the last step because the BERTopic uses a sentence embedding model, and
    that model requires a string, not a list of words:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will split the dataset into training and test sets, specifying the
    size of the test set to be 10%. As a result, we will get 2002 datapoints for training
    and 223 for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the lists of texts from the dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we willinitialize the **BERTopic** object and then fit it on
    the documents extracted in *step 6*. We will specify the number of topics to be
    six, one more than the five that we are looking for. This is because a key difference
    between BERTopic and other topic modeling algorithms is that it has a special
    **discard** topic numbered -1\. We could also specify a larger number of topics.
    In that case, they would be narrower than the general five categories of business,
    politics, entertainment, tech, and sport:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will print out the information about the resulting topic model. Other
    than the **discard** topic, the topics align well with the gold labels assigned
    by human annotators. The function prints out the most representative words for
    each topic, as well as the most representative documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will vary; here is an example result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we will print the topics. We can see from the words that the zeroth
    topic is sport, the first topic is politics, the second topic is entertainment,
    the third topic is tech, and the fourth topic is business.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18411_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – The topics generated by BERTopic
  prefs: []
  type: TYPE_NORMAL
- en: 'In this step, we will generate the topic labels using the **generate_topic_labels**
    function. We will input the number of words to use for the topic label, the separator
    (in this case, this is an underscore), and whether to include the topic number.
    As a result, we will get a list of topic names. We can see from the resulting
    topics that we could include *would* as a stopword:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will define the **get_prediction** function that gives us the topic
    number for a text input and a corresponding model. The function transforms the
    input text and outputs a tuple of two lists. One is a list of topic numbers and
    the other is the list of probabilities of assigning each topic. The lists are
    sorted in the order of the most probable topic, so we can take the first element
    of the first list as the predicted topic and return it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will define a column for predictions in the test dataframe
    and then use the function we defined in the previous step to get predictions for
    each text in the dataframe. We will then create a mapping of topic numbers to
    gold topic labels that we can use to test the effectiveness of the topic model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will create a new column in the test dataframe to record the predicted
    topic name using the mapping we created. We will then filter the test set to only
    use entries that have not been predicted to be the discard topic -1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The test scores are very high. This is reflective of the encoding model, the
    BERTopic model, which is also a sentence transformer model, as in the previous
    recipe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this step, we will define a new example to test with the model and print
    it. We will use the **iloc** function from the **pandas** package to access the
    first element of the **bbc_test** dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The example is about politics, which should be topic 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Obtain a prediction from the model and print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be a correct prediction:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can find topics that are similar to a particular word, phrase, or sentence.
    This way, we could easily find a topic to which a text corresponds within the
    dataset. We will use a word, a phrase, and a sentence to see how well the model
    can show the corresponding topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find topics most similar to the **sports** word with the corresponding similarity
    scores. Combine the topic numbers and similarity scores in a list of tuples and
    print them. The tuples are a combination of the topic number and the similarity
    score between the text that is passed in and the particular topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The most similar topic is topic 0, which is sport:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat the preceding step for the **business and economics** example phrase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, the most similar topic is topic 4, which is business:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now repeat the same process for the following example sentence: **"YouTube
    removed a snippet of code that publicly disclosed whether a channel receives ad
    payouts, obscuring which creators benefit most from the platform."**. We would
    expect this to be most similar to the tech topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the output, we can see that the most similar topic is topic 3, which is
    tech:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using contextualized topic models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will look at another topic model algorithm: contextualized
    topic models. To produce a more effective topic model, it combines embeddings
    with a bag-of-words document representation.'
  prefs: []
  type: TYPE_NORMAL
- en: We will show you how to use the trained topic model with input in other languages.
    This feature is especially useful because we can create a topic model in one language,
    for example, one that has many resources available, and then apply it on another
    language that does not have as many resources. To achieve this, we will utilize
    a multilingual embedding model in order to encode the data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need the `contextualized-topic-models` package for this recipe. It is
    part of the poetry environment and the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will load the data, then divide it into sentences, preprocess
    it, and use the **gsdmm** model to cluster the sentences into topics. If you would
    like more information about the algorithm, please see the package documentation
    at [https://pypi.org/project/contextualized-topic-models/](https://pypi.org/project/contextualized-topic-models/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Suppress the warnings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the stopwords list and read in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will create the preprocessor object and use it to preprocess
    the documents. The **contextualized-topic-models** package provides different
    preprocessors that prepare the data to be used in the topic model algorithm. This
    preprocessor tokenizes the documents, removes the stopwords, and puts them back
    into a string. It returns the list of preprocessed documents, the list of original
    documents, the dataset vocabulary, and a list of document indices in the original
    dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will create the **TopicModelDataPreparation** object. We will pass
    the embedding model name as the parameter. This is a multilingual model that can
    encode text in various languages with good results. We will then fit it on the
    documents. It uses an embedding model to turn the texts into embeddings and also
    creates a bag-of-words model. The output is a **CTMDataset** object that represents
    the training dataset in the format required by the topic model training algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will create the topic model using the **ZeroShotTM** object.
    The term **zero shot** means that the model has no prior information about the
    documents. We will input the size of the vocabulary for the bag-of-words model,
    the size of the embeddings vector, the number of topics (the **n_components**
    parameter), and the number of epochs to train the model for. We will use five
    topics, since the BBC dataset has that many topics. When you apply this algorithm
    to your data, you will need to experiment with different numbers of topics. Finally,
    we will fit the initialized topic model on the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we will inspect the topics. We can see that they fit well with the golden
    labels. Topic 0 is tech, topic 1 is sport, topic 2 is business, topic 3 is entertainment,
    and topic 4 is politics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will vary; this is the output we get:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18411_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – The contextualized model output
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will initialize a new news piece, this time in Spanish, to see how
    effective the topic model trained on English-language documents will be on a news
    article in a different language. This particular news piece should fall into the
    tech topic. We will preprocess it using the **TopicModelDataPreparation** object.
    To then use the model on the encoded text, we need to create a dataset object.
    That is why we have to include the Spanish news piece in a list and then pass
    it on for data preparation. Finally, we must pass the dataset (that consists of
    only one element) through the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will get the topic distribution for the testing dataset we
    created in the previous step. The result is a list of lists, where each individual
    list represents the probability that a particular text belongs to that topic.
    The probabilities have the same indices in individual lists as the topic numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this case, the highest probability is for topic 0, which is indeed tech:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information about contextualized topic models, see [https://contextualized-topic-models.readthedocs.io/en/latest/index.html](https://contextualized-topic-models.readthedocs.io/en/latest/index.html).
  prefs: []
  type: TYPE_NORMAL
