- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Topic Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: In this chapter, we will cover **topic modeling**, or the classification of
    topics present in a corpus of text. Topic modeling is a very useful technique
    that can give us an idea about which topics appear in a document set. For example,
    topic modeling is used for trend discovery on social media. Also, in many cases,
    it is useful to do topic modeling as part of the preliminary data analysis of
    a dataset to understand which topics appear in it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍**主题建模**，即对文本语料库中存在的主题进行分类。主题建模是一种非常有用的技术，可以让我们了解哪些主题出现在文档集中。例如，主题建模用于社交媒体的趋势发现。此外，在许多情况下，将主题建模作为数据集初步数据分析的一部分是有用的，以了解其中包含哪些主题。
- en: There are many different algorithms available to do this. All of them try to
    find similarities between different texts and put them into several clusters.
    These different clusters indicate different topics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的算法可以完成这项工作。所有这些算法都试图在不同文本之间找到相似性，并将它们放入几个聚类中。这些不同的聚类表示不同的主题。
- en: 'You will learn how to create and use topic models via various techniques with
    the BBC news dataset in this chapter. This dataset has news that falls within
    the following topics: politics, sport, business, tech, and entertainment. Thus,
    we know that in each case, we need to have five topic clusters. This is not going
    to be the case in real-life scenarios, and you will need to estimate the number
    of topic clusters. A good reference on how to do this is *The Hundred-Page Machine
    Learning Book* by Andriy Burkov (p. 112).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何通过各种技术使用BBC新闻数据集创建和使用主题模型。这个数据集包含以下主题的新闻：政治、体育、商业、科技和娱乐。因此，我们知道在每种情况下，我们需要有五个主题聚类。在现实场景中情况可能并非如此，您将需要估计主题聚类的数量。关于如何做到这一点的一个很好的参考是Andriy
    Burkov的《百页机器学习书》（第112页）。
- en: 'This chapter contains the following recipes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下食谱：
- en: LDA topic modeling with **gensim**
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**gensim**进行LDA主题建模
- en: Community detection clustering with SBERT
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SBERT进行社区检测聚类
- en: K-Means topic modeling with BERT
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BERT进行K-Means主题建模
- en: Topic modeling using BERTopic
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用BERTopic进行主题建模
- en: Using contextualized topic models
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用上下文化的主题模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will work with the same BBC dataset that we worked with
    in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106). The dataset is located in
    the book GitHub repository:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用与[*第4章*](B18411_04.xhtml#_idTextAnchor106)中相同的BBC数据集。数据集位于本书的GitHub仓库中：
- en: '[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv)'
- en: 'It is also available through Hugging Face:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 它也通过Hugging Face提供：
- en: '[https://huggingface.co/datasets/SetFit/bbc-news](https://huggingface.co/datasets/SetFit/bbc-news)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/datasets/SetFit/bbc-news](https://huggingface.co/datasets/SetFit/bbc-news)'
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is used in this book with permission from the researchers. The
    original paper associated with this dataset is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用此数据集时已获得研究者的许可。与该数据集相关的原始论文如下：
- en: Derek Greene and Pádraig Cunningham. “Practical Solutions to the Problem of
    Diagonal Dominance in Kernel Document Clustering”, in Proc. 23rd International
    Conference on Machine Learning (ICML’06), 2006.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Derek Greene和Pádraig Cunningham. “在核文档聚类中解决对角优势问题的实用解决方案”，载于第23届国际机器学习会议（ICML’06），2006年。
- en: All rights, including copyright, in the text content of the original articles
    are owned by the BBC.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文章文本内容中的所有权利，包括版权，均归BBC所有。
- en: Please make sure to download all the Python notebooks in the `util` folder on
    GitHub into the `util` folder on your computer. The directory structure on your
    computer should mirror the setup in the GitHub repository. We will be accessing
    files in this directory in several recipes in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保将GitHub上`util`文件夹中的所有Python笔记本下载到您计算机上的`util`文件夹中。您计算机上的目录结构应与GitHub仓库中的设置相匹配。在本章的多个食谱中，我们将访问此目录中的文件。
- en: LDA topic modeling with gensim
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用gensim进行LDA主题建模
- en: '**Latent Dirichlet Allocation** (**LDA**) is one of the oldest algorithms for
    topic modeling. It is a statistical generative model that calculates the probabilities
    of different words. In general, LDA is a good choice of model for longer texts.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）是主题建模中最古老的算法之一。它是一种统计生成模型，用于计算不同单词的概率。一般来说，LDA是较长的文本的好模型选择。'
- en: 'We will use one of the main topic modeling algorithms, LDA, to create a topic
    model for the BBC news texts. We know that the BBC news dataset has five topics:
    tech, politics, business, entertainment, and sport. Thus, we will use five as
    the expected number of clusters.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用主主题建模算法之一，LDA，为BBC新闻文本创建一个主题模型。我们知道BBC新闻数据集有五个主题：科技、政治、商业、娱乐和体育。因此，我们将使用五个作为预期的聚类数量。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be using the `gensim` package, which is part of the poetry environment.
    You can also install the `requirements.txt` file to get the package.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`gensim`包，它是poetry环境的一部分。您也可以安装`requirements.txt`文件以获取该包。
- en: The dataset is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv)
    and should be downloaded to the `data` folder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/bbc-text.csv)，应下载到`data`文件夹。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.1_topic_modeling_gensim.ipynb)。
- en: How to do it...
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: An LDA model requires the data to be clean. This means that stopwords and other
    unnecessary tokens need to be removed from the text. This includes digits and
    punctuation. If this step is skipped, topics that center around stopwords, digits,
    or punctuation might appear.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LDA模型需要数据是干净的。这意味着需要从文本中删除停用词和其他不必要的标记。这包括数字和标点符号。如果跳过此步骤，可能会出现围绕停用词、数字或标点的主题。
- en: We will load the data, clean it, and preprocess it using the `simple_preprocess`
    function, which is available through the `gensim` package. Then we will create
    the LDA model. Any topic model requires the engineer to estimate the number of
    topics in advance. We will use five, as we know that there are five topics present
    in the data. For more information on how to estimate the number of topics, please
    see the introductory section of this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`gensim`包中的`simple_preprocess`函数加载数据，清理它并进行预处理。然后我们将创建LDA模型。任何主题模型都需要工程师提前估计主题的数量。我们将使用五个，因为我们知道数据中存在五个主题。有关如何估计主题数量的更多信息，请参阅本章的介绍部分。
- en: 'The steps are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: 'Do the necessary imports:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE0]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the stopwords and the BBC news data, then print the resulting dataframe.
    Here, we use the standard stopword list from NLTK. As we saw in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106),
    in the *Clustering sentences using K-Means: unsupervised text classification*
    recipe, the **said** word is also considered a stopword in this dataset, so we
    must manually add it to the list.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载停用词和BBC新闻数据，然后打印生成的数据框。在这里，我们使用NLTK的标准停用词列表。正如我们在[*第4章*](B18411_04.xhtml#_idTextAnchor106)中看到的，在*使用K-Means聚类句子：无监督文本分类*配方中，**said**这个词也被视为这个数据集中的停用词，因此我们必须手动将其添加到列表中。
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The result will look similar to this:'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将类似于以下内容：
- en: '![](img/B18411_06_1.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18411_06_1.jpg)'
- en: Figure 6.1 – The BBC news dataframe output
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – BBC新闻数据框输出
- en: 'In this step, we will create the **clean_text** function. This function removes
    extra whitespace in the first line and digits in the second line from text. It
    then uses the **simple_preprocess** function from **gensim**. The **simple_preprocess**
    function splits the text into a list of tokens, lowercases them, and removes tokens
    that are too long or too short. We then remove stopwords from the list:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将创建**clean_text**函数。此函数从文本的第一行删除额外的空白，从第二行删除数字。然后它使用来自**gensim**的**simple_preprocess**函数。**simple_preprocess**函数将文本拆分为一个标记列表，将它们转换为小写，并删除过长或过短的标记。然后我们从列表中删除停用词：
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will now apply the function to the data. The text column now contains a
    list of words that are in lowercase and no stopwords:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将函数应用于数据。文本列现在包含一个单词列表，这些单词都是小写且没有停用词：
- en: '[PRE3]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result will look similar to this:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果看起来会类似于这样：
- en: '![](img/B18411_06_2.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_06_2.jpg)'
- en: Figure 6.2 – The processed BBC news output
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 处理后的BBC新闻输出
- en: 'Here, we will use the `gensim.corpora.Dictionary` class to create a mapping
    from a word to its integer ID. This is necessary to then create a bag-of-words
    representation of the text. Then, using this mapping, we create the corpus as
    a bag of words. To learn more about the bag-of-words concept, please see [*Chapter
    3*](B18411_03.xhtml#_idTextAnchor067), *Putting Documents into a Bag of Words*.
    In this recipe, instead of using `sklean`’s `CountVectorizer` class, we will use
    the classes provided by the `gensim` package:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用`gensim.corpora.Dictionary`类来创建一个从单词到其整数ID的映射。这是为了然后创建文本的词袋表示。然后，使用这个映射，我们创建语料库作为词袋。要了解更多关于词袋概念的信息，请参阅[*第3章*](B18411_03.xhtml#_idTextAnchor067)，*将文档放入词袋中*。在这个配方中，我们不会使用`sklearn`的`CountVectorizer`类，而是使用`gensim`包提供的类：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this step, we will initialize and train the LDA model. We will pass in the
    preprocessed and vectorized data (**corpus**), the word-to-ID mapping (**id_dict**),
    the number of topics, which we initialized to five, the chunk size, and the number
    of passes. The chunk size determines the number of documents used in each training
    chunk, and the number of passes specifies the number of passes through the corpus
    during training. You can experiment with these hyperparameters to see whether
    they improve the model. The parameters used here, 100 documents per chunk and
    20 passes, were chosen experimentally to produce a good model:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将初始化并训练LDA模型。我们将传递预处理和向量化的数据（**corpus**），单词到ID映射（**id_dict**），主题数量，我们将其初始化为五个，块大小和遍历次数。块大小决定了每个训练块中使用的文档数量，遍历次数指定了训练过程中对语料库的遍历次数。您可以尝试这些超参数以查看它们是否可以改进模型。这里使用的参数，每个块100个文档和20次遍历，是通过实验选择的，以产生一个好的模型：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The results will vary. Our output looks like this:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果会有所不同。我们的输出看起来像这样：
- en: '![](img/B18411_06_3.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_06_3.jpg)'
- en: Figure 6.3 – Our LDA output
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 我们的LDA输出
- en: There’s more...
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'Now let’s save the model and apply it to novel input:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们保存模型并将其应用于新的输入：
- en: 'Define the **save_model** function. To save the model, we need the model itself,
    the path where we want to save it, the vectorizer (**id_dict**), the path where
    we want to save the vectorizer, the corpus, and the path where the corpus will
    be saved. The function will save these three components to their corresponding
    paths:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义**save_model**函数。要保存模型，我们需要模型本身、我们想要保存它的路径、向量器（**id_dict**）、我们想要保存向量器的路径、语料库以及语料库将被保存的路径。该函数将把这些三个组件保存到它们对应的路径：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Save the model, the word-to-ID dictionary, and the vectorized corpus by using
    the function we just defined:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们刚刚定义的函数保存模型、单词到ID字典和向量化的语料库：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load the saved model:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载保存的模型：
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define a new example for testing. This example is on the topic of sports:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个新的示例进行测试。这个示例是关于体育主题的：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this step, we will preprocess the text using the same **clean_text** function,
    then convert it into a bag-of-words vector and run it through the model. The prediction
    is a list of tuples, where the first element in each tuple is the number of the
    topic and the second element is the probability that this text belongs to this
    particular topic. In this example, we can see that topic 1 is the most probable,
    which is sport, and is the correct identification:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将使用相同的**clean_text**函数对文本进行预处理，然后将其转换为词袋向量并通过模型运行。预测结果是一个元组列表，其中每个元组的第一个元素是主题编号，第二个元素是文本属于该特定主题的概率。在这个例子中，我们可以看到主题1是最可能的，即体育，这是正确的识别：
- en: '[PRE10]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The results will vary. Our results look like this:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果会有所不同。我们的结果看起来像这样：
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Community detection clustering with SBERT
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SBERT进行社区检测聚类
- en: In this recipe, we will use the community detection algorithm included with
    the **SentenceTransformers** (**SBERT**) package. SBERT will allow us to easily
    encode sentences using the BERT model. See the *Using BERT and OpenAI embeddings
    instead of word embeddings* recipe in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067)
    for a more detailed explanation of how to use the sentence transformers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用**SentenceTransformers**（**SBERT**）包中包含的社区检测算法。SBERT将使我们能够轻松地使用BERT模型对句子进行编码。请参阅[*第3章*](B18411_03.xhtml#_idTextAnchor067)中的*使用BERT和OpenAI嵌入代替词嵌入*配方，以获取有关如何使用句子转换器的更详细解释。
- en: This algorithm is frequently used to find **communities** in social media but
    can also be used for topic modeling. The advantage of this algorithm is that it
    is very fast. It works best on shorter texts, such as texts found on social media.
    It also only discovers the main topics in the document dataset, as opposed to
    LDA, which clusters all available text. One use of the community detection algorithm
    is finding duplicate posts on social media.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法常用于在社交媒体中寻找**社区**，但也可以用于主题建模。此算法的优点是它非常快。它最适合较短的文本，例如在社交媒体上找到的文本。它还只发现文档数据集中的主要主题，而LDA则将所有可用文本进行聚类。社区检测算法的一个用途是寻找社交媒体上的重复帖子。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the SBERT package in this recipe. It is included in the poetry environment.
    You can also install it together with other packages by installing the `requirements.txt`
    file.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用SBERT包。它包含在poetry环境中。你也可以通过安装`requirements.txt`文件来一起安装其他包。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.2_community_detection.ipynb)。
- en: How to do it...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We will transform the text using the BERT sentence transformers model and then
    use the community detection clustering algorithm on the resulting embeddings.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用BERT句子转换器模型对文本进行转换，然后对生成的嵌入应用社区检测聚类算法。
- en: Do the necessary imports. Here, you might need to download the stopwords corpus
    from NLTK. Please see the *Removing stopwords* recipe in [*Chapter 1*](B18411_01.xhtml#_idTextAnchor013)
    for detailed instructions on how to do this.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入。在这里，你可能需要从NLTK下载停用词语料库。请参阅[*第1章*](B18411_01.xhtml#_idTextAnchor013)中的*移除停用词*配方，以获取有关如何操作的详细说明。
- en: '[PRE12]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Run the **language** **utilities** file:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行**语言****工具**文件：
- en: '[PRE13]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Load the BBC data and print it:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载BBC数据并打印：
- en: '[PRE14]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result will look like *Figure 6**.1*.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将看起来像*图6**.1*。
- en: 'Load the model and create the embeddings. See the *Using BERT and OpenAI embeddings
    instead of word embeddings* recipe in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067)
    for more information on sentence embeddings. The community detection algorithm
    requires the embeddings to be in the form of tensors; hence, we must set **convert_to_tensor**
    to **True**:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型并创建嵌入。请参阅[*第3章*](B18411_03.xhtml#_idTextAnchor067)中的*使用BERT和OpenAI嵌入代替词嵌入*配方，以获取有关句子嵌入的更多信息。社区检测算法需要嵌入以张量的形式存在；因此，我们必须将**convert_to_tensor**设置为**True**：
- en: '[PRE15]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In this step, we will create the clusters. We will specify the threshold for
    similarity to be **0.7** on a scale from 0 to 1\. This will make sure that the
    resulting communities are very similar to each other. The minimum community size
    is 10; that means that a minimum of 10 news articles are required to form a cluster.
    If we want larger, more general clusters, we should use a larger number for the
    minimum community size. A more granular clustering should use a smaller number.
    Any cluster with fewer members will not appear in the output. The result is a
    list of lists, where each inner list represents a cluster and lists the row IDs
    of cluster members in the original dataframe:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将创建聚类。我们将指定相似度的阈值，在0到1的范围内为**0.7**。这将确保生成的社区彼此非常相似。最小社区大小为10；这意味着至少需要10篇新闻文章来形成一个聚类。如果我们想要更大、更通用的聚类，我们应该使用更大的最小社区大小。更细粒度的聚类应使用更小的数字。任何成员数量较少的聚类将不会出现在输出中。结果是列表的列表，其中每个内部列表代表一个聚类，并列出原始数据框中聚类成员的行ID：
- en: '[PRE16]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result will vary and might look like this:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能会有所不同，可能看起来像这样：
- en: '[PRE17]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we will define a function that prints the most common words by cluster.
    We will take the clusters created by the community detection algorithm and the
    original dataframe. For each cluster, we will first select the sentences that
    represent it and then get the most frequent words by using the **get_most_frequent_words**
    function, which we defined in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106),
    in the *Clustering sentences using K-Means: unsupervised text classification*
    recipe. This function is also located in the **lang_utils** notebook that we ran
    in the second step:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将定义一个函数，用于按簇打印最常见的单词。我们将使用社区检测算法创建的簇和原始数据框。对于每个簇，我们首先选择代表它的句子，然后使用 **get_most_frequent_words**
    函数获取最常见的单词，该函数我们在 [*第4章*](B18411_04.xhtml#_idTextAnchor106) 的 *使用 K-Means 对句子进行聚类：无监督文本分类*
    菜谱中定义。此函数也位于我们在第二步中运行的 **lang_utils** 笔记本中：
- en: '[PRE18]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, use the function on the model output (**truncated**). We can see that
    there are many more specific clusters than just the five topics in the original
    BBC dataset:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用该函数对模型输出（**截断**）进行处理。我们可以看到，与原始 BBC 数据集中的五个主题相比，有更多具体的簇：
- en: '[PRE19]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: K-Means topic modeling with BERT
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BERT 进行 K-Means 主题建模
- en: In this recipe, we will use the K-Means algorithm to do unsupervised topic classification,
    using the BERT embeddings to encode the data. This recipe shares many commonalities
    with the *Clustering sentences using K-Means – unsupervised text classification*
    recipe in [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用 K-Means 算法进行无监督主题分类，使用 BERT 嵌入对数据进行编码。这个菜谱与 [*第4章*](B18411_04.xhtml#_idTextAnchor106)
    中的 *使用 K-Means 对句子进行聚类 – 无监督文本分类* 菜谱有许多相似之处。
- en: The K-Means algorithm is used to find similar clusters with any kind of data
    and is an easy way to see trends in the data. It is frequently used while performing
    preliminary data analysis to quickly check the different types of data that appear
    in a dataset. We can use it with text data and encode the data using a sentence
    transformer model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means 算法用于找到任何类型数据的相似簇，并且是查看数据趋势的一种简单方法。在进行初步数据分析时，它经常被用来快速检查数据集中出现的数据类型。我们可以使用它与文本数据，并使用句子转换器模型对数据进行编码。
- en: Getting ready
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be using the `sklearn.cluster.KMeans` object to do the unsupervised
    clustering, as well as using HuggingFace `sentence transformers`. Both packages
    are part of the poetry environment.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `sklearn.cluster.KMeans` 对象进行无监督聚类，以及使用 HuggingFace `sentence transformers`。这两个包都是
    poetry 环境的一部分。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb)。
- en: How to do it...
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the recipe, we will load the BBC dataset and encode it by using the sentence
    transformers package. We will then use the K-Means clustering algorithm to create
    five clusters. After that, we will test the model on the test set to see how well
    it would perform on unseen data:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将加载 BBC 数据集，并使用句子转换器包对其进行编码。然后，我们将使用 K-Means 聚类算法创建五个簇。之后，我们将对测试集进行模型测试，以查看它在未见数据上的表现如何：
- en: 'Do the necessary imports:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE20]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the language utilities file. This will allow us to reuse the **print_most_common_words_by_cluster**
    function in this recipe:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行语言工具文件。这将使我们能够在这个菜谱中重用 **print_most_common_words_by_cluster** 函数：
- en: '[PRE21]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Read and print the data:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取并打印数据：
- en: '[PRE22]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The result should look like the one in *Figure 6**.1*.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像 *图6**.1* 中的那样。
- en: 'In this step, we will split the data into **training** and **testing**. We
    limit the testing size to 10% of the whole dataset. The length of the training
    set is 2002 and the length of the test set is 223:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将数据分为 **训练** 和 **测试**。我们将测试集的大小限制为整个数据集的 10%。训练集的长度为 2002，测试集的长度为 223：
- en: '[PRE23]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result will be as follows:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE24]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, we will assign the list of texts to the **documents** variable. We will
    then read in the **all-MiniLM-L6-v2** model to use for the sentence embeddings
    and encode the text data. Next, we will initialize a KMeans model with five clusters,
    setting the **n_init** parameter to **auto**, which determines the number of times
    the algorithm is run. We will also set the **init** parameter to **k-means++**.
    This parameter ensures faster convergence of the algorithm. We will then train
    the initialized model:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将把文本列表分配给**documents**变量。然后，我们将读取**all-MiniLM-L6-v2**模型用于句子嵌入并编码文本数据。接下来，我们将初始化一个具有五个聚类的KMeans模型，将**n_init**参数设置为**auto**，这决定了算法运行的次数。我们还将**init**参数设置为**k-means++**。此参数确保算法更快地收敛。然后我们将训练初始化后的模型：
- en: '[PRE25]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Print out the most common words by topic:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按主题打印出最常见的单词：
- en: '[PRE26]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The results will vary; our results look like this:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能会有所不同；我们的结果如下所示：
- en: '[PRE27]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can see that the mapping of topics is as follows: 0 is tech, 1 is sport,
    2 is entertainment, 3 is politics, and 4 is business.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，主题的映射如下：0是技术，1是体育，2是娱乐，3是政治，4是商业。
- en: 'We can now use the test data to see how well the model performs on unseen data.
    First, we must create a prediction column in the test dataframe and populate it
    with the cluster number for each of the test inputs:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用测试数据来查看模型在未见数据上的表现如何。首先，我们必须在测试数据框中创建一个预测列，并用每个测试输入的聚类编号填充它：
- en: '[PRE28]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The results will vary; this is our output:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能会有所不同；这是我们的输出：
- en: '![](img/B18411_06_5.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_06_5.jpg)'
- en: Figure 6.4 – The result of running K-Means on the test dataframe
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 在测试数据框上运行K-Means的结果
- en: 'Now, we will create a mapping between the cluster number and the topic name,
    which we discovered manually by looking at the most frequent words for each cluster.
    We will then create a column with the predicted topic name for every text in the
    test set by using the mapping and the **prediction** column we created in the
    previous step. Now, we can compare the predictions of the model with the true
    value of the data. We will use the **classification_report** function from **sklearn**
    to get the corresponding statistics. Finally, we will print out the classification
    report for the predictions:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个聚类编号和主题名称之间的映射，这是我们通过查看每个聚类的最频繁单词手动发现的。然后，我们将使用映射和之前步骤中创建的**prediction**列，为测试集中的每个文本创建一个预测的主题名称列。现在，我们可以比较模型的预测与数据的真实值。我们将使用**sklearn**中的**classification_report**函数获取相应的统计数据。最后，我们将打印出预测的分类报告：
- en: '[PRE29]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result will be as follows:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE30]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The scores are very high – almost perfect. Most of this speaks to the quality
    of the sentence embedding model that we used.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数非常高——几乎完美。其中大部分归功于我们使用的句子嵌入模型的质量。
- en: 'Define a new example:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个新的示例：
- en: '[PRE31]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Print the prediction for the new example:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印新示例的预测：
- en: '[PRE32]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output will be as follows:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE33]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Cluster number 1 corresponds to sport, which is the correct classification.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类编号1对应于体育，这是正确的分类。
- en: Topic modeling using BERTopic
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERTopic进行主题建模
- en: In this recipe, we will explore the BERTopic package that provides many different
    and versatile tools for topic modeling and visualization. It is especially useful
    if you would like to do different visualizations of the topic clusters created.
    This topic modeling algorithm uses BERT embeddings to encode the data, hence the
    “BERT” in the name. You can learn more about the algorithm and its constituent
    parts at [https://maartengr.github.io/BERTopic/algorithm/algorithm.html](https://maartengr.github.io/BERTopic/algorithm/algorithm.html).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将探索BERTopic包，它提供了许多不同且通用的工具用于主题建模和可视化。如果你想要对创建的主题聚类进行不同的可视化，它特别有用。这个主题建模算法使用BERT嵌入来编码数据，因此得名“BERT”。你可以在[https://maartengr.github.io/BERTopic/algorithm/algorithm.html](https://maartengr.github.io/BERTopic/algorithm/algorithm.html)了解更多关于该算法及其组成部分的信息。
- en: The BERTopic package, by default, uses the HDBSCAN algorithm to create clusters
    from the data in an unsupervised fashion. You can learn more about how the HDBSCAN
    algorithm works at [https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html).
    However, it is also possible to customize the inner workings of a BERTopic object
    to use other algorithms. It is also possible to substitute other custom components
    into its pipeline. In this recipe, we will use the default settings, and you can
    experiment with other components.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，BERTopic包使用HDBSCAN算法以无监督的方式从数据中创建聚类。您可以在[https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)了解更多关于HDBSCAN算法的工作原理。然而，也可以自定义BERTopic对象的内部工作方式以使用其他算法。还可以将其管道中的其他自定义组件替换掉。在本配方中，我们将使用默认设置，您也可以尝试其他组件。
- en: The resulting topics are of very high quality. There might be several reasons
    for this. One of them is the result of using BERT embeddings, which we saw in
    [*Chapter 4*](B18411_04.xhtml#_idTextAnchor106), to positively impact the classification
    results
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最终生成的主题质量非常高。这可能有几个原因。其中之一是使用BERT嵌入的结果，我们曾在[*第4章*](B18411_04.xhtml#_idTextAnchor106)中看到，它对分类结果产生了积极影响。
- en: Getting ready
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the BERTopic package to create topic models for the BBC dataset.
    The package is included in the poetry environment and is also part of the `requirements.txt`
    file.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用BERTopic包为BBC数据集创建主题模型。该包包含在poetry环境中，也是`requirements.txt`文件的一部分。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.3-kmeans_with_bert.ipynb)。
- en: How to do it...
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will load the BBC dataset and preprocess it again. The preprocessing
    step will involve tokenizing the data and removing stopwords. Then we will create
    the topic model using BERTopic and inspect the results. We will also test the
    topic model on unseen data and use `classification_report` to see the accuracy
    statistics:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将加载BBC数据集并再次对其进行预处理。预处理步骤将包括分词和移除停用词。然后我们将使用BERTopic创建主题模型并检查结果。我们还将对未见过的数据进行主题模型测试，并使用`classification_report`查看准确度统计信息：
- en: 'Do the necessary imports:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE34]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Run the language utilities file:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行语言工具文件：
- en: '[PRE35]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define and amend the stopwords, then read in the BBC data:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义和修改停用词，然后读取BBC数据：
- en: '[PRE36]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In this step, we will preprocess the data. We will first tokenize it using
    the `word_tokenize` method from NLTK as shown in the *Dividing sentences into
    words – tokenization* recipe in [*Chapter 1*](B18411_01.xhtml#_idTextAnchor013).
    Then remove stopwords and finally put the text back together into a string. We
    must do the last step because the BERTopic uses a sentence embedding model, and
    that model requires a string, not a list of words:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这一步，我们将预处理数据。我们首先使用NLTK中的`word_tokenize`方法进行分词，正如在[*第1章*](B18411_01.xhtml#_idTextAnchor013)中的*将句子分割成单词
    – 分词*配方所示。然后移除停用词，最后将文本重新组合成一个字符串。我们必须执行最后一步，因为BERTopic使用的是句子嵌入模型，该模型需要一个字符串，而不是单词列表：
- en: '[PRE37]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here, we will split the dataset into training and test sets, specifying the
    size of the test set to be 10%. As a result, we will get 2002 datapoints for training
    and 223 for testing:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将数据集分为训练集和测试集，指定测试集的大小为10%。因此，我们将得到2002个训练数据点和223个测试数据点：
- en: '[PRE38]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The result will be as follows:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE39]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Extract the lists of texts from the dataframe:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据框中提取文本列表：
- en: '[PRE40]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In this step, we willinitialize the **BERTopic** object and then fit it on
    the documents extracted in *step 6*. We will specify the number of topics to be
    six, one more than the five that we are looking for. This is because a key difference
    between BERTopic and other topic modeling algorithms is that it has a special
    **discard** topic numbered -1\. We could also specify a larger number of topics.
    In that case, they would be narrower than the general five categories of business,
    politics, entertainment, tech, and sport:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将初始化**BERTopic**对象，然后将其拟合到在*步骤6*中提取的文档上。我们将指定要生成的主题数量为六个，比我们寻找的五个多一个。这是因为BERTopic与其他主题建模算法的一个关键区别在于它有一个特殊的**丢弃**主题，编号为-1。我们也可以指定更多的主题数量。在这种情况下，它们将比商业、政治、娱乐、科技和体育这五大类别更窄：
- en: '[PRE41]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here, we will print out the information about the resulting topic model. Other
    than the **discard** topic, the topics align well with the gold labels assigned
    by human annotators. The function prints out the most representative words for
    each topic, as well as the most representative documents:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将打印出结果主题模型的信息。除了**丢弃**主题外，主题与人类标注员分配的黄金标签很好地对齐。该函数打印出每个主题的最具代表性的单词，以及最具代表性的文档：
- en: '[PRE42]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The results will vary; here is an example result:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果会有所不同；这里是一个示例结果：
- en: '[PRE43]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In this step, we will print the topics. We can see from the words that the zeroth
    topic is sport, the first topic is politics, the second topic is entertainment,
    the third topic is tech, and the fourth topic is business.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将打印出主题。我们可以从单词中看到，零号主题是体育，第一个主题是政治，第二个主题是娱乐，第三个主题是科技，第四个主题是商业。
- en: '![](img/B18411_06_6.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18411_06_6.jpg)'
- en: Figure 6.5 – The topics generated by BERTopic
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – BERTopic生成的主题
- en: 'In this step, we will generate the topic labels using the **generate_topic_labels**
    function. We will input the number of words to use for the topic label, the separator
    (in this case, this is an underscore), and whether to include the topic number.
    As a result, we will get a list of topic names. We can see from the resulting
    topics that we could include *would* as a stopword:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将使用**generate_topic_labels**函数生成主题标签。我们将输入用于主题标签的单词数量，分隔符（在这种情况下，这是一个下划线），以及是否包含主题编号。结果，我们将得到一个主题名称列表。我们可以从生成的主题中看到，我们可以将*would*作为停用词包括在内：
- en: '[PRE44]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The result will be similar to the following:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将与以下类似：
- en: '[PRE45]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here, we will define the **get_prediction** function that gives us the topic
    number for a text input and a corresponding model. The function transforms the
    input text and outputs a tuple of two lists. One is a list of topic numbers and
    the other is the list of probabilities of assigning each topic. The lists are
    sorted in the order of the most probable topic, so we can take the first element
    of the first list as the predicted topic and return it:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将定义一个**get_prediction**函数，该函数为我们提供文本输入和相应模型的主题编号。该函数转换输入文本，并输出一个包含两个列表的元组。一个是主题编号列表，另一个是分配给每个主题的概率列表。列表按最可能的主题顺序排序，因此我们可以取第一个列表的第一个元素作为预测的主题并返回它：
- en: '[PRE46]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In this step, we will define a column for predictions in the test dataframe
    and then use the function we defined in the previous step to get predictions for
    each text in the dataframe. We will then create a mapping of topic numbers to
    gold topic labels that we can use to test the effectiveness of the topic model:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将在测试数据框中定义一个用于预测的列，然后使用我们在上一步定义的函数为数据框中的每个文本获取预测。然后我们将创建一个主题编号到黄金主题标签的映射，我们可以用它来测试主题模型的有效性：
- en: '[PRE47]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Here, we will create a new column in the test dataframe to record the predicted
    topic name using the mapping we created. We will then filter the test set to only
    use entries that have not been predicted to be the discard topic -1:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将在测试数据框中创建一个新列，用于记录使用我们创建的映射预测的主题名称。然后我们将过滤测试集，只使用那些未被预测为丢弃主题-1的条目：
- en: '[PRE48]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The result will be similar to this:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将与这个类似：
- en: '[PRE49]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The test scores are very high. This is reflective of the encoding model, the
    BERTopic model, which is also a sentence transformer model, as in the previous
    recipe.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试分数非常高。这反映了编码模型，BERTopic模型，它也是一个句子转换模型，就像之前的食谱中一样。
- en: 'In this step, we will define a new example to test with the model and print
    it. We will use the **iloc** function from the **pandas** package to access the
    first element of the **bbc_test** dataframe:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将定义一个新的示例来测试模型并打印它。我们将使用**pandas**包中的**iloc**函数来访问**bbc_test**数据框的第一个元素：
- en: '[PRE50]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The example is about politics, which should be topic 1.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个例子是关于政治的，应该是主题1。
- en: 'Obtain a prediction from the model and print it:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型中获得预测并打印出来：
- en: '[PRE51]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The result will be a correct prediction:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将是一个正确的预测：
- en: '[PRE52]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: There’s more...
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'Now, we can find topics that are similar to a particular word, phrase, or sentence.
    This way, we could easily find a topic to which a text corresponds within the
    dataset. We will use a word, a phrase, and a sentence to see how well the model
    can show the corresponding topics:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以找到与特定单词、短语或句子相似的主题。这样，我们就可以轻松地找到在数据集中与文本相对应的主题。我们将使用一个单词、一个短语和一个句子来查看模型如何展示相应的主题：
- en: 'Find topics most similar to the **sports** word with the corresponding similarity
    scores. Combine the topic numbers and similarity scores in a list of tuples and
    print them. The tuples are a combination of the topic number and the similarity
    score between the text that is passed in and the particular topic:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相应的相似度分数找到与**体育**这个词最相似的主题。将主题编号和相似度分数组合成一个元组列表并打印它们。元组是输入文本与特定主题之间的主题编号和相似度分数的组合：
- en: '[PRE53]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The most similar topic is topic 0, which is sport:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最相似的主题是主题0，它是体育类：
- en: '[PRE54]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Repeat the preceding step for the **business and economics** example phrase:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复前面的步骤，对**商业与经济**示例短语进行操作：
- en: '[PRE55]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here, the most similar topic is topic 4, which is business:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，最相似的主题是主题4，它是商业类：
- en: '[PRE56]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now repeat the same process for the following example sentence: **"YouTube
    removed a snippet of code that publicly disclosed whether a channel receives ad
    payouts, obscuring which creators benefit most from the platform."**. We would
    expect this to be most similar to the tech topic:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在重复以下示例句子的相同过程：**"YouTube删除了一段公开披露频道是否获得广告分成、模糊了哪些创作者从平台上获益最多的代码片段。"**。我们预计这最相似于技术主题：
- en: '[PRE57]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'In the output, we can see that the most similar topic is topic 3, which is
    tech:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在输出中，我们可以看到最相似的主题是主题3，它是技术类：
- en: '[PRE58]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Using contextualized topic models
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用上下文主题模型
- en: 'In this recipe, we will look at another topic model algorithm: contextualized
    topic models. To produce a more effective topic model, it combines embeddings
    with a bag-of-words document representation.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将探讨另一个主题模型算法：上下文主题模型。为了产生更有效的主题模型，它将嵌入与词袋文档表示相结合。
- en: We will show you how to use the trained topic model with input in other languages.
    This feature is especially useful because we can create a topic model in one language,
    for example, one that has many resources available, and then apply it on another
    language that does not have as many resources. To achieve this, we will utilize
    a multilingual embedding model in order to encode the data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向您展示如何使用训练好的主题模型，并输入其他语言的数据。这个特性特别有用，因为我们可以在一种语言中创建一个主题模型，例如，拥有许多资源的语言，然后将其应用于资源较少的另一种语言。为了实现这一点，我们将利用多语言嵌入模型来编码数据。
- en: Getting ready
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will need the `contextualized-topic-models` package for this recipe. It is
    part of the poetry environment and the `requirements.txt` file.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`contextualized-topic-models`包来完成这个菜谱。它是poetry环境的一部分，并在`requirements.txt`文件中。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter06/6.5-contextualized-tm.ipynb)。
- en: How to do it...
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this recipe, we will load the data, then divide it into sentences, preprocess
    it, and use the **gsdmm** model to cluster the sentences into topics. If you would
    like more information about the algorithm, please see the package documentation
    at [https://pypi.org/project/contextualized-topic-models/](https://pypi.org/project/contextualized-topic-models/).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将加载数据，然后将其分为句子，预处理它，并使用**gsdmm**模型将句子聚类为主题。如果您想了解更多关于算法的信息，请参阅[https://pypi.org/project/contextualized-topic-models/](https://pypi.org/project/contextualized-topic-models/)的包文档。
- en: 'Do the necessary imports:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE59]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Suppress the warnings:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抑制警告：
- en: '[PRE60]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Create the stopwords list and read in the data:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建停用词列表并读取数据：
- en: '[PRE61]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In this step, we will create the preprocessor object and use it to preprocess
    the documents. The **contextualized-topic-models** package provides different
    preprocessors that prepare the data to be used in the topic model algorithm. This
    preprocessor tokenizes the documents, removes the stopwords, and puts them back
    into a string. It returns the list of preprocessed documents, the list of original
    documents, the dataset vocabulary, and a list of document indices in the original
    dataframe:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将创建预处理对象并使用它来预处理文档。**contextualized-topic-models**包提供了不同的预处理程序，这些程序准备数据以用于主题模型算法。这个预处理程序对文档进行分词，移除停用词，并将它们放回字符串中。它返回预处理文档的列表、原始文档的列表、数据集词汇表以及原始数据框中的文档索引列表：
- en: '[PRE62]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Here, we will create the **TopicModelDataPreparation** object. We will pass
    the embedding model name as the parameter. This is a multilingual model that can
    encode text in various languages with good results. We will then fit it on the
    documents. It uses an embedding model to turn the texts into embeddings and also
    creates a bag-of-words model. The output is a **CTMDataset** object that represents
    the training dataset in the format required by the topic model training algorithm:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将创建**TopicModelDataPreparation**对象。我们将传递嵌入模型名称作为参数。这是一个多语言模型，可以将文本编码成各种语言并取得良好的效果。然后我们将它在文档上拟合。它使用嵌入模型将文本转换为嵌入，并创建一个词袋模型。输出是一个**CTMDataset**对象，它代表了主题模型训练算法所需格式的训练数据集：
- en: '[PRE63]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'In this step, we will create the topic model using the **ZeroShotTM** object.
    The term **zero shot** means that the model has no prior information about the
    documents. We will input the size of the vocabulary for the bag-of-words model,
    the size of the embeddings vector, the number of topics (the **n_components**
    parameter), and the number of epochs to train the model for. We will use five
    topics, since the BBC dataset has that many topics. When you apply this algorithm
    to your data, you will need to experiment with different numbers of topics. Finally,
    we will fit the initialized topic model on the training dataset:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将使用**ZeroShotTM**对象创建主题模型。术语**zero shot**意味着该模型对文档没有任何先验信息。我们将输入词袋模型的词汇量大小、嵌入向量的大小、主题数量（**n_components**参数）以及训练模型所需的轮数。我们将使用五个主题，因为BBC数据集有那么多主题。当你将此算法应用于你的数据时，你需要对不同数量的主题进行实验。最后，我们将初始化的主题模型拟合到训练数据集上：
- en: '[PRE64]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Here, we will inspect the topics. We can see that they fit well with the golden
    labels. Topic 0 is tech, topic 1 is sport, topic 2 is business, topic 3 is entertainment,
    and topic 4 is politics:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将检查主题。我们可以看到它们很好地与黄金标签匹配。主题0是科技，主题1是体育，主题2是商业，主题3是娱乐，主题4是政治：
- en: '[PRE65]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The results will vary; this is the output we get:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果会有所不同；这是我们得到的结果：
- en: '![](img/B18411_06_7.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18411_06_7.jpg)'
- en: Figure 6.6 – The contextualized model output
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 上下文主题模型输出
- en: 'Now, we will initialize a new news piece, this time in Spanish, to see how
    effective the topic model trained on English-language documents will be on a news
    article in a different language. This particular news piece should fall into the
    tech topic. We will preprocess it using the **TopicModelDataPreparation** object.
    To then use the model on the encoded text, we need to create a dataset object.
    That is why we have to include the Spanish news piece in a list and then pass
    it on for data preparation. Finally, we must pass the dataset (that consists of
    only one element) through the model:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将初始化一个新的新闻稿件，这次使用西班牙语，以查看基于英语文档训练的主题模型在另一种语言的新闻文章上的效果如何。这篇特定的新闻稿件应该属于科技主题。我们将使用**TopicModelDataPreparation**对象对其进行预处理。为了然后在编码后的文本上使用该模型，我们需要创建一个数据集对象。这就是为什么我们必须将西班牙语新闻稿件包含在一个列表中，然后传递给数据准备过程。最后，我们必须将数据集（仅包含一个元素）通过模型传递：
- en: '[PRE66]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In this step, we will get the topic distribution for the testing dataset we
    created in the previous step. The result is a list of lists, where each individual
    list represents the probability that a particular text belongs to that topic.
    The probabilities have the same indices in individual lists as the topic numbers:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们将获取之前步骤中创建的测试数据集的主题分布。结果是列表的列表，其中每个单独的列表代表特定文本属于该主题的概率。这些概率在单独的列表中与主题编号具有相同的索引：
- en: '[PRE67]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'In this case, the highest probability is for topic 0, which is indeed tech:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，最高的概率是主题0，这确实是科技：
- en: '[PRE68]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: See also
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more information about contextualized topic models, see [https://contextualized-topic-models.readthedocs.io/en/latest/index.html](https://contextualized-topic-models.readthedocs.io/en/latest/index.html).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于上下文相关主题模型的信息，请参阅[https://contextualized-topic-models.readthedocs.io/en/latest/index.html](https://contextualized-topic-models.readthedocs.io/en/latest/index.html).
