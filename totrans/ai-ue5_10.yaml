- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving Agents with the Perception System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AI **Perception System** is a powerful tool in the Unreal Engine Gameplay
    Framework, as it allows AI-controlled actors to perceive and react to various
    stimuli in an environment. It provides a way for AI agents to become aware of
    the presence of other actors – such as players or enemies – through different
    senses such as sight, hearing, or touch. By properly configuring and using the
    Perception System, developers can create AI agents that respond appropriately
    to events in their surroundings. What’s more, this system allows developers to
    implement and configure custom senses tailored to their game’s specific needs.
    This flexibility enables developers to create unique and engaging AI experiences.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be covering the main components of the Unreal Engine
    Perception System, starting with a bit of theory and then applying this newly
    acquired knowledge to a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the Perception System
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding perception to an agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging perception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating perception stimuli
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the topics presented in this chapter, you should have completed
    the previous ones and understood their content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, if you would prefer to begin with code from the companion repository
    for this book, you can download the **.zip** project files provided in this book’s
    companion project repository: [https://github.com/PacktPublishing/Artificial-Intelligence-in-Unreal-Engine-5](https://github.com/PacktPublishing/Artificial-Intelligence-in-Unreal-Engine-5)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: To download the files from the end of the last chapter, click the **Unreal Agility
    Arena –** **Chapter 09** **-** **End** link.
  prefs: []
  type: TYPE_NORMAL
- en: Presenting the Perception System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Dr. Markus and Professor Viktoria seem to have a new chapter in their story:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dr. Markus and Professor Viktoria knew that allowing sophisticated synthetic
    beings such as their AI dummy puppets to roam unchecked could prove disastrous.
    Therefore, they started working tirelessly to develop an intricate network of
    hidden security cameras that could monitor the movements and actions of their
    creations at all times. With this vigilant surveillance system in place, they
    hoped to keep them under strict observation and maintain full control, ensuring
    the safety of their* *controversial research.*'
  prefs: []
  type: TYPE_NORMAL
- en: One of the key components when creating intelligent and reactive AI agents in
    Unreal Engine is the AI Perception System; this powerful system allows AI controllers
    – and, consequently, AI agents – to perceive and respond to different stimuli
    in their virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of the AI Perception System are **senses** and **stimuli** . A sense
    – such as sight or hearing – represents a way for an AI agent to perceive its
    surroundings and is configured to detect specific types of stimuli, which are
    sources of perception data emanating from other actors in the game world.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, *sight sense* is preconfigured to detect any visible pawn actors,
    while *damage sense* triggers when the associated AI controller’s pawn takes damage
    from an external source.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As a developer, you can create custom senses tailored to your game’s specific
    needs by extending the **AISense** class, if you are working with C++, or the
    **AISense_Blueprint** class, if you are working with Blueprints.
  prefs: []
  type: TYPE_NORMAL
- en: AI Perception System components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The AI Perception System consists of the following main classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AIPerceptionSystem** : This is the core manager that keeps track of all AI
    stimuli sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AIPerceptionComponent** : This represents the AI agent’s mind and handles
    processing perceived stimuli. It needs to be attached to an AI controller to properly
    work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AIPerceptionStimuliSourceComponent** : This component is added to actors
    that can generate stimuli and is in charge of broadcasting perception data to
    listening elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AIPerceptionSenseConfig** : This defines the properties of a specific sense,
    what actors can be perceived, and how perception decays over time or distance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an actor with **AIPerceptionStimuliSourceComponent** generates a stimulus,
    nearby **AIPerceptionComponents** detect it through their configured senses. This
    perceived data is then processed by the AI controller to trigger desired behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have added **AIPerceptionComponent** to an AI controller, you will
    need to add one or more **AIPerceptionSenseConfig** elements in order to give
    your AI agent dedicated senses. *Figure 10* *.1* shows an example where perception
    is based on touch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – The touch sense config](img/Figure_10.1_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – The touch sense config
  prefs: []
  type: TYPE_NORMAL
- en: From the previous screenshot, you may have noticed a **Dominant Sense** property;
    this property allows you to designate a specific sense that takes priority over
    others when determining the location of a sensed actor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore the available sense configs that, as mentioned earlier, define
    the properties of each specific sense:'
  prefs: []
  type: TYPE_NORMAL
- en: AIPerceptionSenseConfig types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unreal Engine offers a range of predefined **AIPerceptionSenseConfig** classes
    that are highly likely to meet your specific requirements. Let’s take a look at
    the available options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AIDamag** e: Use this configuration if your AI agent needs to respond to
    damage events such as *Any Damage* , *Point Damage* , or *Radial Damage*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AIHearing** : Use this configuration if you need to detect sounds generated
    in the surrounding environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AIPrediction** : Use this configuration when you need to predict the target
    actor location in the next few moments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AISight** : Use this configuration when you want your AI agent to see things
    in the level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AITeam** : Use this configuration if you want to notify the AI agent that
    some ally is nearby'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AITouch** : Use this configuration when the AI agent touches some other actor
    or, vice versa, when something is touching the AI agent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stimuli source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **AIPerceptionStimuliSourceComponent** class allows an actor to register
    itself as a source of stimuli for one or more senses. For instance, you can register
    an actor as a stimuli source for sight. This registration allows an AI agent to
    visually perceive the actor in the game level.
  prefs: []
  type: TYPE_NORMAL
- en: A stimuli source can be registered – or unregistered – for a sense, making it
    detectable – or undetectable – by the Perception System.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The **Pawn** and **Character** classes in Unreal Engine are inherently visible
    to **AISight** perception due to their default behavior as stimuli sources. This
    design choice streamlines AI behavior development by eliminating the need to manually
    configure visibility for each character or pawn. However, if you want the AI to
    ignore specific characters, you’ll need to take additional steps to configure
    them accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we were introduced to the Perception System and its main elements.
    In the next section, we will work on a fully functional AI agent that will let
    you sense other actors in your game.
  prefs: []
  type: TYPE_NORMAL
- en: Adding perception to an agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a new AI agent that will use the Perception
    System. We will create a security camera that will probe nearby surrounding areas,
    looking for some possible targets for the dummy gunner that we created in [*Chapter
    9*](B31016_09.xhtml#_idTextAnchor170) , *Extending Behavior Trees* . Think of
    it as some kind of infrared camera for a dark environment. Once the camera spots
    a target, it will tag it so that the gunner will be able to locate it in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by creating an **Actor** class that will be used as the camera
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the BaseSecurityCam class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even though we will be implementing the Perception System inside the AI controller,
    a nice model to display in the level will help your environment’s look and feel,
    so let’s start by creating a new C++ class, extending from the **Pawn** class
    and named **BaseSecurityCam** . Once the class has been created, open the **BaseSecurityCam.h**
    file and add the following forward declaration after the **#** **include** declarations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, make the class a **Blueprintable** one by changing the **UCLASS()** macro
    to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After that, remove the **BeginPlay()** and **Tick()** declarations, as we won’t
    be using them.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final step, add the following component declarations for the static meshes
    that will display the model just after the **GENERATED_BODY()** macro:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now open the **BaseSecurityCam.cpp** file to implement this class;
    as a first step, remove the **BeginPlay()** and **Tick()** functions. Then, locate
    the constructor and change this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Change it to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, inside the constructor and just after the aforementioned line of code,
    add the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You already know all about this from the previous chapters of the book, so I
    suppose there’s no need to explain it again. With the security camera model created,
    we can now implement the corresponding AI controller, along with its perception
    sense.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the BaseSecurityCamAIController class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To add a proper controller for the security camera, let’s create a C++ class
    extending **AIController** and name it **BaseSecurityCamAIController** . Once
    the class has been created, open the **BaseSecurityCamAIController.h** file and
    add the following forward declarations, just after the **#** **include** declarations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, make the class **Blueprintable** by changing the **UCLASS()** macro to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, add this block of code just after the already existing constructor
    declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You are already familiar with the behavior tree property and the **OnPosses()**
    function from [*Chapter 8*](B31016_08.xhtml#_idTextAnchor148) , *Setting Up a
    Behavior Tree* ; in addition, the **OnTargetPerceptionUpdate()** function will
    be used as an event handler when getting information from the Perception System.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now open **BaseSecurityCamAIController.cpp** and add the following
    **#include** declarations to the top of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, locate the constructor, and inside of it, add the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are creating the sense config for the sight perception, along
    with some of its properties, such as **SightRadius** and **LoseSightRadius** ,
    which will determine the distance at which the Perception System can detect something
    and the distance at which detection will be lost, respectively. As redundant as
    these two attributes may seem, keep in mind that, once a target has been detected,
    it will be more difficult to lose perception of it, unless both attributes have
    the same value. **PeripheralVisionAngleDegrees** will handle the cone that will
    be used to check whether an actor is in the line of sight or not. Lastly, the
    **DetectionByAffiliation** property is used to handle whether the detected actor
    is an enemy, friend, or neutral; in this case, we want to check all of them in
    order to detect anything that is in the line of sight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it’s time to add the actual perception component, so add the following
    piece of code just after the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we create the **AIPerceptionComponent** instance, and then we
    assign the previously created sight configuration. Lastly, we register to the
    **OnTargetPerceptionUpdated** delegate that will notify the component of any changes
    detected by the Perception System.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it’s time to implement the **OnPosses()** function, something that we
    already know how to handle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to implement the event handler. To do so, add the following
    block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This function first checks whether the target has already been tagged; in this
    case, it means that it has already been spotted. It then retrieves the ID of the
    sight sense, calling **GetSenseID()** , and checks whether the stimulus type is
    equal to the sight sense ID and whether the stimulus was successfully sensed.
    If both conditions are **true** , it initializes the **Tags** array, with the
    first element set to a value of **ShootingTarget** , in order to make it a viable
    target for the dummy gunner we have at our disposal.
  prefs: []
  type: TYPE_NORMAL
- en: The security camera is now ready to go; we just need a nice environment to test
    it on.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve shown you how to properly create an AI agent, taking
    advantage of the Perception System. In the next section, we will test this agent
    and learn how to properly debug perception information.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging perception
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s now time to test our perception logic and learn how to properly debug the
    Perception system at runtime. In order to do this, we will need to add some small
    improvements to the base dummy character. As previously mentioned, the **Pawn**
    and **Character** classes are already registered with sight stimuli, so we won’t
    need to implement this logic. However, we will need to handle damage, as we will
    be playing around with both **BP_RoamerDummyCharacter** and **BP_GunnerDummyCharacter**
    . It appears that exciting and enjoyable times are just around the corner!
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the roamer behavior tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in improving our AI agents will be adding some logic to handle
    damage to the dummy roamer behavior tree. In particular, we want the AI agent
    to sit down when it is hit by a Nerf gun projectile. We will start by adding a
    new key to the dedicated Blackboard.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the Blackboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A new flag is required for the Blackboard to effectively monitor and keep a
    record of the character that has been hit. So, open up the **BB_Dummy** asset
    and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the **New Key** button, and from the dropdown menu, select **Bool** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the newly created key **IsHit** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you already know, this will expose a new key available to the behavior tree;
    additionally the key will be exposed to the AI controller, as we will see later
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the behavior tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The behavior tree needs to manage the AI agent being hit; in our case, we want
    to play a montage with the character sitting down, as it has been eliminated from
    the game. So, let’s start by opening the **BT_RoamerDummy** asset and doing the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the **Root Sequence** node and add a **Blackboard** decorator,
    naming it **Is** **Not Hit?** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the decorator selected, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the **Notify Observer** attribute to **On** **Value Change**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the **Observer aborts** attribute to **Self**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the **Key Query** attribute to **Is** **Not Set**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the **Blackboard Key** attribute to **IsHit**
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rename the root sequence **In Game Sequence** and disconnect it from the **ROOT**
    node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a **Selector** node to the **ROOT** node and name it **Root Selector** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect the **Root Selector** node to the **In Game** **Sequence** node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect the **Root Selector** node to a **PlayMontage** task, and name the newly
    created node **Sit Montage** . This node should be at the right of the **In Game**
    **Sequence** node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With the **Sit Montage** node selected, set the **Anim Montage** attribute
    to **AM_Sit** . The modified portion of the behavior tree is depicted in *Figure
    10* *.2* :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.2 – A modified behavior tree](img/Figure_10.2_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – A modified behavior tree
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the behavior tree will keep on working as before, unless the
    AI agent has been hit; in that case, the character will sit down and stop wandering.
  prefs: []
  type: TYPE_NORMAL
- en: It’s now time to improve the AI controller, in order to manage incoming damage.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing BaseDummyAIController
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The AI controller for any dummy character will need to handle any damage. We
    are handling all of this inside the AI controller instead of the character for
    the sake of simplicity; we will need to communicate with the Blackboard, and this
    is much simpler and more direct when done from the controller itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by opening the **BaseDummyAIController.h** file and adding the
    following declaration for the damage handler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open the **BaseDummyAIController.cpp** file, and in the **OnPossess()**
    function, add the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add the following implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This function is called when a pawn associated with the AI controller is damaged;
    the function retrieves the Blackboard component of the AI controller and sets
    the **IsHit** key to a value of **true** . Then, it sets the first tag of the
    actor to a value of **Untagged** so that it won’t be a viable target anymore for
    the gunner dummy.
  prefs: []
  type: TYPE_NORMAL
- en: With this AI controller all set up, it’s time to create a Blueprint for the
    security camera.
  prefs: []
  type: TYPE_NORMAL
- en: Creating security camera Blueprints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, get back to the Unreal Engine Editor, and after compilation has finished,
    create a Blueprint out of the **BaseSecurityCamAIController** class, naming it
    **AISecurityCamController** . You don’t need to add a behavior tree, as all the
    logic is handled inside the controller itself.
  prefs: []
  type: TYPE_NORMAL
- en: Now, create a Blueprint class from the **BaseSecurityCam** class, and name it
    **BP_SecurityCam** . Once it has been created, open it, and in the **Details**
    panel, locate the **AI Controller Class** attribute and set its value to **AISecurityCamController**
    .
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – The security cam Blueprint](img/Figure_10.3_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – The security cam Blueprint
  prefs: []
  type: TYPE_NORMAL
- en: We have gathered all the necessary elements to bring our new gym to life, and
    we are ready to take the next steps and start the process of debugging the Perception
    System.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the gym
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are now going to create a level to test and debug everything. We want to
    achieve this kind of behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: One or more AI agents will move around the level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A security cam will try to spot AI agents and tag them as viable targets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A gunner will wait for the AI agents to be tagged, in order to shoot at them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, let’s start by creating the gym:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a level of your choice, starting from the Level Instances and Packed
    Level Actors I provided in the project template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a **NavMeshBoundsVolume** actor so that it will cover all the walkable areas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add some obstacles to make things more interesting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a **BP_GunnerDummyCharacter** instance to the level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add one or more **BP_RoamerDummyCharacter** instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add some **NS_Target** Niagara actors that will work as target points for the
    pathfinding system; just remember to tag them **TargetPoint** . Make sure that
    the path will bring the AI agents in the line of sight of the gunner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add one or more **BP_SecurityCam** instances to the walls. The final result
    should be similar to *Figure 10* *.4* :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.4 – The gym](img/Figure_10.4_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – The gym
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the type of scenario, where the gunner character will shoot at
    targets located by the security camera, I have decided to make the gym a bit juicier
    by adding a post-process volume that simulates an infra-red scenario, as depicted
    in *Figure 10* *.5* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – The gym with a post-process volume](img/Figure_10.5_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – The gym with a post-process volume
  prefs: []
  type: TYPE_NORMAL
- en: This is obviously not mandatory, and you are free to set your post-process environment
    as you wish.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the gym is finished, it’s time to start testing it and learn how to
    debug the Perception System.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling perception debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you start the simulation, you should see the following things happening:'
  prefs: []
  type: TYPE_NORMAL
- en: As the roamers wander around, the gunner, unaware of their presence, will cheer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As soon as one of the roamers enters the camera’s line of sight, the gunner
    will start aiming at it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time a roamer is hit, it will sit down and stop wandering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can tweak the security camera parameters to make it more or less attentive
    to what’s happening in the level.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be curious about how we determine whether an agent
    is in the camera’s line of sight. Well, it’s actually quite simple to observe
    once you enable the debugging tools!
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start by enabling the debugging tools, as explained in [*Chapter 6*](B31016_06.xhtml#_idTextAnchor116)
    , *Optimizing the* *Navigation Mesh* .
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Once the simulation starts, you may be wondering why the security camera has
    a tiny red icon on top of it, while all the dummy puppets have a green one, as
    displayed in *Figure 10* *.6* . When the AI debugging tools are enabled, a green
    icon is displayed if a pawn has some AI logic set up and running; otherwise, the
    icon will be red. In our case, the security cam is possessed by a dedicated AI
    controller, but it has no behavior tree, so the icon will be red.
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/Figure_10.6_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10 .6 – AI icons
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, while the simulation is going on, select a security camera and enable
    the **Perception** and **Perception System** tools by pressing the *4* and *5*
    keys on your numpad, respectively. You should immediately see a visualization
    of the security camera sight sense, as depicted in *Figure 10* *.7* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Perception debugging tools](img/Figure_10.7_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Perception debugging tools
  prefs: []
  type: TYPE_NORMAL
- en: 'The display will show some important information on the AI agent perception,
    such as the active sense and its data. Additionally, you will also see a visual
    representation of the agent sense. In particular, the sight sense will show the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: A green circular area that represents the range of the agent sight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pink circular area that represents the maximum range of the agent sight. Once
    the spotted agent goes beyond this range, sight contact will be lost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A green angle that represents the peripheral vision of the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once an AI agent enters the green circle, it will be detected by the Perception
    System, and you should see a green line, starting from the security camera and
    ending at the detected pawn, as depicted in *Figure 10* *.8* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – A pawn detected](img/Figure_10.8_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – A pawn detected
  prefs: []
  type: TYPE_NORMAL
- en: A green wireframe sphere will show the detection point, which will be updated
    as the detected AI agent moves around.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once an AI agent moves out of sight, the detection point will stop following
    it, and you should see the **age** label next to the sphere, updating its value;
    this is the time that has passed since detection was lost. *Figure 10* *.9* shows
    this scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Detection lost](img/Figure_10.9_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Detection lost
  prefs: []
  type: TYPE_NORMAL
- en: Each sense has its own way of displaying info, so my advice is to start experimenting
    with each of them to get an understanding of how to debug and get information
    from the debugging tools.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have created a new gym and tested how the Perception System
    works; what’s more, I have shown you how to enable the debugging tools and get
    a better understanding of what’s happening in the level at runtime. In the next
    section, we are going to take a look at perception stimuli, in order to make our
    levels more articulated and engaging.
  prefs: []
  type: TYPE_NORMAL
- en: Creating perception stimuli
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we utilized the out-of-the-box pawn feature that enables
    it to be visible to sight sense. We will now analyze actors that are not perceivable
    by default; this means we will need to add **AIPerceptionStimuliSourceComponent**
    to an actor. What’s more, we will learn how to register or unregister these stimuli,
    in order to make the actor visible or invisible to the Perception System.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the target actor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will create a new actor that will serve as a target for
    the dummy gunner puppet, but with a twist – this actor will create some interference
    in the level and won’t be visible to the security camera. Achieving this kind
    of feature is quite easy, once you know how to register and unregister an actor
    from the Perception System. We are basically creating a scrambler device that
    will disturb – that is, it will be invisible to – the gunner sight sense.
  prefs: []
  type: TYPE_NORMAL
- en: To keep things simple, I will create a Blueprint class; when working with stimuli,
    it is often more convenient to configure settings directly from a Blueprint rather
    than using a C++ class. By utilizing a Blueprint, we can easily adjust and fine-tune
    various aspects of the stimuli, making the whole process more flexible and accessible.
    This approach allows for quicker iterations and modifications, ultimately resulting
    in a smoother and more efficient workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our scrambler, open the **Blueprints** folder, create a new Blueprint
    class extending from **Actor** , and name it **BP_Scrambler** . Once the blueprint
    is opened, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the **Components** panel, add a static mesh component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Details** panel, set the **Static Mesh** property to **SM_RoboGun_BaseRemote**
    and the **Scale** property to **(3.0,** **3.0, 3.0)** .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add an **AIPerceptionStimuliSource** component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Details** panel, locate the **Register as Source for Senses** attribute,
    add a new element by clicking the **+** button, and set the value to **AISense_Sight**
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the **Auto Register as Source** attribute unchecked
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.10 – The stimuli source](img/Figure_10.10_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – The stimuli source
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, open the Event Graph for this Blueprint and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: From the outgoing execution pin of the **Event Begin Play** node, add a **Delay**
    node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the **Completed** pin of the **Delay** node, add a **Register for Sense**
    node; this should automatically add an **AIPerception Stimuli Source** reference
    to the **Target** incoming pin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the **Duration** pin of the **Delay** node, add a **Random Float in Range**
    node, setting its **Min** and **Max** values to **4.0** and **6.0** , respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the dropdown menu of the incoming **Sense Class** pin, select **AISense_Sight**
    . The final graph should look like the one depicted in *Figure 10* *.11* :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.11 – The Event Graph](img/Figure_10.11_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – The Event Graph
  prefs: []
  type: TYPE_NORMAL
- en: This graph will simply register the sight sense for this actor after a random
    interval, making the actor itself visible to the Perception System in the level.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you need to unregister a stimuli source, the corresponding Blueprint node
    is **Unregister** **from Sense** .
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test this functionality in a brand-new gym.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the gym
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create the testing level, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a level of your choice, starting from the Level Instances and Packed
    Level Actors that I provided in the project template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add some obstacles to make things more interesting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add **BP_GunnerDummyCharacter** to the level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add one **BP_Scrambler** instance so that the gunner puppet can shoot at it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add one **BP_SecurityCam** instance to the walls so that it is in the line
    of sight of the scrambler. The final result should be similar to *Figure 10* *.12*
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.12 – The scrambler gym](img/Figure_10.12_B31016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – The scrambler gym
  prefs: []
  type: TYPE_NORMAL
- en: By starting the simulation, you can see that the scrambler will go unnoticed
    by the security camera until it reveals itself after a random interval. After
    that, the scrambler will be tagged as a viable target and the gunner will shoot
    at it. Try enabling the debugging tools to check what’s happening to the Perception
    System.
  prefs: []
  type: TYPE_NORMAL
- en: In this final section, I have presented how to make any actor detectable by
    the Perception System. By following the steps and guidelines provided, you can
    seamlessly integrate the Perception System into your project, allowing your actors
    to be accurately recognized and interacted with within the virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the basics of the Unreal Engine Perception System.
    Firstly, we showed the main elements that will let you add senses to your AI agents;
    after that, you built a pawn with a sight sense that detects moving characters
    around the level. Then, you learned how to debug the active senses at runtime.
    Finally, you added a stimuli source to an actor, in order to make it detectable
    by the Perception System itself. All of this opens up a world of possibilities
    for creating immersive and dynamic experiences, using the power of the Unreal
    Engine AI Framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, I’ll unveil a new method to gather data from the environment;
    brace yourself, as this cutting-edge feature is still in the experimental stage.
    But fear not, my friend, for it is knowledge well worth acquiring!
  prefs: []
  type: TYPE_NORMAL
