<html><head></head><body>
<div id="_idContainer356">
<h1 class="chapter-number" id="_idParaDest-180"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.1.1">10</span></h1>
<h1 id="_idParaDest-181"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.2.1">Building an AI Agent Application</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we discussed how an LLM can extend its capabilities by using other tools. </span><span class="koboSpan" id="kobo.3.2">We also saw some examples of how the use of multiple agents at the same time (instead of one) can be used to solve more complex tasks. </span><span class="koboSpan" id="kobo.3.3">We extensively discussed how these approaches can be used in various industries and how they can be revolutionary for so many applications. </span><span class="koboSpan" id="kobo.3.4">However, we also highlighted two of the limitations of agents: scalability and the complexity of connecting an agent with </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">different tools.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will expand on these challenges and show how we can overcome them. </span><span class="koboSpan" id="kobo.5.2">We will pick up from these two limitations. </span><span class="koboSpan" id="kobo.5.3">So far, we have treated multi-agent systems as standalone entities running on a personal computer. </span><span class="koboSpan" id="kobo.5.4">In the final section of the previous chapter, we explored the exciting new business paradigms emerging with AI. </span><span class="koboSpan" id="kobo.5.5">Agents are poised to play a significant role across industries in the future, but for that to happen, agent systems must be ready for production deployment. </span><span class="koboSpan" id="kobo.5.6">Getting a multi-agent system into production means we’ll have to solve the previously mentioned scalability and complexity issues to avoid harming the </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">customer experience.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will follow a progressive approach in this chapter. </span><span class="koboSpan" id="kobo.7.2">We will use Streamlit, which is a simple but flexible framework that allows us to manage the entire process of creating an application around our agents. </span><span class="koboSpan" id="kobo.7.3">It allows us to conduct rapid prototyping of our application, testing different options until we reach a proof of concept. </span><span class="koboSpan" id="kobo.7.4">With Streamlit, we can seamlessly work with both the backend, where agents operate, and the frontend, which shapes the user experience—all within a </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">single framework.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">Next, we will discuss in more detail the whole set of operations that are necessary to make an LLM and agents functional. </span><span class="koboSpan" id="kobo.9.2">Irrespective of whether you have the opportunity to train a model from scratch, this section will help you understand how to improve scalability and how the industry is handling the complexity of the process. </span><span class="koboSpan" id="kobo.9.3">In addition, we will address asynchronous programming and containerization, two concepts that are useful for scaling not only a multi-agent application but any machine </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">learning project.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">In this chapter, we'll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Introduction </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">to Streamlit</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Developing our frontend </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">with Streamlit</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Creating an application with Streamlit and </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">AI agents</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Machine learning operations and </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">LLM operations</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.21.1">Asynchronous programming</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.22.1">Docker</span></span></li>
</ul>
<h1 id="_idParaDest-182"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.23.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.24.1">Most of the code in this chapter can be run on CPUs. </span><span class="koboSpan" id="kobo.24.2">The </span><em class="italic"><span class="koboSpan" id="kobo.25.1">Introduction to Streamlit</span></em><span class="koboSpan" id="kobo.26.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.27.1">Frontend with Streamlit</span></em><span class="koboSpan" id="kobo.28.1"> sections do not require GPUs. </span><span class="koboSpan" id="kobo.28.2">The libraries to install are </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.30.1">Streamlit</span></strong><span class="koboSpan" id="kobo.31.1">: For managing the frontend and backend of </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">our app</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.33.1">pandas</span></strong><span class="koboSpan" id="kobo.34.1">: For </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">handling DataFrames</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.36.1">Matplotilib</span></strong><span class="koboSpan" id="kobo.37.1">: For </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">plotting graphs</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.39.1">Folium</span></strong><span class="koboSpan" id="kobo.40.1">: For </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">plotting maps</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.42.1">time</span></strong><span class="koboSpan" id="kobo.43.1">: For </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">monitoring runtime</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.45.1">NumPy</span></strong><span class="koboSpan" id="kobo.46.1">: For </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">numerical computation</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.48.1">pydeck</span></strong><span class="koboSpan" id="kobo.49.1">: For </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">map representation</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.51.1">OpenAI</span></strong><span class="koboSpan" id="kobo.52.1">: For building agents using </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">its LLMs</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Sentence Transformer</span></strong><span class="koboSpan" id="kobo.55.1">: To </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">conduct embeddings</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.57.1">The </span><em class="italic"><span class="koboSpan" id="kobo.58.1">Creating an application with Streamlit and AI agents</span></em><span class="koboSpan" id="kobo.59.1"> section can be run on a CPU, but it would be preferred if it were run on </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">a GPU.</span></span></p>
<p><span class="koboSpan" id="kobo.61.1">The OpenAI library requires the use of an OpenAI token, and you should register with OpenAI to obtain it. </span><span class="koboSpan" id="kobo.61.2">The next sections can be run on CPUs and are mainly based on the use of the AsyncIO library. </span><span class="koboSpan" id="kobo.61.3">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr10"><span class="No-Break"><span class="koboSpan" id="kobo.63.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr10</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.64.1">.</span></span></p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.65.1">Introduction to Streamlit</span></h1>
<p><span class="koboSpan" id="kobo.66.1">If </span><a id="_idIndexMarker1267"/><span class="koboSpan" id="kobo.67.1">readers are familiar with Streamlit, they can move on to the </span><em class="italic"><span class="koboSpan" id="kobo.68.1">Creating an application with Streamlit and AI agents</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.69.1">section directly.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">Companies have invested heavily in data science and AI. </span><span class="koboSpan" id="kobo.70.2">The models that are trained can guide business decisions and provide different insights. </span><span class="koboSpan" id="kobo.70.3">Training a model, using it, and extracting insights requires expertise that not everyone has. </span><span class="koboSpan" id="kobo.70.4">A model that is truly useful for a company must provide results that must then be used by other stakeholders as well. </span><span class="koboSpan" id="kobo.70.5">For example, when you train a model, it should generate results that are usable by other people. </span><span class="koboSpan" id="kobo.70.6">It is possible to create static visualizations of the data (exporting graphs), but they convey only limited information. </span><span class="koboSpan" id="kobo.70.7">One could provide information in a Jupyter notebook but not everyone is capable of using such a tool. </span><span class="koboSpan" id="kobo.70.8">One option that might allow easier access by others is to create a dashboard or </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">web application.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">This is where Streamlit </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">comes in.</span></span></p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.74.1">Starting with Streamlit</span></h2>
<p><span class="koboSpan" id="kobo.75.1">Streamlit is </span><a id="_idIndexMarker1268"/><span class="koboSpan" id="kobo.76.1">a web application framework that allows one to easily and intuitively create web applications with Python. </span><span class="koboSpan" id="kobo.76.2">Its library provides a number of built-in components for both the backend and the frontend. </span><span class="koboSpan" id="kobo.76.3">It is also compatible with leading machine learning, graph, and </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">plotting libraries.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">The objective of this section is to understand how Streamlit works and how it can be a </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">powerful tool.</span></span></p>
<p><span class="koboSpan" id="kobo.80.1">One of the advantages of Streamlit is its ease of use and installation. </span><span class="koboSpan" id="kobo.80.2">Streamlit can simply be installed from the terminal and is present in </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">Anaconda distributions:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.82.1">
pip install streamlit</span></pre> <p><span class="koboSpan" id="kobo.83.1">Organizing an app with Streamlit is a simple Python script that typically contains both the backend and the frontend. </span><span class="koboSpan" id="kobo.83.2">This script can then be run either locally or in the cloud. </span><span class="koboSpan" id="kobo.83.3">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.84.1">my_app.py</span></strong><span class="koboSpan" id="kobo.85.1"> should contain within it all the elements to build a web app. </span><span class="koboSpan" id="kobo.85.2">In the simplest cases, with just a few lines of code, we can build a web app. </span><span class="koboSpan" id="kobo.85.3">Once we define our app, running it locally is </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">really simple:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.87.1">
streamlit run my_app.py</span></pre> <p><span class="koboSpan" id="kobo.88.1">What we need to do is call Streamlit and the name of our app (obviously, if we are using a terminal, we need to be in the right directory). </span><span class="koboSpan" id="kobo.88.2">Actually, the script does not have to be in your local directory; it can be on the internet. </span><span class="koboSpan" id="kobo.88.3">For example, our script is in our repository on GitHub, and we want to run </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">it locally:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.90.1">
streamlit run https://raw.githubusercontent.com/streamlit/my_apps/master/my_app.py</span></pre> <p><span class="koboSpan" id="kobo.91.1">Under the hood, Streamlit </span><a id="_idIndexMarker1269"/><span class="koboSpan" id="kobo.92.1">runs through the file and executes the elements it finds sequentially. </span><span class="koboSpan" id="kobo.92.2">After that is done, a local Streamlit server will be initialized and your app will open in a new tab in your default web browser. </span><span class="koboSpan" id="kobo.92.3">Note that everything we write is in Python, and no other language is required. </span><span class="koboSpan" id="kobo.92.4">When we make a change, we must save our source. </span><span class="koboSpan" id="kobo.92.5">Streamlit detects any modifications and prompts us to rerun the app. </span><span class="koboSpan" id="kobo.92.6">This allows for quick iterations while immediately observing the effects, ensuring a seamless feedback loop between writing and running </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">the application.</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">An example of a simple app is </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.96.1">
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
# Title for the app
st.title("Simple Streamlit App with Box Plot")
# Create a sample DataFrame
data = {
    'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
    'Values': [10, 20, 15, 25, 30, 20, 35, 40, 45]
}
df = pd.DataFrame(data)
# Display the DataFrame
st.write("Here is the sample DataFrame:")
st.dataframe(df)
# Create a box plot
fig, ax = plt.subplots()
df.boxplot(column='Values', by='Category', ax=ax, grid=False)
plt.title("Box Plot of Values by Category")
plt.suptitle("")  # Remove the automatic subtitle
plt.xlabel("Category")
plt.ylabel("Values")
# Display the plot in Streamlit
st.pyplot(fig)</span></pre> <p><span class="koboSpan" id="kobo.97.1">The app we generated simply does three things: it creates a DataFrame with pandas, plots it, and then produces a box plot. </span><span class="koboSpan" id="kobo.97.2">In a few lines of code, we have created a mini web application that is accessible on our browser. </span><span class="koboSpan" id="kobo.97.3">Once we have written it, we just have to run it and then Streamlit</span><a id="_idIndexMarker1270"/><span class="koboSpan" id="kobo.98.1"> takes care of </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">everything else.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer302">
<span class="koboSpan" id="kobo.100.1"><img alt="Figure 10.1 – Example of a web application" src="image/B21257_10_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.101.1">Figure 10.1 – Example of a web application</span></p>
<p><span class="koboSpan" id="kobo.102.1">Let’s look at the </span><a id="_idIndexMarker1271"/><span class="koboSpan" id="kobo.103.1">code block in a bit </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">more detail:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">st.title</span></strong><span class="koboSpan" id="kobo.106.1">: This is a text element that allows us to display the title of our app. </span><span class="koboSpan" id="kobo.106.2">It is a good idea to always include it in </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">an app.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.108.1">st.write</span></strong><span class="koboSpan" id="kobo.109.1">: This is considered the Swiss army knife of Streamlit. </span><span class="koboSpan" id="kobo.109.2">Its main purpose is to write both textual and other elements. </span><span class="koboSpan" id="kobo.109.3">In this case, we have shown how passing a DataFrame is written to the app in nice formatting. </span><span class="koboSpan" id="kobo.109.4">In addition, this element is interactive. </span><span class="koboSpan" id="kobo.109.5">In other words, its behavior depends on the input given to it. </span><span class="koboSpan" id="kobo.109.6">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.110.1">write()</span></strong><span class="koboSpan" id="kobo.111.1"> function is not limited to text but can be used with images, other Python elements (such as lists and dictionaries), templates, and so on. </span><span class="koboSpan" id="kobo.111.2">It also allows us to insert commands with HTML if we want to edit </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">our text.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">st.pyplot</span></strong><span class="koboSpan" id="kobo.114.1">: This displays a Matplotlib figure – in our case, a box plot. </span><span class="koboSpan" id="kobo.114.2">As you can see, we generated our figure first and then called </span><strong class="source-inline"><span class="koboSpan" id="kobo.115.1">pyplot()</span></strong><span class="koboSpan" id="kobo.116.1"> for the subsequent plotting. </span><span class="koboSpan" id="kobo.116.2">The figure is generated before being actually shown. </span><span class="koboSpan" id="kobo.116.3">In other words, the figure is already present in memory; we need </span><strong class="source-inline"><span class="koboSpan" id="kobo.117.1">pyplot()</span></strong><span class="koboSpan" id="kobo.118.1"> to display the figure to the user in the app. </span><span class="koboSpan" id="kobo.118.2">Actually, we could also call plotting directly with Matplotlib, but this is not recommended because it could lead to </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">unexpected behavior.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.120.1">Note that we have only shown some basic commands, but Streamlit is quite flexible. </span><span class="koboSpan" id="kobo.120.2">For example, the DataFrame can be written to the app in different ways. </span><span class="koboSpan" id="kobo.120.3">Using </span><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">st.write()</span></strong><span class="koboSpan" id="kobo.122.1"> is just one way: </span><strong class="source-inline"><span class="koboSpan" id="kobo.123.1">st.dataframe()</span></strong><span class="koboSpan" id="kobo.124.1"> does the same as </span><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">st.write()</span></strong><span class="koboSpan" id="kobo.126.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.127.1">st.table()</span></strong><span class="koboSpan" id="kobo.128.1"> allows us to render the table statically, and writing </span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">'df'</span></strong><span class="koboSpan" id="kobo.130.1"> directly acts as if we were using </span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">st.write()</span></strong><span class="koboSpan" id="kobo.132.1">. </span><span class="koboSpan" id="kobo.132.2">It is recommended to use one of the built-in methods because the behavior is known and we can also use additional arguments to handle </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">the output.</span></span></p>
<p><span class="koboSpan" id="kobo.134.1">For example, we can use the flexibility provided by the built-in method,  </span><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">st.dataframe()</span></strong><span class="koboSpan" id="kobo.136.1">, to highlight elements in </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">our DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.138.1">
df = pd.DataFrame(data)
st.dataframe(df.style.highlight_max(axis=0))</span></pre> <div>
<div class="IMG---Figure" id="_idContainer303">
<span class="koboSpan" id="kobo.139.1"><img alt="Figure 10.2 – Change in style in the DataFrame rendering" src="image/B21257_10_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.140.1">Figure 10.2 – Change in style in the DataFrame rendering</span></p>
<p><span class="koboSpan" id="kobo.141.1">In addition, Streamlit </span><a id="_idIndexMarker1272"/><span class="koboSpan" id="kobo.142.1">also makes it easy to add maps to our application. </span><span class="koboSpan" id="kobo.142.2">Just provide the coordinates, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">st.map()</span></strong><span class="koboSpan" id="kobo.144.1"> magically allows us to have a map in our application (a map that we can enlarge and move). </span><span class="koboSpan" id="kobo.144.2">In this case, we provided the coordinates of some </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">Sicilian cities:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.146.1">
city_data = {
    'City': ['Palermo', 'Syracuse', 'Catania', 'Agrigento'],
    'latitude': [38.1157, 37.0757, 37.5079, 37.2982],
    'longitude': [13.3615, 15.2867, 15.0830, 13.5763]
}
city_data = pd.DataFrame(city_data)
st.map(city_data)</span></pre> <div>
<div class="IMG---Figure" id="_idContainer304">
<span class="koboSpan" id="kobo.147.1"><img alt="Figure 10.3 – Plotting a map with Streamlit" src="image/B21257_10_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.148.1">Figure 10.3 – Plotting a map with Streamlit</span></p>
<p><span class="koboSpan" id="kobo.149.1">As can be seen, we </span><a id="_idIndexMarker1273"/><span class="koboSpan" id="kobo.150.1">have added some elements and made some changes to our app (adding a map). </span><span class="koboSpan" id="kobo.150.2">Whenever we modify the code, we should remember to save the changes to the script; then, we go to our app and press the </span><em class="italic"><span class="koboSpan" id="kobo.151.1">R</span></em><span class="koboSpan" id="kobo.152.1"> key, which will reload the app with </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">the updates.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">If there are any errors, Streamlit will provide us with error messages indicating what we need to correct. </span><span class="koboSpan" id="kobo.154.2">An example of an error is shown in the following figure (in this case, about the variable name </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">to use):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer305">
<span class="koboSpan" id="kobo.156.1"><img alt="Figure 10.4 – Example of an error" src="image/B21257_10_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.157.1">Figure 10.4 – Example of an error</span></p>
<p><span class="koboSpan" id="kobo.158.1">For debugging, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.159.1">st.write()</span></strong><span class="koboSpan" id="kobo.160.1"> extensively; this simple function can print almost any Python object by </span><a id="_idIndexMarker1274"/><span class="koboSpan" id="kobo.161.1">guiding us to understand what the error is. </span><span class="koboSpan" id="kobo.161.2">For example, we can use it in this case. </span><span class="koboSpan" id="kobo.161.3">As we can see, we have an error in the column names (</span><em class="italic"><span class="koboSpan" id="kobo.162.1">Latitude</span></em><span class="koboSpan" id="kobo.163.1"> should be lowercase; so, we substitute it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">correct name):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.165.1">
st.write(city_data)</span></pre> <div>
<div class="IMG---Figure" id="_idContainer306">
<span class="koboSpan" id="kobo.166.1"><img alt="Figure 10.5 – Using st.write() to debug" src="image/B21257_10_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.167.1">Figure 10.5 – Using st.write() to debug</span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.168.1">Caching the results</span></h2>
<p><span class="koboSpan" id="kobo.169.1">Caching allows </span><a id="_idIndexMarker1275"/><span class="koboSpan" id="kobo.170.1">our app to remain performant even if data is loaded from the web (we will discuss how to add data from the web or the user later). </span><span class="koboSpan" id="kobo.170.2">It also allows it to manipulate large datasets or use machine learning models. </span><span class="koboSpan" id="kobo.170.3">So far, we have been using small datasets and hence we could load anything, but what if we start putting models of millions of parameters inside our app? </span><span class="koboSpan" id="kobo.170.4">Our app might crash. </span><span class="koboSpan" id="kobo.170.5">If we use models or other elements that require long computations, we need to focus on optimizing our app’s efficiency by caching results in memory and avoiding redundant calculations. </span><span class="koboSpan" id="kobo.170.6">We can see the cache as a kind of short-term memory, where we keep information that we use often or think will be useful to safeguard. </span><span class="koboSpan" id="kobo.170.7">Caching</span><a id="_idIndexMarker1276"/><span class="koboSpan" id="kobo.171.1"> allows us to reuse this information and save computation. </span><span class="koboSpan" id="kobo.171.2">If we have a function that performs a large computation, we can use </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">two alternatives:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.173.1">st.cache_data</span></strong><span class="koboSpan" id="kobo.174.1">: This is a decorator in Streamlit</span><a id="_idIndexMarker1277"/><span class="koboSpan" id="kobo.175.1"> that is used to cache the results of a function so that the function need not be recomputed every time the app is rerun (such as when a user interacts with widgets or the app reloads). </span><span class="koboSpan" id="kobo.175.2">This decorator is recommended for cache computations that return data. </span><span class="koboSpan" id="kobo.175.3">One should use </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">st.cache_data</span></strong><span class="koboSpan" id="kobo.177.1"> when a function returns a serializable data object (e.g., </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">str</span></strong><span class="koboSpan" id="kobo.179.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">int</span></strong><span class="koboSpan" id="kobo.181.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">float</span></strong><span class="koboSpan" id="kobo.183.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">DataFrame</span></strong><span class="koboSpan" id="kobo.185.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.186.1">dict</span></strong><span class="koboSpan" id="kobo.187.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.188.1">list</span></strong><span class="koboSpan" id="kobo.189.1">). </span><span class="koboSpan" id="kobo.189.2">When a function is wrapped with </span><strong class="source-inline"><span class="koboSpan" id="kobo.190.1">@st.cache_data</span></strong><span class="koboSpan" id="kobo.191.1">, the first time the function is called, Streamlit stores the result in memory or a disk cache, depending on the configuration. </span><span class="koboSpan" id="kobo.191.2">On subsequent calls with the same arguments, Streamlit returns the cached result, which is much faster than recomputing it. </span><span class="koboSpan" id="kobo.191.3">It speeds up the app by preventing redundant work, especially for functions that take a long time to execute. </span><span class="koboSpan" id="kobo.191.4">If the inputs to the function change, Streamlit will invalidate the cache and recompute </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">the function.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">st.cache_resource</span></strong><span class="koboSpan" id="kobo.194.1">: This is another decorator in Streamlit, introduced to handle the caching of resources – specifically, objects or expensive operations that do not depend on the function arguments but instead represent reusable resources that can be cached for the lifetime of the app. </span><span class="koboSpan" id="kobo.194.2">While </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">st.cache_data</span></strong><span class="koboSpan" id="kobo.196.1"> is used for caching the results of computations or data loads based on the inputs, </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">st.cache_resource</span></strong><span class="koboSpan" id="kobo.198.1"> is designed to cache resources such as database connections, model objects, or any other object that is expensive to create or initialize but doesn’t change with each function call. </span><span class="koboSpan" id="kobo.198.2">Use this for caching resources such as database connections, machine learning models, network connections, or any expensive resource that needs to be reused across multiple runs of the app. </span><span class="koboSpan" id="kobo.198.3">If an object or resource (e.g., a pre-trained model) is expensive to create, you can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">st.cache_resource</span></strong><span class="koboSpan" id="kobo.200.1"> to avoid reloading or reinitializing it </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">multiple times.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.202.1">For example, for </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">st.cache_data</span></strong><span class="koboSpan" id="kobo.204.1">, in the </span><a id="_idIndexMarker1278"/><span class="koboSpan" id="kobo.205.1">following code, we are simulating a slow operation and showing how caching is </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">saving time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.207.1">
import streamlit as st
import time
# Use @st.cache_data to cache the result
@st.cache_data
def load_data():
    time.sleep(3)  # Simulate a slow operation (e.g., loading a large dataset)
    return "Data loaded!"
</span><span class="koboSpan" id="kobo.207.2"># Call the function
st.write(load_data())
in a similar way for st.cache_resource:
import streamlit as st
import time
# Example: A resource-intensive function (e.g., loading a model)
@st.cache_resource
def load_model():
    time.sleep(5)  # Simulate a slow operation like loading a model
    return "Model loaded!"  # This could be a model object in a real scenario
# Call the function
st.write(load_model())</span></pre> <p><span class="koboSpan" id="kobo.208.1">In the preceding snippet, under</span><a id="_idIndexMarker1279"/><span class="koboSpan" id="kobo.209.1"> the hood, before running a function, Streamlit checks its cache for a previously saved result. </span><span class="koboSpan" id="kobo.209.2">If it finds one, it uses that instead of running the function; if it doesn’t find it, it runs the function and saves it in the cache. </span><span class="koboSpan" id="kobo.209.3">The cache is updated during execution, especially if the </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">code changes.</span></span></p>
<p><span class="koboSpan" id="kobo.211.1">By default, Streamlit doesn’t save the information between app reruns, but with each rerun, it reruns the app from top to bottom. </span><span class="koboSpan" id="kobo.211.2">Normally, Streamlit reruns the entire script whenever there’s an interaction (e.g., when a user adjusts a slider or clicks a button). </span><span class="koboSpan" id="kobo.211.3">With session state, you can store data that persists during these reruns so you don’t lose values when the script reruns. </span><span class="koboSpan" id="kobo.211.4">Each user gets their own independent session state, so data stored in the session state is isolated from other users. </span><span class="koboSpan" id="kobo.211.5">You can use the session state to store things such as form inputs, counters, authentication data, or intermediate </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">computation results.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">Let’s try building an app that makes a shopping list; we will show how to save information about </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">the session:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.215.1">
import streamlit as st
# Define a list of grocery items (the initial list of items to buy)
grocery_items = ['Apple', 'Banana', 'Carrot', 'Milk', 'Eggs']
# Streamlit app interface
st.title('Grocery List App')
# Text input to add a new item to the list
new_item = st.text_input("Add a new item to your grocery list:")
# Button to add the new item to the list
if st.button('Add Item'):
    if new_item:
        grocery_items.append(new_item)
        st.success(f"'{new_item}' has been added to your list!")
    else:
        st.warning("Please enter an item to add.")
# Display the current list of grocery items
st.write("### Items to Buy:")
for item in grocery_items:
    st.write(f"- {item}")</span></pre> <p><span class="koboSpan" id="kobo.216.1">This is our</span><a id="_idIndexMarker1280"/><span class="koboSpan" id="kobo.217.1"> initial app; we will see immediately afterward how we can view information saved by </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">the user:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer307">
<span class="koboSpan" id="kobo.219.1"><img alt="Figure 10.6 – Example of grocery list app" src="image/B21257_10_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.220.1">Figure 10.6 – Example of grocery list app</span></p>
<p><span class="koboSpan" id="kobo.221.1">If we add objects by </span><a id="_idIndexMarker1281"/><span class="koboSpan" id="kobo.222.1">clicking on </span><strong class="bold"><span class="koboSpan" id="kobo.223.1">Add Item</span></strong><span class="koboSpan" id="kobo.224.1">, they will be added to the list (at this time, the information is not saved; it remains only for </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">the session):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer308">
<span class="koboSpan" id="kobo.226.1"><img alt="Figure 10.7 – Example of adding objects to the grocery list app" src="image/B21257_10_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.227.1">Figure 10.7 – Example of adding objects to the grocery list app</span></p>
<p><span class="koboSpan" id="kobo.228.1">However, if we press </span><em class="italic"><span class="koboSpan" id="kobo.229.1">R</span></em><span class="koboSpan" id="kobo.230.1"> and</span><a id="_idIndexMarker1282"/><span class="koboSpan" id="kobo.231.1"> rerun our app, we will lose this information, and the elements will disappear (because the information is not </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">saved anywhere).</span></span></p>
<p><span class="koboSpan" id="kobo.233.1">Now, let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">try </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">session_state</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.237.1">
import streamlit as st
# Initialize session state for grocery_items if it doesn't exist yet
if 'grocery_items' not in st.session_state:
    st.session_state.grocery_items = ['Apple', 'Banana', 'Carrot', 'Milk', 'Eggs']
# Streamlit app interface
st.title('Grocery List App')
# Text input to add a new item to the list
new_item = st.text_input("Add a new item to your grocery list:")
# Button to add the new item to the list
if st.button('Add Item'):
    if new_item:
        # Append the new item to the list stored in session state
        st.session_state.grocery_items.append(new_item)
        st.success(f"'{new_item}' has been added to your list!")
    else:
        st.warning("Please enter an item to add.")
# Display the current list of grocery items
st.write("### Items to Buy:")
for item in st.session_state.grocery_items:
    st.write(f"- {item}")</span></pre> <p><span class="koboSpan" id="kobo.238.1">When we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">st.session_state</span></strong><span class="koboSpan" id="kobo.240.1">, the items we add will be preserved during the current session. </span><span class="koboSpan" id="kobo.240.2">On the first run, the list will contain the initial elements, and as the user adds more items, the list will </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">grow accordingly.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">However, once the</span><a id="_idIndexMarker1283"/><span class="koboSpan" id="kobo.243.1"> page is reloaded or the session ends, the list will reset unless we store the data in a persistent location (e.g., a file </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">or database).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer309">
<span class="koboSpan" id="kobo.245.1"><img alt="Figure 10.8 – Updated list" src="image/B21257_10_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.246.1">Figure 10.8 – Updated list</span></p>
<p><span class="koboSpan" id="kobo.247.1">While using </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">st.session_state</span></strong><span class="koboSpan" id="kobo.249.1"> allows temporary storage of values during a user session—gradually filling up as interactions occur—this data is lost upon a full page reload or app restart. </span><span class="koboSpan" id="kobo.249.2">In contrast, </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">st.connection</span></strong><span class="koboSpan" id="kobo.251.1"> enables Streamlit to maintain persistent access to external resources, ensuring that data remains available across sessions and reloads. </span><span class="koboSpan" id="kobo.251.2">This makes it ideal for applications that require consistent interaction with long-lived data, overcoming the limitations of in-memory session state. </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">st.connection</span></strong><span class="koboSpan" id="kobo.253.1"> allows the connection to external services to be maintained and reused and does so efficiently with each </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">user interaction.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">Let’s see how </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">st.connection</span></strong><span class="koboSpan" id="kobo.257.1"> works</span><a id="_idIndexMarker1284"/> <span class="No-Break"><span class="koboSpan" id="kobo.258.1">in practice:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.259.1">
import streamlit as st
conn = st.connection("my_database_sql")
df = conn.query("select * from my_beautiful_table")
st.dataframe(df)</span></pre> <p><span class="koboSpan" id="kobo.260.1">In this section, we discussed the main components of a Streamlit application. </span><span class="koboSpan" id="kobo.260.2">In the next one, we will discuss how to beautify our app and make the user </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">experience better.</span></span></p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.262.1">Developing our frontend with Streamlit</span></h1>
<p><span class="koboSpan" id="kobo.263.1">In this section, we will begin to discuss some of the elements that allow us to improve the user experience when interacting with </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">our app.</span></span></p>
<p><span class="koboSpan" id="kobo.265.1">We will show the various frontend elements and how to combine them for </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">complex apps.</span></span></p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.267.1">Adding the text elements</span></h2>
<p><span class="koboSpan" id="kobo.268.1">To improve our</span><a id="_idIndexMarker1285"/><span class="koboSpan" id="kobo.269.1"> user experience, we can start by improving the text elements. </span><span class="koboSpan" id="kobo.269.2">The first elements we add are </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">the following:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">st.title()</span></strong><span class="koboSpan" id="kobo.272.1">: This sets the main title of your Streamlit app. </span><span class="koboSpan" id="kobo.272.2">It’s the largest text element and is typically used for the main heading of your app. </span><span class="koboSpan" id="kobo.272.3">Every app should have at least one title, and this is shown in the GitHub-flavored Markdown. </span><span class="koboSpan" id="kobo.272.4">This function obviously takes </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">a string.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">st.header()</span></strong><span class="koboSpan" id="kobo.275.1">: This adds a header to your app. </span><span class="koboSpan" id="kobo.275.2">It’s smaller than the title but still stands out as an important section heading. </span><span class="koboSpan" id="kobo.275.3">This also has a counterpart in GitHub and is similar in purpose. </span><span class="koboSpan" id="kobo.275.4">One attribute you can add is </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">divider</span></strong><span class="koboSpan" id="kobo.277.1">, which shows a colored divider below the header (we can specify a color). </span><span class="koboSpan" id="kobo.277.2">Also, we can add a </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">help</span></strong><span class="koboSpan" id="kobo.279.1"> string that provides a tooltip next to </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">the header.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">st.subheader()</span></strong><span class="koboSpan" id="kobo.282.1">: This adds a subheader, which is smaller than the header and is typically used for subsections or to provide additional structure to the content. </span><span class="koboSpan" id="kobo.282.2">The subheader can also have a colored divider if you want one. </span><span class="koboSpan" id="kobo.282.3">A help </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">string</span></strong><span class="koboSpan" id="kobo.284.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">also possible.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.286.1">Here are some </span><a id="_idIndexMarker1286"/><span class="koboSpan" id="kobo.287.1">examples of how to insert </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">these elements:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.289.1">
st.title("Your Title Here")
st.header("Your Header Here")
st.header("Your Header Here", divider=True, help ="bla bla")
st.subheader("Your Subheader Here")</span></pre> <p><span class="koboSpan" id="kobo.290.1">Now, we can test them directly in </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">our app:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.292.1">
import streamlit as st
# Initialize session state for grocery_items if it doesn't exist yet
if 'grocery_items' not in st.session_state:
    st.session_state.grocery_items = ['Apple', 'Banana', 'Carrot', 'Milk', 'Eggs']
# Streamlit app interface
st.title('Grocery List App :banana: :apple: :egg:')  # Main title of the app
# Display a header for the section where the user can add items
st.header('Add new item')
# Text input to add a new item to the list
new_item = st.text_input("Type an item to add to your grocery list:")
# Button to add the new item to the list
if st.button('Add Item'):
    if new_item:
        # Append the new item to the list stored in session state
        st.session_state.grocery_items.append(new_item)
        st.success(f"'{new_item}' has been added to your list!")
    else:
        st.warning("Please enter an item to add.")
# Display a subheader for the current grocery list
st.subheader('Current Grocery List')
# Display the current list of grocery items
for item in st.session_state.grocery_items:
    st.write(f"- {item}")</span></pre> <p><span class="koboSpan" id="kobo.293.1">This code</span><a id="_idIndexMarker1287"/><span class="koboSpan" id="kobo.294.1"> shows how to start inserting stylistic elements into our app. </span><span class="koboSpan" id="kobo.294.2">The following figure shows the result after </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">these improvements:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer310">
<span class="koboSpan" id="kobo.296.1"><img alt="Figure 10.9 – Updated app" src="image/B21257_10_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.297.1">Figure 10.9 – Updated app</span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.298.1">Inserting images in a Streamlit app</span></h2>
<p><span class="koboSpan" id="kobo.299.1">Next, we begin the </span><a id="_idIndexMarker1288"/><span class="koboSpan" id="kobo.300.1">customization of our app, adding both a logo and an image. </span><span class="koboSpan" id="kobo.300.2">To do this, we will use </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">several elements:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">st.set_page_config(...)</span></strong><span class="koboSpan" id="kobo.303.1">: This function is used to configure the Streamlit app’s page settings, such as the title of the page, favicon (icon in the browser tab), and layout preferences. </span><span class="koboSpan" id="kobo.303.2">In this case, we will use it to add a small icon that will be seen as a browser </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">tab element.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.305.1">st.image(...)</span></strong><span class="koboSpan" id="kobo.306.1">: This function displays an image in the Streamlit app. </span><span class="koboSpan" id="kobo.306.2">It takes the URL or path of the image and can adjust its width to fit the screen with </span><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">use_column_width=True</span></strong><span class="koboSpan" id="kobo.308.1">. </span><span class="koboSpan" id="kobo.308.2">As input, </span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">st.image</span></strong><span class="koboSpan" id="kobo.310.1"> takes either a URL (as we are doing in this case) or a path to a local image or </span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">numpy.array</span></strong><span class="koboSpan" id="kobo.312.1"> (the image can be in </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">number format).</span></span><p class="list-inset"><span class="koboSpan" id="kobo.314.1">One of the keywords is </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">caption</span></strong><span class="koboSpan" id="kobo.316.1">, which allows us to provide a caption for the image directly. </span><span class="koboSpan" id="kobo.316.2">In our case, however, we will add the </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">caption separately.</span></span></p></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.318.1">st.caption(...)</span></strong><span class="koboSpan" id="kobo.319.1">: This function adds a small caption or descriptive text below elements, such as images or charts. </span><span class="koboSpan" id="kobo.319.2">In our app, it provides the </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">image credit.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.321.1">st.sidebar.image(...)</span></strong><span class="koboSpan" id="kobo.322.1">: This places an image in the sidebar, which will be the collapsible menu on the left side of the app. </span><span class="koboSpan" id="kobo.322.2">The sidebar is useful for placing navigation, settings, or </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">additional content.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.324.1">We will now insert </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">an</span></span><span class="No-Break"><a id="_idIndexMarker1289"/></span><span class="No-Break"><span class="koboSpan" id="kobo.326.1"> image:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.327.1">
# Set the page configuration to include a logo
st.set_page_config(page_title="Grocery List App", page_icon="https://github.com/SalvatoreRa/tutorial/blob/main/images/vegetable_basket_logo.jpg?raw=true")
# Display the title image
st.image("https://github.com/SalvatoreRa/tutorial/blob/main/images/vegetables.jpg?raw=true", use_column_width=True)
st.caption("Image from [here](https://unsplash.com/it/@randyfath)")
# Add logo to the sidebar
st.sidebar.image("https://github.com/SalvatoreRa/tutorial/blob/main/images/vegetable_basket_logo.jpg?raw=true", use_column_width=True)</span></pre> <p><span class="koboSpan" id="kobo.328.1">The preceding code shows how to insert an image with the proper caption. </span><span class="koboSpan" id="kobo.328.2">The following figure shows </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">the results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer311">
<span class="koboSpan" id="kobo.330.1"><img alt="Figure 10.10 – Changes in appearance in the app" src="image/B21257_10_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.331.1">Figure 10.10 – Changes in appearance in the app</span></p>
<p><span class="koboSpan" id="kobo.332.1">Here is our </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">browser icon:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer312">
<span class="koboSpan" id="kobo.334.1"><img alt="Figure 10.11 – The browser icon" src="image/B21257_10_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.335.1">Figure 10.11 – The browser icon</span></p>
<p><span class="koboSpan" id="kobo.336.1">Thus far, we have explored the basic features of Streamlit and used them to build a simple and static app. </span><span class="koboSpan" id="kobo.336.2">Now it’s time to move beyond and start exploring what makes a Streamlit app dynamic, responsive, and connected to </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">real use.</span></span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.338.1">Creating a dynamic app</span></h2>
<p><span class="koboSpan" id="kobo.339.1">We can further modify our</span><a id="_idIndexMarker1290"/><span class="koboSpan" id="kobo.340.1"> app to make it more dynamic. </span><span class="koboSpan" id="kobo.340.2">So far, our user can only add items to their list, and then the list is shown. </span><span class="koboSpan" id="kobo.340.3">This app is of little use, so we want to make it more dynamic and allow the user to add quantities. </span><span class="koboSpan" id="kobo.340.4">So, we’re going to do </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.342.1">Allow the user to add an item to buy. </span><span class="koboSpan" id="kobo.342.2">Once the item is added, two sliders are created that represent the quantity the user has at home and how much they have to buy. </span><span class="koboSpan" id="kobo.342.3">To avoid creating an endless list, we will use two columns. </span><span class="koboSpan" id="kobo.342.4">In addition, we will add a button to select whether or not the user has taken </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">the ingredient.</span></span></li>
<li><span class="koboSpan" id="kobo.344.1">Make an interactive display of a table with ingredients, showing how much to buy, how much was taken, and whether it was taken, as well as a completion bar that shows how many items have been taken and how many </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">are missing.</span></span></li>
<li><span class="koboSpan" id="kobo.346.1">In the sidebar, add a button to download </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">the list.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.348.1">Let’s start by </span><a id="_idIndexMarker1291"/><span class="koboSpan" id="kobo.349.1">displaying the grocery list items in a structured manner using two columns, ensuring a more compact and visually </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">balanced layout:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.351.1">
data = []
for i, item in enumerate(st.session_state.grocery_items):
    with col1 if i % 2 == 0 else col2:
        st.markdown(f"**{item}**")
        quantity_at_home = st.slider(f"Quantity at home", 0, 12, st.session_state.quantity_at_home[item], key=f"home_{item}")
        st.session_state.quantity_at_home[item] = quantity_at_home
        quantity_to_take = st.slider(f"Quantity to take", 0, 12, st.session_state.quantity_to_take[item], key=f"take_{item}")
        st.session_state.quantity_to_take[item] = quantity_to_take
        taken = st.checkbox(f"Taken", st.session_state.taken[item], key=f"taken_{item}")
        st.session_state.taken[item] = taken
        data.append([item, quantity_at_home, quantity_to_take, "Yes" if taken else "No"])</span></pre> <p><span class="koboSpan" id="kobo.352.1">For each item, we determine whether it should be placed in the first column (</span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">col1</span></strong><span class="koboSpan" id="kobo.354.1">) or the second column (</span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">col2</span></strong><span class="koboSpan" id="kobo.356.1">) based on whether the index, </span><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">i</span></strong><span class="koboSpan" id="kobo.358.1">, is even or odd. </span><span class="koboSpan" id="kobo.358.2">This ensures that items are distributed evenly between the two columns, preventing a long </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">vertical list.</span></span></p>
<p><span class="koboSpan" id="kobo.360.1">Inside the</span><a id="_idIndexMarker1292"/><span class="koboSpan" id="kobo.361.1"> selected column, the item name is displayed in bold using </span><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">st.markdown()</span></strong><span class="koboSpan" id="kobo.363.1">. </span><span class="koboSpan" id="kobo.363.2">Below the name, two sliders are created: one for the quantity the user has at home and another for the quantity they need to take. </span><span class="koboSpan" id="kobo.363.3">Each slider is assigned a unique key based on the item name to ensure proper tracking and persistence of values. </span><span class="koboSpan" id="kobo.363.4">The values from these sliders are stored back into the session state so they remain updated across app interactions. </span><span class="koboSpan" id="kobo.363.5">In addition, a checkbox is included for each item. </span><span class="koboSpan" id="kobo.363.6">The collected data for each item, including its name, the selected quantities, and whether it has been taken or not, is appended to the </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">data list.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer313">
<span class="koboSpan" id="kobo.365.1"><img alt="Figure 10.12 – Restyling of the app" src="image/B21257_10_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.366.1">Figure 10.12 – Restyling of the app</span></p>
<p><span class="koboSpan" id="kobo.367.1">Notice that the app is interactive (we can interact </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">with sliders):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer314">
<span class="koboSpan" id="kobo.369.1"><img alt="Figure 10.13 – Interactive elements" src="image/B21257_10_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.370.1">Figure 10.13 – Interactive elements</span></p>
<p><span class="koboSpan" id="kobo.371.1">The preceding </span><a id="_idIndexMarker1293"/><span class="koboSpan" id="kobo.372.1">figure shows us how to insert interactive elements and how we can interact with them. </span><span class="koboSpan" id="kobo.372.2">Streamlit allows this in the background, without the need for us to code these complex elements, and we can use </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">simple commands.</span></span></p>
<p><span class="koboSpan" id="kobo.374.1">We can then display </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">the table:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.376.1">
df = pd.DataFrame(data, columns=["Name", "Quantity at Home", "Quantity to Take", "Taken"])
st.table(df)</span></pre> <div>
<div class="IMG---Figure" id="_idContainer315">
<span class="koboSpan" id="kobo.377.1"><img alt="Figure 10.14 – Table obtained" src="image/B21257_10_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.378.1">Figure 10.14 – Table obtained</span></p>
<p><span class="koboSpan" id="kobo.379.1">At this point, we</span><a id="_idIndexMarker1294"/><span class="koboSpan" id="kobo.380.1"> can create our </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">progress bar:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.382.1">
# Progress bar
taken_count = sum(1 for item in st.session_state.taken.values() if item)
total_items = len(st.session_state.grocery_items)
progress = taken_count / total_items if total_items &gt; 0 else 0
st.subheader("Grocery Completion Progress")
st.progress(progress)
st.write(f"{taken_count} out of {total_items} items taken ({progress*100:.2f}%)")</span></pre> <div>
<div class="IMG---Figure" id="_idContainer316">
<span class="koboSpan" id="kobo.383.1"><img alt="Figure 10.15 – Progress bar" src="image/B21257_10_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.384.1">Figure 10.15 – Progress bar</span></p>
<p><span class="koboSpan" id="kobo.385.1">Next, we define a </span><a id="_idIndexMarker1295"/><span class="koboSpan" id="kobo.386.1">function, </span><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">generate_pdf()</span></strong><span class="koboSpan" id="kobo.388.1">, which creates a PDF document containing the grocery list data and allows users to </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">download it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.390.1">
# Function to generate PDF
def generate_pdf():
    pdf = FPDF()
    pdf.set_auto_page_break(auto=True, margin=15)
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    logo_path = "logo.jpg"  # Add logo to PDF
    response = requests.get(logo_url)
    with open(logo_path, "wb") as f:
        f.write(response.content)
    pdf.image(logo_path, 10, 10, 30)  # Position and size of the logo
    pdf.cell(200, 10, "Grocery List", ln=True, align='C')
    pdf.ln(20)  # Added extra spacing to prevent text overlapping the logo
    for index, row in df.iterrows():
        pdf.cell(0, 10, f"{row['Name']} - At Home: {row['Quantity at Home']} - To Take: {row['Quantity to Take']} - Taken: {row['Taken']}", ln=True)
    pdf_output = os.path.join(os.getcwd(), "grocery_list.pdf")
    pdf.output(pdf_output)
    return pdf_output
# Directly download the PDF when the button is clicked
if st.sidebar.button("Download List as PDF"):
    pdf_file = generate_pdf()
    with open(pdf_file, "rb") as f:
        st.sidebar.download_button("Download Grocery List PDF", f, file_name="grocery_list.pdf", mime="application/pdf", key="download_pdf", on_click=None)</span></pre> <p><span class="koboSpan" id="kobo.391.1">First, we initialize an </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">FPDF</span></strong><span class="koboSpan" id="kobo.393.1"> object with automatic page breaks and add a new page. </span><span class="koboSpan" id="kobo.393.2">The font is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.394.1">Arial</span></strong><span class="koboSpan" id="kobo.395.1"> with a size of </span><strong class="source-inline"><span class="koboSpan" id="kobo.396.1">12</span></strong><span class="koboSpan" id="kobo.397.1"> for consistent formatting. </span><span class="koboSpan" id="kobo.397.2">To enhance the PDF visually, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">generate_pdf()</span></strong><span class="koboSpan" id="kobo.399.1"> function downloads a logo from a specified URL, saves it locally as </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">logo.jpg</span></strong><span class="koboSpan" id="kobo.401.1">, and embeds it in the top-left corner of the page. </span><span class="koboSpan" id="kobo.401.2">A centered title, </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">Grocery List</span></strong><span class="koboSpan" id="kobo.403.1">, is added, followed by some spacing to ensure the text does not overlap with the logo. </span><span class="koboSpan" id="kobo.403.2">The </span><a id="_idIndexMarker1296"/><span class="koboSpan" id="kobo.404.1">function then iterates through the grocery list stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">DataFrame (df)</span></strong><span class="koboSpan" id="kobo.406.1">, adding each item’s name, quantities at home and to take, and whether the item has been marked as taken. </span><span class="koboSpan" id="kobo.406.2">Once the document is populated, it is saved in the current working directory as </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">grocery_list.pdf</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.408.1">and returned.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer317">
<span class="koboSpan" id="kobo.409.1"><img alt="Figure 10.16 – The PDF button" src="image/B21257_10_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.410.1">Figure 10.16 – The PDF button</span></p>
<p><span class="koboSpan" id="kobo.411.1">Here is the </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">generated PDF:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer318">
<span class="koboSpan" id="kobo.413.1"><img alt="Figure 10.17 – The obtained PDF file" src="image/B21257_10_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.414.1">Figure 10.17 – The obtained PDF file</span></p>
<p><span class="koboSpan" id="kobo.415.1">Our users may want </span><a id="_idIndexMarker1297"/><span class="koboSpan" id="kobo.416.1">to add notes; for this, we can take advantage of the fact that Streamlit allows other pages to be added to create a section for notes. </span><span class="koboSpan" id="kobo.416.2">Note that we now have a second page that we can access through our sidebar. </span><span class="koboSpan" id="kobo.416.3">This way, we can enter notes and then </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">save them:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.418.1">
elif page == "Notes":
    st.title("Notes")
    st.session_state.notes = st.text_area("Write your notes here:", st.session_state.notes)
    if st.button("Save Notes"):
        st.success("Notes saved successfully!")</span></pre> <div>
<div class="IMG---Figure" id="_idContainer319">
<span class="koboSpan" id="kobo.419.1"><img alt="Figure 10.18 – Adding another page to the app" src="image/B21257_10_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.420.1">Figure 10.18 – Adding another page to the app</span></p>
<p><span class="koboSpan" id="kobo.421.1">Now, we can also</span><a id="_idIndexMarker1298"/><span class="koboSpan" id="kobo.422.1"> note that the information has been updated in </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">our PDF:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer320">
<span class="koboSpan" id="kobo.424.1"><img alt="Figure 10.19 – Updated PDF" src="image/B21257_10_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.425.1">Figure 10.19 – Updated PDF</span></p>
<p><span class="koboSpan" id="kobo.426.1">If our users want to know where the nearest supermarkets are, we could add the following functionality to </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">our app:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.428.1">
elif page == "Find Supermarkets":
    st.title("Find Nearby Supermarkets (OSM)")
        # Get user's location
    location_input = st.text_input("Enter your location (City, Address, or Coordinates):")
    if st.button("Find Supermarkets") and location_input:
        geolocator = Nominatim(user_agent="grocery_app")
        location = geolocator.geocode(location_input)
        if location:
            st.success(f"Location found: {location.address}")
            # Create map
            m = folium.Map(location=[location.latitude, location.longitude], zoom_start=14)
            folium.Marker([location.latitude, location.longitude], tooltip="Your Location", icon=folium.Icon(color="blue")).add_to(m)
            # Use Overpass API to find nearby supermarkets
            overpass_url = "http://overpass-api.de/api/interpreter"
            overpass_query = f"""
            [out:json];
            node["shop"="supermarket"](around:5000,{location.latitude},{location.longitude});
            out;
            """
            response = requests.get(overpass_url, params={'data': overpass_query})
            data = response.json()
            for element in data["elements"]:
                lat, lon = element["lat"], element["lon"]
                name = element.get("tags", {}).get("name", "Unnamed Supermarket")
                folium.Marker([lat, lon], tooltip=name, icon=folium.Icon(color="green")).add_to(m)
            folium_static(m)
        else:
            st.error("Location not found. </span><span class="koboSpan" id="kobo.428.2">Please try a different input.")</span></pre> <p><span class="koboSpan" id="kobo.429.1">In the code, we</span><a id="_idIndexMarker1299"/><span class="koboSpan" id="kobo.430.1"> are adding a new page to the Streamlit app where users can find</span><a id="_idIndexMarker1300"/><span class="koboSpan" id="kobo.431.1"> nearby supermarkets using </span><strong class="bold"><span class="koboSpan" id="kobo.432.1">OpenStreetMap</span></strong><span class="koboSpan" id="kobo.433.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.434.1">OSM</span></strong><span class="koboSpan" id="kobo.435.1">). </span><span class="koboSpan" id="kobo.435.2">First, we display a title for the page and add a text input field where users can enter their location, which could be a city name, an address, or coordinates. </span><span class="koboSpan" id="kobo.435.3">When the user clicks the </span><strong class="bold"><span class="koboSpan" id="kobo.436.1">Find Supermarkets</span></strong><span class="koboSpan" id="kobo.437.1"> button, we use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.438.1">Nominatim</span></strong><span class="koboSpan" id="kobo.439.1"> geocoder from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.440.1">geopy</span></strong><span class="koboSpan" id="kobo.441.1"> library to convert the location input into latitude and longitude coordinates. </span><span class="koboSpan" id="kobo.441.2">If a valid location is found, we confirm this to the user and create an interactive map centered at the given coordinates using Folium. </span><span class="koboSpan" id="kobo.441.3">A marker is added to indicate the user’s location. </span><span class="koboSpan" id="kobo.441.4">Next, we use the Overpass API, which queries OSM data, to find supermarkets within a 5-kilometer radius. </span><span class="koboSpan" id="kobo.441.5">We send a request to the Overpass API and parse the JSON response to extract the coordinates and names of nearby supermarkets. </span><span class="koboSpan" id="kobo.441.6">Each supermarket is then added as a green marker on the map. </span><span class="koboSpan" id="kobo.441.7">Finally, we display the generated map inside the Streamlit app using </span><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">folium_static</span></strong><span class="koboSpan" id="kobo.443.1">. </span><span class="koboSpan" id="kobo.443.2">If the location input is invalid or not found, we show an error message prompting the user to </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">try again.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer321">
<span class="koboSpan" id="kobo.445.1"><img alt="Figure 10.20 – Find Supermarkets page" src="image/B21257_10_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.446.1">Figure 10.20 – Find Supermarkets page</span></p>
<p><span class="koboSpan" id="kobo.447.1">When we click </span><strong class="bold"><span class="koboSpan" id="kobo.448.1">Find Supermarkets</span></strong><span class="koboSpan" id="kobo.449.1">, we get </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer322">
<span class="koboSpan" id="kobo.451.1"><img alt="Figure 10.21 – Supermarket map" src="image/B21257_10_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.452.1">Figure 10.21 – Supermarket map</span></p>
<p><span class="koboSpan" id="kobo.453.1">Now that we know how to build an app, we can build one </span><a id="_idTextAnchor189"/><span class="No-Break"><span class="koboSpan" id="kobo.454.1">with agents.</span></span></p>
<h1 id="_idParaDest-190"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.455.1">Creating an application with Streamlit and AI agents</span></h1>
<p><span class="koboSpan" id="kobo.456.1">In this section, we will look at</span><a id="_idIndexMarker1301"/><span class="koboSpan" id="kobo.457.1"> integrating the multi-agent system described in </span><a href="B21257_09.xhtml#_idTextAnchor156"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.458.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.459.1"> into an app with Streamlit. </span><span class="koboSpan" id="kobo.459.2">Here, we will describe only the code parts we change; the structure remains the same. </span><span class="koboSpan" id="kobo.459.3">In the previous chapter, we built a script that allowed a travel program to be defined; in this chapter, the output is the same, but the system is encapsulated in an app. </span><span class="koboSpan" id="kobo.459.4">In other words, our app will run in the browser and can be used even by a user who does not </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">know programming.</span></span></p>
<p><span class="koboSpan" id="kobo.461.1">As a brief recap, the multi-model </span><em class="italic"><span class="koboSpan" id="kobo.462.1">Travel Planning System</span></em><span class="koboSpan" id="kobo.463.1"> is an AI-driven assistant that integrates multiple specialized models to generate personalized travel plans. </span><span class="koboSpan" id="kobo.463.2">It consists of four </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">key agents:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">WeatherAnalysisAgent</span></strong><span class="koboSpan" id="kobo.466.1">: Predicts the best travel months using historical </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">weather data</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">HotelRecommenderAgent</span></strong><span class="koboSpan" id="kobo.469.1">: Uses a transformer model to find accommodations that match </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">user preferences</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">ItineraryPlannerAgent</span></strong><span class="koboSpan" id="kobo.472.1">: Employs GPT-2 to generate detailed day-by-day </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">travel plans</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">SummaryAgent</span></strong><span class="koboSpan" id="kobo.475.1">: Creates professional trip summaries and </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">cost estimates</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.477.1">The system follows a structured data flow, where the user inputs their destination, preferences, and duration, and the agents collaborate to deliver a complete travel plan. </span><span class="koboSpan" id="kobo.477.2">The core AI models include </span><strong class="source-inline"><span class="koboSpan" id="kobo.478.1">RandomForestRegressor</span></strong><span class="koboSpan" id="kobo.479.1"> for weather predictions, </span><strong class="source-inline"><span class="koboSpan" id="kobo.480.1">SentenceTransformer</span></strong><span class="koboSpan" id="kobo.481.1"> for hotel recommendations, and GPT-2 for itinerary and </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">summary generation.</span></span></p>
<p><span class="koboSpan" id="kobo.483.1">To better</span><a id="_idIndexMarker1302"/><span class="koboSpan" id="kobo.484.1"> understand the internal structure of the </span><em class="italic"><span class="koboSpan" id="kobo.485.1">Travel Planning System</span></em><span class="koboSpan" id="kobo.486.1">, this section provides three UML diagrams. </span><span class="koboSpan" id="kobo.486.2">These visualizations illustrate the architecture, execution flow, and system interactions of the application described in </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">this chapter:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.488.1">Class diagram</span></strong><span class="koboSpan" id="kobo.489.1">: The following class diagram</span><a id="_idIndexMarker1303"/><span class="koboSpan" id="kobo.490.1"> shows the main components of the application, including the core AI agents (such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">WeatherAnalysisAgent</span></strong><span class="koboSpan" id="kobo.492.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">ItineraryPlannerAgent</span></strong><span class="koboSpan" id="kobo.494.1">), the underlying models (</span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">RandomForest</span></strong><span class="koboSpan" id="kobo.496.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.497.1">SentenceTransformer</span></strong><span class="koboSpan" id="kobo.498.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">OpenAI</span></strong><span class="koboSpan" id="kobo.500.1"> GPT), and the Streamlit app that connects the user interface to the </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">backend logic:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer323">
<span class="koboSpan" id="kobo.502.1"><img alt="Figure 10.22 – Structural UML Diagram for the multi-model Travel Planning System" src="image/B21257_10_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.503.1">Figure 10.22 – Structural UML Diagram for the multi-model Travel Planning System</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.504.1">Activity diagram</span></strong><span class="koboSpan" id="kobo.505.1">: The activity diagram</span><a id="_idIndexMarker1304"/><span class="koboSpan" id="kobo.506.1"> describes the control flow of the </span><a id="_idIndexMarker1305"/><span class="koboSpan" id="kobo.507.1">application, starting from user input collection through to the generation of a complete travel plan. </span><span class="koboSpan" id="kobo.507.2">It illustrates how each agent is triggered and how their outputs </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">are merged:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer324">
<span class="koboSpan" id="kobo.509.1"><img alt="Figure 10.23 – UML activity diagram for the multi-model Travel Planning System" src="image/B21257_10_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.510.1">Figure 10.23 – UML activity diagram for the multi-model Travel Planning System</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.511.1">Sequence diagram</span></strong><span class="koboSpan" id="kobo.512.1">: Finally, the sequence diagram</span><a id="_idIndexMarker1306"/><span class="koboSpan" id="kobo.513.1"> outlines the </span><a id="_idIndexMarker1307"/><span class="koboSpan" id="kobo.514.1">time-based interactions between the Streamlit frontend, the database, and the AI agents. </span><span class="koboSpan" id="kobo.514.2">It shows the order of method calls, the data exchanged, and the points where the system waits for responses. </span><span class="koboSpan" id="kobo.514.3">It makes clear when and how each agent </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">is called:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer325">
<span class="koboSpan" id="kobo.516.1"><img alt="Figure 10.24 – UML sequence diagram for the multi-model Travel Planning System" src="image/B21257_10_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.517.1">Figure 10.24 – UML sequence diagram for the multi-model Travel Planning System</span></p>
<p><span class="koboSpan" id="kobo.518.1">First, we start by</span><a id="_idIndexMarker1308"/><span class="koboSpan" id="kobo.519.1"> importing the libraries </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">we need:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.521.1">
import streamlit as st
import numpy as np
import pandas as pd
import pydeck as pdk
import openai
from sklearn.ensemble import RandomForestRegressor
from sentence_transformers import SentenceTransformer</span></pre> <ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.522.1">streamlit</span></strong><span class="koboSpan" id="kobo.523.1">: Our library to create the interactive </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">web application</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">numpy</span></strong><span class="koboSpan" id="kobo.526.1">: A library for all the </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">numerical operations</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">pandas</span></strong><span class="koboSpan" id="kobo.529.1">: A library to </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">handle DataFrames</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">pydeck</span></strong><span class="koboSpan" id="kobo.532.1">: A visualization library built on top of Deck.gl, specifically for rendering large-scale </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">geographical data</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">openai</span></strong><span class="koboSpan" id="kobo.535.1">: The OpenAI </span><a id="_idIndexMarker1309"/><span class="koboSpan" id="kobo.536.1">Python library, which provides access to models such as GPT-3.5 and GPT-4 for </span><strong class="bold"><span class="koboSpan" id="kobo.537.1">natural language processing</span></strong><span class="koboSpan" id="kobo.538.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.539.1">NLP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">) tasks.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.541.1">RandomForestRegressor</span></strong><span class="koboSpan" id="kobo.542.1">: The scikit-learn model we use in </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">our app</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">SentenceTransformer</span></strong><span class="koboSpan" id="kobo.545.1">: The</span><a id="_idIndexMarker1310"/><span class="koboSpan" id="kobo.546.1"> library for the embeddings (see the </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">previous chapter)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.548.1">The code for agents is the same, except for </span><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">ItineraryPlannerAgent</span></strong><span class="koboSpan" id="kobo.550.1">. </span><span class="koboSpan" id="kobo.550.2">For a better and smoother response, we use OpenAI’s GPT-4 </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">model here:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.552.1">
class ItineraryPlannerAgent:
    def __init__(self, api_key):
        self.api_key = api_key
    def create_itinerary(self, destination, best_month, hotel, duration):
        client = openai.OpenAI(api_key=self.api_key)
        prompt = f"""
        Create a {duration}-day travel itinerary for {destination} in the best month: {best_month}.
</span><span class="koboSpan" id="kobo.552.2">        Recommended Hotel: {hotel['name']}.
</span><span class="koboSpan" id="kobo.552.3">        """
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are an expert travel planner."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300
        )
        return response.choices[0].message.content</span></pre> <p><span class="koboSpan" id="kobo.553.1">The operation is the same: it takes in a travel destination, the best time to visit, a recommended hotel, and the trip duration, following which a structured itinerary is generated. </span><span class="koboSpan" id="kobo.553.2">Note that we need to use an API key to authenticate requests to OpenAI’s API. </span><span class="koboSpan" id="kobo.553.3">Again, the agent does nothing more than generate an itinerary based on the same inputs: travel </span><a id="_idIndexMarker1311"/><span class="koboSpan" id="kobo.554.1">location, the best months to travel, hotel details, and the number of travel days. </span><span class="koboSpan" id="kobo.554.2">GPT-4 also works similarly to GPT-2: we have to provide a prompt with the information and the model then autoregressively generates the </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">travel itinerary</span></span></p>
<p><span class="koboSpan" id="kobo.556.1">Here again, we provide the same data that we provided to our system previously (you can find it in </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">the repository):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer326">
<span class="koboSpan" id="kobo.558.1"><img alt="Figure 10.25 – Screenshot of the code" src="image/B21257_10_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.559.1">Figure 10.25 – Screenshot of the code</span></p>
<p><span class="koboSpan" id="kobo.560.1">At this point, we </span><a id="_idIndexMarker1312"/><span class="koboSpan" id="kobo.561.1">can initialize our agents, each with its own </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">different purpose:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.563.1">
openai_api_key = st.secrets["general"]["openai_api_key"]
weather_agent = WeatherAnalysisAgent()
hotel_agent = HotelRecommenderAgent()
itinerary_agent = ItineraryPlannerAgent(api_key=openai_api_key)
weather_agent.train(historical_weather_data)
hotel_agent.add_hotels(hotels_database)
Your API should be in a file TOML, like this:
[general]
openai_api_key = "YOUR_API"</span></pre> <p><span class="koboSpan" id="kobo.564.1">Note, </span><strong class="source-inline"><span class="koboSpan" id="kobo.565.1">openai_api_key = st.secrets["general"]["openai_api_key"]</span></strong><span class="koboSpan" id="kobo.566.1"> uses Streamlit’s secrets manager to securely access the OpenAI API key. </span><span class="koboSpan" id="kobo.566.2">In fact, </span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">st.secrets</span></strong><span class="koboSpan" id="kobo.568.1"> is a way to store and retrieve sensitive credentials in Streamlit apps. </span><span class="koboSpan" id="kobo.568.2">The API key is stored under </span><strong class="source-inline"><span class="koboSpan" id="kobo.569.1">st.secrets["general"]["openai_api_key"]</span></strong><span class="koboSpan" id="kobo.570.1">, indicating it is saved inside a </span><strong class="source-inline"><span class="koboSpan" id="kobo.571.1">"general"</span></strong><span class="koboSpan" id="kobo.572.1"> section within the </span><strong class="source-inline"><span class="koboSpan" id="kobo.573.1">secrets</span></strong><span class="koboSpan" id="kobo.574.1"> configuration. </span><span class="koboSpan" id="kobo.574.2">The purpose of </span><strong class="source-inline"><span class="koboSpan" id="kobo.575.1">st.secrets</span></strong><span class="koboSpan" id="kobo.576.1"> is to prevent sensitive credentials from being hardcoded in the script, reducing the risk of </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">privacy breaches.</span></span></p>
<p><span class="koboSpan" id="kobo.578.1">Now, let’s start building </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">our interface:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.580.1">
st.title("AI Travel Planner ✈️")
st.write("Find the best time to travel and discover the perfect hotel!")
destination = st.text_input("Enter your destination (e.g., Rome):", "Rome")
preferences = st.text_area("Describe your ideal hotel:", "Luxury hotel in city center with spa.")
duration = st.slider("Trip duration (days):", 1, 14, 5)</span></pre> <p><span class="koboSpan" id="kobo.581.1">First, we add a</span><a id="_idIndexMarker1313"/><span class="koboSpan" id="kobo.582.1"> title: </span><strong class="source-inline"><span class="koboSpan" id="kobo.583.1">st.title()</span></strong><span class="koboSpan" id="kobo.584.1"> sets the title of the Streamlit web app. </span><span class="koboSpan" id="kobo.584.2">This title will appear at the top of the page. </span><span class="koboSpan" id="kobo.584.3">At this point, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.585.1">st.write()</span></strong><span class="koboSpan" id="kobo.586.1"> to give a brief explanation of the app’s purpose. </span><span class="koboSpan" id="kobo.586.2">Next, </span><strong class="source-inline"><span class="koboSpan" id="kobo.587.1">st.text_input()</span></strong><span class="koboSpan" id="kobo.588.1"> is used to create a box where the user can enter their destination. </span><span class="koboSpan" id="kobo.588.2">Note that we are providing a hint about what the user can enter – </span><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">"Enter your destination (e.g., Rome):"</span></strong><span class="koboSpan" id="kobo.590.1"> – and there is a default value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">"Rome"</span></strong><span class="koboSpan" id="kobo.592.1"> (if the user doesn’t input anything, it defaults to </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">Rome</span></strong><span class="koboSpan" id="kobo.594.1">). </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">st.text_area()</span></strong><span class="koboSpan" id="kobo.596.1"> creates a multi-line text box where users can describe their ideal hotel. </span><span class="koboSpan" id="kobo.596.2">We use </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">text_area</span></strong><span class="koboSpan" id="kobo.598.1"> to allow users to provide detailed hotel preferences. </span><strong class="source-inline"><span class="koboSpan" id="kobo.599.1">st.slider()</span></strong><span class="koboSpan" id="kobo.600.1"> creates a slider input for selecting the trip duration (there are parameters that define a minimum duration of </span><strong class="source-inline"><span class="koboSpan" id="kobo.601.1">1</span></strong><span class="koboSpan" id="kobo.602.1"> day and a maximum of </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">14</span></strong><span class="koboSpan" id="kobo.604.1">, with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.605.1">5</span></strong><span class="koboSpan" id="kobo.606.1">-day trip being the </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">default duration).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer327">
<span class="koboSpan" id="kobo.608.1"><img alt="Figure 10.26 – Input preferences in the app" src="image/B21257_10_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.609.1">Figure 10.26 – Input preferences in the app</span></p>
<p><span class="koboSpan" id="kobo.610.1">At this point, we will</span><a id="_idIndexMarker1314"/><span class="koboSpan" id="kobo.611.1"> deal with what happens after the user adds the information and presses a button. </span><span class="koboSpan" id="kobo.611.2">To recap, the system predicts the best travel months based on weather conditions (through the use of historical data and random forest algorithms), finds a hotel that matches the user’s preferences (using data on hotels and similarity of embeddings), and finally, creates a personalized itinerary using </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">OpenAI’s GPT-4.</span></span></p>
<p><span class="koboSpan" id="kobo.613.1">We have created the framework to be able to visualize the results: the best months to visit, the recommended hotel, the AI-generated itinerary, and finally, a map visualization of the destination. </span><span class="koboSpan" id="kobo.613.2">All this happens only when our user presses the button, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">create next:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.615.1">
if st.button("Generate Travel Plan ✨"):
    best_months = weather_agent.predict_best_time({'latitude': 41.9028, 'longitude': 12.4964})
    best_month = best_months[0]['month']
    recommended_hotels = hotel_agent.find_hotels(preferences)
    itinerary = itinerary_agent.create_itinerary(destination, best_month, recommended_hotels[0], duration)
    st.subheader("📆 Best Months to Visit")
    for m in best_months:
        st.write(f"Month {m['month']}: Score {m['score']:.2f}")
    st.subheader("🏨 Recommended Hotel")
    st.write(f"**{recommended_hotels[0]['name']}** - {recommended_hotels[0]['description']}")
    st.subheader("📜 Generated Itinerary")
    st.write(itinerary)
    # -------------------------------
    # Interactive Map
    # -------------------------------
    st.subheader("🗺 Destination Map")
    map_data = pd.DataFrame(
        {'lat': [41.9028], 'lon': [12.4964]},
    )
    st.map(map_data)</span></pre> <p><strong class="source-inline"><span class="koboSpan" id="kobo.616.1">if st.button("Generate Travel Plan </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">✨</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">"):</span></strong><span class="koboSpan" id="kobo.619.1"> creates an interactive button labeled </span><strong class="bold"><span class="koboSpan" id="kobo.620.1">Generate Travel Plan </span></strong><strong class="bold"><span class="koboSpan" id="kobo.621.1">✨</span></strong><span class="koboSpan" id="kobo.622.1"> and defines a block of actions that are executed when </span><a id="_idIndexMarker1315"/><span class="koboSpan" id="kobo.623.1">the user clicks the button. </span><span class="koboSpan" id="kobo.623.2">First, the best month to visit the city is predicted: </span><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">best_months = weather_agent.predict_best_time({'latitude': 41.9028, 'longitude': 12.4964})</span></strong><span class="koboSpan" id="kobo.625.1">. </span><span class="koboSpan" id="kobo.625.2">Note that we entered the destination’s latitude (</span><strong class="source-inline"><span class="koboSpan" id="kobo.626.1">41.9028</span></strong><span class="koboSpan" id="kobo.627.1">) and longitude (</span><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">12.4964</span></strong><span class="koboSpan" id="kobo.629.1">) for Rome, got our best months based on the weather score, and selected the best month. </span><span class="koboSpan" id="kobo.629.2">At this point, we identify the best hotels based on our user’s preferences with </span><strong class="source-inline"><span class="koboSpan" id="kobo.630.1">hotel_agent.find_hotels(preferences)</span></strong><span class="koboSpan" id="kobo.631.1">. </span><span class="koboSpan" id="kobo.631.2">This agent will return a list of hotels matching the </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">user’s description.</span></span></p>
<p><span class="koboSpan" id="kobo.633.1">Since we have all the details, we can generate our itinerary. </span><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">itinerary = itinerary_agent.create_itinerary(destination, best_month, recommended_hotels[0], duration)</span></strong><span class="koboSpan" id="kobo.635.1"> does exactly that; it takes the inputs defined earlier and produces a structured AI-generated itinerary. </span><span class="koboSpan" id="kobo.635.2">Once we have our itinerary, we start the display of it for the user. </span><span class="koboSpan" id="kobo.635.3">We use </span><strong class="source-inline"><span class="koboSpan" id="kobo.636.1">st.subheader("</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.637.1">📆</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.638.1"> Best Months to Visit")</span></strong><span class="koboSpan" id="kobo.639.1"> to create a subsection and then iterate over </span><strong class="source-inline"><span class="koboSpan" id="kobo.640.1">best_months</span></strong><span class="koboSpan" id="kobo.641.1"> and print each month with its weather score. </span><span class="koboSpan" id="kobo.641.2">At this point, we show the best hotels in an additional subsection after </span><strong class="source-inline"><span class="koboSpan" id="kobo.642.1">st.subheader("</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">🏨</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.644.1"> Recommended Hotel")</span></strong><span class="koboSpan" id="kobo.645.1">. </span><span class="koboSpan" id="kobo.645.2">Finally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.646.1">st.subheader("</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.647.1">📜</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.648.1"> Generated Itinerary")</span></strong><span class="koboSpan" id="kobo.649.1"> allows us to create a subsection where our itinerary will be inserted. </span><span class="koboSpan" id="kobo.649.2">In the last part, we show the </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">city map.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer328">
<span class="koboSpan" id="kobo.651.1"><img alt="Figure 10.27 – Generated output (part 1)" src="image/B21257_10_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.652.1">Figure 10.27 – Generated output (part 1)</span></p>
<div>
<div class="IMG---Figure" id="_idContainer329">
<span class="koboSpan" id="kobo.653.1"><img alt="Figure 10.28 – Generated output (part 2)" src="image/B21257_10_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.654.1">Figure 10.28 – Generated output (part 2)</span></p>
<div>
<div class="IMG---Figure" id="_idContainer330">
<span class="koboSpan" id="kobo.655.1"><img alt="Figure 10.29 – Generated output (part 3)" src="image/B21257_10_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.656.1">Figure 10.29 – Generated output (part 3)</span></p>
<p><span class="koboSpan" id="kobo.657.1">In this section, we created a multi-agent system and embedded it within an app. </span><span class="koboSpan" id="kobo.657.2">In this way, even users with no programming knowledge can interact with our system. </span><span class="koboSpan" id="kobo.657.3">The system can be run by a user by clicking a </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">simple button.</span></span></p>
<p><span class="koboSpan" id="kobo.659.1">We discussed an app as an isolated system; in the next section, we will see how a model is not an isolated concept but part of an ecosystem. </span><span class="koboSpan" id="kobo.659.2">This complexity must be taken into account, and in the next section, we will discuss the life cycle of a model, from conception </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">to</span><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.661.1"> deployment.</span></span></p>
<h1 id="_idParaDest-191"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.662.1">Machine learning operations and LLM operations</span></h1>
<p><span class="koboSpan" id="kobo.663.1">We have seen how to create an app containing a multi-agent system. </span><span class="koboSpan" id="kobo.663.2">When we create a script with Python, we create an element that can run on our computer, but this is not a product. </span><span class="koboSpan" id="kobo.663.3">Turning a script into an app allows a user to be able to interact with our app even if they do not know how to program. </span><span class="koboSpan" id="kobo.663.4">Streamlit allows us to be able to run a quick prototype of our app. </span><span class="koboSpan" id="kobo.663.5">This is not optimal for a product, especially if it is to be used by several users. </span><span class="koboSpan" id="kobo.663.6">In this section, we will discuss all those operations necessary to make our model function as </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">a product.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.665.1">Machine Learning Operations</span></strong><span class="koboSpan" id="kobo.666.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.667.1">MLOps</span></strong><span class="koboSpan" id="kobo.668.1">) is </span><a id="_idIndexMarker1316"/><span class="koboSpan" id="kobo.669.1">a set of practices and tools designed to streamline and manage the life cycle of </span><strong class="bold"><span class="koboSpan" id="kobo.670.1">machine learning</span></strong><span class="koboSpan" id="kobo.671.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.672.1">ML</span></strong><span class="koboSpan" id="kobo.673.1">) models</span><a id="_idIndexMarker1317"/><span class="koboSpan" id="kobo.674.1"> in production. </span><span class="koboSpan" id="kobo.674.2">It combines ML, DevOps, and data engineering practices to ensure</span><a id="_idIndexMarker1318"/><span class="koboSpan" id="kobo.675.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.676.1">continuous integration/continuous delivery</span></strong><span class="koboSpan" id="kobo.677.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.678.1">CI/CD</span></strong><span class="koboSpan" id="kobo.679.1">), monitoring, and scaling of </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">ML systems.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer331">
<span class="koboSpan" id="kobo.681.1"><img alt="Figure 10.30 – MLOps combination (https://arxiv.org/pdf/2202.10169)" src="image/B21257_10_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.682.1">Figure 10.30 – MLOps combination (</span><a href="https://arxiv.org/pdf/2202.10169"><span class="koboSpan" id="kobo.683.1">https://arxiv.org/pdf/2202.10169</span></a><span class="koboSpan" id="kobo.684.1">)</span></p>
<p><span class="koboSpan" id="kobo.685.1">MLOps plays a </span><a id="_idIndexMarker1319"/><span class="koboSpan" id="kobo.686.1">key role in turning a model into a useful application in the real world. </span><span class="koboSpan" id="kobo.686.2">In short, MLOps encompass the development, monitoring, and maintenance of models in a production environment, enabling the transition from a research product to a functional product. </span><span class="koboSpan" id="kobo.686.3">Here are the various </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">stages involved:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.688.1">Model development</span></strong><span class="koboSpan" id="kobo.689.1">: This is the</span><a id="_idIndexMarker1320"/><span class="koboSpan" id="kobo.690.1"> first step, in which an ML model is designed and trained. </span><span class="koboSpan" id="kobo.690.2">Typically, at this stage, both data scientists and data engineers collaborate on the choice of model, datasets, and training and </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">testing process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.692.1">Testing</span></strong><span class="koboSpan" id="kobo.693.1">: Normally, the</span><a id="_idIndexMarker1321"/><span class="koboSpan" id="kobo.694.1"> testing phase is part of model development; however, today, there is a greater emphasis on testing the model. </span><span class="koboSpan" id="kobo.694.2">Hence, we consider it a separate stage. </span><span class="koboSpan" id="kobo.694.3">In fact, complex models in particular can exhibit unexpected behaviors, so testing is often considered a </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">separate phase.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.696.1">Deployment</span></strong><span class="koboSpan" id="kobo.697.1">: Once the</span><a id="_idIndexMarker1322"/><span class="koboSpan" id="kobo.698.1"> model has been developed and tested, it can be deployed in a production environment. </span><span class="koboSpan" id="kobo.698.2">This delicate step requires that the model be integrated with other existing systems (which have been developed previously) and that it can be used in </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">real time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.700.1">Monitoring and maintenance</span></strong><span class="koboSpan" id="kobo.701.1">: Once the model is deployed, we must ensure its performance doesn’t degrade and prevent operational problems. </span><span class="koboSpan" id="kobo.701.2">At the same time, we may need to update</span><a id="_idIndexMarker1323"/><span class="koboSpan" id="kobo.702.1"> the model or</span><a id="_idIndexMarker1324"/><span class="koboSpan" id="kobo.703.1"> ensure compatibility with new </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">system elements.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer332">
<span class="koboSpan" id="kobo.705.1"><img alt="Figure 10.31 – High-level process view of MLOps (https://arxiv.org/pdf/2202.10169)" src="image/B21257_10_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.706.1">Figure 10.31 – High-level process view of MLOps (</span><a href="https://arxiv.org/pdf/2202.10169"><span class="koboSpan" id="kobo.707.1">https://arxiv.org/pdf/2202.10169</span></a><span class="koboSpan" id="kobo.708.1">)</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.709.1">Large Language Model Operations</span></strong><span class="koboSpan" id="kobo.710.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.711.1">LLMOps</span></strong><span class="koboSpan" id="kobo.712.1">) is an</span><a id="_idIndexMarker1325"/><span class="koboSpan" id="kobo.713.1"> extension of MLOps specifically focused on the deployment, maintenance, and management of LLMs. </span><span class="koboSpan" id="kobo.713.2">It incorporates the principles of MLOps but also addresses the unique challenges and needs associated with working with large-scale </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">NLP models.</span></span></p>
<p><span class="koboSpan" id="kobo.715.1">However, LLMOps adds additional complexity. </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">Here’s why:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.717.1">Model size and complexity</span></strong><span class="koboSpan" id="kobo.718.1">: In MLOps, models can vary in size and complexity, but they typically don’t require as much computational power or memory as </span><a id="_idIndexMarker1326"/><span class="koboSpan" id="kobo.719.1">LLMs. </span><span class="koboSpan" id="kobo.719.2">Models may include traditional ML algorithms, smaller deep learning models, or specialized models for structured data. </span><span class="koboSpan" id="kobo.719.3">LLMs can be in the order of billions of parameters and thus require optimized infrastructure (often involving specialized hardware such as GPUs or TPUs) or distributed training. </span><span class="koboSpan" id="kobo.719.4">This means more expertise and dedicated infrastructure (dedicated hardware and storage), which can be </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">very expensive.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.721.1">Training and fine-tuning</span></strong><span class="koboSpan" id="kobo.722.1">: In MLOps, training is much</span><a id="_idIndexMarker1327"/><span class="koboSpan" id="kobo.723.1"> more manageable. </span><span class="koboSpan" id="kobo.723.2">Many of the models are small in size and can therefore be easily retrained. </span><span class="koboSpan" id="kobo.723.3">Retraining itself can be conducted programmatically. </span><span class="koboSpan" id="kobo.723.4">Fine-tuning LLMs is more complex and resource-intensive. </span><span class="koboSpan" id="kobo.723.5">Collecting and processing the datasets needed for an LLM </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">is</span></span><span class="No-Break"><a id="_idIndexMarker1328"/></span><span class="No-Break"><span class="koboSpan" id="kobo.725.1"> resource-intensive.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.726.1">Scalability and deployment</span></strong><span class="koboSpan" id="kobo.727.1">: In MLOps, deploying models to production is usually straightforward. </span><span class="koboSpan" id="kobo.727.2">Scaling</span><a id="_idIndexMarker1329"/><span class="koboSpan" id="kobo.728.1"> LLMs, on the other hand, requires dedicated infrastructure that can ensure necessary support when there is high demand. </span><span class="koboSpan" id="kobo.728.2">In fact, latency can increase considerably when there are many users. </span><span class="koboSpan" id="kobo.728.3">Optimizing latency during inference can be a delicate process that risks </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">degrading performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.730.1">Monitoring and maintenance</span></strong><span class="koboSpan" id="kobo.731.1">: Monitoring </span><a id="_idIndexMarker1330"/><span class="koboSpan" id="kobo.732.1">ML models in production involves tracking key metrics such as accuracy, precision, and recall, as well as model drift or data drift. </span><span class="koboSpan" id="kobo.732.2">Monitoring LLMs involves not only the usual performance metrics but also the quality of text generation, user feedback, and ethical concerns such as biased or harmful outputs. </span><span class="koboSpan" id="kobo.732.3">While it is straightforward to evaluate an output in terms of accuracy, it is more complex to assess whether an LLM produces hallucinations or inappropriate or harmful content. </span><span class="koboSpan" id="kobo.732.4">Some biases might be subtle but still be noticed </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">by users.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.734.1">Model governance and compliance</span></strong><span class="koboSpan" id="kobo.735.1">: While governance and compliance are critical in any ML deployment, MLOps</span><a id="_idIndexMarker1331"/><span class="koboSpan" id="kobo.736.1"> primarily focuses on ensuring data privacy and model transparency, especially when dealing with sensitive or regulated data. </span><span class="koboSpan" id="kobo.736.2">For LLMOps, there is not only privacy, but it can also be used to generate text on a wide variety of topics with the risk of generating inappropriate content. </span><span class="koboSpan" id="kobo.736.3">With regulations in development, assessing bias, fairness, and ethical issues is complex </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">and evolving.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.738.1">Here’s an example of the added complexity involved when performing LLMOps. </span><span class="koboSpan" id="kobo.738.2">If we wanted to train a model from scratch, we would have to retrieve a corpus of at least 1B tokens. </span><span class="koboSpan" id="kobo.738.3">These tokens would have to be collected from different sources (books, websites, articles, code repositories, and so on). </span><span class="koboSpan" id="kobo.738.4">In MLOPs, we usually create a model when a dataset is already present (e.g., through user interactions with our site). </span><span class="koboSpan" id="kobo.738.5">The steps of preprocessing a dataset for a classical model (images or tabular) are much simpler than a large corpus (steps such as debiasing, eliminating duplicates, and so on). </span><span class="koboSpan" id="kobo.738.6">Also, since our dataset can be over hundreds of terabytes in size, there is more complexity. </span><span class="koboSpan" id="kobo.738.7">While we can train an ML model easily (even on a consumer computer), this is no longer possible with an LLM. </span><span class="koboSpan" id="kobo.738.8">Especially for larger ones, we have to use dedicated infrastructure, and we cannot do many experiments (testing different hyperparameters or different architecture combinations). </span><span class="koboSpan" id="kobo.738.9">Similarly, fine-tuning will be preferred to having to retrain </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">our model.</span></span></p>
<p><span class="koboSpan" id="kobo.740.1">Testing also no longer relies on simple measures (such as accuracy) but requires human-in-the-loop evaluations. </span><span class="koboSpan" id="kobo.740.2">Given the language-centric nature of the system, a metric such as accuracy gives us only partial information about the output of our model. </span><span class="koboSpan" id="kobo.740.3">Only humans (even if we use other LLMs to check at scale) can evaluate the output of an LLM in terms of creativity, bias, quality, and the presence of inappropriate content. </span><span class="koboSpan" id="kobo.740.4">Also, after pre-training, there is usually a step where human feedback is used to be able to further improve the output of a model. </span><span class="koboSpan" id="kobo.740.5">In addition, we must then continue to evaluate our LLM, because the traffic may grow or there may be evolutions in the language and knowledge that our model must have. </span><span class="koboSpan" id="kobo.740.6">For example, an LLM for medical use needs to be updated on </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">new therapies.</span></span></p>
<p><span class="koboSpan" id="kobo.742.1">In the next section, we will start with the complexities of developing a model as complex as </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">an LLM.</span></span></p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.744.1">Model development</span></h2>
<p><span class="koboSpan" id="kobo.745.1">The development of a model</span><a id="_idIndexMarker1332"/><span class="koboSpan" id="kobo.746.1"> starts with the collection of a corpus. </span><span class="koboSpan" id="kobo.746.2">This collection is generally divided into two types: general data and specialized data. </span><span class="koboSpan" id="kobo.746.3">General data represents data such as web pages, books, and conversational text. </span><span class="koboSpan" id="kobo.746.4">Specialized data, on the other hand, is data that is designed for a specific task, such as multilingual data, scientific data, </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">and code:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.748.1">General data</span></strong><span class="koboSpan" id="kobo.749.1">: Considering the</span><a id="_idIndexMarker1333"/><span class="koboSpan" id="kobo.750.1"> large amount of data on the internet, it is now common for data collection to start with using datasets of downloaded pages or even conducting crawling to collect new data. </span><span class="koboSpan" id="kobo.750.2">In addition, there are also datasets of conversations (such as discussions on Reddit or other platforms), chats with LLMs, and other sources. </span><span class="koboSpan" id="kobo.750.3">Books are another popular source for training, as they generally contain coherent, quality text on disparate topics. </span><span class="koboSpan" id="kobo.750.4">These datasets contain a mixture of quality data (such as Wikipedia and blog posts) but also a large amount of data that needs to be removed, such as spam, toxic posts, and</span><a id="_idIndexMarker1334"/> <span class="No-Break"><span class="koboSpan" id="kobo.751.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.752.1">Specialized text data</span></strong><span class="koboSpan" id="kobo.753.1">: Today, it is common to add a multilingual corpus to improve the language </span><a id="_idIndexMarker1335"/><span class="koboSpan" id="kobo.754.1">capabilities of LLMs (e.g., PaLM covers 122 languages due to the addition of a </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">multilingual corpus).</span></span><p class="list-inset"><span class="koboSpan" id="kobo.756.1">Adding scientific text enables improved performance in scientific and reasoning tasks. </span><span class="koboSpan" id="kobo.756.2">Huge datasets of articles exist today that are ready to use and can be directly added. </span><span class="koboSpan" id="kobo.756.3">Almost all modern pre-training datasets also insert code. </span><span class="koboSpan" id="kobo.756.4">The addition of code and other structured data appears to be related to an increase in performance in </span><a id="_idIndexMarker1336"/><span class="koboSpan" id="kobo.757.1">some </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">reasoning tasks.</span></span></p></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer333">
<span class="koboSpan" id="kobo.759.1"><img alt="Figure 10.32 – Ratios of various data sources in the pre-training data for existing LLMs (https://arxiv.org/pdf/2303.18223)" src="image/B21257_10_32.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.760.1">Figure 10.32 – Ratios of various data sources in the pre-training data for existing LLMs (</span><a href="https://arxiv.org/pdf/2303.18223"><span class="koboSpan" id="kobo.761.1">https://arxiv.org/pdf/2303.18223</span></a><span class="koboSpan" id="kobo.762.1">)</span></p>
<p><span class="koboSpan" id="kobo.763.1">Once the data has been collected, it must be preprocessed to remove unnecessary tokens such as HTML tags or other presentation elements, reduce text variation, and eliminate duplicate data. </span><span class="koboSpan" id="kobo.763.2">Today, we try to eliminate data that is of low quality, using either heuristic algorithms or classifiers. </span><span class="koboSpan" id="kobo.763.3">For example, we can train a classifier on quality data such as Wikipedia to recognize what content we want to preserve. </span><span class="koboSpan" id="kobo.763.4">Heuristic algorithms, on the other hand, rely on a set of rules that are defined upstream (such as statistical properties, the presence or absence of keywords, and so on). </span><span class="koboSpan" id="kobo.763.5">Deduplication is an important step because it impacts model diversity and training stability. </span><span class="koboSpan" id="kobo.763.6">Typically, different granularities, such as sentence or document level, are used to avoid repetitive word patterns. </span><span class="koboSpan" id="kobo.763.7">In addition, another common step today is privacy reduction, in which an attempt is made to remove </span><strong class="bold"><span class="koboSpan" id="kobo.764.1">personally identifiable information</span></strong><span class="koboSpan" id="kobo.765.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.766.1">PII</span></strong><span class="koboSpan" id="kobo.767.1">), often through a set of rules that are defined</span><a id="_idIndexMarker1337"/><span class="koboSpan" id="kobo.768.1"> upstream. </span><span class="koboSpan" id="kobo.768.2">Once these steps are conducted, tokenization can be done. </span><span class="koboSpan" id="kobo.768.3">Tokenization is considered a crucial step because it largely impacts model performance. </span><strong class="bold"><span class="koboSpan" id="kobo.769.1">Byte-pair encoding</span></strong><span class="koboSpan" id="kobo.770.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.771.1">BPE</span></strong><span class="koboSpan" id="kobo.772.1">) tokenization</span><a id="_idIndexMarker1338"/><span class="koboSpan" id="kobo.773.1"> is generally one of the most widely </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">used</span></span><span class="No-Break"><a id="_idIndexMarker1339"/></span><span class="No-Break"><span class="koboSpan" id="kobo.775.1"> methods.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer334">
<span class="koboSpan" id="kobo.776.1"><img alt="Figure 10.33 – Illustration of a typical data preprocessing pipeline for pre-training LLMs (https://arxiv.org/pdf/2303.18223)" src="image/B21257_10_33.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.777.1">Figure 10.33 – Illustration of a typical data preprocessing pipeline for pre-training LLMs (</span><a href="https://arxiv.org/pdf/2303.18223"><span class="koboSpan" id="kobo.778.1">https://arxiv.org/pdf/2303.18223</span></a><span class="koboSpan" id="kobo.779.1">)</span></p>
<p><span class="koboSpan" id="kobo.780.1">Once we have preprocessed the corpus, we can train the model in the next phase. </span><span class="koboSpan" id="kobo.780.2">To train the model, we need to define a strategy to schedule the multi-sources (different types of data such as Wikipedia, text from the internet, books, etc.) previously introduced. </span><span class="koboSpan" id="kobo.780.3">In fact, two important aspects are decided: the proportion of each data source (data mixture) and the order in which each data source is scheduled for training (data curriculum). </span><span class="koboSpan" id="kobo.780.4">Since each type of data has an impact on performance, the data must be mixed in a precise distribution. </span><span class="koboSpan" id="kobo.780.5">This distribution can be global or local (at certain training steps). </span><span class="koboSpan" id="kobo.780.6">To do this, we can then decide to conduct upsampling and downsampling of the various sources in order to respect the mixture we have decided on. </span><span class="koboSpan" id="kobo.780.7">For example, in the case of LLaMA pre-training, the authors chose to train with the following proportion (based on experimental results, which have shown that this proportion works well): 80% web pages, 6.5% code-related data from GitHub and Stack Exchange, 4.5% from books, and 2.5% of scientific data sourced from arXiv. </span><span class="koboSpan" id="kobo.780.8">These values do not sum to exactly 100%, as the remaining portion includes other minor sources not explicitly detailed in the original paper. </span><span class="koboSpan" id="kobo.780.9">Today, this recipe has been used for many different types of LLMs, while LLMs with a specific purpose have a different proportion of code and </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">scientific articles.</span></span></p>
<p><span class="koboSpan" id="kobo.782.1">Generally, a heterogeneous corpus is preferred, as diversity enhances a model’s ability to generalize across domains. </span><span class="koboSpan" id="kobo.782.2">In contrast, an overly homogeneous dataset can hinder generalization. </span><span class="koboSpan" id="kobo.782.3">Additionally, the sequence in which data is presented—often referred to as a data curriculum—is crucial. </span><span class="koboSpan" id="kobo.782.4">The training data is thus typically organized to first develop foundational skills, followed by more </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">specialized capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.784.1">To do this, you first use easy/general examples and then add examples that are more complex or more specific. </span><span class="koboSpan" id="kobo.784.2">For example, for models that are code-specific such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.785.1">CodeLLaMA-Python</span></strong><span class="koboSpan" id="kobo.786.1">, the order is as follows: 2T general tokens, 500B code-heavy tokens, and 100B </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">Python-heavy tokens.</span></span></p>
<p><span class="koboSpan" id="kobo.788.1">In general, it is important</span><a id="_idIndexMarker1340"/><span class="koboSpan" id="kobo.789.1"> that we create pipelines that allow us to collect and organize data. </span><span class="koboSpan" id="kobo.789.2">Generally, these kinds of pipelines are </span><a id="_idIndexMarker1341"/><span class="koboSpan" id="kobo.790.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.791.1">extract, transform, load</span></strong><span class="koboSpan" id="kobo.792.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.793.1">ETL</span></strong><span class="koboSpan" id="kobo.794.1">) pipelines. </span><span class="koboSpan" id="kobo.794.2">So, if we want to download a set of web pages, we will need to create an ETL pipeline that allows us to download the pages and load them into a database along with a set of metadata. </span><span class="koboSpan" id="kobo.794.3">The metadata will then be used both to clean the data and for data scheduling. </span><span class="koboSpan" id="kobo.794.4">Once the data is downloaded it needs to be transformed. </span><span class="koboSpan" id="kobo.794.5">Because our corpus contains different types of data, it is good to have different pipelines for preprocessing the different types (removing HTML tags from web pages, removing comments from code, and </span><span class="No-Break"><span class="koboSpan" id="kobo.795.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.796.1">In addition, data is an important resource, and access must be controlled. </span><span class="koboSpan" id="kobo.796.2">Indeed, we need to prevent data </span><a id="_idIndexMarker1342"/><span class="koboSpan" id="kobo.797.1">leakage and ensure that our corpus complies with regulations such as the </span><strong class="bold"><span class="koboSpan" id="kobo.798.1">General Data Protection Regulation</span></strong><span class="koboSpan" id="kobo.799.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.800.1">GDPR</span></strong><span class="koboSpan" id="kobo.801.1">). </span><span class="koboSpan" id="kobo.801.2">Often, </span><strong class="bold"><span class="koboSpan" id="kobo.802.1">role-based access control</span></strong><span class="koboSpan" id="kobo.803.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.804.1">RBAC</span></strong><span class="koboSpan" id="kobo.805.1">) is also implemented, where </span><a id="_idIndexMarker1343"/><span class="koboSpan" id="kobo.806.1">different users have control over a different corpus of data. </span><span class="koboSpan" id="kobo.806.2">For example, administrators or analysts may have different privileges so as to avoid contamination or problems with </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.808.1">Once we have our data and have cleaned it, we create features (i.e., the data that will be used for training). </span><span class="koboSpan" id="kobo.808.2">The feature store is typically a database that is optimized to enable training. </span><span class="koboSpan" id="kobo.808.3">The idea is to have a dedicated database that we can efficiently use </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">for training.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer335">
<span class="koboSpan" id="kobo.810.1"><img alt="Figure 10.34 – Automation of the ML pipeline for continuous training (https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)" src="image/B21257_10_34.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.811.1">Figure 10.34 – Automation of the ML pipeline for continuous training (</span><a href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning"><span class="koboSpan" id="kobo.812.1">https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning</span></a><span class="koboSpan" id="kobo.813.1">)</span></p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.814.1">Model training</span></h2>
<p><span class="koboSpan" id="kobo.815.1">Once we have our features, we need to</span><a id="_idIndexMarker1344"/><span class="koboSpan" id="kobo.816.1"> decide what our foundation model will be. </span><span class="koboSpan" id="kobo.816.2">There are two alternatives: use an LLM that has already been trained or conduct fine-tuning of an already trained model. </span><span class="koboSpan" id="kobo.816.3">In the first case, most models today are causal decoders (as we saw in </span><em class="italic"><span class="koboSpan" id="kobo.817.1">Chapters 2</span></em><span class="koboSpan" id="kobo.818.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.819.1">3</span></em><span class="koboSpan" id="kobo.820.1">). </span><span class="koboSpan" id="kobo.820.2">Although the structure remains the base, there are now different alternatives and modifications (such as the mixture of experts architecture) and modifications to the attention mechanism to increase context and reduce computational cost. </span><span class="koboSpan" id="kobo.820.3">Training an LLM from scratch is very expensive, however, so most companies focus on using a pre-trained model and </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">conducting fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.822.1">Therefore, choosing the foundation model will be an important task. </span><span class="koboSpan" id="kobo.822.2">First, we must choose a model that has the desired performance in terms of output quality. </span><span class="koboSpan" id="kobo.822.3">Obviously, the chosen model must be compatible with the resources available to us (hardware and cost). </span><span class="koboSpan" id="kobo.822.4">In addition, we may want to choose a model that exhibits lower performance on general benchmarks but superior performance on some other aspects. </span><span class="koboSpan" id="kobo.822.5">For example, if our application focuses on having a coding assistant, it is better to have an LLM with superior performance on coding benchmarks than an LLM that has better </span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">wide-ranging capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.824.1">When choosing a model, we need to take into account that its size impacts both its memory footprint and its storage. </span><span class="koboSpan" id="kobo.824.2">A larger size means higher costs in general, especially if we use a cloud provider. </span><span class="koboSpan" id="kobo.824.3">Also, not all models can be used for all applications (for example, we cannot use large models for specific devices). </span><span class="koboSpan" id="kobo.824.4">In addition, a larger model also has higher latency (the time to process an input and produce an output). </span><span class="koboSpan" id="kobo.824.5">A high latency disrupts the user experience and may lead the user to choose a competitor. </span><span class="koboSpan" id="kobo.824.6">As we saw in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.825.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.826.1">, techniques (distillation, quantization, and pruning) to reduce model size while maintaining performance exist today. </span><span class="koboSpan" id="kobo.826.2">Another important point is the licensing of the model. </span><span class="koboSpan" id="kobo.826.3">Not all models have an open source license; some models may be available in repositories but may not be </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">commercially usable.</span></span></p>
<p><span class="koboSpan" id="kobo.828.1">Fine-tuning is intended </span><a id="_idIndexMarker1345"/><span class="koboSpan" id="kobo.829.1">to enable the model to acquire specific skills or some particular knowledge. </span><span class="koboSpan" id="kobo.829.2">In the former case, it is often referred to as instruction tuning. </span><span class="koboSpan" id="kobo.829.3">Instruction tuning is a subcategory of the supervised training process that aims to make the model more capable of following instructions or being trained for specific tasks. </span><span class="koboSpan" id="kobo.829.4">In repositories, there are often models that have been simply pre-trained or ones that have already undergone an instruction-tuning step. </span><span class="koboSpan" id="kobo.829.5">If we want the model to acquire a specific set of skills, it might be more interesting for us to collect a dataset for instruction tuning. </span><span class="koboSpan" id="kobo.829.6">Again, some </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">caveats apply:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.831.1">Data</span></strong><strong class="bold"><span class="koboSpan" id="kobo.832.1"> distribution</span></strong><span class="koboSpan" id="kobo.833.1">: Instruction tuning considers a mix of different tasks, so our dataset should respect this principle and contain several examples. </span><span class="koboSpan" id="kobo.833.2">Ideally, these examples should be of different topics, different contexts, different lengths, different styles, and different types </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">of tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.835.1">Dataset quality</span></strong><span class="koboSpan" id="kobo.836.1">: Generally, in this step (quality check), it is important to use examples that are correct not only in terms of factual correctness but also in terms of ensuring that the task is done correctly and is well explained. </span><span class="koboSpan" id="kobo.836.2">For example, chain-of-thought examples are used today, where the intermediate thinking is explained instead of just the solution. </span><span class="koboSpan" id="kobo.836.3">The examples are human-generated; however, to save costs, a larger model can be used initially to create the dataset for instruction tuning. </span><span class="koboSpan" id="kobo.836.4">For instance, a 70-billion-parameter model could be used to prepare the dataset for tuning a </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">7-billion-parameter</span></span><span class="No-Break"><span class="koboSpan" id="kobo.838.1"> model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.839.1">Complexity</span></strong><span class="koboSpan" id="kobo.840.1">: In general, we want our model to acquire capabilities. </span><span class="koboSpan" id="kobo.840.2">Through simple examples, the model will learn structure and gain a general understanding of the task. </span><span class="koboSpan" id="kobo.840.3">However, there should also be examples in the dataset that are difficult, require multi-step reasoning, or are complex in nature. </span><span class="koboSpan" id="kobo.840.4">These examples reflect the complexity of real-world problems and have been seen to help the model improve its </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">reasoning skills.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.842.1">Quantity</span></strong><span class="koboSpan" id="kobo.843.1">: There is also a discourse associated with quantity. </span><span class="koboSpan" id="kobo.843.2">According to some studies, larger models need fewer examples. </span><span class="koboSpan" id="kobo.843.3">For example, models with 70 billion parameters might require as few as 1,000 quality examples. </span><span class="koboSpan" id="kobo.843.4">In contrast, smaller models might need many more examples. </span><span class="koboSpan" id="kobo.843.5">Smaller models may need many examples just to understand the task and many more to master it. </span><span class="koboSpan" id="kobo.843.6">A 7 billion model may use up to a </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">million examples.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.845.1">Building a dataset of</span><a id="_idIndexMarker1346"/><span class="koboSpan" id="kobo.846.1"> thousands of examples can be particularly expensive. </span><span class="koboSpan" id="kobo.846.2">In many studies, only a small portion is created by humans. </span><span class="koboSpan" id="kobo.846.3">To reach the desired number of examples, one can either use a model to generate them or integrate already available datasets. </span><span class="koboSpan" id="kobo.846.4">Hugging Face contains many datasets for instruction tuning, for both general purposes as well as </span><span class="No-Break"><span class="koboSpan" id="kobo.847.1">specific domains.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer336">
<span class="koboSpan" id="kobo.848.1"><img alt="Figure 10.35 – Constructing an instruction-tuning dataset (https://arxiv.org/pdf/2303.18223)" src="image/B21257_10_35.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.849.1">Figure 10.35 – Constructing an instruction-tuning dataset (</span><a href="https://arxiv.org/pdf/2303.18223"><span class="koboSpan" id="kobo.850.1">https://arxiv.org/pdf/2303.18223</span></a><span class="koboSpan" id="kobo.851.1">)</span></p>
<p><span class="koboSpan" id="kobo.852.1">The construction of</span><a id="_idIndexMarker1347"/><span class="koboSpan" id="kobo.853.1"> these datasets, especially for particular domains, also requires the presence of experts (for example, if the dataset is for finance or medicine, collaboration with experts in the field or other institutions is common). </span><span class="koboSpan" id="kobo.853.2">Similar to a pre-training dataset, this dataset will undergo preprocessing. </span><span class="koboSpan" id="kobo.853.3">For example, examples of poor quality will be filtered out (one of the most commonly used methods is to have a list of keywords that indicate inappropriate content, off-topic examples, and so on), and filters will be used for length (e.g., examples that are too short or too long for the model) and for format (for some tasks, examples are formatted in a particular way, and examples that do not comply are removed). </span><span class="koboSpan" id="kobo.853.4">This dataset will also be deduplicated, and examples that are too similar are also often removed (if you ask an LLM to generate the examples, it might happen that examples that are too similar are generated). </span><span class="koboSpan" id="kobo.853.5">Patterns such as embeddings can be used for this task, where examples that have too high a similarity are filtered out. </span><strong class="bold"><span class="koboSpan" id="kobo.854.1">MinHash</span></strong><span class="koboSpan" id="kobo.855.1"> is </span><a id="_idIndexMarker1348"/><span class="koboSpan" id="kobo.856.1">another popular alternative to reduce the computational cost of the task. </span><span class="koboSpan" id="kobo.856.2">MinHash generates compact representations of patterns (of vectors), which are then compared with a </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">similarity function.</span></span></p>
<p><span class="koboSpan" id="kobo.858.1">Because we are interested in model performance for specific tasks, an additional step is also conducted: </span><strong class="bold"><span class="koboSpan" id="kobo.859.1">data decontamination</span></strong><span class="koboSpan" id="kobo.860.1">. </span><span class="koboSpan" id="kobo.860.2">This is a </span><a id="_idIndexMarker1349"/><span class="koboSpan" id="kobo.861.1">process in which we ensure that our instruction-tuning dataset does not contain examples that are the same or too similar to those in the evaluation or test set. </span><span class="koboSpan" id="kobo.861.2">In fact, once we have instruction-tuned our model, we want to test it on test sets that we set aside. </span><span class="koboSpan" id="kobo.861.3">If there were examples that were too similar, we could not verify overfitting or storage phenomena. </span><span class="koboSpan" id="kobo.861.4">Data decontamination is conducted with techniques similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">data deduplication.</span></span></p>
<p><span class="koboSpan" id="kobo.863.1">Before proceeding to the actual training, an </span><a id="_idIndexMarker1350"/><span class="koboSpan" id="kobo.864.1">additional step, </span><strong class="bold"><span class="koboSpan" id="kobo.865.1">data quality evaluation</span></strong><span class="koboSpan" id="kobo.866.1">, is usually conducted. </span><span class="koboSpan" id="kobo.866.2">The dataset is evaluated for several criteria such as quality, accuracy, and complexity. </span><span class="koboSpan" id="kobo.866.3">Usually, some statistical parameters (such as the loss) are calculated and some examples are manually inspected. </span><span class="koboSpan" id="kobo.866.4">Recently, it has become increasingly popular to </span><a id="_idIndexMarker1351"/><span class="koboSpan" id="kobo.867.1">use </span><strong class="bold"><span class="koboSpan" id="kobo.868.1">LLM-as-a-judge</span></strong><span class="koboSpan" id="kobo.869.1">, a strategy in which an LLM evaluates the quality of some examples. </span><span class="koboSpan" id="kobo.869.2">In such cases, an LLM is given a kind of template to check the quality of the examples by providing a score. </span><span class="koboSpan" id="kobo.869.3">Alternatively, today, there are also specific templates trained to provide a quality score. </span><span class="koboSpan" id="kobo.869.4">For example, reward models such </span><a id="_idIndexMarker1352"/><span class="koboSpan" id="kobo.870.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.871.1">ArmoRM-Llama3-8B-v0.1</span></strong><span class="koboSpan" id="kobo.872.1"> are trained to produce an output that represents the quality of a text in terms of helpfulness, correctness, coherence, complexity, </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">and verbosity.</span></span></p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.874.1">Model testing</span></h2>
<p><span class="koboSpan" id="kobo.875.1">Once we have our dataset, we can </span><a id="_idIndexMarker1353"/><span class="koboSpan" id="kobo.876.1">conduct fine-tuning. </span><span class="koboSpan" id="kobo.876.2">Fine-tuning allows us to steer the capabilities and knowledge of our LLM. </span><span class="koboSpan" id="kobo.876.3">We must keep in mind that fine-tuning is not a magic potion; it has both risks and benefits. </span><span class="koboSpan" id="kobo.876.4">For example, fine-tuning exploits pre-existing knowledge of the model, but also conducts a refocus for a specific domain. </span><span class="koboSpan" id="kobo.876.5">This can lead to performance degradation and hallucinations. </span><span class="koboSpan" id="kobo.876.6">For this reason, in </span><em class="italic"><span class="koboSpan" id="kobo.877.1">Chapters 5</span></em><span class="koboSpan" id="kobo.878.1">–</span><em class="italic"><span class="koboSpan" id="kobo.879.1">7</span></em><span class="koboSpan" id="kobo.880.1">, we looked at alternatives (RAG and GraphRAG). </span><span class="koboSpan" id="kobo.880.2">In </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.881.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.882.1">, we saw that there are now also efficient fine-tuning techniques such as LoRA and QLoRA that make the process much less expensive. </span><span class="koboSpan" id="kobo.882.2">Today, different libraries can conduct fine-tuning of these models, such as TRL (a library created by Hugging Face), Unsloth, and Axolotl based on Unsloth; these libraries also have </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">additional features.</span></span></p>
<p><span class="koboSpan" id="kobo.884.1">After training, the key step is LLM evaluation. </span><span class="koboSpan" id="kobo.884.2">In general, evaluation is carried out in </span><span class="No-Break"><span class="koboSpan" id="kobo.885.1">three stages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.886.1">During </span></strong><strong class="bold"><span class="koboSpan" id="kobo.887.1">pre-training</span></strong><span class="koboSpan" id="kobo.888.1">: During this step, the training of the model is monitored, and, in general, metrics such as training loss (a metric based on cross-entropy), loss on the validation set, perplexity (the exponential of training loss, one of the most commonly used metrics), and gradient norm (which indicates whether there were any instabilities in the training) </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">are evaluated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.890.1">After </span></strong><strong class="bold"><span class="koboSpan" id="kobo.891.1">pre-training</span></strong><span class="koboSpan" id="kobo.892.1">: Once pre-training is completed, a capability analysis is conducted on the benchmark datasets. </span><span class="koboSpan" id="kobo.892.2">In these datasets, both model knowledge and the ability to solve certain problems are evaluated. </span><span class="koboSpan" id="kobo.892.3">For example, MMLU tests model knowledge on a large number of domains, while datasets such as HellaSwag test the model on </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">reasoning skills.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.894.1">After fine-tuning</span></strong><span class="koboSpan" id="kobo.895.1">: After instruction tuning, qualities such as the LLM’s ability to follow instructions, converse, and use tools, for example, are usually evaluated. </span><span class="koboSpan" id="kobo.895.2">Since fine-tuning allows you to adapt the model to a specialized domain, it is beneficial to use specialized benchmarks in such cases. </span><span class="koboSpan" id="kobo.895.3">For example, for medical knowledge, a dataset such as Open Medical-LLM Leaderboard can be used, or for coding skills, BigCodeBench Leaderboard is a </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">popular choice.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer337">
<span class="koboSpan" id="kobo.897.1"><img alt="Figure 10.36 – Taxonomy of LLM evaluation (https://arxiv.org/pdf/2310.19736)" src="image/B21257_10_36.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.898.1">Figure 10.36 – Taxonomy of LLM evaluation (</span><a href="https://arxiv.org/pdf/2310.19736"><span class="koboSpan" id="kobo.899.1">https://arxiv.org/pdf/2310.19736</span></a><span class="koboSpan" id="kobo.900.1">)</span></p>
<p><span class="koboSpan" id="kobo.901.1">The last </span><a id="_idIndexMarker1354"/><span class="koboSpan" id="kobo.902.1">two steps (</span><em class="italic"><span class="koboSpan" id="kobo.903.1">After </span></em><em class="italic"><span class="koboSpan" id="kobo.904.1">pre-training</span></em><span class="koboSpan" id="kobo.905.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.906.1">After fine-tuning</span></em><span class="koboSpan" id="kobo.907.1">) can also be conducted by manual inspection or using LLM-as-a-judge. </span><span class="koboSpan" id="kobo.907.2">For example, for open-ended text generation, it is more difficult to evaluate the capabilities of a model with standard metrics. </span><span class="koboSpan" id="kobo.907.3">Moreover, evaluating a model’s capabilities in a specific domain requires more </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">in-depth analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.909.1">If our LLM is a component of a system such as RAG, not only should the capabilities of the LLM be evaluated but the whole system as well. </span><span class="koboSpan" id="kobo.909.2">Indeed, we can evaluate the reasoning or hallucination capabilities of a model alone, but since the model will then be part of a system, we need to evaluate the whole product. </span><span class="koboSpan" id="kobo.909.3">For example, we should evaluate the whole RAG system for accuracy in retrieval and response generation. </span><span class="koboSpan" id="kobo.909.4">Even for RAG, there are </span><a id="_idIndexMarker1355"/><span class="koboSpan" id="kobo.910.1">both metrics and specific libraries for evaluating the system. </span><span class="koboSpan" id="kobo.910.2">For example, RAGAS (Retrieval-Augmented Generation Assessment) uses an LLM to evaluate the RAG response. </span><span class="koboSpan" id="kobo.910.3">ARES (Automatic RAG Evaluation through Synthetic data) is a comprehensive tool that takes advantage of synthetic data generation to assess </span><span class="No-Break"><span class="koboSpan" id="kobo.911.1">model quality.</span></span></p>
<h2 id="_idParaDest-195"><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.912.1">Inference optimization</span></h2>
<p><span class="koboSpan" id="kobo.913.1">Our LLM has to be deployed and will</span><a id="_idIndexMarker1356"/><span class="koboSpan" id="kobo.914.1"> consume resources; our goal now is to optimize the inference process to avoid users encountering latency and reduce costs for us. </span><span class="koboSpan" id="kobo.914.2">Basically, three processes occur </span><span class="No-Break"><span class="koboSpan" id="kobo.915.1">in inference:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.916.1">Tokenization and embedding</span></strong><span class="koboSpan" id="kobo.917.1">: Input is transformed into a numerical representation and </span><span class="No-Break"><span class="koboSpan" id="kobo.918.1">then vector.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.919.1">Computation</span></strong><span class="koboSpan" id="kobo.920.1">: A key and value are computed for each </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">multi-head attention.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.922.1">Generation</span></strong><span class="koboSpan" id="kobo.923.1">: Output is </span><span class="No-Break"><span class="koboSpan" id="kobo.924.1">produced sequentially.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.925.1">The first two steps are expensive but are easily parallelized on GPUs. </span><span class="koboSpan" id="kobo.925.2">The third step, on the other hand, is sequential because each output token depends on the previous token. </span><span class="koboSpan" id="kobo.925.3">The purpose of inference optimization is to speed up these three steps, and in this subsection, we will look at </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">some techniques.</span></span></p>
<h3><span class="koboSpan" id="kobo.927.1">Model inference optimization</span></h3>
<p><span class="koboSpan" id="kobo.928.1">To produce a token</span><a id="_idIndexMarker1357"/><span class="koboSpan" id="kobo.929.1"> output, we </span><a id="_idIndexMarker1358"/><span class="koboSpan" id="kobo.930.1">need all the previous context. </span><span class="koboSpan" id="kobo.930.2">For example, for the 15th token produced, we should calculate the </span><strong class="bold"><span class="koboSpan" id="kobo.931.1">key-value</span></strong><span class="koboSpan" id="kobo.932.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.933.1">KV</span></strong><span class="koboSpan" id="kobo.934.1">) product</span><a id="_idIndexMarker1359"/><span class="koboSpan" id="kobo.935.1"> of all tokens, 1 through 14. </span><span class="koboSpan" id="kobo.935.2">This makes the process very slow, reducing over time such that the attention has a quadratic cost (</span><em class="italic"><span class="koboSpan" id="kobo.936.1">O(n²)</span></em><span class="koboSpan" id="kobo.937.1">). </span><span class="koboSpan" id="kobo.937.2">The KV cache caches and reuses the key (</span><em class="italic"><span class="koboSpan" id="kobo.938.1">K</span></em><span class="koboSpan" id="kobo.939.1">) and value (</span><em class="italic"><span class="koboSpan" id="kobo.940.1">V</span></em><span class="koboSpan" id="kobo.941.1">) tensors from previous tokens, allowing faster computation of attention scores. </span><span class="koboSpan" id="kobo.941.2">This reduces memory and computational cost, enabling near-linear time (</span><em class="italic"><span class="koboSpan" id="kobo.942.1">O(n)</span></em><span class="koboSpan" id="kobo.943.1">) inference. </span><span class="koboSpan" id="kobo.943.2">Typically, the process works like this: for the first token, we compute and store </span><em class="italic"><span class="koboSpan" id="kobo.944.1">(K,V)</span></em><span class="koboSpan" id="kobo.945.1">. </span><span class="koboSpan" id="kobo.945.2">For the second, we find </span><em class="italic"><span class="koboSpan" id="kobo.946.1">(K,V)</span></em><span class="koboSpan" id="kobo.947.1"> again and add </span><em class="italic"><span class="koboSpan" id="kobo.948.1">K,V</span></em><span class="koboSpan" id="kobo.949.1">. </span><span class="koboSpan" id="kobo.949.2">In other words, attention is applied only to the new tokens. </span><span class="koboSpan" id="kobo.949.3">As we saw in </span><a href="B21257_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.950.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.951.1">, this is the calculation </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">of attention:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer338">
<span class="koboSpan" id="kobo.953.1"><img alt="Figure 10.37 – Attention calculation" src="image/B21257_10_37.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.954.1">Figure 10.37 – Attention calculation</span></p>
<p><span class="koboSpan" id="kobo.955.1">In the </span><a id="_idIndexMarker1360"/><span class="koboSpan" id="kobo.956.1">KV cache, we</span><a id="_idIndexMarker1361"/><span class="koboSpan" id="kobo.957.1"> calculate the KV product, and</span><a id="_idIndexMarker1362"/><span class="koboSpan" id="kobo.958.1"> then we save the product result in memory. </span><span class="koboSpan" id="kobo.958.2">At the time of a new token, we retrieve this information (the KV product) and calculate the KV product only for </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">that token.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer339">
<span class="koboSpan" id="kobo.960.1"><img alt="Figure 10.38 – KV cache process" src="image/B21257_10_38.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.961.1">Figure 10.38 – KV cache process</span></p>
<p><span class="koboSpan" id="kobo.962.1">The KV cache</span><a id="_idIndexMarker1363"/><span class="koboSpan" id="kobo.963.1"> speeds up inference by eliminating some redundant computation (it prevents us from reprocessing all the previous parts of the sequence), scales well with long context windows, and is now optimized for major libraries and hardware. </span><span class="koboSpan" id="kobo.963.2">Of course, using the KV cache means we use more memory. </span><span class="koboSpan" id="kobo.963.3">In fact, it means that we have to keep in memory each KV cache per token, per attention head, and per layer. </span><span class="koboSpan" id="kobo.963.4">This, in practice, also places a limit on the size of the context window we can use. </span><span class="koboSpan" id="kobo.963.5">Obviously, during model training, it is of little use because we have to conduct parameter updates. </span><span class="koboSpan" id="kobo.963.6">Therefore, today, there are approaches that try to </span><a id="_idIndexMarker1364"/><span class="koboSpan" id="kobo.964.1">compress the KV cache so as to reduce the cost in terms </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">of memory.</span></span></p>
<p><span class="koboSpan" id="kobo.966.1">Another technique used to speed up inference </span><a id="_idIndexMarker1365"/><span class="koboSpan" id="kobo.967.1">is </span><strong class="bold"><span class="koboSpan" id="kobo.968.1">continuous batching</span></strong><span class="koboSpan" id="kobo.969.1">. </span><span class="koboSpan" id="kobo.969.2">The main purpose of this technique is to parallelize the various queries, then divide the model memory cost by the batch and transfer more data to the GPU. </span><span class="koboSpan" id="kobo.969.3">Traditional batching leads to slower input processing and is not optimized for inference, where the various queries may differ in size. </span><span class="koboSpan" id="kobo.969.4">Continuous batching, on the other hand, allows multiple user requests to be handled dynamically, allowing multiple inference requests to be processed in parallel, even if they arrive at different times. </span><span class="koboSpan" id="kobo.969.5">Requests that arrive at a different time are dynamically grouped into a series of batches, instead of having a fixed batch to fill. </span><span class="koboSpan" id="kobo.969.6">A batching engine merges multiple users’ prompts into a single batch. </span><span class="koboSpan" id="kobo.969.7">Instead of waiting for an entire batch, new tokens are processed when resources are available. </span><span class="koboSpan" id="kobo.969.8">This technique also works well with the KV cache; some tokens may have already been processed and we can recall what is in memory to further speed up the process. </span><span class="koboSpan" id="kobo.969.9">Continuous batching</span><a id="_idIndexMarker1366"/><span class="koboSpan" id="kobo.970.1"> thus allows lower latency, allows streaming for several users at the same time, and improves resource utilization. </span><span class="koboSpan" id="kobo.970.2">Of course, it is more complex than the standard implementation of attention and requires a different implementation: we have to manage users optimally, and numerous requests are made to the </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">KV cache.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.972.1">Speculative decoding</span></strong><span class="koboSpan" id="kobo.973.1"> is </span><a id="_idIndexMarker1367"/><span class="koboSpan" id="kobo.974.1">another optimization technique used in autoregressive language models to accelerate text generation. </span><span class="koboSpan" id="kobo.974.2">Classic LLMs generate only one token at a time, and token generation is not parallelizable, leading to inefficient inference. </span><span class="koboSpan" id="kobo.974.3">In speculative decoding, we have two models </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">working together:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.976.1">A small, faster “draft” model that generates multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">candidate tokens</span></span></li>
<li><span class="koboSpan" id="kobo.978.1">The main, larger LLM that verifies the candidates and either accepts or </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">corrects them</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.980.1">The draft model (a small model of the same LLM architecture as the main one, but with fewer parameters) generates multiple speculative tokens at once. </span><span class="koboSpan" id="kobo.980.2">The main LLM checks these proposed tokens; if they match those of the larger LLM’s output, they are accepted. </span><span class="koboSpan" id="kobo.980.3">If, however, there is no match, the LLM discards them and continues to generate. </span><span class="koboSpan" id="kobo.980.4">The process is iterative until the output is finished. </span><span class="koboSpan" id="kobo.980.5">Speculative decoding makes it possible to reduce the number of sequential steps in inference, speed up the response, and maximize GPU consumption without losing quality. </span><span class="koboSpan" id="kobo.980.6">Of course, the draft model must generate good candidates; if the small model is not accurate, we lose the advantage in speedup, which means we would require another model. </span><span class="koboSpan" id="kobo.980.7">This approach works better with long-form than </span><span class="No-Break"><span class="koboSpan" id="kobo.981.1">small outputs.</span></span></p>
<p><span class="koboSpan" id="kobo.982.1">Another way to </span><a id="_idIndexMarker1368"/><span class="koboSpan" id="kobo.983.1">speed up inference is to use specific</span><a id="_idIndexMarker1369"/><span class="koboSpan" id="kobo.984.1"> forms of attention. </span><strong class="bold"><span class="koboSpan" id="kobo.985.1">Paged attention</span></strong><span class="koboSpan" id="kobo.986.1"> is an </span><a id="_idIndexMarker1370"/><span class="koboSpan" id="kobo.987.1">optimized memory management technique for handling large KV caches efficiently during LLM inference. </span><span class="koboSpan" id="kobo.987.2">It works like a virtual memory system by dynamically managing memory allocation and preventing fragmentation. </span><span class="koboSpan" id="kobo.987.3">It is inspired by the management of memory systems in computers, and instead of storing KV caches in a continuous memory block (which can lead to fragmentation), it stores them in smaller memory pages. </span><span class="koboSpan" id="kobo.987.4">This allows faster retrieval of information (and only necessary information) from the KV cache. </span><span class="koboSpan" id="kobo.987.5">Paged attention thus prevents GPU memory fragmentation, makes the system more efficient for long context (reduces memory consumption for long chats between the user and the system), and decreases latency by allowing easier fetching from the</span><a id="_idIndexMarker1371"/><span class="koboSpan" id="kobo.988.1"> KV cache. </span><strong class="bold"><span class="koboSpan" id="kobo.989.1">FlashAttention</span></strong><span class="koboSpan" id="kobo.990.1"> is another way to make the inference process more efficient, allowing faster processing of attention with decreased memory consumption. </span><span class="koboSpan" id="kobo.990.2">It achieves this by processing attention in small blocks instead of storing large intermediate matrices. </span><span class="koboSpan" id="kobo.990.3">In this way, it makes more efficient use of GPU resources. </span><span class="koboSpan" id="kobo.990.4">In FlashAttention, only small blocks of various tokens are stored in the RAM. </span><span class="koboSpan" id="kobo.990.5">Today, many models use forms of attention during training that are aimed at faster reasoning. </span><strong class="bold"><span class="koboSpan" id="kobo.991.1">Multi-grouped attention</span></strong><span class="koboSpan" id="kobo.992.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.993.1">MGA</span></strong><span class="koboSpan" id="kobo.994.1">) is a</span><a id="_idIndexMarker1372"/><span class="koboSpan" id="kobo.995.1"> hybrid between </span><strong class="bold"><span class="koboSpan" id="kobo.996.1">multi-head attention</span></strong><span class="koboSpan" id="kobo.997.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.998.1">MHA</span></strong><span class="koboSpan" id="kobo.999.1">) and</span><a id="_idIndexMarker1373"/><span class="koboSpan" id="kobo.1000.1"> sparse attention. </span><span class="koboSpan" id="kobo.1000.2">Instead of each attention head attending to all tokens, MGA groups multiple heads together to enable more efficient computation. </span><span class="koboSpan" id="kobo.1000.3">In MGA, the heads are not separated but grouped into specific clusters and process a group of characters. </span><span class="koboSpan" id="kobo.1000.4">This makes it possible to reduce computational costs, is more flexible for sparse attention forms, and makes it possible to speed up training and reasoning. </span><span class="koboSpan" id="kobo.1000.5">Another popular alternative is </span><strong class="bold"><span class="koboSpan" id="kobo.1001.1">multi-head latent attention</span></strong><span class="koboSpan" id="kobo.1002.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1003.1">MLA</span></strong><span class="koboSpan" id="kobo.1004.1">), which</span><a id="_idIndexMarker1374"/><span class="koboSpan" id="kobo.1005.1"> is used in modern LLMs. </span><span class="koboSpan" id="kobo.1005.2">In standard MHA, we explicitly compute attention for all heads. </span><span class="koboSpan" id="kobo.1005.3">In MLA, we use latent heads that indirectly encode relationships between tokens without the need for a full pairwise </span><a id="_idIndexMarker1375"/><span class="koboSpan" id="kobo.1006.1">computation of attention. </span><span class="koboSpan" id="kobo.1006.2">In</span><a id="_idIndexMarker1376"/><span class="koboSpan" id="kobo.1007.1"> this way, the model has better generalization by learning a compressed representation without sacrificing accuracy. </span><span class="koboSpan" id="kobo.1007.2">This requires less attention during inference and </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">saves memory.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer340">
<span class="koboSpan" id="kobo.1009.1"><img alt="Figure 10.39 – Overview of methods for speeding inference (https://arxiv.org/pdf/2407.18003)" src="image/B21257_10_39.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1010.1">Figure 10.39 – Overview of methods for speeding inference (</span><a href="https://arxiv.org/pdf/2407.18003"><span class="koboSpan" id="kobo.1011.1">https://arxiv.org/pdf/2407.18003</span></a><span class="koboSpan" id="kobo.1012.1">)</span></p>
<p><span class="koboSpan" id="kobo.1013.1">These techniques, as illustrated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1014.1">Figure 10</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1015.1">.39</span></em><span class="koboSpan" id="kobo.1016.1">, demonstrate how inference efficiency can be improved across multiple stages—compression, caching, and memory optimization. </span><span class="koboSpan" id="kobo.1016.2">With this foundation, we can now explore how such optimizations are applied in real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">deployment scenarios.</span></span></p>
<h3><span class="koboSpan" id="kobo.1018.1">Data, pipeline, and tensor parallelism</span></h3>
<p><span class="koboSpan" id="kobo.1019.1">Another way to make training more efficient is to</span><a id="_idIndexMarker1377"/><span class="koboSpan" id="kobo.1020.1"> parallelize it. </span><strong class="bold"><span class="koboSpan" id="kobo.1021.1">Model parallelism</span></strong><span class="koboSpan" id="kobo.1022.1"> for a neural network is to distribute the model across multiple devices (such as GPUs or TPUs) to overcome memory and computation limitations. </span><span class="koboSpan" id="kobo.1022.2">While this can be useful to speed up training, in other cases, it is necessary because the model is too large to fit on a single device. </span><span class="koboSpan" id="kobo.1022.3">There are several ways to parallelize a model, as we will </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">see next:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1024.1">Data parallelism</span></strong><span class="koboSpan" id="kobo.1025.1"> is </span><a id="_idIndexMarker1378"/><span class="koboSpan" id="kobo.1026.1">considered the simplest approach, in which replicas of the model are distributed across multiple computing devices (e.g., GPUs, TPUs, or even different machines), and different subsets of the training dataset are fed into each replica. </span><span class="koboSpan" id="kobo.1026.2">During training, averaging of the gradients of the various GPUs is conducted; this is used for model updates. </span><span class="koboSpan" id="kobo.1026.3">Then, each model is replicated across workers (GPUs/TPUs), and the input data batch is split into mini-batches assigned to different workers. </span><span class="koboSpan" id="kobo.1026.4">During the forward pass, each worker computes predictions and losses for its mini-batch. </span><span class="koboSpan" id="kobo.1026.5">Subsequently, each worker calculates gradients for its assigned data. </span><span class="koboSpan" id="kobo.1026.6">These gradients are aggregated either by averaging or using a more complex method, and the aggregated gradients are used to update all model replicas, ensuring synchronization across workers. </span><span class="koboSpan" id="kobo.1026.7">Data parallelism can be implemented in several ways, the most common being synchronous data parallelism, in which all devices compute the gradient before synchronization. </span><span class="koboSpan" id="kobo.1026.8">Once all gradients are available, averaging is conducted. </span><span class="koboSpan" id="kobo.1026.9">Although this approach ensures that there is consistency, a worker can slow down the training. </span><span class="koboSpan" id="kobo.1026.10">To overcome this, we have asynchronous data parallelism, where each device conducts the local model update independently, at the risk of introducing stale gradients (outdated updates). </span><span class="koboSpan" id="kobo.1026.11">An intermediate approach (stale-sync data parallelism) is also available, where workers perform multiple local updates before synchronizing with others. </span><span class="koboSpan" id="kobo.1026.12">Data parallelism can also be centralized with a central server or decentralized with the various workers exchanging gradients in a ring topology. </span><span class="koboSpan" id="kobo.1026.13">Data parallelism </span><a id="_idIndexMarker1379"/><span class="koboSpan" id="kobo.1027.1">allows the workload to be distributed among different devices, increasing the speed of training, scales well when you have several devices, is not complex to implement, and is efficient because the model stays on the various devices and is not swapped. </span><span class="koboSpan" id="kobo.1027.2">On the other hand, gradient synchronization can be slow due to communication overhead, especially if communication is inefficient. </span><span class="koboSpan" id="kobo.1027.3">Variations in device speed, such as using different hardware or GPU versions, can further exacerbate this issue. </span><span class="koboSpan" id="kobo.1027.4">Additionally, large batch sizes may cause convergence problems, and managing synchronization becomes increasingly complex as the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.1028.1">devices grows.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer341">
<span class="koboSpan" id="kobo.1029.1"><img alt="Figure 10.40 – Processing of mini-batches over time in data parallelism. Each GPU has a copy of all the layers (shown in different colors) and different mini-batches (numbered) are processed by different GPUs (https://arxiv.org/pdf/2111.04949)" src="image/B21257_10_40.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1030.1">Figure 10.40 – Processing of mini-batches over time in data parallelism. </span><span class="koboSpan" id="kobo.1030.2">Each GPU has a copy of all the layers (shown in different colors) and different mini-batches (numbered) are processed by different GPUs (</span><a href="https://arxiv.org/pdf/2111.04949"><span class="koboSpan" id="kobo.1031.1">https://arxiv.org/pdf/2111.04949</span></a><span class="koboSpan" id="kobo.1032.1">)</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1033.1">Pipeline parallelism</span></strong><span class="koboSpan" id="kobo.1034.1"> is a</span><a id="_idIndexMarker1380"/><span class="koboSpan" id="kobo.1035.1"> distributed training technique where different layers of a deep learning model are assigned to different devices (e.g., GPUs or TPUs), and mini-batches are processed sequentially through the pipeline. </span><span class="koboSpan" id="kobo.1035.2">This technique helps in training extremely large models that do not fit into a single device’s memory. </span><span class="koboSpan" id="kobo.1035.3">Pipeline parallelism is commonly used in transformer models such as GPT-3, GPT-4, LLaMA, and DeepSeek, where model sizes exceed the memory capacity of a single GPU. </span><span class="koboSpan" id="kobo.1035.4">The model is divided into multiple stages, where each stage represents a subset of consecutive layers and is assigned to a different GPU. </span><span class="koboSpan" id="kobo.1035.5">A batch is split into mini-batches, and a mini-batch is split into micro-batches. </span><span class="koboSpan" id="kobo.1035.6">One micro-batch is then processed from the first stage and passed to the next. </span><span class="koboSpan" id="kobo.1035.7">The second micro-batch starts being processed before the first micro-batch has finished all the stages (as soon as the first stage clears, it can start processing the second micro-batch, without the first micro-batch having to pass all the layers, thus allowing the process to be parallelized in an efficient manner). </span><span class="koboSpan" id="kobo.1035.8">The backward pass follows the same pipeline as the forward pass but in reverse order; the gradient starts from the last stages to the first stages. </span><span class="koboSpan" id="kobo.1035.9">Once all micro-batches are completed, the model update can </span><a id="_idIndexMarker1381"/><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">be conducted.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer342">
<span class="koboSpan" id="kobo.1037.1"><img alt="Figure 10.41 – Forward and backward update for a single micro-batch (https://arxiv.org/pdf/2403.03699v1)" src="image/B21257_10_41.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1038.1">Figure 10.41 – Forward and backward update for a single micro-batch (</span><a href="https://arxiv.org/pdf/2403.03699v1"><span class="koboSpan" id="kobo.1039.1">https://arxiv.org/pdf/2403.03699v1</span></a><span class="koboSpan" id="kobo.1040.1">)</span></p>
<div>
<div class="IMG---Figure" id="_idContainer343">
<span class="koboSpan" id="kobo.1041.1"><img alt="Figure 10.42 – Forward and backward update for two micro-batches in parallel (https://arxiv.org/pdf/2403.03699v1)" src="image/B21257_10_42.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1042.1">Figure 10.42 – Forward and backward update for two micro-batches in parallel (</span><a href="https://arxiv.org/pdf/2403.03699v1"><span class="koboSpan" id="kobo.1043.1">https://arxiv.org/pdf/2403.03699v1</span></a><span class="koboSpan" id="kobo.1044.1">)</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.1045.1">Pipeline parallelism can be conducted in </span><a id="_idIndexMarker1382"/><span class="koboSpan" id="kobo.1046.1">different manners such as </span><strong class="bold"><span class="koboSpan" id="kobo.1047.1">one forward, one backward</span></strong><span class="koboSpan" id="kobo.1048.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1049.1">1F1B</span></strong><span class="koboSpan" id="kobo.1050.1">) scheduling, in which each GPU conducts one forward pass and one backward pass at the same time. </span><span class="koboSpan" id="kobo.1050.2">Alternatively, each device could contain multiple model partitions and thus conduct more flexible scheduling. </span><span class="koboSpan" id="kobo.1050.3">Pipeline parallelism allows the training of very large models that do not fit into a single GPU, allows better utilization of the various devices (each device constantly processes micro-batches), reduces the risk of memory bottlenecks, and is well adapted to transformers. </span><span class="koboSpan" id="kobo.1050.4">On the other hand, it is a more complex system, where one has to manage the stages so that some of them do not have more computation-heavy layers and thus become bottlenecks (careful layer partitioning to balance the workload among the various devices). </span><span class="koboSpan" id="kobo.1050.5">In the first iterations, the system is</span><a id="_idIndexMarker1383"/><span class="koboSpan" id="kobo.1051.1"> less efficient as it waits to be filled with micro-batches (the first stage starts working before the other stages), communication is more complex due to gradient aggregation, and there is increased complexity in designing </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">the system.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1053.1">Tensor parallelism</span></strong><span class="koboSpan" id="kobo.1054.1"> is a </span><a id="_idIndexMarker1384"/><span class="koboSpan" id="kobo.1055.1">model parallelism technique where individual weight tensors (matrices) within a model are split across multiple GPUs. </span><span class="koboSpan" id="kobo.1055.2">Unlike traditional model parallelism, which assigns entire layers to different GPUs, tensor parallelism breaks down the computations within a single layer and distributes them across multiple devices. </span><span class="koboSpan" id="kobo.1055.3">This approach is particularly useful for large-scale transformer models where certain operations (such as matrix multiplications in attention layers) require enormous memory and computational power. </span><span class="koboSpan" id="kobo.1055.4">Instead of computing and storing entire weight matrices on a single GPU, tensor parallelism divides them among multiple GPUs. </span><span class="koboSpan" id="kobo.1055.5">For example, a fully connected layer applies a weight matrix, </span><em class="italic"><span class="koboSpan" id="kobo.1056.1">W</span></em><span class="koboSpan" id="kobo.1057.1">, to an input, </span><em class="italic"><span class="koboSpan" id="kobo.1058.1">X</span></em><span class="koboSpan" id="kobo.1059.1">, to obtain an output, </span><em class="italic"><span class="koboSpan" id="kobo.1060.1">Y</span></em><span class="koboSpan" id="kobo.1061.1">. </span><span class="koboSpan" id="kobo.1061.2">If </span><em class="italic"><span class="koboSpan" id="kobo.1062.1">W</span></em><span class="koboSpan" id="kobo.1063.1"> is too large for a single GPU, we can divide it among multiple GPUs. </span><span class="koboSpan" id="kobo.1063.2">Each GPU will then conduct only part of the computation, producing part of the output, which is then later aggregated. </span><span class="koboSpan" id="kobo.1063.3">Similarly, during the backward pass, we must then redistribute the gradient computation to allow proper updates of the weights of the various matrices, </span><em class="italic"><span class="koboSpan" id="kobo.1064.1">W</span></em><span class="koboSpan" id="kobo.1065.1">. </span><span class="koboSpan" id="kobo.1065.2">Column-wise tensor parallelism is among the most widely used for transformers, where the weight matrix is split column-wise across GPUs, and each GPU then computes part of the output, which is then concatenated. </span><span class="koboSpan" id="kobo.1065.3">Considering the self-attention mechanism of a model, the query (</span><em class="italic"><span class="koboSpan" id="kobo.1066.1">Q</span></em><span class="koboSpan" id="kobo.1067.1">), key (</span><em class="italic"><span class="koboSpan" id="kobo.1068.1">K</span></em><span class="koboSpan" id="kobo.1069.1">), and value (</span><em class="italic"><span class="koboSpan" id="kobo.1070.1">V</span></em><span class="koboSpan" id="kobo.1071.1">) matrices are split column-wise across multiple GPUs. </span><span class="koboSpan" id="kobo.1071.2">Each GPU then computes a partial attention score, following which the various results are aggregated across GPUs to reconstruct the finished output. </span><span class="koboSpan" id="kobo.1071.3">The advantage of this approach is that instead of storing entire weight matrices, each GPU stores only a portion. </span><span class="koboSpan" id="kobo.1071.4">Also, the multiplication of large </span><a id="_idIndexMarker1385"/><span class="koboSpan" id="kobo.1072.1">matrices can be distributed and thus make the computation faster, making it particularly efficient for large models. </span><span class="koboSpan" id="kobo.1072.2">On the other hand, there is always the risk of communication overhead (GPUs must frequently exchange partial results, which can slow down training), it can be complex to implement, and it is not worthwhile except for </span><span class="No-Break"><span class="koboSpan" id="kobo.1073.1">large models.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer344">
<span class="koboSpan" id="kobo.1074.1"><img alt="Figure 10.43 – Tensor parallelism (https://arxiv.org/pdf/2311.01635)" src="image/B21257_10_43.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1075.1">Figure 10.43 – Tensor parallelism (</span><a href="https://arxiv.org/pdf/2311.01635"><span class="koboSpan" id="kobo.1076.1">https://arxiv.org/pdf/2311.01635</span></a><span class="koboSpan" id="kobo.1077.1">)</span></p>
<p><span class="koboSpan" id="kobo.1078.1">The following table compares tensor parallelism, data parallelism, and pipeline parallelism across key dimensions such as memory usage, communication overhead, </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">and complexity:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1080.1">Feature</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1081.1">Tensor parallelism</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1082.1">Data parallelism</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1083.1">Pipeline parallelism</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1084.1">How </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1085.1">it works</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1086.1">Splits</span><a id="_idIndexMarker1386"/><span class="koboSpan" id="kobo.1087.1"> individual tensors </span><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">across GPUs</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1089.1">Replicates </span><a id="_idIndexMarker1387"/><span class="koboSpan" id="kobo.1090.1">full model on each device; </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">splits data</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1092.1">Splits </span><a id="_idIndexMarker1388"/><span class="koboSpan" id="kobo.1093.1">model layers </span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1">across GPUs</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1095.1">Memory usage</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1096.1">Low (weights </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">are sharded)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1098.1">High (full model stored on </span><span class="No-Break"><span class="koboSpan" id="kobo.1099.1">each GPU)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1100.1">Medium (</span><span class="No-Break"><span class="koboSpan" id="kobo.1101.1">layers distributed)</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1102.1">Communication </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1103.1">overhead</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1104.1">High (frequent </span><span class="No-Break"><span class="koboSpan" id="kobo.1105.1">cross-GPU communication)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1106.1">High (gradient </span><span class="No-Break"><span class="koboSpan" id="kobo.1107.1">synchronization)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1108.1">Moderate (</span><span class="No-Break"><span class="koboSpan" id="kobo.1109.1">micro-batch passing)</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1110.1">Best for</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1111.1">Very large models with huge </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">weight matrices</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1113.1">Medium-sized models with </span><span class="No-Break"><span class="koboSpan" id="kobo.1114.1">large datasets</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1115.1">Deep models such </span><span class="No-Break"><span class="koboSpan" id="kobo.1116.1">as transformers</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1117.1">Complexity</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">High</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1119.1">Low</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">Medium</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1121.1">Table 10.1 – Comparison of tensor, data, and pipeline parallelism in large-scale model training</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.1122.1">Hybrid parallelism</span></strong><span class="koboSpan" id="kobo.1123.1"> integrates </span><a id="_idIndexMarker1389"/><span class="koboSpan" id="kobo.1124.1">different types of parallelism trying to optimize training across multiple GPUs. </span><span class="koboSpan" id="kobo.1124.2">Generally, the various approaches can be combined, although this requires more complexity. </span><span class="koboSpan" id="kobo.1124.3">For example, data parallelism ensures that GPUs process different batches while model parallelism (tensor or pipeline parallelism) ensures that the model is optimized across multiple GPUs. </span><span class="koboSpan" id="kobo.1124.4">For example, if the model is too large for a single GPU, we can use model parallelism and split the model across multiple GPUs. </span><span class="koboSpan" id="kobo.1124.5">We can then use 16 GPUs to split a batch of data across 4 copies of </span><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.1126.1">So far, we have explored how to build a fully working AI-driven Streamlit app that integrates multiple agents and external APIs such as OpenAI. </span><span class="koboSpan" id="kobo.1126.2">However, when an application moves from development to production, some important challenges need to be taken </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">into account.</span></span></p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.1128.1">Handling errors in production</span></h2>
<p><span class="koboSpan" id="kobo.1129.1">In this section, we’ll explore</span><a id="_idIndexMarker1390"/><span class="koboSpan" id="kobo.1130.1"> some of the approaches we can adopt to handle issues that may occur when an application moves from development to production. </span><span class="koboSpan" id="kobo.1130.2">Typical problems you might </span><span class="No-Break"><span class="koboSpan" id="kobo.1131.1">encounter include:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1132.1">The OpenAI API is </span><span class="No-Break"><span class="koboSpan" id="kobo.1133.1">temporarily unavailable</span></span></li>
<li><span class="koboSpan" id="kobo.1134.1">Intermittent network failures or exceeding </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">rate limits</span></span></li>
<li><span class="koboSpan" id="kobo.1136.1">Incomplete or missing </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">logging system</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1138.1">Let’s see how we can mitigate these </span><span class="No-Break"><span class="koboSpan" id="kobo.1139.1">issues effectively:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1140.1">The OpenAI API is temporarily unavailable</span></strong><span class="koboSpan" id="kobo.1141.1">: One simple and effective way to start handling these issues is by wrapping your API calls in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1142.1">try</span></strong><span class="koboSpan" id="kobo.1143.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1144.1">except</span></strong><span class="koboSpan" id="kobo.1145.1"> blocks. </span><span class="koboSpan" id="kobo.1145.2">Here’s an example of how you can handle different types of errors when calling the </span><span class="No-Break"><span class="koboSpan" id="kobo.1146.1">OpenAI API:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1147.1">
try:
     response = client.chat.completions.create(
         model="gpt-4",
         messages=[...],
         timeout=10           # optional timeout
     )
     return response.choices[0].message.content
except openai.RateLimitError:
     st.error("Rate limit exceeded. </span><span class="koboSpan" id="kobo.1147.2">Please try again later.")
except openai.APIError as e:
     st.error(f"OpenAI API error: {str(e)}")
except Exception as e:
     st.error(f"Unexpected error: {str(e)}")</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1148.1">Temporary issues</span></strong><span class="koboSpan" id="kobo.1149.1">: When there are intermittent network failures or momentary unavailability of external APIs, instead of immediately failing, the app can retry the </span><a id="_idIndexMarker1391"/><span class="koboSpan" id="kobo.1150.1">operation a </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">few times:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1152.1">
import time
 import random
def call_openai_with_retry(prompt, retries=3):
     for i in range(retries):
         try:
             return client.chat.completions.create(
                 model="gpt-4",
                 messages=[ {"role": "user", "content": prompt}]
             )
        except openai.APIError:
             wait = 2 ** i + random.random()
             time.sleep(wait)
    st.error("Failed after multiple retries.")    return None</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1153.1">Logging system</span></strong><span class="koboSpan" id="kobo.1154.1">: Using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1155.1">st.write()</span></strong><span class="koboSpan" id="kobo.1156.1"> is fine for quick debugging, but in production, you need a more persistent and structured way to track what’s happening in </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">your</span></span><span class="No-Break"><a id="_idIndexMarker1392"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1"> app.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.1159.1">A basic logging system helps you record important events and catch errors that may not appear in </span><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">the UI:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1161.1">
import logging
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
try:
     logger.info("Calling OpenAI API")
     response = client.chat.completions.create(...)
except Exception as e:
     logger.exception("API call failed")
     st.error("Something went wrong.")</span></pre></li> </ul>
<h2 id="_idParaDest-197"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.1162.1">Security considerations for production</span></h2>
<p><span class="koboSpan" id="kobo.1163.1">Applications deployed in production</span><a id="_idIndexMarker1393"/><span class="koboSpan" id="kobo.1164.1"> often involve API keys and potentially sensitive user data, so security must be carefully addressed from </span><span class="No-Break"><span class="koboSpan" id="kobo.1165.1">the beginning.</span></span></p>
<p><span class="koboSpan" id="kobo.1166.1">One of the most fundamental practices is to avoid hardcoding credentials such as API keys directly into the source code. </span><span class="koboSpan" id="kobo.1166.2">Instead, credentials should be managed securely using environment variables or a dedicated secrets </span><span class="No-Break"><span class="koboSpan" id="kobo.1167.1">management system.</span></span></p>
<p><span class="koboSpan" id="kobo.1168.1">Security in production typically involves three </span><span class="No-Break"><span class="koboSpan" id="kobo.1169.1">key areas:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.1170.1">Managing secrets</span></span></li>
<li><span class="koboSpan" id="kobo.1171.1">Data </span><span class="No-Break"><span class="koboSpan" id="kobo.1172.1">exposure prevention</span></span></li>
<li><span class="koboSpan" id="kobo.1173.1">Securing your </span><span class="No-Break"><span class="koboSpan" id="kobo.1174.1">deployment environment</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1175.1">Let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.1176.1">these next.</span></span></p>
<h3><span class="koboSpan" id="kobo.1177.1">Managing secrets in production</span></h3>
<p><span class="koboSpan" id="kobo.1178.1">There are two </span><a id="_idIndexMarker1394"/><span class="koboSpan" id="kobo.1179.1">common ways to securely manage secrets in </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">production</span></span><span class="No-Break"><a id="_idIndexMarker1395"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1181.1"> environments:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1182.1">Using </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1183.1">st.secrets</span></strong><span class="koboSpan" id="kobo.1184.1">: This is </span><a id="_idIndexMarker1396"/><span class="koboSpan" id="kobo.1185.1">ideal for applications deployed on </span><span class="No-Break"><span class="koboSpan" id="kobo.1186.1">Streamlit Cloud</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1187.1">Using environment variables</span></strong><span class="koboSpan" id="kobo.1188.1">: This is recommended for Docker containers or local </span><span class="No-Break"><span class="koboSpan" id="kobo.1189.1">server deployments</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1190.1">Both approaches allow you to keep sensitive information out of your source code, but the right choice depends on your </span><span class="No-Break"><span class="koboSpan" id="kobo.1191.1">deployment context.</span></span></p>
<p><span class="koboSpan" id="kobo.1192.1">Here are some</span><a id="_idIndexMarker1397"/><span class="koboSpan" id="kobo.1193.1"> examples for </span><a id="_idIndexMarker1398"/><span class="No-Break"><span class="koboSpan" id="kobo.1194.1">each method:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1195.1">Using </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1196.1">st.secrets</span></strong><span class="koboSpan" id="kobo.1197.1">: When using </span><a id="_idIndexMarker1399"/><span class="koboSpan" id="kobo.1198.1">Streamlit, create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1199.1">.streamlit/secrets.toml</span></strong><span class="koboSpan" id="kobo.1200.1"> file that lets you define secrets into it. </span><span class="koboSpan" id="kobo.1200.2">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.1201.1">an example:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1202.1">
[general]
 openai_api_key = "application-api-key"</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1203.1">Access it in your code </span><span class="No-Break"><span class="koboSpan" id="kobo.1204.1">like this:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1205.1">import openai
 openai.api_key = st.secrets["general"]["openai_api_key"]</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1206.1">Using environment variables</span></strong><span class="koboSpan" id="kobo.1207.1">: For Dockerization or local deployments, it is recommended to store secrets as environment variables, keeping them separate from the source code. </span><span class="koboSpan" id="kobo.1207.2">To use environment variables, you must define them in your terminal or deployment environment before running </span><span class="No-Break"><span class="koboSpan" id="kobo.1208.1">your application.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.1209.1">For example, in a Unix-based terminal (Linux, macOS, or WSL), you can define the variable </span><span class="No-Break"><span class="koboSpan" id="kobo.1210.1">like this:</span></span></p><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.1211.1">export OPENAI_API_KEY="your-api-key"</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.1212.1">Then, in your Python code, access the variable </span><span class="No-Break"><span class="koboSpan" id="kobo.1213.1">as follows:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.1214.1">import os
openai.api_key = os.getenv("OPENAI_API_KEY")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1215.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1216.1">export</span></strong><span class="koboSpan" id="kobo.1217.1"> command sets an environment variable only for the current terminal session. </span><span class="koboSpan" id="kobo.1217.2">This means it will remain active only until you close the terminal. </span><span class="koboSpan" id="kobo.1217.3">To launch your app using the variable, you must run it in the same </span><span class="No-Break"><span class="koboSpan" id="kobo.1218.1">shell session:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.1219.1">export OPENAI_API_KEY="your-api-key"</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.1220.1"> streamlit run app.py</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.1221.1">To make the variable available every time you open a terminal, you can add it to your shell’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">startup file.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1223.1">On Linux, this file is usually </span><span class="No-Break"><span class="koboSpan" id="kobo.1224.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1225.1">~/.bash_profile</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1226.1">.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1227.1">These are called initialization scripts. </span><span class="koboSpan" id="kobo.1227.2">They are automatically executed every time you start a new terminal session and are used to configure the shell environment, including setting environment variables, aliases, </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">and paths.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.1229.1">To add the API key to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1230.1">~/.bash_profile</span></strong><span class="koboSpan" id="kobo.1231.1">, open the terminal and run </span><span class="No-Break"><span class="koboSpan" id="kobo.1232.1">the following:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.1233.1">nano ~/.bashrc</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.1234.1">Then, save</span><a id="_idIndexMarker1400"/><span class="koboSpan" id="kobo.1235.1"> and close it. </span><span class="koboSpan" id="kobo.1235.2">From now on, your app will automatically find</span><a id="_idIndexMarker1401"/><span class="koboSpan" id="kobo.1236.1"> the API key every time it is launched from a new </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">terminal session.</span></span></p></li> </ul>
<h3><span class="koboSpan" id="kobo.1238.1">Data exposure prevention</span></h3>
<p><span class="koboSpan" id="kobo.1239.1">In production, one of the most</span><a id="_idIndexMarker1402"/><span class="koboSpan" id="kobo.1240.1"> overlooked security risks is the unintentional exposure of sensitive data through logging, error messages, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1241.1">misconfigured URLs.</span></span></p>
<p><span class="koboSpan" id="kobo.1242.1">While logging is essential for debugging and observability, it can easily become a liability if secrets, tokens, or user data are captured without </span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">proper filtering.</span></span></p>
<p><span class="koboSpan" id="kobo.1244.1">Here are a few best practices to minimize </span><span class="No-Break"><span class="koboSpan" id="kobo.1245.1">the risk:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1246.1">Avoid logging secrets</span></strong><span class="koboSpan" id="kobo.1247.1">: Never print API keys, access tokens, or passwords to logs, even in debug mode. </span><span class="koboSpan" id="kobo.1247.2">This applies to both client-side and </span><span class="No-Break"><span class="koboSpan" id="kobo.1248.1">server-side logs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1249.1">Sanitize user data</span></strong><span class="koboSpan" id="kobo.1250.1">: If your application logs inputs or error traces that include user-provided data (e.g., form submissions, headers, and payloads), be sure to mask or strip sensitive fields (such as email addresses, credit card numbers, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1251.1">personal identifiers).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1252.1">Configure logging levels appropriately</span></strong><span class="koboSpan" id="kobo.1253.1">: Use different log levels (e.g., </span><strong class="source-inline"><span class="koboSpan" id="kobo.1254.1">INFO</span></strong><span class="koboSpan" id="kobo.1255.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1256.1">WARNING</span></strong><span class="koboSpan" id="kobo.1257.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1258.1">ERROR</span></strong><span class="koboSpan" id="kobo.1259.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.1260.1">DEBUG</span></strong><span class="koboSpan" id="kobo.1261.1">) and restrict debug-level logs in production. </span><span class="koboSpan" id="kobo.1261.2">Enable only what is necessary to diagnose issues without </span><span class="No-Break"><span class="koboSpan" id="kobo.1262.1">overexposing internals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1263.1">Handle errors</span></strong><span class="koboSpan" id="kobo.1264.1">: Avoid sending raw stack traces or system error messages directly to users. </span><span class="koboSpan" id="kobo.1264.2">These </span><a id="_idIndexMarker1403"/><span class="koboSpan" id="kobo.1265.1">can leak details about your backend, framework, </span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">or database.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1267.1">Preventing data exposure is about designing systems that assume that secrets and user data must always be protected, even in edge cases </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">or failures.</span></span></p>
<h3><span class="koboSpan" id="kobo.1269.1">Securing your deployment environment</span></h3>
<p><span class="koboSpan" id="kobo.1270.1">Even if your code avoids data exposure and </span><a id="_idIndexMarker1404"/><span class="koboSpan" id="kobo.1271.1">your secrets are properly managed, your application can still be vulnerable if the environment in which it runs </span><span class="No-Break"><span class="koboSpan" id="kobo.1272.1">is misconfigured.</span></span></p>
<p><span class="koboSpan" id="kobo.1273.1">For example, in modern workflows, containerization is one of the most common ways to package and deploy applications. </span><span class="koboSpan" id="kobo.1273.2">In fact, containers offer portability and consistency across environments, but they also introduce specific </span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">security risks.</span></span></p>
<p><span class="koboSpan" id="kobo.1275.1">A wrong or poor Dockerfile configuration can introduce multiple vulnerabilities, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1277.1">Increased exposure to known exploits if the image includes unnecessary packages </span><span class="No-Break"><span class="koboSpan" id="kobo.1278.1">or tools</span></span></li>
<li><span class="koboSpan" id="kobo.1279.1">Credential leaks if secrets are stored directly in </span><span class="No-Break"><span class="koboSpan" id="kobo.1280.1">the image</span></span></li>
<li><span class="koboSpan" id="kobo.1281.1">Privilege escalation if the container runs as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1282.1">root user</span></span></li>
<li><span class="koboSpan" id="kobo.1283.1">Unsafe access to host resources if volumes are not </span><span class="No-Break"><span class="koboSpan" id="kobo.1284.1">properly restricted</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1285.1">To mitigate these risks, it is important to follow a set of container security best practices. </span><span class="koboSpan" id="kobo.1285.2">Let’s look at a few simple guidelines to make your Docker-based deployment more secure </span><span class="No-Break"><span class="koboSpan" id="kobo.1286.1">and production-ready:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1287.1">Bloated images = more attack surface</span></strong><span class="koboSpan" id="kobo.1288.1">: Using a full </span><strong class="source-inline"><span class="koboSpan" id="kobo.1289.1">python:3.11</span></strong><span class="koboSpan" id="kobo.1290.1"> image instead of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1291.1">python:3.11-slim</span></strong><span class="koboSpan" id="kobo.1292.1"> can include dozens of unnecessary system tools. </span><span class="koboSpan" id="kobo.1292.2">If any of these have known vulnerabilities, they become an unintentional attack, even if your app doesn’t </span><span class="No-Break"><span class="koboSpan" id="kobo.1293.1">use them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1294.1">Secrets in the image = easy leaks</span></strong><span class="koboSpan" id="kobo.1295.1">: Hardcoding API keys or copying </span><strong class="source-inline"><span class="koboSpan" id="kobo.1296.1">.env</span></strong><span class="koboSpan" id="kobo.1297.1"> files into the Docker image allows anyone with access to the image to extract and </span><span class="No-Break"><span class="koboSpan" id="kobo.1298.1">appropriate them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1299.1">Running as </span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.1300.1">root</span></strong><strong class="bold"><span class="koboSpan" id="kobo.1301.1"> = dangerous escalation</span></strong><span class="koboSpan" id="kobo.1302.1">: If no user is specified, containers run as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1303.1">root</span></strong><span class="koboSpan" id="kobo.1304.1">, and combined with an exploit in a Python dependency, this could give an attacker full control of the container and possibly the </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">host too.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1306.1">Unsafe volume mounting = host access</span></strong><span class="koboSpan" id="kobo.1307.1">: Mounting critical paths gives containers access to critical host files including SSH keys and system configuration. </span><span class="koboSpan" id="kobo.1307.2">If the container is compromised, the host is compromised as well. </span><span class="koboSpan" id="kobo.1307.3">Examples of critical</span><a id="_idIndexMarker1405"/><span class="koboSpan" id="kobo.1308.1"> paths include </span><span class="No-Break"><span class="koboSpan" id="kobo.1309.1">the following:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.1310.1">/</span></strong><span class="koboSpan" id="kobo.1311.1">: Root of the host filesystem. </span><span class="koboSpan" id="kobo.1311.2">Grants full access to the entire filesystem of the host, including sensitive system directories, user data, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1">configuration files.</span></span></li><li> <strong class="source-inline"><span class="koboSpan" id="kobo.1313.1">/etc</span></strong><span class="koboSpan" id="kobo.1314.1">: System configuration directory. </span><span class="koboSpan" id="kobo.1314.2">Contains critical configuration files, including </span><strong class="source-inline"><span class="koboSpan" id="kobo.1315.1">/etc/passwd</span></strong><span class="koboSpan" id="kobo.1316.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1317.1">/etc/shadow</span></strong><span class="koboSpan" id="kobo.1318.1">, network settings, and user permissions. </span><span class="koboSpan" id="kobo.1318.2">Exposing this can allow manipulation of how the host </span><span class="No-Break"><span class="koboSpan" id="kobo.1319.1">system behaves.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.1320.1">/var/run/docker.sock</span></strong><span class="koboSpan" id="kobo.1321.1">: Docker daemon socket. </span><span class="koboSpan" id="kobo.1321.2">Gives the container direct control over the Docker engine running on the host. </span><span class="koboSpan" id="kobo.1321.3">This lets the container start, stop, and manage other containers, including mounting volumes and executing code on </span><span class="No-Break"><span class="koboSpan" id="kobo.1322.1">the host.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.1323.1">Here is an example of a minimal and </span><span class="No-Break"><span class="koboSpan" id="kobo.1324.1">secure Dockerfile:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1325.1">
# Minimal python image
 FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
 </span><span class="koboSpan" id="kobo.1325.2">RUN pip install --no-cache-dir -r requirements.txt
COPY . </span><span class="koboSpan" id="kobo.1325.3">.
</span><span class="koboSpan" id="kobo.1325.4"># Create and use a non-root user through the keyword USER
 RUN useradd -m appuser
 USER appuser
CMD ["streamlit", "run", "app.py"]</span></pre> <p><span class="koboSpan" id="kobo.1326.1">To inject secrets</span><a id="_idIndexMarker1406"/><span class="koboSpan" id="kobo.1327.1"> securely at runtime, use environment variables </span><a id="_idIndexMarker1407"/><span class="koboSpan" id="kobo.1328.1">passed with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1329.1">docker run</span></strong><span class="koboSpan" id="kobo.1330.1"> or use secret management tools such as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1331.1">Docker secrets</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1333.1">
# Runtime execution
 docker run -e OPENAI_API_KEY="your-api-key" my-streamlit-app</span></pre> <p><span class="koboSpan" id="kobo.1334.1">MLOPs and LLMOPs are important concepts for anyone who wants to use an ML model or LLM in production. </span><span class="koboSpan" id="kobo.1334.2">In the next section, we will discuss other important concepts in production deployment, such as asynchronous programming, which allows us to handle multiple concurr</span><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.1335.1">ent </span><span class="No-Break"><span class="koboSpan" id="kobo.1336.1">user requests.</span></span></p>
<h1 id="_idParaDest-198"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.1337.1">Asynchronous programming</span></h1>
<p><span class="koboSpan" id="kobo.1338.1">So far, you’ve seen examples where tasks are executed one after the other. </span><span class="koboSpan" id="kobo.1338.2">But what if some tasks don’t need to block the flow of the entire program while waiting? </span><span class="koboSpan" id="kobo.1338.3">That’s where asynchronous programming</span><a id="_idIndexMarker1408"/> <span class="No-Break"><span class="koboSpan" id="kobo.1339.1">comes in.</span></span></p>
<p><span class="koboSpan" id="kobo.1340.1">Asynchronous programming allows tasks to cooperatively share the CPU. </span><span class="koboSpan" id="kobo.1340.2">Instead of each task waiting for the previous one to finish, tasks can voluntarily pause and let others run, making better use of the single processor’s time. </span><span class="koboSpan" id="kobo.1340.3">This does not imply simultaneous execution; instead, it indicates a smart interleaving of their operations.; this is especially useful when waiting for things such as </span><span class="No-Break"><span class="koboSpan" id="kobo.1341.1">I/O operations.</span></span></p>
<p><span class="koboSpan" id="kobo.1342.1">Think of it as multiple conversations happening with one person switching between them, efficiently and politely. </span><span class="koboSpan" id="kobo.1342.2">In Python, this is achieved using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1343.1">asyncio</span></strong><span class="koboSpan" id="kobo.1344.1"> module, which supports cooperative multitasking on a </span><span class="No-Break"><span class="koboSpan" id="kobo.1345.1">single CPU.</span></span></p>
<p><span class="koboSpan" id="kobo.1346.1">As you’ll see in the </span><a id="_idIndexMarker1409"/><span class="koboSpan" id="kobo.1347.1">comparison table, asynchronous code is different from using threads or multiple processes. </span><span class="koboSpan" id="kobo.1347.2">It runs on just one core, but it can still feel fast, especially when dealing with many </span><span class="No-Break"><span class="koboSpan" id="kobo.1348.1">I/O-bound tasks.</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1349.1">Python module</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1350.1">Number </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1351.1">of CPUs</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1352.1">Task </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1353.1">switching style</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1354.1">Switching decision</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1355.1">asyncio</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1356.1">Single</span></span></p>
</td>
<td class="No-Table-Style">
<p><span lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1357.1">Cooperative </span></span><span lang="en-US" xml:lang="en-US"><a id="_idIndexMarker1410"/></span><span lang="en-US" xml:lang="en-US">
</span><span class="No-Break" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1358.1">multitaskin</span></span><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">g</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1360.1">Tasks yield control voluntarily through the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1361.1">await</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1362.1"> keyword</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1363.1">threading</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1364.1">Single</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1365.1">Preemptive </span><span class="No-Break"><span class="koboSpan" id="kobo.1366.1">multitasking</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1367.1">OS</span><a id="_idIndexMarker1411"/><span class="koboSpan" id="kobo.1368.1"> decides when to </span><span class="No-Break"><span class="koboSpan" id="kobo.1369.1">switch threads</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1370.1">multiprocessing</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">Multiple</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1372.1">Preemptive </span><a id="_idIndexMarker1412"/>
<span class="No-Break"><span class="koboSpan" id="kobo.1373.1">multitasking</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1374.1">Separate processes run independently, but on the same machine; the OS decides when </span><span class="No-Break"><span class="koboSpan" id="kobo.1375.1">to switch</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1376.1">Table 10.2 – Concurrency mechanisms in Python: differences between asyncio, threading, and multiprocessing</span></p>
<p><span class="koboSpan" id="kobo.1377.1">Concurrency</span><a id="_idIndexMarker1413"/><span class="koboSpan" id="kobo.1378.1"> is particularly useful in two types of scenarios: when a program is waiting for responses from external systems (I/O-bound), and when it is handling a high computational </span><span class="No-Break"><span class="koboSpan" id="kobo.1379.1">workload (CPU-bound).</span></span></p>
<p><span class="koboSpan" id="kobo.1380.1">In I/O-bound situations, a script spends most of its time waiting for data to arrive from a source, such as a filesystem, a network connection, a database, or an API. </span><span class="koboSpan" id="kobo.1380.2">During this time, the CPU is often idle, making it a perfect opportunity to run other </span><span class="No-Break"><span class="koboSpan" id="kobo.1381.1">tasks concurrently.</span></span></p>
<p><span class="koboSpan" id="kobo.1382.1">In contrast, CPU-bound tasks </span><a id="_idIndexMarker1414"/><span class="koboSpan" id="kobo.1383.1">keep the processor fully occupied with calculations such as rendering images, parsing large datasets, or performing cryptographic operations. </span><span class="koboSpan" id="kobo.1383.2">In these cases, concurrency helps by distributing the workload across multiple CPU cores, enabling true parallel execution. </span><span class="koboSpan" id="kobo.1383.3">This form of concurrency (better described as parallelism) can significantly reduce total processing time for </span><span class="No-Break"><span class="koboSpan" id="kobo.1384.1">heavy computations.</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1385.1">Type </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1386.1">of task</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1387.1">Main limitation</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1388.1">Examples</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1389.1">Concurrency benefit</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1390.1">Execution style</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1391.1">I/O-bound</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1392.1">Slow external </span><span class="No-Break"><span class="koboSpan" id="kobo.1393.1">systems</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1394.1">Reading files, API requests, </span><span class="No-Break"><span class="koboSpan" id="kobo.1395.1">database queries</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1396.1">Keeps CPU busy while waiting </span><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">for I/O</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1398.1">Cooperative (</span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1399.1">asyncio</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1400.1">)</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.1401.1">CPU-bound</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1402.1">Intensive </span><span class="No-Break"><span class="koboSpan" id="kobo.1403.1">computation</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1404.1">Data crunching, image processing, </span><span class="No-Break"><span class="koboSpan" id="kobo.1405.1">encryption</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1406.1">Distributes load across multiple cores for </span><span class="No-Break"><span class="koboSpan" id="kobo.1407.1">real parallelism</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1408.1">Preemptive (</span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1409.1">threading</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1410.1">, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1411.1">multiprocessing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1412.1">)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1413.1">Table 10.3 – I/O-bound vs. </span><span class="koboSpan" id="kobo.1413.2">CPU-bound: task types and optimal concurrency models</span></p>
<p><span class="koboSpan" id="kobo.1414.1">The following diagram illustrates how task execution differs between synchronous and asynchronous when dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.1415.1">I/O-bound operations.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer345">
<span class="koboSpan" id="kobo.1416.1"><img alt="Figure 10.44 – Comparison of blocking vs non-blocking I/O execution" src="image/B21257_10_44.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1417.1">Figure 10.44 – Comparison of blocking vs non-blocking I/O execution</span></p>
<p><span class="koboSpan" id="kobo.1418.1">In the first row, each request</span><a id="_idIndexMarker1415"/><span class="koboSpan" id="kobo.1419.1"> blocks the CPU until the I/O completes. </span><span class="koboSpan" id="kobo.1419.2">In the second row (async), the CPU switches between tasks during I/O wait times, improving efficiency on a single core and minimizing the </span><span class="No-Break"><span class="koboSpan" id="kobo.1420.1">idle time.</span></span></p>
<p><span class="koboSpan" id="kobo.1421.1">When multiple I/O-bound requests arrive in sequence, using a single thread to handle each of them one after the other would block the program during </span><span class="No-Break"><span class="koboSpan" id="kobo.1422.1">I/O waits.</span></span></p>
<p><span class="koboSpan" id="kobo.1423.1">To improve responsiveness, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1424.1">threading</span></strong><span class="koboSpan" id="kobo.1425.1"> module can be used to delegate each request to a </span><span class="No-Break"><span class="koboSpan" id="kobo.1426.1">separate thread.</span></span></p>
<p><span class="koboSpan" id="kobo.1427.1">In the following diagram, each</span><a id="_idIndexMarker1416"/><span class="koboSpan" id="kobo.1428.1"> incoming request is assigned to one of four worker threads. </span><span class="koboSpan" id="kobo.1428.2">The actual workload (T1, T2, T3, ...) represents short bursts of CPU activity interleaved with </span><span class="No-Break"><span class="koboSpan" id="kobo.1429.1">I/O waits:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer346">
<span class="koboSpan" id="kobo.1430.1"><img alt="Figure 10.45 – Concurrent request handling with four worker threads and interleaved CPU/I/O workloads" src="image/B21257_10_45.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1431.1">Figure 10.45 – Concurrent request handling with four worker threads and interleaved CPU/I/O workloads</span></p>
<p><span class="koboSpan" id="kobo.1432.1">This pattern is useful when your program must remain responsive while interacting with slow external systems such as APIs, databases, filesystems, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1433.1">even GUIs.</span></span></p>
<p><span class="koboSpan" id="kobo.1434.1">Asynchronous programming is a type of parallel programming that allows programs to perform tasks concurrently, without blocking the main execution thread. </span><span class="koboSpan" id="kobo.1434.2">For example, when we have multiple users interacting with the system at the same time, we will have more tasks to handle at the same time, with some tasks taking more time by blocking our agent. </span><span class="koboSpan" id="kobo.1434.3">In traditional synchronous programming, tasks are executed one after the other, where each task must wait for the previous one to finish before it can begin; tasks are executed sequentially in the order in which they are written. </span><span class="koboSpan" id="kobo.1434.4">Each task must complete fully before the next begins, which can lead to delays if a task involves waiting, such as for file I/O or network operations. </span><span class="koboSpan" id="kobo.1434.5">Asynchronous programming, on the other hand, allows tasks that may block execution to be initiated and handled concurrently. </span><span class="koboSpan" id="kobo.1434.6">Instead of waiting for a task to finish, the program can move on to other tasks, returning to the blocked task once it is ready. </span><span class="koboSpan" id="kobo.1434.7">This approach improves efficiency by making better use of system resources, particularly in scenarios involving high-latency operations, such as web requests or database </span><a id="_idIndexMarker1417"/><span class="koboSpan" id="kobo.1435.1">queries, enabling more responsive and </span><span class="No-Break"><span class="koboSpan" id="kobo.1436.1">performant applications.</span></span></p>
<p><span class="koboSpan" id="kobo.1437.1">There are some key concepts for discussing </span><span class="No-Break"><span class="koboSpan" id="kobo.1438.1">asynchronous programming:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1439.1">Concurrency</span></strong><span class="koboSpan" id="kobo.1440.1">: This</span><a id="_idIndexMarker1418"/><span class="koboSpan" id="kobo.1441.1"> refers to the ability to handle </span><a id="_idIndexMarker1419"/><span class="koboSpan" id="kobo.1442.1">different tasks at the same time; however, this does not mean that tasks are handled simultaneously. </span><span class="koboSpan" id="kobo.1442.2">Tasks are started and completed in overlapping time periods, but </span><span class="No-Break"><span class="koboSpan" id="kobo.1443.1">not simultaneously.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1444.1">Parallelism</span></strong><span class="koboSpan" id="kobo.1445.1">: This </span><a id="_idIndexMarker1420"/><span class="koboSpan" id="kobo.1446.1">refers to the ability to accomplish </span><a id="_idIndexMarker1421"/><span class="koboSpan" id="kobo.1447.1">tasks at exactly the same time, usually by using multiple processors or cores. </span><span class="koboSpan" id="kobo.1447.2">While concurrency may or may not involve parallelism, parallelism always </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">involves concurrency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1449.1">Blocking operations</span></strong><span class="koboSpan" id="kobo.1450.1">: These</span><a id="_idIndexMarker1422"/><span class="koboSpan" id="kobo.1451.1"> are operations </span><a id="_idIndexMarker1423"/><span class="koboSpan" id="kobo.1452.1">that wait for a task to complete before starting a new operation (e.g., reading a file from disk before starting to </span><span class="No-Break"><span class="koboSpan" id="kobo.1453.1">process text).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1454.1">Non-blocking operations</span></strong><span class="koboSpan" id="kobo.1455.1">: This</span><a id="_idIndexMarker1424"/><span class="koboSpan" id="kobo.1456.1"> refers to the </span><a id="_idIndexMarker1425"/><span class="koboSpan" id="kobo.1457.1">ability to start a task and continue the program with other tasks without waiting for the task to complete (making an HTTP request and continuing to generate more text with an LLM while waiting for </span><span class="No-Break"><span class="koboSpan" id="kobo.1458.1">the response).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1459.1">Callbacks</span></strong><span class="koboSpan" id="kobo.1460.1">: These are </span><a id="_idIndexMarker1426"/><span class="koboSpan" id="kobo.1461.1">functions passed as arguments to other </span><a id="_idIndexMarker1427"/><span class="koboSpan" id="kobo.1462.1">functions that are executed when a </span><span class="No-Break"><span class="koboSpan" id="kobo.1463.1">task completes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1464.1">Promises and futures</span></strong><span class="koboSpan" id="kobo.1465.1">: These are abstractions that represent the eventual result of an asynchronous operation. </span><span class="koboSpan" id="kobo.1465.2">A</span><a id="_idIndexMarker1428"/><span class="koboSpan" id="kobo.1466.1"> promise</span><a id="_idIndexMarker1429"/><span class="koboSpan" id="kobo.1467.1"> is a value (or result) that may be unavailable at that time but will be available at some later point. </span><span class="koboSpan" id="kobo.1467.2">A</span><a id="_idIndexMarker1430"/><span class="koboSpan" id="kobo.1468.1"> future </span><a id="_idIndexMarker1431"/><span class="koboSpan" id="kobo.1469.1">is the same thing but is commonly used in languages such as Python </span><span class="No-Break"><span class="koboSpan" id="kobo.1470.1">and Java.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1471.1">Event loop</span></strong><span class="koboSpan" id="kobo.1472.1">: This is </span><a id="_idIndexMarker1432"/><span class="koboSpan" id="kobo.1473.1">the fundamental component of an asynchronous program, where tasks, events, or signals are listed and scheduled for execution when the resources are available. </span><span class="koboSpan" id="kobo.1473.2">In other words, we use an event loop to allow tasks to run without blocking the main program. </span><span class="koboSpan" id="kobo.1473.3">The event loop</span><a id="_idIndexMarker1433"/><span class="koboSpan" id="kobo.1474.1"> waits for an event to occur and calls an appropriate callback function at </span><span class="No-Break"><span class="koboSpan" id="kobo.1475.1">this point.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1476.1">Coroutines</span></strong><span class="koboSpan" id="kobo.1477.1">: These are special functions that can be paused and then resumed during their </span><a id="_idIndexMarker1434"/><span class="koboSpan" id="kobo.1478.1">execution. </span><span class="koboSpan" id="kobo.1478.2">In other words, a function can start and then be paused to wait for the result of another task. </span><span class="koboSpan" id="kobo.1478.3">For example, when we </span><a id="_idIndexMarker1435"/><span class="koboSpan" id="kobo.1479.1">start an analysis of some documents, the function pauses while we conduct an HTTP request to find more information that is needed to accomplish our function. </span><span class="koboSpan" id="kobo.1479.2">When the results of the HTTP request arrive, the </span><span class="No-Break"><span class="koboSpan" id="kobo.1480.1">function resumes.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1481.1">It may seem counterintuitive how asynchronous programming makes code execution faster (after all,  no additional resources are being used). </span><span class="koboSpan" id="kobo.1481.2">I have made extensive changes here for conciseness and clarity. </span><span class="koboSpan" id="kobo.1481.3">Please confirm whether your intended meaning has been retained. </span><span class="koboSpan" id="kobo.1481.4">In the synchronous format, she completes each game one at a time before moving to the next. </span><span class="koboSpan" id="kobo.1481.5">With each move taking her 10 seconds and her opponent 60 seconds, a full game of 30 moves per player (60 moves total) takes 2,100 seconds. </span><span class="koboSpan" id="kobo.1481.6">Playing all 24 games sequentially requires 50,400 seconds, or roughly </span><span class="No-Break"><span class="koboSpan" id="kobo.1482.1">14 hours.</span></span></p>
<p><span class="koboSpan" id="kobo.1483.1">In contrast, the asynchronous format has Judit moving from board to board, making one move per game while each opponent thinks during her rotation. </span><span class="koboSpan" id="kobo.1483.2">One full round of 24 moves takes 240 seconds, and since each player takes 60 seconds to respond, Judit returns to each board just as the opponent is ready. </span><span class="koboSpan" id="kobo.1483.3">Over 30 rounds, the entire session lasts only 7,200 seconds, or approximately 2 hours—making asynchronous play significantly </span><span class="No-Break"><span class="koboSpan" id="kobo.1484.1">more time-efficient.</span></span></p>
<p><span class="koboSpan" id="kobo.1485.1">In async programming, we do exactly the same, the event loop allows us to manage the various tasks in an optimal time management manner. </span><span class="koboSpan" id="kobo.1485.2">A function that would block other tasks can be optimally blocked when we need to run other tasks, allowing optimized management of the entire program. </span><span class="koboSpan" id="kobo.1485.3">Here, we do not want to optimize the time of each game but the </span><span class="No-Break"><span class="koboSpan" id="kobo.1486.1">whole performance.</span></span></p>
<p><span class="koboSpan" id="kobo.1487.1">We can then manage multiple processes at the same time in </span><span class="No-Break"><span class="koboSpan" id="kobo.1488.1">different ways:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1489.1">Multiple processes</span></strong><span class="koboSpan" id="kobo.1490.1">: A process</span><a id="_idIndexMarker1436"/><span class="koboSpan" id="kobo.1491.1"> is an independent program in execution. </span><span class="koboSpan" id="kobo.1491.2">Each </span><a id="_idIndexMarker1437"/><span class="koboSpan" id="kobo.1492.1">process has its own memory, resources, and execution context. </span><span class="koboSpan" id="kobo.1492.2">In the simplest way, we can manage different processes at the same time (for example, several players playing the 24 games is a simple example of multiple processes occurring at the same time during performance). </span><span class="koboSpan" id="kobo.1492.3">In the case of programming, this means that different scripts or processes can run at the same time (e.g., four functions and each of them runs on a different CPU). </span><span class="koboSpan" id="kobo.1492.4">However, this approach is </span><span class="No-Break"><span class="koboSpan" id="kobo.1493.1">very inefficient.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1494.1">Multiple threads</span></strong><span class="koboSpan" id="kobo.1495.1">: This is a</span><a id="_idIndexMarker1438"/><span class="koboSpan" id="kobo.1496.1"> variation of the previous approach. </span><span class="koboSpan" id="kobo.1496.2">A thread is the smallest unit of execution within a process. </span><span class="koboSpan" id="kobo.1496.3">Multiple threads can be within the same process and share the same memory, but each thread has its own execution stack. </span><span class="koboSpan" id="kobo.1496.4">In this case, several threads are executed at the </span><span class="No-Break"><span class="koboSpan" id="kobo.1497.1">same time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1498.1">Asynchronous programming</span></strong><span class="koboSpan" id="kobo.1499.1">: In this</span><a id="_idIndexMarker1439"/><span class="koboSpan" id="kobo.1500.1"> case, we have a single process and a single thread but conduct several things at the same time. </span><span class="koboSpan" id="kobo.1500.2">In Python, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1501.1">asyncio</span></strong><span class="koboSpan" id="kobo.1502.1"> does exactly this by exploiting coroutines and futures to simplify </span><span class="No-Break"><span class="koboSpan" id="kobo.1503.1">asynchronous code.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1504.1">Asynchronous programming, therefore, improves performance when some tasks are time-consuming and can block the execution of a program. </span><span class="koboSpan" id="kobo.1504.2">In this way, the system can continue executing other tasks while it waits for them to complete. </span><span class="koboSpan" id="kobo.1504.3">It also allows better utilization of system resources (for example, while waiting for a network request, the program can perform calculations or handle other requests). </span><span class="koboSpan" id="kobo.1504.4">Asynchronous programming also helps to achieve systems that are more scalable and can handle multiple requests in parallel, reducing </span><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.1505.1">the number </span><span class="No-Break"><span class="koboSpan" id="kobo.1506.1">of threads.</span></span></p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.1507.1">asyncio</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1508.1">asyncio</span></strong><span class="koboSpan" id="kobo.1509.1"> is a </span><a id="_idIndexMarker1440"/><span class="koboSpan" id="kobo.1510.1">Python library that allows you to write concurrent code using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1511.1">async</span></strong><span class="koboSpan" id="kobo.1512.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1513.1">await</span></strong><span class="koboSpan" id="kobo.1514.1"> syntax. </span><span class="koboSpan" id="kobo.1514.2">It provides a framework for running asynchronous operations, without relying on multithreading or multiprocessing. </span><span class="koboSpan" id="kobo.1514.3">The heart of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1515.1">asyncio</span></strong><span class="koboSpan" id="kobo.1516.1"> is the event loop, which schedules and executes asynchronous tasks (called coroutines) in the background. </span><span class="koboSpan" id="kobo.1516.2">A coroutine is similar to a generator in Python: it can pause execution and let other tasks run and then resume later. </span><span class="koboSpan" id="kobo.1516.3">It is the event loop that tracks the state of these coroutines and their results,</span><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.1517.1"> which are presented </span><span class="No-Break"><span class="koboSpan" id="kobo.1518.1">as </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1519.1">futures</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1520.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1521.1">Here is a basic example of </span><span class="No-Break"><span class="koboSpan" id="kobo.1522.1">a coroutine:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1523.1">
import asyncio
async def my_coroutine():
    print("Hello, world!")
# Create an event loop and run the coroutine
asyncio.run(my_coroutine())</span></pre> <p><span class="koboSpan" id="kobo.1524.1">While this code shows how to define and run a coroutine by using the event loop, it does not yet take advantage of concurrent execution. </span><span class="koboSpan" id="kobo.1524.2">In fact, to execute multiple asynchronous tasks concurrently, we can use either </span><strong class="source-inline"><span class="koboSpan" id="kobo.1525.1">asyncio.gather()</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1526.1">or </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1527.1">async</span><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.1528.1">io.create_task()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1529.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1530.1">While </span><strong class="source-inline"><span class="koboSpan" id="kobo.1531.1">gather()</span></strong><span class="koboSpan" id="kobo.1532.1"> is useful when you want to run several coroutines and wait for all of them to finish together, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1533.1">create_task()</span></strong><span class="koboSpan" id="kobo.1534.1"> provides more flexibility. </span><span class="koboSpan" id="kobo.1534.2">It allows you to launch coroutines in the background and decide when (or whether) to await their results later in your program. </span><span class="koboSpan" id="kobo.1534.3">Let’s look at some </span><span class="No-Break"><span class="koboSpan" id="kobo.1535.1">examples together.</span></span></p>
<p><span class="koboSpan" id="kobo.1536.1">The following example uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.1537.1">asyncio.gather()</span></strong><span class="koboSpan" id="kobo.1538.1"> to execute multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.1539.1">coroutines concurrently:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1540.1">
async def task1():
    await asyncio.sleep(2)
    print("Task 1 completed!")
async def task2():
    await asyncio.sleep(1)
    print("Task 2 completed!")
async def main():
    await asyncio.gather(task1(), task2())  # Run both tasks concurrently
asyncio.run(main())</span></pre> <p><span class="koboSpan" id="kobo.1541.1">In this case, both tasks are executed concurrently, and the total execution time will be close to 2 seconds: the time taken by the </span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">longest task.</span></span></p>
<p><span class="koboSpan" id="kobo.1543.1">We can achieve the same result using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1544.1">asyncio.create_task()</span></strong><span class="koboSpan" id="kobo.1545.1">, which offers more control over task scheduling. </span><span class="koboSpan" id="kobo.1545.2">Unlike </span><strong class="source-inline"><span class="koboSpan" id="kobo.1546.1">asyncio.gather()</span></strong><span class="koboSpan" id="kobo.1547.1">, which groups coroutines and waits for all of them together, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1548.1">create_task()</span></strong><span class="koboSpan" id="kobo.1549.1"> lets us launch coroutines individually and decide </span><a id="_idIndexMarker1441"/><span class="koboSpan" id="kobo.1550.1">when to await their results. </span><span class="koboSpan" id="kobo.1550.2">This is particularly useful when we want to run background tasks while doing </span><span class="No-Break"><span class="koboSpan" id="kobo.1551.1">other work.</span></span></p>
<p><span class="koboSpan" id="kobo.1552.1">Here is the same example rewritten </span><span class="No-Break"><span class="koboSpan" id="kobo.1553.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1554.1">create_task()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1555.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1556.1">
import asyncio
async def task1():
    aw</span><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.1557.1">ait asyncio.sleep(2)
    print("Task 1 completed!")
async def task2():
    aw</span><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.1558.1">ait asyncio.sleep(1)
    print("Task 2 </span><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.1559.1">completed!")
async def main():
    t1 = asyncio.create_task(task1())
    t2 = asyncio.create_task(task2())
    # Both tasks start running in the background immediately
    await t1
    await t2
asyncio.run(main())</span></pre> <p><span class="koboSpan" id="kobo.1560.1">Each call to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1561.1">create_task()</span></strong><span class="koboSpan" id="kobo.1562.1"> returns a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1563.1">Task</span></strong><span class="koboSpan" id="kobo.1564.1"> object, which represents the running coroutine and can be awaited, cancelled, </span><span class="No-Break"><span class="koboSpan" id="kobo.1565.1">or monitored.</span></span></p>
<p><span class="koboSpan" id="kobo.1566.1">The result is the same: both tasks run concurrently, and the output is printed once each finish. </span><span class="koboSpan" id="kobo.1566.2">However, with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1567.1">create_task()</span></strong><span class="koboSpan" id="kobo.1568.1">, we gain </span><span class="No-Break"><span class="koboSpan" id="kobo.1569.1">more flexibility.</span></span></p>
<p><span class="koboSpan" id="kobo.1570.1">For example, we can</span><a id="_idIndexMarker1442"/><span class="koboSpan" id="kobo.1571.1"> start several background tasks and continue executing other logic in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1572.1">main()</span></strong><span class="koboSpan" id="kobo.1573.1">. </span><span class="koboSpan" id="kobo.1573.2">Then, we can await only the results we need at a specific point in the workflow. </span><span class="koboSpan" id="kobo.1573.3">This flexibility makes </span><strong class="source-inline"><span class="koboSpan" id="kobo.1574.1">create_task()</span></strong><span class="koboSpan" id="kobo.1575.1"> especially useful in complex workflows where not all tasks are equally important </span><span class="No-Break"><span class="koboSpan" id="kobo.1576.1">or time-sensitive.</span></span></p>
<p><span class="koboSpan" id="kobo.1577.1">To better understand the real-world impact of asynchronous programming, let’s compare an example of synchronous versus asynchronous execution. </span><span class="koboSpan" id="kobo.1577.2">Specifically, we will simulate fetching data from a website using HTTP requests by using Python </span><strong class="source-inline"><span class="koboSpan" id="kobo.1578.1">requests</span></strong><span class="koboSpan" id="kobo.1579.1"> library. </span><span class="koboSpan" id="kobo.1579.2">This will highlight how asynchronous code can significantly improve performance when dealing with I/O-bound tasks such as </span><span class="No-Break"><span class="koboSpan" id="kobo.1580.1">network calls.</span></span></p>
<p><span class="koboSpan" id="kobo.1581.1">Here is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1582.1">synchronous code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1583.1">
import requests
import time
def fetch_url(url):
    response = requests.get(url)
    return f"Fetched {url}"
def sync_fetch():
    urls = ['https://httpbin.org/get'] * 5  # Simulating 5 requests to the same URL
    results = [fetch_url(url) for url in urls]
    for result in results:
        print(result)
def main():
    start_time = time.time()
    sync_fetch()
    end_time = time.time()
    print(f"Synchronous version took {end_time - start_time:.4f} seconds")
# Run the synchronous example
main()</span></pre> <p><span class="koboSpan" id="kobo.1584.1">Here is the</span><a id="_idIndexMarker1443"/> <span class="No-Break"><span class="koboSpan" id="kobo.1585.1">asynchronous code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1586.1">
import asyncio
import aiohttp
import time
async def fetch_url(session, url):
    async with session.get(url) as response:
        await response.text()  # Simulate processing the response
        return f"Fetched {url}"
async def async_fetch():
    urls = ['https://httpbin.org/get'] * 5  # Simulating 5 requests to the same URL
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        for result in results:
            print(result)
async def main():
    start_time = time.time()
    await async_fetch()
    end_time = time.time()
    print(f"Asynchronous version took {end_time - start_time:.4f} seconds")
# Directly calling the asynchronous function in Jupyter
await main()</span></pre> <p><span class="koboSpan" id="kobo.1587.1">The following </span><a id="_idIndexMarker1444"/><span class="koboSpan" id="kobo.1588.1">figures show the output of the synchronous and asynchronous implementations described above, respectively. </span><span class="koboSpan" id="kobo.1588.2">As we can see, the synchronous version performs the HTTP requests one after the other, resulting in a longer total execution time. </span><span class="koboSpan" id="kobo.1588.3">The asynchronous version, on the other hand, sends all requests concurrently, significantly reducing the total </span><span class="No-Break"><span class="koboSpan" id="kobo.1589.1">time required.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer347">
<span class="koboSpan" id="kobo.1590.1"><img alt="Figure 10.46 – Synchronous result" src="image/B21257_10_46.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1591.1">Figure 10.46 – Synchronous result</span></p>
<div>
<div class="IMG---Figure" id="_idContainer348">
<span class="koboSpan" id="kobo.1592.1"><img alt="Figure 10.47 – Asynchronous result" src="image/B21257_10_47.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1593.1">Figure 10.47 – Asynchronous result</span></p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.1594.1">Asynchronous programming and ML</span></h2>
<p><span class="koboSpan" id="kobo.1595.1">Conjugating asynchronous programming</span><a id="_idIndexMarker1445"/><span class="koboSpan" id="kobo.1596.1"> with ML in Python can be a powerful combination. </span><span class="koboSpan" id="kobo.1596.2">Asynchronous programming can improve performance by allowing non-blocking operations, such as loading large datasets, running hyperparameter tuning, or interacting with APIs. </span><span class="koboSpan" id="kobo.1596.3">For example, we can see </span><span class="No-Break"><span class="koboSpan" id="kobo.1597.1">different possibilities:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1598.1">Data loading</span></strong><span class="koboSpan" id="kobo.1599.1">: In ML workflows, especially when working with large datasets, loading and preprocessing data can often be a bottleneck. </span><span class="koboSpan" id="kobo.1599.2">Asynchronous programming can help speed this up by loading different parts of the data concurrently. </span><span class="koboSpan" id="kobo.1599.3">For example, you can asynchronously load multiple chunks of a dataset while concurrently performing some I/O-bound tasks (such as data augmentation, cleaning, </span><span class="No-Break"><span class="koboSpan" id="kobo.1600.1">or transformation).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1601.1">Hyperparameter tuning</span></strong><span class="koboSpan" id="kobo.1602.1">: The tuning of hyperparameters is one of the most time-consuming and slowest processes, which can benefit from conducting some tasks asynchronously. </span><span class="koboSpan" id="kobo.1602.2">For example, when performing a grid or random search on hyperparameters, different configurations can be evaluated simultaneously rather </span><span class="No-Break"><span class="koboSpan" id="kobo.1603.1">than sequentially.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1604.1">Asynchronous inference</span></strong><span class="koboSpan" id="kobo.1605.1">: You can use asynchronous programming to create a non-blocking API to serve trained ML models. </span><span class="koboSpan" id="kobo.1605.2">This is especially useful when deploying a model for real-time inference and wanting to handle multiple </span><span class="No-Break"><span class="koboSpan" id="kobo.1606.1">queries simultaneously.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1607.1">Model training</span></strong><span class="koboSpan" id="kobo.1608.1">: Although training is usually conducted on different GPUs/CPUs in parallel, asynchronous scheduling can be conjugated to allow better loading and preprocessing of data while training appears in parallel. </span><span class="koboSpan" id="kobo.1608.2">This is particularly useful when we have different data </span><span class="No-Break"><span class="koboSpan" id="kobo.1609.1">to retrieve.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1610.1">We can observe a classic example of hyperparameter tuning. </span><span class="koboSpan" id="kobo.1610.2">In this simple example with the classic Iris dataset and</span><a id="_idIndexMarker1446"/><span class="koboSpan" id="kobo.1611.1"> a simple model, we’ll show how using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1612.1">asyncio</span></strong><span class="koboSpan" id="kobo.1613.1"> saves </span><span class="No-Break"><span class="koboSpan" id="kobo.1614.1">some time.</span></span></p>
<p><span class="koboSpan" id="kobo.1615.1">Here is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1616.1">synchronous code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1617.1">
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
def train_and_evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):
    # Load dataset
    data = load_iris()
    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)
        # Initialize and train the model
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf
    )
    model.fit(X_train, y_train)
     # Evaluate the model
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
        return (n_estimators, max_depth, min_samples_split, min_samples_leaf, accuracy)
def tune_hyperparameters():
    n_estimators_values = [10, 50, 100, 150, 200]  # Hyperparameter values to tune
    max_depth_values = [5, 10, None]
    min_samples_split_values = [2, 5]
    min_samples_leaf_values = [1, 2, 4]
    results = []
    for n_estimators in n_estimators_values:
        for max_depth in max_depth_values:
            for min_samples_split in min_samples_split_values:
                for min_samples_leaf in min_samples_leaf_values:
                    results.append(train_and_evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf))
        # Find the best hyperparameters and accuracy
    best_params = max(results, key=lambda x: x[4])
    print(f"Best hyperparameters: {best_params[:4]} with accuracy: {best_params[4]:.4f}")
# Measure time for synchronous execution
start_time = time.time()
tune_hyperparameters()
end_time = time.time()
print(f"Synchronous version took {end_time - start_time:.4f} seconds")</span></pre> <p><span class="koboSpan" id="kobo.1618.1">In the preceding script, we run </span><a id="_idIndexMarker1447"/><span class="koboSpan" id="kobo.1619.1">an ML model and search for the best parameters. </span><span class="koboSpan" id="kobo.1619.2">This script shows how even a small model takes a lot of time to </span><span class="No-Break"><span class="koboSpan" id="kobo.1620.1">be executed.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer349">
<span class="koboSpan" id="kobo.1621.1"><img alt="Figure 10.48 – Synchronous result" src="image/B21257_10_48.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1622.1">Figure 10.48 – Synchronous result</span></p>
<p><span class="koboSpan" id="kobo.1623.1">Here is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1624.1">asynchronous code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1625.1">
import asyncio
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
async def train_and_evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf):
    # Load dataset
    data = load_iris()
    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)
    # Initialize and train the model
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf
    )
    model.fit(X_train, y_train)
    # Evaluate the model
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    return (n_estimators, max_depth, min_samples_split, min_samples_leaf, accuracy)
async def tune_hyperparameters():
    n_estimators_values = [10, 50, 100, 150, 200]  # Hyperparameter values to tune
    max_depth_values = [5, 10, None]
    min_samples_split_values = [2, 5]
    min_samples_leaf_values = [1, 2, 4]
    tasks = []
    for n_estimators in n_estimators_values:
        for max_depth in max_depth_values:
            for min_samples_split in min_samples_split_values:
                for min_samples_leaf in min_samples_leaf_values:
                    tasks.append(train_and_evaluate_model(n_estimators, max_depth, min_samples_split, min_samples_leaf))
        results = await asyncio.gather(*tasks)
        # Find the best hyperparameters and accuracy
    best_params = max(results, key=lambda x: x[4])
    print(f"Best hyperparameters: {best_params[:4]} with accuracy: {best_params[4]:.4f}")
# Measure time for asynchronous execution
start_time = time.time()
await tune_hyperparameters()
end_time = time.time()
print(f"Asynchronous version took {end_time - start_time:.4f} seconds")</span></pre> <p><span class="koboSpan" id="kobo.1626.1">In this case, we trained the same model using asynchronous programming. </span><span class="koboSpan" id="kobo.1626.2">This approach allowed us to save time and thus reduce the </span><span class="No-Break"><span class="koboSpan" id="kobo.1627.1">execution time.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer350">
<span class="koboSpan" id="kobo.1628.1"><img alt="Figure 10.49 – Asynchronous result" src="image/B21257_10_49.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1629.1">Figure 10.49 – Asynchronous result</span></p>
<p><span class="koboSpan" id="kobo.1630.1">This can also be applied to an</span><a id="_idIndexMarker1448"/><span class="koboSpan" id="kobo.1631.1"> LLM as an agent. </span><span class="koboSpan" id="kobo.1631.2">Traditionally, function calls block LLM inference, making the process inefficient as each function call must complete before moving to the next. </span><span class="koboSpan" id="kobo.1631.3">Some authors propose instead to implement an async approach even with LLMs (or generate tokens and execute function calls concurrently) when tools are connected like in agents. </span><span class="koboSpan" id="kobo.1631.4">For example, one can consider interruptible LLM decoding, where the function executor notifies the LLM asynchronously, allowing it to continue generating tokens while waiting for function call results. </span><span class="koboSpan" id="kobo.1631.5">The purpose of this approach is to reduce latency by conducting an overlap of function </span><a id="_idIndexMarker1449"/><span class="koboSpan" id="kobo.1632.1">execution and </span><span class="No-Break"><span class="koboSpan" id="kobo.1633.1">token generation.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer351">
<span class="koboSpan" id="kobo.1634.1"><img alt="Figure 10.50 – Synchronous vs. asynchronous function calling (https://arxiv.org/pdf/2412.07017)" src="image/B21257_10_50.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1635.1">Figure 10.50 – Synchronous vs. </span><span class="koboSpan" id="kobo.1635.2">asynchronous function calling (</span><a href="https://arxiv.org/pdf/2412.07017"><span class="koboSpan" id="kobo.1636.1">https://arxiv.org/pdf/2412.07017</span></a><span class="koboSpan" id="kobo.1637.1">)</span></p>
<p><span class="koboSpan" id="kobo.1638.1">So, in theory, we can have three approaches for an </span><span class="No-Break"><span class="koboSpan" id="kobo.1639.1">LLM agent:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1640.1">Synchronous LLM function calling</span></strong><span class="koboSpan" id="kobo.1641.1">: Each function is executed one after the other. </span><span class="koboSpan" id="kobo.1641.2">An LLM must wait for each function to complete before it can continue with the next one. </span><span class="koboSpan" id="kobo.1641.3">This approach is the simplest, but it adds latency to the system since it must wait for each operation to finish (e.g., reading HTML, reading XLS files, generating tokens, etc.) before it can continue. </span><span class="koboSpan" id="kobo.1641.4">This leads to high inefficiency, especially if there are many functions or some functions lose a lot </span><span class="No-Break"><span class="koboSpan" id="kobo.1642.1">of time.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1643.1">Synchronous LLM function calling with parallel optimization</span></strong><span class="koboSpan" id="kobo.1644.1">: This process tries to optimize each task in parallel (e.g., reading HTML, reading XLS, and reading text simultaneously), but each task still blocks the next one. </span><span class="koboSpan" id="kobo.1644.2">The advantage over the previous approach is that each function can be conducted concurrently, with an increase in speed over the previous one. </span><span class="koboSpan" id="kobo.1644.3">Synchronization is required to conduct the tasks in the right order. </span><span class="koboSpan" id="kobo.1644.4">Although the tasks are optimized, they are still synchronous, so we have to wait for a function to finish before completing </span><span class="No-Break"><span class="koboSpan" id="kobo.1645.1">some tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1646.1">Asynchronous LLM function calling</span></strong><span class="koboSpan" id="kobo.1647.1">: In this approach, tasks are executed asynchronously, meaning that functions do not block one another. </span><span class="koboSpan" id="kobo.1647.2">The system can read HTML, read XLS, and read text while simultaneously performing other operations (such as summarizing or saving data). </span><span class="koboSpan" id="kobo.1647.3">This leads to a noticeable improvement in latency, improving the use of resources. </span><span class="koboSpan" id="kobo.1647.4">The system ensures that dependent tasks (e.g., summarizing and saving PDFs) are only performed once the necessary data (e.g., reading text) is available. </span><span class="koboSpan" id="kobo.1647.5">Dependencies are managed dynamically without halting other operations. </span><span class="koboSpan" id="kobo.1647.6">Multiprocessing parallelization (the previous approach) creates different processes or threads in order to handle tasks concurrently, thus allocating resources and memory. </span><span class="koboSpan" id="kobo.1647.7">This leads to more resource</span><a id="_idIndexMarker1450"/><span class="koboSpan" id="kobo.1648.1"> consumption than in an asynchronous version, and consumption can explode depending on how many functions we have. </span><span class="koboSpan" id="kobo.1648.2">Also, this approach is </span><span class="No-Break"><span class="koboSpan" id="kobo.1649.1">more scalable.</span></span><div class="IMG---Figure" id="_idContainer352"><span class="koboSpan" id="kobo.1650.1"><img alt="Figure 10.51 – Comparison of LLM-executor interactions (https:﻿//arxiv.org/pdf/2412.07017)" src="image/B21257_10_51.jpg"/></span></div></li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1651.1">Figure 10.51 – Comparison of LLM-executor interactions (</span><a href="https://arxiv.org/pdf/2412.07017"><span class="koboSpan" id="kobo.1652.1">https:</span><span id="_idTextAnchor209"/><span class="koboSpan" id="kobo.1653.1">//arxiv.org/pdf/2412.07017</span></a><span class="koboSpan" id="kobo.1654.1">)</span></p>
<p><span class="koboSpan" id="kobo.1655.1">Once we have made our system (our application) efficient, it should be placed in isolation to avoid external problems. </span><span class="koboSpan" id="kobo.1655.2">In the next section, we will explain in detail exactly how Docker allows us to </span><span class="No-Break"><span class="koboSpan" id="kobo.1656.1">do this.</span></span></p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.1657.1">Docker</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.1658.1">Docker</span></strong><span class="koboSpan" id="kobo.1659.1"> is </span><a id="_idIndexMarker1451"/><span class="koboSpan" id="kobo.1660.1">an open source platform that enables developers and system administrators to create, deploy, and run applications in containers. </span><span class="koboSpan" id="kobo.1660.2">Containers allow software to be packaged along with all its dependencies (such as libraries, configurations, etc.) and run consistently across different environments, whether it’s a developer’s laptop, a test server, or a </span><span class="No-Break"><span class="koboSpan" id="kobo.1661.1">production machine.</span></span></p>
<p><span class="koboSpan" id="kobo.1662.1">Containers</span><a id="_idIndexMarker1452"/><span class="koboSpan" id="kobo.1663.1"> can then be viewed as virtual machines, allowing for reduced overhead and better utilization of resources and the system itself (especially if we have to use a single model on several systems). </span><span class="koboSpan" id="kobo.1663.2">The idea is that our software (of which our model or an LLM plus agents is a component) can run in isolation to prevent problems from arising that impact its execution and performance. </span><span class="koboSpan" id="kobo.1663.3">The use of virtual machines is an example of how a system can run in a guest </span><strong class="bold"><span class="koboSpan" id="kobo.1664.1">operating system</span></strong><span class="koboSpan" id="kobo.1665.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1666.1">OS</span></strong><span class="koboSpan" id="kobo.1667.1">) and use resources. </span><span class="koboSpan" id="kobo.1667.2">Optimizing a system for a guest OS, however, requires considerable resources. </span><span class="koboSpan" id="kobo.1667.3">Containers try to reduce resource consumption and overhead in order to run the application. </span><span class="koboSpan" id="kobo.1667.4">Containers offer a way to package an application to make it abstract from the environment in which it runs. </span><span class="koboSpan" id="kobo.1667.5">This decoupling then allows a container to run in any target environment, predictably and isolated from other applications. </span><span class="koboSpan" id="kobo.1667.6">At the same time, the container provides the ability to control our environment in a granular manner. </span><span class="koboSpan" id="kobo.1667.7">Docker containers are lightweight and portable and ensure that the application behaves the same way everywhere. </span><span class="koboSpan" id="kobo.1667.8">Given these benefits, Docker containers have been adopted by </span><span class="No-Break"><span class="koboSpan" id="kobo.1668.1">many companies.</span></span></p>
<p><span class="koboSpan" id="kobo.1669.1">Docker is based on a few </span><span class="No-Break"><span class="koboSpan" id="kobo.1670.1">main concepts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1671.1">Containers</span></strong><span class="koboSpan" id="kobo.1672.1">: These are the </span><a id="_idIndexMarker1453"/><span class="koboSpan" id="kobo.1673.1">basic units of Docker and contain an application and its dependencies in a single package that can be easily moved between environments. </span><span class="koboSpan" id="kobo.1673.2">A container also contains the OS kernels, to reduce the resources needed. </span><span class="koboSpan" id="kobo.1673.3">Unlike a virtual machine that contains the entire OS, Docker containers contain only the information needed to run the application. </span><span class="koboSpan" id="kobo.1673.4">This makes Docker containers much faster and more efficient </span><span class="No-Break"><span class="koboSpan" id="kobo.1674.1">to run.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1675.1">Images</span></strong><span class="koboSpan" id="kobo.1676.1">: An image</span><a id="_idIndexMarker1454"/><span class="koboSpan" id="kobo.1677.1"> is a read-only template used to create containers. </span><span class="koboSpan" id="kobo.1677.2">It contains the application code, runtime, libraries, and environment variables. </span><span class="koboSpan" id="kobo.1677.3">Docker images contain the blueprint of the application – all the information to be able to execute a code. </span><span class="koboSpan" id="kobo.1677.4">There are many ready-made images in Docker Hub that can be used to efficiently create containers and reduce the need to start </span><span class="No-Break"><span class="koboSpan" id="kobo.1678.1">from scratch.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1679.1">Docker Engine</span></strong><span class="koboSpan" id="kobo.1680.1">: This is</span><a id="_idIndexMarker1455"/><span class="koboSpan" id="kobo.1681.1"> the component responsible for managing and running containers (runtime environment for Docker). </span><span class="koboSpan" id="kobo.1681.2">Docker Engine runs on both Linux and </span><span class="No-Break"><span class="koboSpan" id="kobo.1682.1">Windows OSs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1683.1">Dockerfile</span></strong><span class="koboSpan" id="kobo.1684.1">: A Dockerfile</span><a id="_idIndexMarker1456"/><span class="koboSpan" id="kobo.1685.1"> is a script containing instructions on how to build a Docker image. </span><span class="koboSpan" id="kobo.1685.2">This file specifies which base image to use, how to install dependencies, environment configurations, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1686.1">other details.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1687.1">Docker Compose</span></strong><span class="koboSpan" id="kobo.1688.1">: This </span><a id="_idIndexMarker1457"/><span class="koboSpan" id="kobo.1689.1">is a tool for defining and running multi-container </span><span class="No-Break"><span class="koboSpan" id="kobo.1690.1">Docker applications.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1691.1">Docker containers thus</span><a id="_idIndexMarker1458"/><span class="koboSpan" id="kobo.1692.1"> have a number </span><span class="No-Break"><span class="koboSpan" id="kobo.1693.1">of advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1694.1">Portability</span></strong><span class="koboSpan" id="kobo.1695.1">: Docker containers encapsulate an application and its dependencies in a single, portable unit. </span><span class="koboSpan" id="kobo.1695.2">In this way, the system abstracts away differences between environments, making it more reliable and consistent </span><span class="No-Break"><span class="koboSpan" id="kobo.1696.1">in deployment.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1697.1">Efficiency</span></strong><span class="koboSpan" id="kobo.1698.1">: The system is more efficient compared to traditional virtual machines. </span><span class="koboSpan" id="kobo.1698.2">By using only the kernel, the system uses far fewer resources, thus making it easier to deploy and more scalable. </span><span class="koboSpan" id="kobo.1698.3">Docker integrates well with other orchestration tools, such as Kubernetes and Docker Swarm, making it easier to scale an application both horizontally (more containers) and vertically (increasing the resources available </span><span class="No-Break"><span class="koboSpan" id="kobo.1699.1">to containers).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1700.1">Isolation</span></strong><span class="koboSpan" id="kobo.1701.1">: Docker provides strong isolation between containers, allowing them to run independently and not interfere with each other, thus improving security and avoiding conflicts between </span><span class="No-Break"><span class="koboSpan" id="kobo.1702.1">different applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1703.1">Version control and reproducibility</span></strong><span class="koboSpan" id="kobo.1704.1">: A container allows you to store, share, and deploy specific versions of an application, ensuring that a single version is used in different environments, thus </span><span class="No-Break"><span class="koboSpan" id="kobo.1705.1">improving reproducibility.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1706.1">Like any system, there are </span><span class="No-Break"><span class="koboSpan" id="kobo.1707.1">also </span></span><span class="No-Break"><a id="_idIndexMarker1459"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1708.1">disadvantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1709.1">Security concerns</span></strong><span class="koboSpan" id="kobo.1710.1">: It can introduce some vulnerabilities, especially if you’re not shrewd and handy with Docker. </span><span class="koboSpan" id="kobo.1710.2">It has a certain learning curve, especially if you want to use the </span><span class="No-Break"><span class="koboSpan" id="kobo.1711.1">system efficiently.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1712.1">Data management</span></strong><span class="koboSpan" id="kobo.1713.1">: Containers are ephemeral by design, meaning that any data inside a container will be lost if the container is destroyed. </span><span class="koboSpan" id="kobo.1713.2">Although there are solutions to this problem, it requires more complexity than </span><span class="No-Break"><span class="koboSpan" id="kobo.1714.1">traditional systems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1715.1">Complexity</span></strong><span class="koboSpan" id="kobo.1716.1">: Docker makes deploying and managing individual containers easy; scaling and orchestrating large numbers of containers across many nodes can become complex. </span><span class="koboSpan" id="kobo.1716.2">Docker’s networking model, while flexible, can be difficult to set up and manage, particularly when containers are spread across multiple hosts. </span><span class="koboSpan" id="kobo.1716.3">In addition, complexities increase if there are several containers and associated tools. </span><span class="koboSpan" id="kobo.1716.4">Also, the OS kernel is limited, making debugging and implementing</span><a id="_idIndexMarker1460"/><span class="koboSpan" id="kobo.1717.1"> certain features </span><span class="No-Break"><span class="koboSpan" id="kobo.1718.1">more complex.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1719.1">While Docker containers offer many advantages such as portability, efficiency, isolation, and scalability, they also come with challenges, especially related to security, complexity, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1720.1">data management.</span></span></p>
<p><span class="koboSpan" id="kobo.1721.1">Sometimes, our system can be particularly complex and have more than one container; in the following subsection, we will discuss Kubernetes, which allows us to orchestrate </span><span class="No-Break"><span class="koboSpan" id="kobo.1722.1">multiple containers.</span></span></p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.1723.1">Kubernetes</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1724.1">Kubernetes</span></strong><span class="koboSpan" id="kobo.1725.1"> is an </span><a id="_idIndexMarker1461"/><span class="koboSpan" id="kobo.1726.1">open source container orchestration platform that automates the deployment, scaling, management, and operation of containerized applications. </span><span class="koboSpan" id="kobo.1726.2">It manages and orchestrates containers in </span><span class="No-Break"><span class="koboSpan" id="kobo.1727.1">production environments.</span></span></p>
<p><span class="koboSpan" id="kobo.1728.1">In Kubernetes, a Pod</span><a id="_idIndexMarker1462"/><span class="koboSpan" id="kobo.1729.1"> is a group of one or more containers that are tied together and share resources such as networks and storage. </span><span class="koboSpan" id="kobo.1729.2">The containers in a Pod are always deployed together and share the same environment. </span><span class="koboSpan" id="kobo.1729.3">A Service</span><a id="_idIndexMarker1463"/><span class="koboSpan" id="kobo.1730.1"> is an abstraction that defines a logical set of Pods and a policy to access them. </span><span class="koboSpan" id="kobo.1730.2">Services allow us to manage how our Pods are connected internally or are open to the outside world (during production deployment). </span><span class="koboSpan" id="kobo.1730.3">A node, on the other hand, is a physical or virtual machine that runs containers in the Kubernetes cluster. </span><span class="koboSpan" id="kobo.1730.4">Each node in the cluster runs at least</span><a id="_idIndexMarker1464"/><span class="koboSpan" id="kobo.1731.1"> one kubelet (the agent that runs containers) and a kube-proxy (networking proxy for managing communication between containers). </span><span class="koboSpan" id="kobo.1731.2">A group of nodes is called a cluster, and </span><a id="_idIndexMarker1465"/><span class="koboSpan" id="kobo.1732.1">clusters are the backbone of a Kubernetes environment that provides resources such as CPU, memory, and storage to applications. </span><span class="koboSpan" id="kobo.1732.2">Kubernetes facilitates the deployment and maintenance of containers, allowing easier scaling and production of applications. </span><span class="koboSpan" id="kobo.1732.3">It also allows us to better manage sensitive data configuration and data management </span><span class="No-Break"><span class="koboSpan" id="kobo.1733.1">in general.</span></span></p>
<p><span class="koboSpan" id="kobo.1734.1">Kubernetes</span><a id="_idIndexMarker1466"/><span class="koboSpan" id="kobo.1735.1"> is widely used to deploy, manage, and scale microservices-based applications. </span><span class="koboSpan" id="kobo.1735.2">It is also a popular choice in DevOps practices due to its ability to automate deployments and </span><span class="No-Break"><span class="koboSpan" id="kobo.1736.1">scale applications.</span></span></p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.1737.1">Docker with ML</span></h2>
<p><span class="koboSpan" id="kobo.1738.1">Over the years, Docker</span><a id="_idIndexMarker1467"/><span class="koboSpan" id="kobo.1739.1"> has been extensively used with ML models, both for running models and for ML-based creations. </span><span class="koboSpan" id="kobo.1739.2">It allows you to set up a workspace that is ready to code, where all the dependencies needed are managed so that the process of using a model is expedited. </span><span class="koboSpan" id="kobo.1739.3">Docker also allows for improved reproducibility of models, both for training </span><span class="No-Break"><span class="koboSpan" id="kobo.1740.1">and inference.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer353">
<span class="koboSpan" id="kobo.1741.1"><img alt="Figure 10.52 – Overview of the purposes of using Docker for ML-based software projects (https://arxiv.org/pdf/2206.00699)" src="image/B21257_10_52.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1742.1">Figure 10.52 – Overview of the purposes of using Docker for ML-based software projects (</span><a href="https://arxiv.org/pdf/2206.00699"><span class="koboSpan" id="kobo.1743.1">https://arxiv.org/pdf/2206.00699</span></a><span class="koboSpan" id="kobo.1744.1">)</span></p>
<p><span class="koboSpan" id="kobo.1745.1">Docker can be </span><a id="_idIndexMarker1468"/><span class="koboSpan" id="kobo.1746.1">used with any ML application, including using LLMs and agents. </span><span class="koboSpan" id="kobo.1746.2">For example, Ollama has its own Docker image available on Docker Hub, thus making it easy to create applications with LLMs and be able to directly deploy them to a server. </span><span class="koboSpan" id="kobo.1746.3">Our application can also contain RAG or </span><span class="No-Break"><span class="koboSpan" id="kobo.1747.1">other components.</span></span></p>
<p><span class="koboSpan" id="kobo.1748.1">Also, as Docker containers are now used in various applications and LLMs are used to generate code, an LLM can be used to address the challenges of environment configuration in software development, particularly when using Docker for containerization. </span><span class="koboSpan" id="kobo.1748.2">In fact, many software repositories require specific dependencies to function properly, and setting up the environment correctly is error-prone, time-consuming, and difficult for users. </span><span class="koboSpan" id="kobo.1748.3">Docker allows the process to be more robust and reproducible, but Dockerfiles must be configured manually and can be complex when a project has many dependencies or when the configuration involves multiple steps that need to be executed in a specific order. </span><span class="koboSpan" id="kobo.1748.4">Therefore, it was proposed to use an LLM to act as an intelligent agent that understands the dependencies and requirements of a repository and can generate a fully automated configuration that works in a Docker container. </span><strong class="bold"><span class="koboSpan" id="kobo.1749.1">Repo2Run</span></strong><span class="koboSpan" id="kobo.1750.1"> is an</span><a id="_idIndexMarker1469"/><span class="koboSpan" id="kobo.1751.1"> approach that leverages an LLM as an agent to control the process and ensure that the environment is properly configured before </span><span class="No-Break"><span class="koboSpan" id="kobo.1752.1">being deployed.</span></span></p>
<p><span class="koboSpan" id="kobo.1753.1">Repo2Run automatically</span><a id="_idIndexMarker1470"/><span class="koboSpan" id="kobo.1754.1"> generates Dockerfiles, which are used to configure Docker containers. </span><span class="koboSpan" id="kobo.1754.2">Dockerfiles contain a set of instructions for setting up a Docker container environment, including installing dependencies and setting up necessary configurations. </span><span class="koboSpan" id="kobo.1754.3">The system inspects a given Python repository, detects its dependencies (e.g., from files such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1755.1">requirements.txt</span></strong><span class="koboSpan" id="kobo.1756.1"> or Pipfile), and then formulates a Dockerfile to recreate the necessary environment. </span><span class="koboSpan" id="kobo.1756.2">The core innovation in Repo2Run lies in its use of LLMs to drive the configuration process. </span><span class="koboSpan" id="kobo.1756.3">The LLM intelligently understands the structure of the repository and its dependencies, reducing the need for manual intervention. </span><span class="koboSpan" id="kobo.1756.4">It automates steps that are traditionally tedious and prone to errors, such as dependency resolution and </span><span class="No-Break"><span class="koboSpan" id="kobo.1757.1">configuration setup.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer354">
<span class="koboSpan" id="kobo.1758.1"><img alt="Figure 10.53 – Example process of Repo2Run (https://www.arxiv.org/pdf/2502.13681)" src="image/B21257_10_53.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1759.1">Figure 10.53 – Example process of Repo2Run (</span><a href="https://www.arxiv.org/pdf/2502.13681"><span class="koboSpan" id="kobo.1760.1">https://www.arxiv.org/pdf/2502.13681</span></a><span class="koboSpan" id="kobo.1761.1">)</span></p>
<p><span class="koboSpan" id="kobo.1762.1">Moving Docker </span><a id="_idIndexMarker1471"/><span class="koboSpan" id="kobo.1763.1">containers to Kubernetes requires a set of configuration files that describe how applications run within Kubernetes clusters (Kubernetes manifests). </span><span class="koboSpan" id="kobo.1763.2">This migration can be complex, especially for large applications that contain several containers and services. </span><span class="koboSpan" id="kobo.1763.3">Conducting this process can be error-prone, time-consuming, and difficult to manage, especially for teams without in-depth Kubernetes expertise. </span><span class="koboSpan" id="kobo.1763.4">Therefore, some works (such as Ueno, 2024) propose to use an LLM to assist in this process and generate </span><span class="No-Break"><span class="koboSpan" id="kobo.1764.1">the manifest.</span></span></p>
<p><span class="koboSpan" id="kobo.1765.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.1766.1">LLMSecConfig</span></strong><span class="koboSpan" id="kobo.1767.1"> framework</span><a id="_idIndexMarker1472"/><span class="koboSpan" id="kobo.1768.1"> aims to address a critical problem in the security of containerized applications and </span><strong class="bold"><span class="koboSpan" id="kobo.1769.1">container orchestrators</span></strong><span class="koboSpan" id="kobo.1770.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1771.1">COs</span></strong><span class="koboSpan" id="kobo.1772.1">) such as</span><a id="_idIndexMarker1473"/><span class="koboSpan" id="kobo.1773.1"> Kubernetes. </span><span class="koboSpan" id="kobo.1773.2">CO tools are used to manage the deployment, scaling, and networking of containerized applications. </span><span class="koboSpan" id="kobo.1773.3">However, due to their complexity, many possible misconfigurations can expose security vulnerabilities. </span><span class="koboSpan" id="kobo.1773.4">For instance, misconfigured access controls, improper resource limitations, or insecure network policies can leave applications open </span><span class="No-Break"><span class="koboSpan" id="kobo.1774.1">to attacks.</span></span></p>
<p><span class="koboSpan" id="kobo.1775.1">These misconfigurations are common because the process requires a high level of expertise and is manual. </span><strong class="bold"><span class="koboSpan" id="kobo.1776.1">Static analysis tools</span></strong><span class="koboSpan" id="kobo.1777.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1778.1">SATs</span></strong><span class="koboSpan" id="kobo.1779.1">) are </span><a id="_idIndexMarker1474"/><span class="koboSpan" id="kobo.1780.1">used to detect misconfigurations by analyzing the configuration files of containerized applications, such as Kubernetes YAML files or Dockerfiles. </span><span class="koboSpan" id="kobo.1780.2">Although SATs are a good solution for detecting vulnerabilities, they lack automation and require manual effort. </span><span class="koboSpan" id="kobo.1780.3">LLMSecConfig </span><a id="_idIndexMarker1475"/><span class="koboSpan" id="kobo.1781.1">proposes to use RAG and LLMs to find relevant information from external sources to identify misconfigurations. </span><span class="koboSpan" id="kobo.1781.2">The goal then is to make the process automated, in which vulnerabilities are identified and fixed at the same time while maintaining </span><span class="No-Break"><span class="koboSpan" id="kobo.1782.1">operational containers.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer355">
<span class="koboSpan" id="kobo.1783.1"><img alt="Figure 10.54 – Architecture overview of the LLMSecConfig framework for automated Kubernetes security configuration (https://arxiv.org/pdf/2502.02009)" src="image/B21257_10_54.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1784.1">Figure 10.54 – Architecture overview of the LLMSecConfig framework for automated Kubernetes security configuration (</span><a href="https://arxiv.org/pdf/2502.02009"><span class="koboSpan" id="kobo.1785.1">https://arxiv.org/pdf/2502.02009</span></a><span class="koboSpan" id="kobo.1786.1">)</span></p>
<p><span class="koboSpan" id="kobo.1787.1">These approaches show that not only can Docker be used for LLM applications but also, conversely, LLMs can be used to enhance the use of containers, especially when the application goes </span><span class="No-Break"><span class="koboSpan" id="kobo.1788.1">into production.</span></span></p>
<h1 id="_idParaDest-204"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.1789.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1790.1">This chapter focused on an important aspect of how we plan a multi-agent system. </span><span class="koboSpan" id="kobo.1790.2">Whatever form our system takes, it must eventually go into production and be used by users. </span><span class="koboSpan" id="kobo.1790.3">The experience for users is pivotal to whatever project we have in mind. </span><span class="koboSpan" id="kobo.1790.4">That is why we started by using Streamlit, a framework that allows us to experiment quickly and get an initial proof of concept. </span><span class="koboSpan" id="kobo.1790.5">Being able to get a prototype of our system allows us to understand both strengths and weaknesses before investing large resources in scaling. </span><span class="koboSpan" id="kobo.1790.6">The advantage of Streamlit is that it allows us to analyze both the backend and the frontend, enabling us to interact with an application as if we were one of the users. </span><span class="koboSpan" id="kobo.1790.7">Streamlit allows us to test what a complete product may look like before we conduct scaling and </span><span class="No-Break"><span class="koboSpan" id="kobo.1791.1">system optimization.</span></span></p>
<p><span class="koboSpan" id="kobo.1792.1">Obviously, an application will then have to pass this prototype stage to enter production. </span><span class="koboSpan" id="kobo.1792.2">This step requires that we conduct scaling of our application. </span><span class="koboSpan" id="kobo.1792.3">LLMs are complex products that need a lot of resources, so during the second half of the chapter, we dealt with all those operations that enable the training and what happens afterward. </span><span class="koboSpan" id="kobo.1792.4">Although we kept a main focus on LLMs, everything we saw can be useful for any </span><span class="No-Break"><span class="koboSpan" id="kobo.1793.1">ML application.</span></span></p>
<p><span class="koboSpan" id="kobo.1794.1">In the next and final chapter of the book, we will discuss the perspectives of a field that is constantly evolving. </span><span class="koboSpan" id="kobo.1794.2">We will discuss some important open questions and some of the future opportunities and developments that the exciting field of agents holds </span><span class="No-Break"><span class="koboSpan" id="kobo.1795.1">for us.</span></span></p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.1796.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.1797.1">Hewage, </span><em class="italic"><span class="koboSpan" id="kobo.1798.1">Machine Learning Operations: A Survey on MLOps Tool Support</span></em><span class="koboSpan" id="kobo.1799.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1800.1">2022,</span></span><span class="No-Break"> </span><a href="https://arxiv.org/abs/2202.10169"><span class="No-Break"><span class="koboSpan" id="kobo.1801.1">https://arxiv.org/abs/2202.10169</span></span></a></li>
<li><span class="koboSpan" id="kobo.1802.1">Park, </span><em class="italic"><span class="koboSpan" id="kobo.1803.1">LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs</span></em><span class="koboSpan" id="kobo.1804.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1805.1">2024,</span></span><span class="No-Break"> </span><a href="https://arxiv.org/abs/2408.13467"><span class="No-Break"><span class="koboSpan" id="kobo.1806.1">https://arxiv.org/abs/2408.13467</span></span></a></li>
<li><span class="koboSpan" id="kobo.1807.1">Zhao, </span><em class="italic"><span class="koboSpan" id="kobo.1808.1">A Survey of Large Language Models</span></em><span class="koboSpan" id="kobo.1809.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1810.1">2023, </span></span><a href="https://arxiv.org/abs/2303.18223"><span class="No-Break"><span class="koboSpan" id="kobo.1811.1">https://arxiv.org/abs/2303.18223</span></span></a></li>
<li><span class="koboSpan" id="kobo.1812.1">Chang, </span><em class="italic"><span class="koboSpan" id="kobo.1813.1">A Survey on Evaluation of Large Language Models</span></em><span class="koboSpan" id="kobo.1814.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1815.1">2023, </span></span><a href="https://arxiv.org/abs/2307.03109"><span class="No-Break"><span class="koboSpan" id="kobo.1816.1">https://arxiv.org/abs/2307.03109</span></span></a></li>
<li><span class="koboSpan" id="kobo.1817.1">IBM, </span><em class="italic"><span class="koboSpan" id="kobo.1818.1">LLM evaluation: Why </span></em><em class="italic"><span class="koboSpan" id="kobo.1819.1">Testing AI Models </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1820.1">Matters</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1821.1">, </span></span><a href="https://www.ibm.com/think/insights/llm-evaluation"><span class="No-Break"><span class="koboSpan" id="kobo.1822.1">https://www.ibm.com/think/insights/llm-evaluation</span></span></a></li>
<li><span class="koboSpan" id="kobo.1823.1">Guo, </span><em class="italic"><span class="koboSpan" id="kobo.1824.1">Evaluating Large Language Models: A Comprehensive Survey</span></em><span class="koboSpan" id="kobo.1825.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1826.1">2023, </span></span><a href="https://arxiv.org/abs/2310.19736"><span class="No-Break"><span class="koboSpan" id="kobo.1827.1">https://arxiv.org/abs/2310.19736</span></span></a></li>
<li><span class="koboSpan" id="kobo.1828.1">Shi, </span><em class="italic"><span class="koboSpan" id="kobo.1829.1">Keep the Cost Down: A Review on Methods to Optimize LLM’s KV-Cache Consumption</span></em><span class="koboSpan" id="kobo.1830.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1831.1">2024, </span></span><a href="https://arxiv.org/abs/2407.18003"><span class="No-Break"><span class="koboSpan" id="kobo.1832.1">https://arxiv.org/abs/2407.18003</span></span></a></li>
<li><span class="koboSpan" id="kobo.1833.1">Li, </span><em class="italic"><span class="koboSpan" id="kobo.1834.1">A Survey on Large Language Model Acceleration based on KV Cache Management</span></em><span class="koboSpan" id="kobo.1835.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1836.1">2024, </span></span><a href="https://arxiv.org/abs/2412.19442"><span class="No-Break"><span class="koboSpan" id="kobo.1837.1">https://arxiv.org/abs/2412.19442</span></span></a></li>
<li><span class="koboSpan" id="kobo.1838.1">Zhou, </span><em class="italic"><span class="koboSpan" id="kobo.1839.1">A Survey on Efficient Inference for Large Language Models</span></em><span class="koboSpan" id="kobo.1840.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1841.1">2024, </span></span><a href="https://arxiv.org/abs/2404.14294"><span class="No-Break"><span class="koboSpan" id="kobo.1842.1">https://arxiv.org/abs/2404.14294</span></span></a></li>
<li><span class="koboSpan" id="kobo.1843.1">Leviathan, </span><em class="italic"><span class="koboSpan" id="kobo.1844.1">Looking </span></em><em class="italic"><span class="koboSpan" id="kobo.1845.1">Back at Speculative Decoding,</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.1846.1">2024, </span></span><a href="https://research.google/blog/looking-back-at-speculative-decoding/"><span class="No-Break"><span class="koboSpan" id="kobo.1847.1">https://research.google/blog/looking-back-at-speculative-decoding/</span></span></a></li>
<li><span class="koboSpan" id="kobo.1848.1">Determined AI, </span><em class="italic"><span class="koboSpan" id="kobo.1849.1">Tensor Parallelism in Three Levels of </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1850.1">Difficulty</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1851.1">, </span></span><a href="https://www.determined.ai/blog/tp"><span class="No-Break"><span class="koboSpan" id="kobo.1852.1">https://www.determined.ai/blog/tp</span></span></a></li>
<li><span class="koboSpan" id="kobo.1853.1">Geeksforgeeks, </span><em class="italic"><span class="koboSpan" id="kobo.1854.1">asyncio in </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1855.1">Python</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1856.1">, </span></span><a href="https://www.geeksforgeeks.org/asyncio-in-python/"><span class="No-Break"><span class="koboSpan" id="kobo.1857.1">https://www.geeksforgeeks.org/asyncio-in-python/</span></span></a></li>
<li><span class="koboSpan" id="kobo.1858.1">Gim, </span><em class="italic"><span class="koboSpan" id="kobo.1859.1">Asynchronous LLM Function Calling</span></em><span class="koboSpan" id="kobo.1860.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1861.1">2024, </span></span><a href="https://arxiv.org/abs/2412.07017"><span class="No-Break"><span class="koboSpan" id="kobo.1862.1">https://arxiv.org/abs/2412.07017</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1863.1">Asynchronous </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1864.1">Computation</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1865.1">, </span></span><a href="https://d2l.ai/chapter_computational-performance/async-computation.html"><span class="No-Break"><span class="koboSpan" id="kobo.1866.1">https://d2l.ai/chapter_computational-performance/async-computation.html</span></span></a></li>
<li><span class="koboSpan" id="kobo.1867.1">Openja, </span><em class="italic"><span class="koboSpan" id="kobo.1868.1">Studying the Practices of Deploying Machine Learning Projects on Docker</span></em><span class="koboSpan" id="kobo.1869.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1870.1">2022, </span></span><a href="https://arxiv.org/abs/2206.00699"><span class="No-Break"><span class="koboSpan" id="kobo.1871.1">https://arxiv.org/abs/2206.00699</span></span></a></li>
<li><span class="koboSpan" id="kobo.1872.1">Muzumdar, </span><em class="italic"><span class="koboSpan" id="kobo.1873.1">Navigating the Docker Ecosystem: A Comprehensive Taxonomy and Survey</span></em><span class="koboSpan" id="kobo.1874.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1875.1">2024, </span></span><a href="https://arxiv.org/abs/2403.17940"><span class="No-Break"><span class="koboSpan" id="kobo.1876.1">https://arxiv.org/abs/2403.17940</span></span></a></li>
<li><span class="koboSpan" id="kobo.1877.1">Saha, </span><em class="italic"><span class="koboSpan" id="kobo.1878.1">Evaluation of Docker Containers for Scientific Workloads in the Cloud</span></em><span class="koboSpan" id="kobo.1879.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1880.1">2019, </span></span><a href="https://arxiv.org/abs/1905.08415"><span class="No-Break"><span class="koboSpan" id="kobo.1881.1">https://arxiv.org/abs/1905.08415</span></span></a></li>
<li><span class="koboSpan" id="kobo.1882.1">Ru, </span><em class="italic"><span class="koboSpan" id="kobo.1883.1">An LLM-based Agent for Reliable Docker Environment Configuration</span></em><span class="koboSpan" id="kobo.1884.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1885.1">2025, </span></span><a href="https://www.arxiv.org/abs/2502.13681"><span class="No-Break"><span class="koboSpan" id="kobo.1886.1">https://www.arxiv.org/abs/2502.13681</span></span></a></li>
<li><span class="koboSpan" id="kobo.1887.1">Ueno, </span><em class="italic"><span class="koboSpan" id="kobo.1888.1">Migrating Existing Container Workload to Kubernetes -- LLM Based Approach and Evaluation</span></em><span class="koboSpan" id="kobo.1889.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1890.1">2024, </span></span><a href="https://arxiv.org/abs/2408.11428v1"><span class="No-Break"><span class="koboSpan" id="kobo.1891.1">https://arxiv.org/abs/2408.11428v1</span></span></a></li>
<li><span class="koboSpan" id="kobo.1892.1">Ye, </span><em class="italic"><span class="koboSpan" id="kobo.1893.1">LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations</span></em><span class="koboSpan" id="kobo.1894.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1895.1">2025, </span></span><a href="https://arxiv.org/abs/2502.02009"><span class="No-Break"><span class="koboSpan" id="kobo.1896.1">https://arxiv.org/abs/2502.02009</span></span></a></li>
<li><span class="koboSpan" id="kobo.1897.1">Docker, </span><em class="italic"><span class="koboSpan" id="kobo.1898.1">LLM Everywhere: Docker for Local and Hugging Face </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1899.1">Hosting</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1900.1">, </span></span><a href="https://www.docker.com/blog/llm-docker-for-local-and-hugging-face-hosting/"><span class="No-Break"><span class="koboSpan" id="kobo.1901.1">https://www.docker.com/blog/llm-docker-for-local-and-hugging-face-hosting/</span></span></a></li>
</ul>
</div>
</body></html>