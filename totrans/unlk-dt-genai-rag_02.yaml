- en: <st c="0">2</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Code Lab – An Entire RAG Pipeline</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="35">This code lab lays the foundation for the rest of the code in this
    book.</st> <st c="109">We will spend this entire chapter giving you an entire</st>
    **<st c="164">retrieval-augmented generation</st>** <st c="194">(</st>**<st c="196">RAG</st>**<st
    c="199">) pipeline.</st> <st c="212">Then, as we step through the book, we will
    look at different parts of the code, adding enhancements along the way so that
    you have a comprehensive understanding of how your code can evolve to tackle more
    and more</st> <st c="425">difficult problems.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="444">We will spend this chapter walking through each component of the
    RAG pipeline, including the</st> <st c="538">following aspects:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="556">No interface</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="569">Setting up a large language model (LLM) account</st> <st c="618">with
    OpenAI</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="629">Installing the required</st> <st c="654">Python packages</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="669">Indexing data by web crawling, splitting documents, and embedding</st>
    <st c="736">the chunks</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="746">Retrieving relevant documents using vector</st> <st c="790">similarity
    search</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="807">Generating responses by integrating retrieved context into</st>
    <st c="867">LLM prompts</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="878">As we step through the code, you will gain a comprehensive understanding
    of each step in the RAG process programmatically by using tools such as LangChain,
    Chroma DB, and OpenAI’s APIs.</st> <st c="1065">This will provide you with a strong
    foundation that we will build upon in subsequent chapters, enhancing and evolving
    the code to tackle increasingly</st> <st c="1215">complex problems.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1232">In later chapters, we will explore techniques that can help improve
    and customize the pipeline for different use cases and overcome common challenges
    that arise when building RAG-powered applications.</st> <st c="1434">Let’s dive
    in and</st> <st c="1452">start building!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1467">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1490">The code for this chapter is available</st> <st c="1530">here:</st>
    [<st c="1536">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_02</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_02
    )
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1633">You will need to run this chapter’s code in an environment that’s
    been set up to run Jupyter notebooks.</st> <st c="1738">Experience with Jupyter
    notebooks is a prerequisite for using this book, and it is too difficult to cover
    it in a short amount of text.</st> <st c="1874">There are numerous ways to set
    up a notebook environment.</st> <st c="1932">There are online versions, versions
    you can download, notebook environments that universities provide students, and
    different interfaces you can use.</st> <st c="2082">If you are doing this at a
    company, they will likely have an environment you will want to get familiar with.</st>
    <st c="2191">Each of these options takes very different instructions to set up,
    and those instructions change often.</st> <st c="2295">If you need to brush up
    on your knowledge about this type of environment, you can start on the Jupyter
    website:</st> [<st c="2407">https://docs.jupyter.org/en/latest/</st>](https://docs.jupyter.org/en/latest/)<st
    c="2442">. Start here, then ask your favorite LLM for more help to get your environment</st>
    <st c="2521">set up.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2528">What do I use?</st> <st c="2544">When I use my Chromebook, often
    when I am traveling, I use a notebook set up in one of the cloud environments.</st>
    <st c="2655">I prefer Google Colab or their Colab Enterprise notebooks, which
    you can find in the Vertex AI section of Google Cloud Platform.</st> <st c="2784">But
    these environments cost money, often exceeding $20 a month if you are active.</st>
    <st c="2866">If you are as active as me, it can exceed $1,000</st> <st c="2915">per
    month!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2925">As a cost-effective alternative for when I am that active, I use
    Docker Desktop on my Mac, which hosts a Kubernetes cluster locally, and set up
    my notebook environment in the cluster.</st> <st c="3110">All these approaches
    have several environmental requirements that are often changing.</st> <st c="3196">It
    is best to do a little research and figure out what works best for your situation.</st>
    <st c="3282">There are similar solutions for</st> <st c="3314">Windows-based computers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3338">Ultimately, the primary requirement is to find an environment in
    which you can run a Jupyter notebook using Python 3\.</st> <st c="3457">The code
    we will provide will indicate what other packages you will need</st> <st c="3530">to
    install.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3541">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3546">All of this code assumes you are working in a Jupyter notebook.</st>
    <st c="3611">You could do this directly in a Python file (</st>`<st c="3656">.py</st>`<st
    c="3659">), but you may have to change some of it.</st> <st c="3702">Running this
    in a notebook gives you the ability to step through it cell by cell and see what
    happens at each point to better understand the</st> <st c="3843">entire process.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3858">No interface!</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3872">In the following coding example, we are not going to work with
    interfaces; we will cover that in</st> [*<st c="3970">Chapter 6</st>*](B22475_06.xhtml#_idTextAnchor114)<st
    c="3979">. In the meantime, we will simply create a string variable that represents
    the prompt users would enter and use that as a fill-in for a full-fledged</st>
    <st c="4128">interface input.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4144">Setting up a large language model (LLM) account</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="4192">For the</st> <st c="4200">general public, OpenAI’s ChatGPT models</st>
    <st c="4240">are currently the most popular and well-known LLMs.</st> <st c="4293">However,
    there are many other LLMs available in the market that fit a myriad of purposes.</st>
    <st c="4383">You do not always need to use the most expensive, most powerful LLM.</st>
    <st c="4452">Some LLMs focus on one area, such as the Meditron LLMs, which are
    medical research-focused fine-tuned versions of Llama 2\.</st> <st c="4575">If
    you are in the medical area, you may want to use that LLM instead as it may do
    better than a big general LLM in your domain.</st> <st c="4703">Often, LLMs can
    be used to double-check other LLMs, so you have to have more than one in those
    cases.</st> <st c="4805">I strongly encourage you to not just use the first LLM
    you have worked with and to look for the LLM that best suits your needs.</st>
    <st c="4933">But to keep things simpler this early in this book, I am going to
    talk about setting up</st> <st c="5021">OpenAI’s ChatGPT:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5038">Go to the</st> **<st c="5049">API</st>** <st c="5052">section of
    the OpenAI</st> <st c="5075">website:</st> [<st c="5084">https://openai.com/api/</st>](https://openai.com/api/)<st
    c="5107">.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="5108">If you have not set up an account yet, do so now.</st> <st c="5159">The
    web page can change often, but look for where to</st> <st c="5212">sign up.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="5220">Warning</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5228">Using OpenAI’s API costs money!</st> <st c="5261">Use</st> <st
    c="5265">it sparingly!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5278">Once you’ve signed up, go to the documentation at</st> [<st c="5329">https://platform.openai.com/docs/quickstart</st>](https://platform.openai.com/docs/quickstart)
    <st c="5372">and follow the instructions to set up your first</st> <st c="5422">API
    key.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="5430">When creating an API key, give it a memorable name and select the
    type of permissions you want to implement (</st>**<st c="5540">All</st>**<st c="5544">,</st>
    **<st c="5546">Restricted</st>**<st c="5556">, or</st> **<st c="5561">Read Only</st>**<st
    c="5570">).</st> <st c="5574">If you do not know what option to select, it is
    best to go with</st> **<st c="5638">All</st>** <st c="5641">for now.</st> <st
    c="5651">However, be aware of the other options – you may want to share various
    responsibilities with other team members but restrict certain types</st> <st c="5790">of
    access:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**<st c="5800">All</st>**<st c="5804">: This key will have read/write access
    to all of the</st> <st c="5858">OpenAI APIs.</st>'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**<st c="5870">Restricted</st>**<st c="5881">: A list of available APIs will
    appear, providing you with granular control over which APIs the key has access
    to.</st> <st c="5997">You have the option of giving just read or write access
    to each API.</st> <st c="6066">Make sure you have at least enabled the models
    and embedding APIs you will use in</st> <st c="6148">these demos.</st>'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**<st c="6160">Read Only</st>**<st c="6170">: This option gives you read-only
    access to</st> <st c="6215">all APIs.</st>'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="6224">Copy the key provided.</st> <st c="6248">You will add this to your
    code shortly.</st> <st c="6288">In the meantime, keep in mind that if this key
    is shared with anyone else, whomever you provide this key can use it and you will
    be charged.</st> <st c="6429">So, this is a key that you want to consider top
    secret and take the proper precautions to prevent unauthorized use</st> <st c="6544">of
    it.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="6550">The OpenAI API requires you to buy credits in advance to use the
    API.</st> <st c="6621">Buy what you are comfortable with, and then for more safety,
    make sure the</st> **<st c="6696">Enable auto recharge</st>** <st c="6716">option
    is off.</st> <st c="6732">This</st> <st c="6737">will ensure you</st> <st c="6753">are
    only spending what you intend</st> <st c="6787">to spend.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '<st c="6796">With that, you have set up the key component that will serve as
    the</st> *<st c="6865">brains</st>* <st c="6871">in your RAG pipeline: the LLM!</st>
    <st c="6903">Next, we will set up your development environment so that you can
    connect to</st> <st c="6980">the LLM.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6988">Installing the necessary packages</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="7022">Make sure these packages are installed in your Python environment.</st>
    <st c="7090">Add the following lines of code in the first cell of</st> <st c="7143">your
    notebook:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="7355">The preceding code installs several Python libraries using the</st>
    `<st c="7419">pip</st>` <st c="7422">package manager, something you will need
    to run the code I am providing.</st> <st c="7496">Here’s a breakdown of</st> <st
    c="7518">each library:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="7531">langchain_community</st>`<st c="7551">: This is a</st> <st c="7563">community-driven
    package for the LangChain library, which is an open source framework for building
    applications with LLMs.</st> <st c="7687">It provides a set of tools and components
    for working with LLMs and integrating them into</st> <st c="7777">various applications.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="7798">langchain_experimental</st>`<st c="7821">: The</st> `<st c="7828">langchain_experimental</st>`
    <st c="7850">library</st> <st c="7858">offers additional capabilities and tools
    beyond the core LangChain library that are not yet fully stable or production-ready
    but are still available for experimentation</st> <st c="8028">and exploration.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="8044">langchain-openai</st>`<st c="8061">: This</st> <st c="8069">package
    provides integration between LangChain and OpenAI’s language models.</st> <st
    c="8146">It allows you to easily incorporate OpenAI’s models, such as ChatGPT
    4 or the OpenAI embeddings service, into your</st> <st c="8261">LangChain applications.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="8284">langchainhub</st>`<st c="8297">: This</st> <st c="8305">package
    provides a collection of pre-built components and templates for LangChain applications.</st>
    <st c="8401">It includes various agents, memory components, and utility functions
    that can be used to accelerate the development of</st> <st c="8520">LangChain-based
    applications.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="8549">chromadb</st>`<st c="8558">: This is the</st> <st c="8573">package
    name for Chroma DB, a high-performance embedding/vector database designed for
    efficient similarity search</st> <st c="8687">and retrieval.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="8701">langchain</st>`<st c="8711">: This is the</st> <st c="8725">core
    LangChain library itself.</st> <st c="8757">It provides a framework and a set
    of abstractions for building applications with LLMs.</st> <st c="8844">LangChain
    includes the components needed for an effective RAG pipeline, including prompting,
    memory management, agents, and other integrations with various external tools</st>
    <st c="9015">and services.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9028">After running the preceding first line, you will need to restart
    your kernel to be able to access all of the new packages you just installed in
    the environment.</st> <st c="9190">Depending on what environment you are in, this
    can be done in a variety of ways.</st> <st c="9271">Typically, you will see a
    refresh button you can use or a</st> **<st c="9329">Restart kernel</st>** <st
    c="9343">option in</st> <st c="9354">the menu.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9363">If you have trouble finding a way to restart the kernel, add this
    cell and</st> <st c="9439">run it:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: <st c="9527">This is a code version for performing a kernel restart in an IPython
    environment (notebooks).</st> <st c="9622">You shouldn’t need it, but it is here
    for you just</st> <st c="9673">in case!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9681">Once you have installed these packages and restarted your kernel,
    you are ready to start coding!</st> <st c="9779">Let’s start with importing many
    of the packages you just installed in</st> <st c="9849">your environment.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9866">Imports</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="9874">Now, let’s import all of the libraries needed to</st> <st c="9923">perform
    the RAG-related tasks.</st> <st c="9955">I have provided comments at the top of
    each group of imports to indicate what area of RAG the imports are relevant to.</st>
    <st c="10074">This, combined with the description in the following list, provides
    a basic introduction to everything you need for your first</st> <st c="10201">RAG
    pipeline:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: <st c="10644">Let’s step through each of</st> <st c="10672">these imports:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="10686">import os</st>`<st c="10696">: This</st> <st c="10703">provides
    a way to interact with the operating system.</st> <st c="10758">It is useful for
    performing operations such as accessing environment variables and working with</st>
    <st c="10854">file paths.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="10865">from langchain_community.document_loaders import WebBaseLoader</st>`<st
    c="10928">: The</st> `<st c="10935">WebBaseLoader</st>` <st c="10948">class is
    a document loader that can fetch and load web pages</st> <st c="11010">as documents.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="11023">import bs4</st>`<st c="11034">: The</st> `<st c="11041">bs4</st>`
    <st c="11044">module, which stands for</st> **<st c="11070">Beautiful Soup 4</st>**<st
    c="11086">, is a popular library for web scraping and parsing HTML</st> <st c="11143">or
    XML documents.</st> <st c="11161">Since we will be working with a web page, this
    gives us a simple way to pull out the title, content, and</st> <st c="11266">headers
    separately.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="11285">import openai</st>`<st c="11299">: This provides an interface
    to interact with OpenAI’s language models</st> <st c="11371">and APIs.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="11380">from langchain_openai import ChatOpenAI, OpenAIEmbeddings</st>`<st
    c="11438">: This imports both</st> `<st c="11459">ChatOpenAI</st>` <st c="11469">(for
    the LLM) and</st> `<st c="11488">OpenAIEmbeddings</st>` <st c="11504">(for the
    embeddings), which are specific implementations of language models and embeddings
    that use OpenAI’s models that work directly</st> <st c="11640">with LangChain.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="11655">from langchain import hub</st>`<st c="11681">: The</st> `<st
    c="11688">hub</st>` <st c="11691">component provides access to various pre-built
    components and utilities for working with</st> <st c="11781">language models.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="11797">from langchain_core.output_parsers import StrOutputParser</st>`<st
    c="11855">: This component parses the output generated by the language model and
    extracts the relevant information.</st> <st c="11962">In this case, it assumes
    that the language model’s output is a string and returns</st> <st c="12044">it
    as-is.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="12053">from langchain_core.runnables import RunnablePassthrough</st>`<st
    c="12110">: This component passes through the question or query without any modifications.</st>
    <st c="12192">It allows the question to be used as-is in the subsequent steps
    of</st> <st c="12259">the chain.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="12269">Import chromadb</st>`<st c="12285">: As mentioned previously,</st>
    `<st c="12313">chromadb</st>` <st c="12321">imports the Chroma DB vector store,
    a high-performance embedding/vector database designed for efficient similarity
    search</st> <st c="12444">and retrieval.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="12458">from langchain_community.vectorstores import Chroma</st>`<st
    c="12510">: This provides an interface to interact with the Chroma vector database</st>
    <st c="12584">using LangChain.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="12600">from langchain_experimental.text_splitter import SemanticChunker</st>`<st
    c="12665">: A text splitter is typically a function that we use to split the text
    into small chunks based on a specified chunk size and overlap.</st> <st c="12801">This
    splitter is called</st> `<st c="12825">SemanticChunker</st>`<st c="12840">, an
    experimental text-splitting utility provided by the</st> `<st c="12897">Langchain_experimental</st>`
    <st c="12919">library.</st> <st c="12929">The main purpose of</st> `<st c="12949">SemanticChunker</st>`
    <st c="12964">is to break down long text into more manageable pieces while preserving
    the</st> <st c="13040">semantic coherence and context of</st> <st c="13075">each
    chunk.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="13086">These imports provide the essential Python packages that will
    be needed to set up your RAG pipeline.</st> <st c="13188">Your next step will
    be to connect your environment to</st> <st c="13242">OpenAI’s API.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13255">OpenAI connection</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="13273">The following line of code is a very</st> <st c="13310">simple
    demonstration of how your API key will be ingested into the system.</st> <st c="13386">However,
    this is not a secure way to use an API key.</st> <st c="13439">There are many
    ways to do this more securely.</st> <st c="13485">If you have a preference, go
    ahead and implement it now, but otherwise, we will cover a popular way to make
    this more secure in</st> [*<st c="13613">Chapter 5</st>*](B22475_05.xhtml#_idTextAnchor095)*<st
    c="13622">.</st>*
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13623">You are going to need replace</st> `<st c="13654">sk-###################</st>`
    <st c="13676">with your actual OpenAI</st> <st c="13701">API key:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: <st c="13811">Important</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13821">This is just a simple example; please use a secure approach to
    hide your</st> <st c="13895">API key!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13903">You have probably guessed that this OpenAI API key will be used
    to connect to the ChatGPT LLM.</st> <st c="13999">But ChatGPT is not the only
    service we will use from OpenAI.</st> <st c="14060">This API key is also used
    to access the OpenAI embedding service.</st> <st c="14126">In the next section,
    which focuses on coding the indexing stage of the RAG process, we will utilize
    the OpenAI embedding service to convert your content into vector embeddings, a
    key aspect of the</st> <st c="14323">RAG pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14336">Indexing</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="14345">The next few steps represent the</st> *<st c="14379">indexing</st>*
    <st c="14387">stage, where we obtain our target data, pre-process it, and vectorize
    it.</st> <st c="14462">These</st> <st c="14467">steps are often done</st> *<st
    c="14489">offline</st>*<st c="14496">, meaning they are done to</st> <st c="14523">prepare
    the application for usage later.</st> <st c="14564">But in some cases, it may
    make sense to do this all in real time, such as in rapidly changing data environments
    where the data that is used is relatively small.</st> <st c="14725">In this particular
    example, the steps are</st> <st c="14767">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14778">Web loading</st> <st c="14791">and crawling.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="14804">Splitting the data into digestible chunks for the Chroma DB</st>
    <st c="14865">vectorizing algorithm.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="14887">Embedding and indexing</st> <st c="14911">those chunks.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="14924">Adding those chunks and embeddings to the Chroma DB</st> <st c="14977">vector
    store.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '<st c="14990">Let’s start with the first step: web loading</st> <st c="15036">and
    crawling.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15049">Web loading and crawling</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="15074">To start, we need to pull</st> <st c="15101">in our data.</st>
    <st c="15114">This could be anything of</st> <st c="15140">course, but we have
    to</st> <st c="15163">start somewhere!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15179">For our example, I am providing a web page example based on some
    of the content from</st> [*<st c="15265">Chapter 1</st>*](B22475_01.xhtml#_idTextAnchor015)<st
    c="15274">. I have adopted the original structure from an example provided by
    LangChain</st> <st c="15352">at</st> [<st c="15355">https://lilianweng.github.io/posts/2023-06-23-agent/</st>](https://lilianweng.github.io/posts/2023-06-23-agent/)<st
    c="15407">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15408">You can try that web page as well if it is still available when
    you read this, but be sure to change the question you use to query the content
    to a question more suitable to the content on that page.</st> <st c="15609">You
    also need to restart your kernel if you change web pages; otherwise, it will include
    content from both web pages if you rerun the loader.</st> <st c="15751">That may
    be what you want, but I’m just letting</st> <st c="15799">you know!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15808">I also encourage you to try this with other web pages and see
    what challenges these other pages present.</st> <st c="15914">This example involves
    a very clean piece of data compared to most web pages, which tend to be rife with
    ads and other content you do not want showing up.</st> <st c="16068">But maybe
    you can find a relatively clean blog post and pull that in?</st> <st c="16138">Maybe
    you can create your own?</st> <st c="16169">Try different web pages</st> <st c="16193">and
    see!</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: <st c="16407">The preceding</st> <st c="16421">code starts with using the</st>
    `<st c="16449">WebBaseLoader</st>` <st c="16462">class from the</st> `<st c="16478">langchain_community
    document_loaders</st>` <st c="16514">module</st> <st c="16521">to load web pages
    as documents.</st> <st c="16554">Let’s break</st> <st c="16566">it down:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="16574">Creating the</st> `<st c="16588">WebBaseLoader</st>` <st c="16601">instance:
    The</st> `<st c="16616">WebBaseLoader</st>` <st c="16629">class is instantiated
    with the</st> <st c="16661">following parameters:</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`<st c="16682">web_paths</st>`<st c="16692">: A tuple containing the URLs of
    the web pages to be loaded.</st> <st c="16754">In this case, it contains a single</st>
    <st c="16789">URL:</st> `<st c="16794">https://kbourne.github.io/chapter1.html</st>`<st
    c="16833">.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="16834">bs_kwargs</st>`<st c="16844">: A dictionary of keyword arguments
    to be passed to the</st> `<st c="16901">BeautifulSoup</st>` <st c="16914">parser.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="16922">parse_only</st>`<st c="16933">: A</st> `<st c="16938">bs4.SoupStrainer</st>`
    <st c="16954">object specifies the HTML elements to parse.</st> <st c="17000">In
    this case, it is set to parse only the elements with the CSS classes, such as</st>
    `<st c="17081">post-content</st>`<st c="17093">,</st> `<st c="17095">post-title</st>`<st
    c="17105">,</st> <st c="17107">and</st> `<st c="17111">post-header</st>`<st c="17122">.</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="17123">The</st> `<st c="17128">WebBaseLoader</st>` <st c="17141">instance
    initiates a series of steps that represent the loading of the document into your
    environment: The load method is called on</st> `<st c="17274">loader</st>`<st
    c="17280">, the</st> `<st c="17286">WebBaseLoader</st>` <st c="17299">instance
    that fetches and loads the specified web pages as documents.</st> <st c="17370">Internally,</st>
    `<st c="17382">loader</st>` <st c="17388">is doing</st> <st c="17398">a lot!</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="17404">Here are the steps it performs just based on this small amount</st>
    <st c="17468">of code:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="17476">Makes HTTP requests to the specified URLs to fetch the</st> <st
    c="17532">web pages.</st>
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="17542">Parses the HTML content of the web pages using</st> `<st c="17590">BeautifulSoup</st>`<st
    c="17603">, considering only the elements specified by the</st> `<st c="17652">parse_only</st>`
    <st c="17662">parameter.</st>
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="17673">Extracts the relevant text content from the parsed</st> <st c="17725">HTML
    elements.</st>
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="17739">Creates</st> `<st c="17748">Document</st>` <st c="17756">objects
    for each web page that contain the extracted text content, along with metadata
    such as the</st> <st c="17856">source URL.</st>
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="17867">The resulting</st> `<st c="17882">Document</st>` <st c="17890">objects
    are stored in the</st> `<st c="17917">docs</st>` <st c="17921">variable for further
    use in</st> <st c="17950">our code!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17959">The classes that</st> <st c="17977">we are passing to</st> `<st
    c="17995">bs4</st>` <st c="17998">(</st>`<st c="18000">post-content</st>`<st c="18012">,</st>
    `<st c="18014">post-title</st>`<st c="18024">, and</st> `<st c="18030">post-header</st>`<st
    c="18041">) are CSS classes.</st> <st c="18061">If you are using an HTML page</st>
    <st c="18090">that does not have those CSS classes, this will not work.</st> <st
    c="18149">So, if you are using a different URL and are not getting data, take
    a look at what the CSS tags are in the HTML you are crawling.</st> <st c="18279">Many
    web pages do use this pattern, but not all!</st> <st c="18328">Crawling web pages
    presents many challenges</st> <st c="18372">like this.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18382">Once you have collected the documents from your data source, you
    need to pre-process them.</st> <st c="18474">In this case, this</st> <st c="18493">involves
    splitting.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18512">Splitting</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18522">If you are using the provided URL, you</st> <st c="18562">will
    only parse the elements with the</st> `<st c="18600">post-content</st>`<st c="18612">,</st>
    `<st c="18614">post-title</st>`<st c="18624">, and</st> `<st c="18630">post-header</st>`
    <st c="18641">CSS classes.</st> <st c="18655">This will extract the text content
    from the main article body (usually identified by the</st> `<st c="18744">post-content</st>`
    <st c="18756">class), the title of the blog post (usually identified by the</st>
    `<st c="18819">post-title</st>` <st c="18829">class), and any header information
    (usually identified by the</st> `<st c="18892">post-header</st>` <st c="18903">class).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18911">In case you were curious, this is what this document looks like
    on the web (</st>*<st c="18988">Figure 2</st>**<st c="18997">.1</st>*<st c="18999">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – A web page that we will process](img/B22475_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="21860">Figure 2.1 – A web page that we will process</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21904">It goes down many pages too!</st> <st c="21934">There</st> <st
    c="21939">is a lot of content here, too much for an LLM to process directly.</st>
    <st c="22007">So, we will need to split the document into</st> <st c="22051">digestible
    chunks:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: <st c="22166">There are many text splitters available in LangChain, but I chose
    to start with an experimental, but very interesting, option called</st> `<st c="22300">SemanticChunker</st>`<st
    c="22315">. As I mentioned previously, when talking about the imports,</st> `<st
    c="22376">SemanticChunker</st>` <st c="22391">focuses on breaking down long text
    into more manageable pieces while preserving the semantic coherence and context
    of</st> <st c="22510">each chunk.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22521">Other text splitters typically take an arbitrary chunk length
    that is not context-aware, something that creates issues when important content
    gets split by the chunker.</st> <st c="22691">There are ways to address this that
    we will talk about in</st> [*<st c="22749">Chapter 11</st>*](B22475_11.xhtml#_idTextAnchor229)<st
    c="22759">, but for now, just know that</st> `<st c="22789">SemanticChunker</st>`
    <st c="22804">focuses on accounting for context rather than just arbitrary length
    in your chunks.</st> <st c="22889">It should also be noted that it is still considered
    experimental and it is under continual development.</st> <st c="22993">In</st>
    [*<st c="22996">Chapter 11</st>*](B22475_11.xhtml#_idTextAnchor229)<st c="23006">,
    we will put it to the test against probably the other most important text splitter,</st>
    `<st c="23092">RecursiveCharacter TextSplitter</st>`<st c="23123">, and see which
    splitter works best with</st> <st c="23164">this content.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23177">It should also be noted that the</st> `<st c="23211">SemanticChunker</st>`
    <st c="23226">splitter you use in this code uses</st> `<st c="23262">OpenAIEmbeddings</st>`<st
    c="23278">, and it costs money to process the embeddings.</st> <st c="23326">The
    OpenAI embedding models currently cost between $0.02 and $0.13 per million tokens,
    depending on what model you use.</st> <st c="23446">At the time of writing, if
    do not designate an embedding model, OpenAI will use the</st> `<st c="23530">text-embedding-ada-002</st>`
    <st c="23552">model by default, which costs $0.02 per million tokens.</st> <st
    c="23609">If you want to avoid the cost, fall back to</st> `<st c="23653">RecursiveCharacter
    TextSplitter</st>`<st c="23684">, something we will cover in</st> [*<st c="23713">Chapter
    11</st>*](B22475_11.xhtml#_idTextAnchor229)<st c="23723">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23724">I encourage you to go ahead and try different splitters and see
    what happens!</st> <st c="23803">For example, do you think you get better results
    from</st> `<st c="23857">RecursiveCharacter TextSplitter</st>` <st c="23888">than
    from</st> `<st c="23899">SemanticChunker</st>`<st c="23914">, which we are using
    here?</st> <st c="23941">Maybe speed is more important</st> <st c="23971">than
    quality in your particular case – which one</st> <st c="24020">is faster?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24030">Once you have chunked up your content, the next step is to convert
    it into the vector embeddings we have talked so</st> <st c="24146">much about!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24157">Embedding and indexing the chunks</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="24191">The next few steps</st> <st c="24211">represent the retrieval
    and generation steps, where we will use Chroma DB</st> <st c="24284">as the vector
    database.</st> <st c="24309">As mentioned multiple times now, Chroma DB is a great
    vector store!</st> <st c="24377">I selected this vector store because it is easy
    to run locally and it works well for demos like this, but it is a fairly powerful
    vector store.</st> <st c="24521">As you may recall when we talked about vocabulary
    and the difference between vector stores and vector databases, Chroma DB is indeed
    both!</st> <st c="24660">Chroma is one of many options for your vector store though.</st>
    <st c="24720">In</st> [*<st c="24723">Chapter 7</st>*](B22475_07.xhtml#_idTextAnchor122)<st
    c="24732">, we will discuss many of the vector store options and reasons to choose
    one over the other.</st> <st c="24825">Some of these options even provide free
    vector</st> <st c="24872">embedding generation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24893">We are using OpenAI embeddings here as well, which will use your
    OpenAI key to send your chunks of data to the OpenAI API, convert them into embeddings,
    and then send them back in their mathematical form.</st> <st c="25099">Note that
    this</st> *<st c="25114">does</st>* <st c="25118">cost money!</st> <st c="25131">It
    is a fraction of a penny for each embedding, but it is worth noting.</st> <st
    c="25203">So, please use caution when using this code if you are doing this on
    a tight budget!</st> <st c="25288">In</st> [*<st c="25291">Chapter 7</st>*](B22475_07.xhtml#_idTextAnchor122)<st
    c="25300">, we will review some ways to use free vectorization services to generate
    these embeddings</st> <st c="25391">for free:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: <st c="25524">First, we create</st> <st c="25542">the Chroma vector store with
    the</st> `<st c="25575">Chroma.from_documents</st>` <st c="25596">method, which
    is called to create a Chroma vector store from the split documents.</st> <st c="25679">This
    is one of many methods we can use to create a Chroma database.</st> <st c="25747">This
    typically depends on the source, but for this particular method, it takes the</st>
    <st c="25830">following parameters:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="25851">documents</st>`<st c="25861">: The list of split documents (splits)
    obtained from the previous</st> <st c="25928">code snippet</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="25940">embedding</st>`<st c="25950">: An instance of the</st> `<st
    c="25972">OpenAIEmbeddings</st>` <st c="25988">class, which is used to generate
    embeddings for</st> <st c="26037">the documents</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="26050">Internally, the method is doing a</st> <st c="26085">few things:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26096">It iterates over each</st> `<st c="26119">Document</st>` <st c="26127">object
    in the</st> <st c="26142">splits list.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="26154">For each</st> `<st c="26164">Document</st>` <st c="26172">object,
    it uses the provided</st> `<st c="26202">OpenAIEmbeddings</st>` <st c="26218">instance
    to generate an</st> <st c="26243">embedding vector.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="26260">It stores the document text and its corresponding embedding vector
    in the Chroma</st> <st c="26342">vector database.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="26358">At this point, you now have a vector database called</st> `<st
    c="26412">vectorstore</st>`<st c="26423">, and it is full of embeddings, which
    are…?</st> <st c="26467">That’s right – mathematical representations of all of
    the content from the web page you just crawled!</st> <st c="26569">So cool!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26577">But what is this next part – a retriever?</st> <st c="26620">Is
    this of the canine variety?</st> <st c="26651">Nope.</st> <st c="26657">This is
    creating the mechanism that you will use to perform vector similarity searches
    on your new vector database.</st> <st c="26773">You call the</st> `<st c="26786">as_retriever</st>`
    <st c="26798">method right on the</st> `<st c="26819">vectorstore</st>` <st c="26830">instance
    to create the retriever.</st> <st c="26865">The retriever is an object that provides
    a convenient interface for performing these similarity searches and retrieving
    the relevant documents from the vector database based on</st> <st c="27042">those
    searches.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27057">If you just want to perform the document retrieval process, you
    can.</st> <st c="27127">This is not officially part of the code, but if you want
    to test this out, add this in an extra cell and</st> <st c="27232">run it:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27358">The output</st> <st c="27370">should be what I list later in this
    code when I indicate what is passed to the LLM, but it is essentially a list of
    the content stored in the</st> `<st c="27512">vectorstore</st>` <st c="27523">vector
    database that is most similar to</st> <st c="27564">the query.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27574">Aren’t you impressed?</st> <st c="27597">This is a simple example
    of course, but this is the foundation for much more powerful tools that you can
    use to access your data and supercharge generative AI applications for</st> <st
    c="27773">your organization!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27791">However, at this point in the application, you have only created
    the receiver.</st> <st c="27871">You have not used it within the RAG pipeline
    yet.</st> <st c="27921">We will review how to do</st> <st c="27946">that next!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27956">Retrieval and generation</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="27981">In the code, the retrieval and generation stages</st> <st c="28030">are
    combined within the chain we set up to represent the entire RAG process.</st>
    <st c="28108">This leverages pre-built components from the</st> **<st c="28153">LangChain
    Hub</st>**<st c="28166">, such as</st> **<st c="28176">prompt templates</st>**<st
    c="28192">, and</st> <st c="28197">integrates them with a selected LLM.</st> <st
    c="28235">We will also</st> <st c="28247">utilize the</st> **<st c="28260">LangChain
    Expression Language</st>** <st c="28289">(</st>**<st c="28291">LCEL</st>**<st
    c="28295">) to</st> <st c="28301">define a chain of operations that retrieves
    relevant documents based on an input question, formats the retrieved content,
    and feeds it into the LLM to generate a response.</st> <st c="28473">Overall,
    the steps we take in retrieval and generation are</st> <st c="28532">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28543">Take in a</st> <st c="28554">user query.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="28565">Vectorize that</st> <st c="28581">user query.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="28592">Perform a similarity search of the vector store to find the closest
    vectors to the user query vector, as well as their</st> <st c="28712">associated
    content.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="28731">Pass the</st> <st c="28740">retrieved content into a prompt template,
    a process known</st> <st c="28799">as</st> **<st c="28802">hydrating</st>**<st
    c="28811">.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="28812">Pass that</st> *<st c="28823">hydrated</st>* <st c="28831">prompt
    to</st> <st c="28842">the LLM.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="28850">Once you</st> <st c="28859">receive a response from the LLM, present
    it to</st> <st c="28907">the user.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="28916">From a coding standpoint, we will start by defining the prompt
    template so that we have something to hydrate when we receive the user query.</st>
    <st c="29058">We will cover this in the</st> <st c="29084">next section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29097">Prompt templates from the LangChain Hub</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="29137">The LangChain Hub</st> <st c="29155">is a collection of pre-built
    components and templates that can be easily integrated into LangChain applications.</st>
    <st c="29269">It provides a centralized repository for</st> <st c="29310">sharing
    and discovering reusable components, such as prompts, agents, and utilities.</st>
    <st c="29395">Here, we are calling a prompt template from the LangChain Hub and
    assigning it to</st> `<st c="29477">prompt</st>`<st c="29483">, a prompt template
    representing what we will pass to</st> <st c="29537">the LLM:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: <st c="29602">This code retrieves a pre-built prompt template from the LangChain
    Hub using the</st> `<st c="29684">pull</st>` <st c="29688">method of the</st>
    `<st c="29703">hub</st>` <st c="29706">module.</st> <st c="29715">The prompt template
    is identified by the</st> `<st c="29756">jclemens24/rag-prompt</st>` <st c="29777">string.</st>
    <st c="29786">This identifier follows the</st> *<st c="29814">repository/component</st>*
    <st c="29834">convention, where</st> *<st c="29853">repository</st>* <st c="29863">represents
    the organization or user hosting the component, and</st> *<st c="29927">component</st>*
    <st c="29936">represents the specific component being pulled.</st> <st c="29985">The</st>
    `<st c="29989">rag-prompt</st>` <st c="29999">component indicates it is a prompt
    designed for</st> <st c="30048">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30065">If you print out the prompt with</st> `<st c="30099">print(prompt)</st>`<st
    c="30112">, you can see what is used here, as well as what the</st> <st c="30165">inputs
    are:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30564">This is the initial part of the prompt that gets passed to the
    LLM, which in this case, tells</st> <st c="30659">it this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30898">Later, you add</st> <st c="30913">the</st> `<st c="30918">question</st>`
    <st c="30926">and</st> `<st c="30931">context</st>` <st c="30938">variables to</st>
    *<st c="30952">hydrate</st>* <st c="30959">the prompt, but starting with this
    format optimizes it to work better for</st> <st c="31034">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31051">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31056">The</st> `<st c="31061">jclemens24/rag-prompt</st>` <st c="31082">string
    is one version of the predefined starting prompts.</st> <st c="31141">Visit the
    LangChain Hub to find many more – you may even find one that better fits your</st>
    <st c="31229">needs:</st> [<st c="31236">https://smith.langchain.com/hub/search?q=rag-prompt</st>](https://smith.langchain.com/hub/search?q=rag-prompt)<st
    c="31287">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31288">You can also use your own!</st> <st c="31316">I can count over
    30 options at the time</st> <st c="31356">of writing!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31367">The prompt template is a key part of the RAG pipeline as it represents
    how you communicate with the LLM to receive the response you are seeking.</st>
    <st c="31513">But in most RAG pipelines, getting the prompt into a format so that
    it can work with the prompt template is not as straightforward as just passing
    it a string.</st> <st c="31673">In this example, the</st> `<st c="31694">context</st>`
    <st c="31701">variable represents the content we get from the retriever and that
    is not in a string format</st> <st c="31795">yet!</st> <st c="31800">We will walk
    through how to convert our retrieved content into the proper string format we</st>
    <st c="31891">need next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31901">Formatting a function so that it matches the next step’s input</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="31964">First, we will set up a</st> <st c="31988">function that takes
    the list of retrieved documents (docs)</st> <st c="32048">as input:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <st c="32133">Inside this function, a generator expression,</st> `<st c="32180">(doc.page_content
    for doc in docs)</st>`<st c="32214">, is used to extract the</st> `<st c="32239">page_content</st>`
    <st c="32251">attribute from each document object.</st> <st c="32289">The</st>
    `<st c="32293">page_content</st>` <st c="32305">attribute represents the text
    content of</st> <st c="32347">each document.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32361">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32366">In this case, a</st> *<st c="32383">document</st>* <st c="32391">is
    not the entire document that you crawled earlier.</st> <st c="32445">It is just
    one small section of it, but we generally call</st> <st c="32503">these documents.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32519">The</st> `<st c="32524">join</st>` <st c="32528">method is called
    on the</st> `<st c="32553">\n\n</st>` <st c="32557">string to concatenate</st>
    `<st c="32580">page_content</st>` <st c="32592">of each document with two newline
    characters between each document’s content.</st> <st c="32671">The formatted string
    is returned by the</st> `<st c="32711">format_docs</st>` <st c="32722">function
    to represent the</st> `<st c="32749">context</st>` <st c="32756">key in the dictionary
    that is piped into the</st> <st c="32802">prompt object.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32816">The purpose of this function is to format the output of the retriever
    into the string format that it will need to be in for the next step in the chain,
    after the retriever step.</st> <st c="32995">We will explain this further in a
    moment, but short functions like this are often necessary for LangChain chains
    to match up inputs and outputs across the</st> <st c="33150">entire chain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33163">Next, we will review the last step before we can create our LangChain
    chain – that is, defining the LLM we will use in</st> <st c="33283">that chain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33294">Defining your LLM</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33312">Let’s set up the</st> <st c="33329">LLM model you</st> <st c="33344">will
    use:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <st c="33411">The preceding code creates an instance of the</st> `<st c="33458">ChatOpenAI</st>`
    <st c="33468">class from the</st> `<st c="33484">langchain_openai</st>` <st c="33500">module,
    which serves as an interface to OpenAI’s language models, specifically the</st>
    <st c="33583">GPT-4o mini model.</st> <st c="33603">Even though this model is
    newer, it was released at a significant discount to the older models.</st> <st
    c="33699">Using this model will help keep your inference costs down while still
    allowing you to use a recent model!</st> <st c="33805">If you would like to try
    a different version of ChatGPT, such as</st> `<st c="33870">gpt-4</st>`<st c="33875">,
    you can just change the model name.</st> <st c="33913">Look up the newest models
    on the OpenAI API website – they add</st> <st c="33976">them often!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33987">Setting up a LangChain chain using LCEL</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="34027">This</st> *<st c="34033">chain</st>* <st c="34038">is in</st>
    <st c="34045">a code format specific to LangChain called</st> <st c="34087">LCEL.</st>
    <st c="34094">You will see me using LCEL throughout the code from here on out.</st>
    <st c="34159">Not only does it make the code easier to read and more concise,
    but it opens up new techniques focused on improving the speed and efficiency of
    your</st> <st c="34308">LangChain code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34323">If you walk through this chain, you’ll see it provides a great
    representation of the entire</st> <st c="34416">RAG process:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: <st c="34551">All of these components have already been described, but to summarize,
    the</st> `<st c="34627">rag_chain</st>` <st c="34636">variable represents a chain
    of operations using the LangChain framework.</st> <st c="34710">Let’s walk through
    each step of the chain, digging into what is happening at</st> <st c="34787">each
    point:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="34990">rag_chain</st>` <st c="34999">variable in a moment, we will
    pass it a “question.” As shown in the preceding code, the chain starts with a
    dictionary that defines two keys:</st> `<st c="35142">"context"</st>` <st c="35151">and</st>
    `<st c="35156">"question"</st>`<st c="35166">. The question part is pretty straightforward,
    but where does the context come from?</st> <st c="35251">The</st> `<st c="35255">"context"</st>`
    <st c="35264">key assigned is the result of the</st> `<st c="35299">retriever</st>`
    <st c="35308">|</st> `<st c="35311">format_docs</st>` <st c="35322">operation.</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="35333">Does</st> `<st c="35339">format_docs</st>` <st c="35350">sound
    familiar?</st> <st c="35367">Yes!</st> <st c="35372">That’s because we just set
    up that function previously.</st> <st c="35428">Here, we use that function alongside</st>
    `<st c="35465">retriever</st>`<st c="35474">. The</st> `<st c="35480">|</st>`
    <st c="35481">operator, called a pipe, between the retriever and</st> `<st c="35533">format_docs</st>`
    <st c="35544">indicates that we are chaining these operations together.</st> <st
    c="35603">So, in this case, the</st> `<st c="35625">retriever</st>` <st c="35634">object
    is</st> *<st c="35645">piped</st>* <st c="35650">into the</st> `<st c="35660">format_docs</st>`
    <st c="35671">function.</st> <st c="35682">We are running the</st> `<st c="35701">retriever</st>`
    <st c="35710">operation here, which is the vector similarity search.</st> <st
    c="35766">The similarity search should return a set of matches; that set of matches
    is what is passed to the function.</st> <st c="35875">Our</st> `<st c="35879">format_docs</st>`
    <st c="35890">function, as described earlier, is then used on the content provided
    by the retriever to format all the results of that retriever into a single string.</st>
    <st c="36043">That complete string is then assigned to the</st> *<st c="36088">context</st>*<st
    c="36095">, which as you may remember is a variable in our prompt.</st> <st c="36152">The
    expected input format of the next step is a dictionary with two keys – that is,</st>
    `<st c="36236">"context"</st>` <st c="36245">and</st> `<st c="36250">"question"</st>`<st
    c="36260">. The values that are assigned to these keys are expected to be strings.</st>
    <st c="36333">So, we can’t just pass retriever output, which is a list of objects.</st>
    <st c="36402">This is why we use the</st> `<st c="36425">format_docs</st>` <st
    c="36436">function – to convert the retriever results into the string we need
    for the next step.</st> <st c="36524">Let’s go back to the</st> *<st c="36545">question</st>*
    <st c="36553">that was passed into the chain, which is already in the string format
    we require.</st> <st c="36636">We don’t need any formatting!</st> <st c="36666">So,
    we use the</st> `<st c="36681">RunnablePassthrough()</st>` <st c="36702">object
    to just let that input (the</st> *<st c="36738">question</st>* <st c="36746">provided)
    pass through as the string that it is already formatted as.</st> <st c="36817">That
    object takes the</st> *<st c="36839">question</st>* <st c="36847">we pass into
    the</st> `<st c="36865">rag_chain</st>` <st c="36874">variable and passes it through
    without any modification.</st> <st c="36932">We now have our first step in the
    chain, which is defining the two variables that the prompt in the next</st> <st
    c="37037">step accepts.</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <st c="37050">We can see another pipe (</st>`<st c="37076">|</st>`<st c="37078">)
    followed by the</st> `<st c="37096">prompt</st>` <st c="37102">object, and we</st>
    *<st c="37118">pipe</st>* <st c="37122">the variables (in a dictionary) into that
    prompt object.</st> <st c="37180">This is known as hydrating the prompt.</st>
    <st c="37219">As mentioned previously, the</st> `<st c="37248">prompt</st>` <st
    c="37254">object is a prompt template that defines what we will pass to the LLM,
    and it typically includes input variables (context and question) that are filled/hydrated
    first.</st> <st c="37423">The result of this second step is the full prompt text
    as a string, with the variables filling in the placeholders for context and question.</st>
    <st c="37564">Then, we have another pipe (</st>`<st c="37592">|</st>`<st c="37594">)
    and the</st> `<st c="37604">llm</st>` <st c="37607">object that we defined earlier.</st>
    <st c="37640">As we have seen already, this step in the chain takes the output
    from the previous step, which is the prompt string that includes all the information
    from previous steps.</st> <st c="37811">The</st> `<st c="37815">llm</st>` <st
    c="37818">object represents the language model</st> <st c="37855">we set up, which
    in this case is</st> `<st c="37889">ChatGPT 4o</st>`<st c="37899">. The formatted
    prompt string is passed as input to the language model, which generates a response
    based on the provided context</st> <st c="38028">and question.</st>*   <st c="38041">It
    almost seems like this would be enough, but when you use an LLM API, it is not
    just sending you the text you might see when you type something into ChatGPT.</st>
    <st c="38202">It is in a JSON format and has a lot of other data included with
    it.</st> <st c="38271">So, to keep things simple, we are going to</st> *<st c="38314">pipe</st>*
    <st c="38318">the LLM’s output to the next step and use LangChain’s</st> `<st
    c="38373">StrOutputParser()</st>` <st c="38390">object.</st> <st c="38399">Note
    that</st> `<st c="38409">StrOutputParser()</st>` <st c="38426">is a utility class
    in LangChain that parses the key output of the language model into a string format.</st>
    <st c="38530">Not only does it strip away all the information you did not want
    to deal with right now, but it ensures that the generated response is returned
    as</st> <st c="38677">a string.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="38686">Let’s take a moment to appreciate everything we just did here.</st>
    <st c="38750">This</st> *<st c="38755">chain</st>* <st c="38760">we created using
    LangChain represents the core code for our entire RAG pipeline, and it is just
    a few</st> <st c="38863">strings long!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38876">When the user uses your application, it will start with the user
    query.</st> <st c="38949">But from a coding standpoint, we set up everything else
    so that we can process the query properly.</st> <st c="39048">At this point, we
    are ready to accept the user query, so let’s review this last step in</st> <st
    c="39136">our code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39145">Submitting a question for RAG</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="39175">So far, you have</st> <st c="39192">defined the chain, but you
    haven’t run it.</st> <st c="39236">So, let’s run the entire RAG pipeline in this
    one line, using a query you are</st> <st c="39314">feeding in:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: <st c="39383">As mentioned when stepping through what happens in the chain,</st>
    `<st c="39446">"What are the advantages of using RAG?"</st>` <st c="39485">is
    the string we are going to pass into the chain to begin with.</st> <st c="39551">The
    first step in the chain expects this string as the</st> *<st c="39606">question</st>*
    <st c="39614">we discussed in the previous section as one of the two expected
    variables.</st> <st c="39690">In some applications, this may not be in the proper
    format and will need an extra function to prepare it, but for this application,
    it is already in the string format we are expecting, so we pass it right into
    that</st> `<st c="39905">RunnablePassThrough()</st>` <st c="39926">object.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39934">In the future, this prompt will include a query from a user interface,
    but for now, we will represent it as this variable string.</st> <st c="40065">Keep
    in mind that this is not the only text the LLM will see; you added a more robust
    prompt defined by</st> `<st c="40169">prompt</st>` <st c="40175">previously, hydrated
    by the</st> `<st c="40204">"context"</st>` <st c="40213">and</st> `<st c="40218">"</st>``<st
    c="40219">question"</st>` <st c="40228">variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40239">And that is it from a coding standpoint!</st> <st c="40281">But
    what happens when you run the code?</st> <st c="40321">Let’s review the output
    you can expect from this RAG</st> <st c="40374">pipeline code.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40388">Final output</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="40401">The</st> <st c="40405">final output will look something</st> <st
    c="40439">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="41649">This has some</st> <st c="41664">basic formatting in it, so when
    it’s displayed, it will look like this (including the bullets and</st> <st c="41762">bolded
    text):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="41775">The advantages of using Retrieval Augmented Generation (</st>``<st
    c="41832">RAG) include:</st>`'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="41846">Improved Accuracy and Relevance: RAG enhances the accuracy and
    relevance of responses generated by large language models (LLMs) by fetching and
    incorporating specific information from databases or datasets in real time.</st>
    <st c="42067">This ensures outputs are based on both the model''s pre-existing
    knowledge and the most current and relevant</st>` `<st c="42175">data provided.</st>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="42189">Customization and Flexibility: RAG allows for the customization
    of responses based on domain-specific needs by integrating a company''s internal
    databases into the model''s response generation process.</st> <st c="42390">This
    level of customization is invaluable for creating personalized experiences and
    for applications requiring high specificity</st>` `<st c="42518">and detail.</st>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="42529">Expanding Model Knowledge Beyond Training Data: RAG overcomes
    the limitations of LLMs, which are bound by the scope of their training data.</st>
    <st c="42670">By enabling models to access and utilize information not included
    in their initial training sets, RAG effectively expands the knowledge base of
    the model without the need for retraining.</st> <st c="42857">This makes LLMs
    more versatile and adaptable to new domains or rapidly</st>` `<st c="42928">evolving
    topics.</st>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="42944">In your use cases, you will need to make decisions by asking questions
    such as, could a less expensive model do a good enough job at a significantly
    reduced cost?</st> <st c="43108">Or do I need to spend the extra money to get
    more robust responses?</st> <st c="43176">Your prompt may have said to keep it
    very brief and you end up with the same shorter response as a less expensive model
    anyway, so why spend the extra money?</st> <st c="43334">This is a common consideration
    when using these models, and in many cases, the largest, most expensive models
    are not always what is needed to meet the requirements of</st> <st c="43502">the
    application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43518">Here’s what the LLM will</st> <st c="43544">see when you combine
    this with the RAG-focused prompt</st> <st c="43598">from earlier:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45116">As you can see, the context is quite large—it returns all of the
    most relevant information from the original document to help the LLM determine
    how to answer the new question.</st> <st c="45293">The context</st> <st c="45304">is
    what was returned by the vector similarity search, something we will talk about
    in more depth in</st> [*<st c="45405">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="45414">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45415">Complete code</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="45429">Here is the code in</st> <st c="45450">its entirety:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45661">Restart the kernel before running the</st> <st c="45700">following
    code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: <st c="47070">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="47078">This chapter provided a comprehensive code lab that walked through
    the implementation of a complete RAG pipeline.</st> <st c="47193">We began by
    installing the necessary Python packages, including LangChain, Chroma DB, and
    various LangChain extensions.</st> <st c="47313">Then, we learned how to set up
    an OpenAI API key, load documents from a web page using</st> `<st c="47400">WebBaseLoader</st>`<st
    c="47413">, and preprocess the HTML content with BeautifulSoup to extract</st>
    <st c="47477">relevant sections.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47495">Next, the loaded documents were split into manageable chunks using</st>
    `<st c="47563">SemanticChunker</st>` <st c="47578">from LangChain’s experimental
    module.</st> <st c="47617">These chunks were then embedded into vector representations
    using OpenAI’s embedding model and stored in a Chroma DB</st> <st c="47734">vector
    database.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47750">Next, we introduced the concept of a retriever, which is used
    to perform a vector similarity search on the embedded documents based on a given
    query.</st> <st c="47901">We stepped through the retrieval and generation stages
    of RAG, which in this case are combined into a LangChain chain using the LCEL.</st>
    <st c="48035">The chain integrates pre-built prompt templates from the LangChain
    Hub, a selected LLM, and utility functions for formatting retrieved documents
    and parsing</st> <st c="48192">LLM outputs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48204">Finally, we learned how to submit a question to the RAG pipeline
    and receive a generated response that incorporates the retrieved context.</st>
    <st c="48344">We saw the output from the LLM model and discussed key considerations
    for choosing the appropriate model based on accuracy, depth,</st> <st c="48475">and
    cost.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48484">Finally, the complete code for a RAG pipeline was provided!</st>
    <st c="48545">That’s it – you can close this book for now and still be able to
    build an entire RAG application.</st> <st c="48643">Good luck!</st> <st c="48654">But
    before you go, there are still many concepts to review so that you can optimize
    your RAG pipeline.</st> <st c="48757">If you do a quick search of the web for</st>
    `<st c="48797">trouble with RAG</st>` <st c="48813">or something similar, you
    will likely find millions of questions and problems highlighted where RAG applications
    have issues with all but the simplest of applications.</st> <st c="48982">There
    are also many other solutions that RAG can solve that need the code just provided
    to be adjusted.</st> <st c="49086">The rest of this book is dedicated to helping
    you build up knowledge that will help you get past any of these problems and form
    many new solutions.</st> <st c="49234">If you hit a similar challenge, don’t despair!</st>
    <st c="49281">There is a solution!</st> <st c="49302">It just might take the time
    to go beyond</st> [*<st c="49343">Chapter 2</st>*](B22475_02.xhtml#_idTextAnchor035)<st
    c="49352">!</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="49353">In the next chapter, we will take some of the practical applications
    we discussed in</st> [*<st c="49438">Chapter 1</st>*](B22475_01.xhtml#_idTextAnchor015)
    <st c="49447">and dive much deeper into how they are being implemented in various
    organizations.</st> <st c="49531">We will also provide some hands-on code related
    to one of the most common practical applications of RAG: providing the sources
    of the content that the RAG application is quoting</st> <st c="49709">to you.</st>'
  prefs: []
  type: TYPE_NORMAL
