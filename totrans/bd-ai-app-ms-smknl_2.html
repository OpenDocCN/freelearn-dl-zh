<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer020">
			<h1 id="_idParaDest-46" class="chapter-number"><a id="_idTextAnchor045"/>2</h1>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Creating Better Prompts</h1>
			<p>As a developer, you can request that an LLM completes a task by submitting a prompt to it. In the previous chapter, we saw some examples of prompts, such as “<em class="italic">Tell me a knock-knock joke</em>” and “<em class="italic">What is the flight duration between New York City and Rio de Janeiro?</em>” As LLMs became more powerful, the tasks that they could accomplish became <span class="No-Break">more complex.</span></p>
			<p>Researchers discovered that using different techniques to build prompts yielded vastly different results. The process of crafting prompts that improve the likelihood of getting the desired answer is called prompt engineering, and the value of creating better prompts gave birth <a id="_idIndexMarker134"/>to a new profession: <strong class="bold">prompt engineer</strong>. This is someone who doesn’t need to know how to code in any programming language but can create prompts using natural language that return the <span class="No-Break">desired results.</span></p>
			<p>Microsoft Semantic <a id="_idIndexMarker135"/>Kernel uses the concept of <strong class="bold">prompt templating</strong>, the creation of structured templates for prompts that contain placeholders for specific types of information and instructions that can be filled in or customized by the user or by the developer. By using prompt templates, developers can introduce multiple variables in prompts, separate the prompt engineering function from the coding function, and use advanced prompt techniques to increase accuracy <span class="No-Break">in responses.</span></p>
			<p>In this chapter, you’ll learn about several techniques that will make your prompts more likely to return the results you want your users to see in the first attempt. You’ll learn how to employ prompts with multiple variables as well as how to create and use prompts that have multiple parameters to complete more complex tasks. Finally, you’ll uncover techniques that combine prompts in creative ways to improve accuracy in scenarios where LLMs are not very accurate – for example, when solving <span class="No-Break">math problems.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Engineering prompts</span></li>
				<li>Prompts with <span class="No-Break">multiple variables</span></li>
				<li>Prompts with <span class="No-Break">multiple stages</span></li>
			</ul>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Technical requirements</h1>
			<p>To complete this chapter, you will need to have a recent, supported version of your preferred Python or C# <span class="No-Break">development environment:</span></p>
			<ul>
				<li>For Python, the minimum supported version is Python 3.10, and the recommended version is <span class="No-Break">Python 3.11</span></li>
				<li>For C#, the minimum supported version is .<span class="No-Break">NET 8</span></li>
			</ul>
			<p>You will also need an <strong class="bold">OpenAI API</strong> key, obtained either directly through <strong class="bold">OpenAI</strong> or through <strong class="bold">Microsoft</strong>, through the <strong class="bold">Azure OpenAI</strong> service. Instructions on how to obtain these keys can be found in <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
			<p>If you are using .NET, the code for this chapter can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2"><span class="No-Break">https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2</span></a><span class="No-Break">.</span></p>
			<p>If you are using Python, the code for this chapter can be found <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2"><span class="No-Break">https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2</span></a><span class="No-Break">.</span></p>
			<p>You can install the required packages by going to the GitHub repository and using the following: <strong class="source-inline">pip install -</strong><span class="No-Break"><strong class="source-inline">r requirements.txt</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>A simple plugin template</h1>
			<p>There are two simple ways of creating <span class="No-Break">prompt templates.</span></p>
			<p>The first is to <a id="_idIndexMarker136"/>generate the prompt from a string variable inside your code. This way is simple and convenient. We covered this method in the <em class="italic">Running a simple prompt</em> section of <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
			<p>The second is to use Semantic Kernel to help separate the development function from the prompt engineering function. As you saw in <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, you can create your requests to LLMs as functions in plugins. A plugin is a directory that contains multiple sub-directories, one per function. Each subdirectory will have exactly <span class="No-Break">two files:</span></p>
			<ul>
				<li>A text file <a id="_idIndexMarker137"/>called <strong class="source-inline">skprompt.txt</strong> that contains <span class="No-Break">the prompt</span></li>
				<li>A configuration <a id="_idIndexMarker138"/>file called <strong class="source-inline">config.json</strong> that contains the parameters that will be used in the <span class="No-Break">API call</span></li>
			</ul>
			<p>Since the prompt is maintained separately from the code, you, as an application developer, can focus on the code of your application and let a specialized prompt engineer work on the <span class="No-Break"><strong class="source-inline">skprompt.txt</strong></span><span class="No-Break"> files.</span></p>
			<p>In this chapter, we will focus on the second method – creating plugins in dedicated directories – because this method is more robust to changes. For example, if you are switching from <a id="_idIndexMarker139"/>Python to C# or using a new version of the .NET library and these changes require a lot of changes to be made to your code, at least you don’t need to change your prompts and <span class="No-Break">function configurations.</span></p>
			<p>The code for this prompt plugin is like the one we used in the previous chapter. We are going to take the plugin we built in <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> and make several changes to <strong class="source-inline">skprompt.txt</strong> to observe the results and learn how different prompting techniques can substantially <span class="No-Break">change outcomes.</span></p>
			<p>We are doing this again in detail at the beginning of this chapter even though we did something very similar in <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> because we’re going to use these steps several times as we explore <span class="No-Break">prompt engineering.</span></p>
			<p>For the examples we’ll cover here, you can use both GPT-3.5 and GPT-4, but remember that GPT-4 is 30x more expensive. The results that are shown are from GPT-3.5 unless <span class="No-Break">indicated otherwise.</span></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>The skprompt.txt file</h2>
			<p>We will <a id="_idIndexMarker140"/>start with a very simple prompt, just asking <a id="_idIndexMarker141"/>directly what we want, without any <span class="No-Break">additional context:</span></p>
			<pre class="source-code">
Create an itinerary of three must-see attractions in {{$city}}.</pre>			<p>That’s it – that’s the <span class="No-Break">whole file.</span></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>The config.json file</h2>
			<p>The <strong class="source-inline">config.json</strong> file is <a id="_idIndexMarker142"/>very similar to the one we used in <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, but we <a id="_idIndexMarker143"/>changed <span class="No-Break">three things:</span></p>
			<ul>
				<li>The description of <span class="No-Break">the function</span></li>
				<li>The name <a id="_idIndexMarker144"/>of the input variable <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">input_variables</strong></span></li>
				<li>The <a id="_idIndexMarker145"/>description of the input variable <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">input_variables</strong></span></li>
			</ul>
			<pre class="source-code">
{
    "schema": 1,
    "type": "completion",
    "description": "Creates a list of three must-see attractions for someone traveling to a city",
    "default_services": [
        "gpt35"
    ],
    "execution_settings": {
        "default": {
            "temperature": 0.8,
            "number_of_responses": 1,
            "top_p": 1,
            "max_tokens": 4000,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0
        }
    },
    "input_variables": [
        {
            "name": "city",
            "description": "The city the person wants to travel to",
            "required": true
        }
    ]
}</pre>			<p>Now that <a id="_idIndexMarker146"/>we have <a id="_idIndexMarker147"/>defined the function, let’s <span class="No-Break">call it.</span></p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Calling the plugin from Python</h2>
			<p>The code we’re using here is very similar to what we used in <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. We’ve made three <a id="_idIndexMarker148"/>small changes: use only GPT-3.5, point to <a id="_idIndexMarker149"/>the appropriate plugin directory, <strong class="source-inline">prompt_engineering</strong>, and change the input variable’s name from <strong class="source-inline">input</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">city</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
import asyncio
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
from semantic_kernel.utils.settings import openai_settings_from_dot_env
import semantic_kernel as sk
from semantic_kernel.functions.kernel_arguments import KernelArguments
async def main():
    kernel = sk.Kernel()
    api_key, org_id = openai_settings_from_dot_env()
    gpt35 = OpenAIChatCompletion("gpt-3.5-turbo", api_key, org_id, "gpt35")
    kernel.add_service(gpt35)
    pe_plugin = kernel.add_plugin(None, parent_directory="../../plugins", plugin_name="prompt_engineering")
    response = await kernel.invoke(pe_plugin["attractions_single_variable"], KernelArguments(city="New York City"))
    print(response)
if __name__ == "__main__":
    asyncio.run(main())</pre>			<p>Now, let’s learn <a id="_idIndexMarker150"/>how to call the plugin in C#. Note <a id="_idIndexMarker151"/>that we don’t make any changes to <strong class="source-inline">skprompt.txt</strong> or <strong class="source-inline">config.json</strong>. We can use the same prompt and configuration, independent of <span class="No-Break">the language.</span></p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>Calling the plugin from C#</h2>
			<p>As we <a id="_idIndexMarker152"/>did in Python, we just need to make three small <a id="_idIndexMarker153"/>changes: use only GPT-3.5, point to the appropriate plugin directory, <strong class="source-inline">prompt_engineering</strong>, and change the input variable’s name from <strong class="source-inline">input</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">city</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
using Microsoft.SemanticKernel;
var (apiKey, orgId) = Settings.LoadFromFile();
Kernel kernel = Kernel.CreateBuilder()
                        .AddOpenAIChatCompletion("gpt-3.5-turbo", apiKey, orgId, serviceId: "gpt35")
                        .Build();
var pluginsDirectory = Path.Combine(System.IO.Directory.GetCurrentDirectory(),
        "..", "..", "..", "plugins", "prompt_engineering");
var promptPlugin = kernel.ImportPluginFromPromptDirectory(pluginsDirectory, "prompt_engineering");
var result = await kernel.InvokeAsync(promptPlugin["attractions_single_variable"], new KernelArguments() {["city"] = "New York City"});
Console.WriteLine(result);</pre>			<p>Let’s <a id="_idIndexMarker154"/>see <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker155"/></span><span class="No-Break">results.</span></p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Results</h2>
			<p>Instead of giving the top three attractions that I wanted to see, in one of the calls, GPT-3.5 created a 3-day list of attractions and gave me a list of six attractions. This is a typical problem <a id="_idIndexMarker156"/>that can happen when the prompt is not very specific. The list of attractions is very good and considers how hard it is to move around in New York City due to traffic: attractions on the same day are close to each other, or at least there is enough time to travel between them. Since a lot of GPT-3.5 training was done with data during the COVID period, the itinerary even includes a note to check for <span class="No-Break">COVID closures.</span></p>
			<p>Note that your result may be different, and in some cases, you may get a response that looks like the original intent, with only three <span class="No-Break">must-see attractions:</span></p>
			<pre class="console">
Itinerary: Must-See Attractions in New York City
Day 1:
1. Statue of Liberty and Ellis Island: Start your visit to New York City by taking a ferry to the Statue of Liberty and Ellis Island. Explore the grounds of Lady Liberty, marvel at this iconic symbol of freedom and take in stunning views of the city skyline from the observation deck. Then, head to Ellis Island to learn about the millions of immigrants who passed through its gates and contributed to the cultural fabric of America.
2. Times Square: In the afternoon, immerse yourself in the vibrant energy of Times Square. This bustling, neon-lit intersection is known for its towering billboards, dazzling Broadway theaters, and bustling crowds. Take a stroll along the pedestrian-friendly plazas, snap photos in front of iconic landmarks like the TKTS booth, and soak up the excitement of this iconic New York City landmark.
Day 2:
3. The Metropolitan Museum of Art (The Met): Spend the morning exploring one of the world's largest and most famous art museums, The Met. This cultural treasure houses an extensive collection spanning 5,000 years of human history, encompassing art from various regions and civilizations. Admire masterpieces from ancient Egypt, classical antiquity, European Renaissance artists, and contemporary art. Don't miss the rooftop garden for panoramic views of Central Park and the Manhattan skyline.
4. Central Park: After the museum, take a leisurely stroll through Central Park, an urban oasis in the heart of the city. This sprawling green space offers a refreshing break from the bustling streets. Enjoy a picnic near the Bethesda Terrace and Fountain, rent a rowboat on the lake, visit the Central Park Zoo, or simply relax and people-watch in this iconic park.
Day 3:
5. The High Line: Start your day with a visit to the High Line, a unique elevated park built on a historic freight rail line. This linear park stretches for 1.45 miles and offers stunning views of the city, beautifully landscaped gardens, public art installations, and a variety of seating areas. Take a leisurely walk along the promenade, enjoy the greenery, and appreciate the innovative urban design.
6. The 9/11 Memorial &amp; Museum: In the afternoon, visit the 9/11 Memorial &amp; Museum, a deeply moving tribute to the victims of the September 11, 2001, terrorist attacks. The memorial features two reflecting pools set in the footprints of the Twin Towers, surrounded by bronze parapets inscribed with the names of those who lost their lives. Inside the museum, you'll find artifacts, multimedia displays, and personal accounts that document the events and aftermath of 9/11.
Note: This itinerary includes popular attractions in New York City, but it's always worth checking for any COVID-19-related restrictions or closures before planning your visit.</pre>			<p>This quick, simple <a id="_idIndexMarker157"/>prompt didn’t work consistently as expected, so let’s explore how to make things better by adding more information to <span class="No-Break">the prompt.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Improving the prompt to get better results</h1>
			<p>If the main way you interact with LLMs is using ChatGPT or Microsoft Copilot, you may have the impression <a id="_idIndexMarker158"/>that you can use very short prompts to get the results you want. As explained in <a href="B21826_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, these applications modify your submission and add a lot of instructions to your prompt. As a developer, you’ll have to do the same. Here are a few tips to improve your prompt and obtain <span class="No-Break">better results:</span></p>
			<ul>
				<li><strong class="bold">Provide context</strong>: Instead of simply asking for three must-see attractions, provide as much context as you can. Think about everything that you would ask someone who asked you the same question – for example, “<em class="italic">How many days are you staying?</em>,” “<em class="italic">What kind of things do you like and dislike?</em>,” or “<em class="italic">How are you going to be </em><span class="No-Break"><em class="italic">getting around?</em></span><span class="No-Break">”</span></li>
				<li><strong class="bold">Be explicit about the format of the response</strong>: If you want the answer to come out in a specific format, make sure you tell the LLM this. You may even provide some examples – for example, “<em class="italic">Answer with a single word – ‘Yes’ </em><span class="No-Break"><em class="italic">or ‘No.’</em></span><span class="No-Break">”</span></li>
				<li><strong class="bold">Specify the response’s length</strong>: If you want a concise answer, specify the desired length – for example, “<em class="italic">In two sentences.</em>” If you want a longer answer, make it clear – for example, “<em class="italic">600 words</em>” or “<span class="No-Break"><em class="italic">five paragraphs.</em></span><span class="No-Break">”</span></li>
			</ul>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>Revising the skprompt.txt file</h2>
			<p>Using <a id="_idIndexMarker159"/>these hints, we’re going to rewrite the prompt to see if we can get improved <span class="No-Break">responses consistently:</span></p>
			<pre class="source-code">
I am traveling to {{$city}} and staying only 24 hours.
I am afraid of heights.
I will be downtown.
I like movies and TV shows a lot.
I am traveling by myself.
I will get around using public transportation or Uber.
I will be there on a Saturday.
Create an itinerary of three attractions for me to visit.
The attractions should be listed in the following format:
- Attraction name: [name]
- What is the best thing to see there: [one sentence description]
Your whole answer should be less than 500 words.</pre>			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>The result</h2>
			<p>Here’s <a id="_idIndexMarker160"/>the response we get from <span class="No-Break">the prompt:</span></p>
			<pre class="console">
Itinerary for a 24-hour trip to New York City:
Attraction 1: Times Square
- What is the best thing to see there: The vibrant neon lights, billboards, and bustling atmosphere of one of the world's most famous intersections.
Start your day by visiting Times Square, the iconic heart of New York City. Immerse yourself in the vibrant energy and larger-than-life atmosphere of this bustling area. Marvel at the enormous billboards and screens displaying advertisements for Broadway shows, movies, and various brands. While you're there, make sure to take a stroll along the pedestrian plazas, people-watch, and soak in the bustling activity. Don't forget to snap some photos to capture the essence of this iconic location.
Attraction 2: Central Park
- What is the best thing to see there: Explore the beautiful green landscape, visit popular landmarks like Bethesda Terrace and Strawberry Fields, and enjoy a peaceful escape from the city.
Escape the concrete jungle and head to Central Park, an oasis of tranquility and natural beauty right in the heart of Manhattan. Take a leisurely walk through the park's lush greenery, visit famous spots such as Bethesda Terrace and Bethesda Fountain, and enjoy the peacefulness that surrounds you. If you're a fan of movies or TV shows, keep an eye out for locations you may recognize from various film shoots. Grab a coffee or snack from one of the park's vendors and find a cozy spot to relax and people-watch, or rent a bike and explore the park's vast expanse at your own pace.
Attraction 3: Museum of the Moving Image
- What is the best thing to see there: Delve into the world of movies and TV shows through interactive exhibits, iconic props, and behind-the-scenes insights.
As a movie and TV show enthusiast, a visit to the Museum of the Moving Image in Queens is a must. This unique museum offers a fascinating exploration of the history, art, and technology behind the moving image. Get a glimpse of iconic props, costumes, and memorabilia from beloved films and TV shows. Engage in interactive exhibits that allow you to create your own animations or dub your voice into famous movie scenes. Learn about the evolution of filmmaking techniques and gain behind-the-scenes insights into the process of creating the entertainment you love. With its fascinating exhibits and hands-on experiences, the Museum of the Moving Image is a perfect destination for any movie and TV aficionado.
Remember to plan your itinerary in a way that allows for travel time between attractions, especially if you're relying on public transportation or Uber. Be sure to check opening hours and any reservation requirements for the attractions you wish to visit, as they may vary. Enjoy your 24-hour adventure in the vibrant city of New York!</pre>			<p>Note that <a id="_idIndexMarker161"/>we’ve created the prompt in such a way that you can use it with different cities. Here’s a run with London. Note that the response acknowledged my fear of heights, but simply told me to <span class="No-Break">overcome it:</span></p>
			<pre class="console">
Attraction 1: Warner Bros. Studio Tour London - The Making of Harry Potter
- What is the best thing to see there: Immerse yourself in the magical world of Harry Potter by exploring the actual sets, costumes, and props used in the film series.
Start your day by visiting the Warner Bros. Studio Tour London - The Making of Harry Potter. As a fan of movies and TV shows, this attraction will be a dream come true for you. Located just outside of London, this studio tour offers a unique behind-the-scenes experience of the Harry Potter film series.
Attraction 2: The British Museum
- What is the best thing to see there: Discover the vast collection of art and artifacts from around the world, including the Rosetta Stone and the Egyptian mummies.
Next, head to The British Museum, located in the heart of downtown London. This world-renowned museum houses a vast collection of art and historical artifacts, spanning over two million years of human history. As a lover of movies and TV shows, you'll appreciate the opportunity to see real-life artifacts and treasures from various civilizations.
Attraction 3: The London Eye
- What is the best thing to see there: Overcome your fear of heights and enjoy breathtaking panoramic views of London's iconic landmarks.
To end your day, challenge yourself and conquer your fear of heights by visiting the London Eye. This giant Ferris wheel, located on the South Bank of the River Thames, offers spectacular views of the city's skyline. Take a ride in one of the glass capsules and enjoy a 30-minute rotation, offering uninterrupted 360-degree views of London.
With 24 hours in London, these three attractions will provide you with a well-rounded experience that combines your love for movies and TV shows with iconic landmarks and breathtaking views. Don't forget to make use of public transportation or Uber to efficiently move between these attractions and make the most of your limited time in the city. Enjoy your adventure!</pre>			<p>For many <a id="_idIndexMarker162"/>applications, you’ll need to make your prompt more configurable, and that will require adding more variables. We’ll do <span class="No-Break">that next.</span></p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Prompts with multiple variables</h1>
			<p>You can parameterize a lot of information in a prompt. For example, in our prompt that finds the <a id="_idIndexMarker163"/>best attractions, you can add multiple variables, such as the number of days that the person is staying, the things they like and dislike, and how many attractions they want <span class="No-Break">to see.</span></p>
			<p>In such a case, our prompt will become more complex, so we will need to create a new <strong class="source-inline">skprompt.txt</strong> file. Since our prompt will be a new function and have multiple parameters, we will also need to create a new <span class="No-Break"><strong class="source-inline">config.json</strong></span><span class="No-Break"> file.</span></p>
			<p>These two files can be found in the <span class="No-Break"><strong class="source-inline">plugins/prompt_engineering/attractions_multiple_variables</strong></span><span class="No-Break"> folder.</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Skprompt.txt</p>
			<p>To add more variables to a prompt, simply add them within double curly brackets with a dollar sign before their name. The following code shows how to add several variables (<strong class="source-inline">city</strong>, <strong class="source-inline">n_days</strong>, <strong class="source-inline">likes</strong>, <strong class="source-inline">dislikes</strong>, and <strong class="source-inline">n_attractions</strong>) to a <span class="No-Break">single prompt:</span></p>
			<pre class="source-code">
I am traveling to {{$city}} and staying {{$n_days}} days.
I like {{$likes}}.
I dislike {{$dislikes}}.
I am traveling by myself.
Create an itinerary of up to {{$n_attractions}} must-see attractions for me to visit.
The attractions should be listed in the following format:
- Attraction name: [name]
- What is the best thing to see there: [one sentence description]
Your whole answer should be less than 300 words.</pre>			<p>Now, let’s <a id="_idIndexMarker164"/>see the changes in the <span class="No-Break">function configuration.</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Config.json</p>
			<p>Our <strong class="source-inline">config.json</strong> file for multiple variables is almost the same as the one we use with a single variable, but we need to add the details for all <span class="No-Break">the variables:</span></p>
			<pre class="source-code">
    "input_variables": [
        {
            "name": "city",
            "description": "The city the person wants to travel to",
            "required": true
        },
        {
            "name": "n_days",
            "description": "The number of days the person will be in the city",
            "required": true
        },
        {
            "name": "likes",
            "description": "The interests of the person traveling to the city",
            "required": true
        },
        {
            "name": "dislikes",
            "description": "The dislikes of the person traveling to the city",
            "required": true
        },
        {
            "name": "n_attractions",
            "description": "The number of attractions to recommend",
            "required": true
        }
    ]</pre>			<p>Now that <a id="_idIndexMarker165"/>we’ve configured the new function, let’s learn how to call it in Python <span class="No-Break">and C#.</span></p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor058"/>Requesting a complex itinerary with Python</h2>
			<p>Compared <a id="_idIndexMarker166"/>to calling a <a id="_idIndexMarker167"/>prompt with a single parameter, the <a id="_idIndexMarker168"/>only change we need to make to call a prompt with multiple parameters is on the <strong class="source-inline">KernelArguments</strong> object. When passing <strong class="source-inline">KernelArguments</strong> as a parameter to <strong class="source-inline">kernel.invoke</strong>, we must add all the parameters we need to the object, as shown here. One thing to note is that <a id="_idIndexMarker169"/>the parameters <a id="_idIndexMarker170"/>are all strings since LLMs <a id="_idIndexMarker171"/>work best <span class="No-Break">with text:</span></p>
			<pre class="source-code">
        response = await kernel.invoke(pe_plugin["attractions_multiple_variables"], KernelArguments(
        city = "New York City",
        n_days = "3",
        likes = "restaurants, Ghostbusters, Friends tv show",
        dislikes = "museums, parks",
        n_attractions = "5"
    ))</pre>			<p>Let’s see the <span class="No-Break">C# code.</span></p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Requesting a complex itinerary with C#</h2>
			<p>This <a id="_idIndexMarker172"/>time, we will create a <strong class="source-inline">KernelArguments</strong> object called <strong class="source-inline">function_arguments</strong> outside of the function invocation <a id="_idIndexMarker173"/>and pre-fill the five variables <a id="_idIndexMarker174"/>with the content we want. Then, we will pass this object to the <span class="No-Break">invocation call:</span></p>
			<pre class="source-code">
 var function_arguments = new KernelArguments()
    {["city"] = "New York City",
    ["n_days"] = "3",
    ["likes"] = "restaurants, Ghostbusters, Friends tv show",
    ["dislikes"] = "museums, parks",
    ["n_attractions"] = "5"};
var result = await kernel.InvokeAsync(promptPlugin["attractions_multiple_variables"], function_arguments );</pre>			<p>Now, let’s see <span class="No-Break">the results.</span></p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/>The result of the complex itinerary</h2>
			<p>The result considers all the input variables. It suggests a park – the High Line – even though I <a id="_idIndexMarker175"/>told the LLM that I dislike parks. It does explain that it is a very unusual park, and if you know New York, it doesn’t feel like a park at all. I think that a person who doesn’t enjoy the traditional park experience would enjoy the High Line, so the LLM did a very good job once it got <span class="No-Break">more context:</span></p>
			<pre class="console">
Itinerary for Three Days in New York City:
Day 1:
1. Attraction name: Ghostbusters Firehouse
   What is the best thing to see there: Visit the iconic firehouse featured in the Ghostbusters movies and take pictures in front of the famous logo.
2. Attraction name: Friends Apartment Building
   What is the best thing to see there: Pay a visit to the apartment building that served as the exterior shot for Monica and Rachel's apartment in the beloved TV show Friends.
Day 2:
3. Attraction name: Restaurant Row
   What is the best thing to see there: Explore Restaurant Row on West 46th Street, known for its diverse culinary scene, offering a plethora of international cuisines to satisfy your food cravings.
4. Attraction name: High Line Park
   What is the best thing to see there: Although you mentioned disliking parks, the High Line is a unique urban park built on a historic freight rail line, offering beautiful views of the city and a different experience compared to traditional parks.
Day 3:
5. Attraction name: Times Square
   What is the best thing to see there: Immerse yourself in the vibrant atmosphere of Times Square, known for its dazzling billboards, bustling streets, and renowned theaters, making it a must-see destination in NYC.
This itinerary focuses on your interests while also incorporating some iconic NYC experiences. You can explore famous film locations like the Ghostbusters Firehouse and Friends Apartment Building. Since you enjoy restaurants, Restaurant Row will offer a variety of dining options to suit your taste. Although you specified disliking parks, the High Line Park provides a unique urban green space experience. Finally, no trip to NYC would be complete without a visit to the energetic Times Square. Enjoy your trip!</pre>			<p>With that, we’ve learned how to improve our prompts to get better results. However, there are some <a id="_idIndexMarker176"/>cases in which LLMs fail to provide good answers, even after we use all the techniques mentioned previously. A very common case is solving math problems. We will explore <span class="No-Break">this next.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Issues when answering math problems</h1>
			<p>Although the results <a id="_idIndexMarker177"/>from LLMs are impressive, sometimes, models can get confused by seemingly simple questions. This tends to be more frequent when the question involves math. For example, I ran the following prompt in GPT-3.5 <span class="No-Break">five times:</span></p>
			<pre class="source-code">
When I was 6 my sister was half my age.
Now I'm 70. How old is my sister?</pre>			<p>The plugin I used to run this prompt is <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">plugins/prompt_engineering/solve_math_problem</strong></span><span class="No-Break">.</span></p>
			<p>The correct answer is 67 because when I was 6, my sister was 3. Now, 64 years later, I’m 70, so she would <span class="No-Break">be 67.</span></p>
			<p>Here are the <a id="_idIndexMarker178"/>results of the five runs on GPT-3.5. The first result was incorrect, saying that “<em class="italic">my sister is 64 years younger than her </em><span class="No-Break"><em class="italic">current age</em></span><span class="No-Break">:”</span></p>
			<pre class="console">
If your sister was half your age when you were 6, that means she was 3 years younger than you at that time (6/2 = 3).
Since then, you have aged 64 years (70 - 6 = 64). Therefore, your sister would also be 64 years younger than her current age, which would make her 70 - 64 = 6 years old.
So, your sister is currently 6 years old.</pre>			<p>The second and third attempts gave the correct answer. Here’s the <span class="No-Break">second attempt:</span></p>
			<pre class="console">
If your sister was half your age when you were 6, it means she was 6/2 = &lt;&lt;6/2=3&gt;&gt;3 years old at that time.
Since then, there is a constant age difference of 3 years between the two of you.
Now that you are 70, your sister would be 70 - 3 = &lt;&lt;70-3=67&gt;&gt;67 years old.</pre>			<p>Here’s the <span class="No-Break">third attempt:</span></p>
			<pre class="console">
If your sister was half your age when you were 6, that means she was 3 years old at that time.
Since then, the age difference between you and your sister remains constant.
Therefore, if you are currently 70 years old, your sister would be 67 years old.</pre>			<p>But the fourth and fifth attempts gave wrong answers again. Here’s the <span class="No-Break">fourth attempt:</span></p>
			<pre class="console">
If your sister was half your age when you were 6, that means she was 3 years old at that time. Since then, the age difference between you and your sister remains constant. Therefore, if you are currently 70 years old, your sister is 73 years old.</pre>			<p>Here’s the <span class="No-Break">fifth attempt:</span></p>
			<pre class="console">
If your sister was half your age when you were 6, that means she was 3 years younger than you at that time.
Now, you are 70, which means there is a 67-year age difference between when you were 6 and now.
So, if your sister was 3 years younger than you when you were 6, she would be 67 years younger than you now.
Therefore, your sister would be 70 - 67 = 3 years old.</pre>			<p>Therefore, in five <a id="_idIndexMarker179"/>attempts, only two (40%) returned correct answers, which is not an experience that you want your users <span class="No-Break">to have.</span></p>
			<p>We will learn how to address this issue in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>Multistage prompts</h1>
			<p>One way to <a id="_idIndexMarker180"/>improve the accuracy of LLMs when doing math is to use multistage prompts. In this technique, the answer from the first prompt is passed to the second as a parameter. We’re going to illustrate this with the <strong class="bold">Chain-of-Thought</strong> (<span class="No-Break"><strong class="bold">CoT</strong></span><span class="No-Break">) technique.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor063"/>CoT – “Let’s think step by step”</h2>
			<p>In the <a id="_idIndexMarker181"/>paper <em class="italic">Large Language Models are Zero-Shot Reasoners</em> [1], the authors found that simply adding “Let’s think step <a id="_idIndexMarker182"/>by step” right after the question can help improve the accuracy of LLMs a lot. Their proposed process works <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Ask the intended question, but instead of asking the LLM to answer, simply append “<em class="italic">Let’s think step by step</em>” at <span class="No-Break">the end.</span></li>
				<li>The LLM will answer with a process to answer <span class="No-Break">the question.</span></li>
				<li>Combine the question from <em class="italic">step 1</em> with the process from s<em class="italic">tep 2</em> in a new prompt, and finish with “<em class="italic">Therefore, the </em><span class="No-Break"><em class="italic">answer is…</em></span><span class="No-Break">:”</span></li>
			</ol>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B21826_02_1.jpg" alt="Figure 2.1 – The Zero-shot-CoT method" width="1650" height="877"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The Zero-shot-CoT method</p>
			<p>They called their process <em class="italic">Zero-shot-Chain-of-Thought</em> or <em class="italic">Zero-shot-CoT</em>. The <em class="italic">Zero-shot</em> part means that you don’t need to give any example answers to the LLM; you can give the <a id="_idIndexMarker183"/>question directly. This is to differentiate the process from <strong class="bold">few-shot</strong>, which is when you provide the LLM several examples of expected <a id="_idIndexMarker184"/>answers in your prompt, making the prompt substantially larger. The <em class="italic">CoT</em> part describes the process of asking the LLM to provide a reasoning framework and adding the LLM’s reasoning to <span class="No-Break">the question.</span></p>
			<p>The authors tested several different phrases to obtain the CoT from the LLM, such as “<em class="italic">Let’s think about it logically</em>” and “<em class="italic">Let’s think like a detective step by step</em>,” and found that simply “<em class="italic">Let’s think step by step</em>” yielded the <span class="No-Break">best results.</span></p>
			<h3>Implementing Zero-shot-CoT</h3>
			<p>We need two prompts – one for both steps. For the first step, we will call <strong class="source-inline">solve_math_problem_v2</strong>. The prompt simply restates the problem and adds “<em class="italic">Let’s think step by step</em>” at <span class="No-Break">the end:</span></p>
			<pre class="source-code">
{{$problem}}
Let's think step by step.</pre>			<p>The prompt <a id="_idIndexMarker185"/>for the second step, which we will call <strong class="source-inline">chain_of_thought</strong>, repeats the first prompt, includes the answer for <a id="_idIndexMarker186"/>the first prompt, and then asks for <span class="No-Break">the solution:</span></p>
			<pre class="source-code">
{{$problem}}
Let's think step by step.
{{$input}}
Therefore, the answer is…</pre>			<p>Given this prompt, the <strong class="source-inline">config.json</strong> file needs <span class="No-Break">two </span><span class="No-Break"><strong class="source-inline">input_variables</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
    "input_variables": [
        {
            "name": "problem",
            "description": "The problem that needs to be solved",
            "required": true
        },
        {
            "name": "input",
            "description": "The steps to solve the problem",
            "required": true
        }
    ]</pre>			<p>Let’s learn how to call <span class="No-Break">the prompts.</span></p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor064"/>Implementing CoT with Python</h2>
			<p>To make <a id="_idIndexMarker187"/>the steps explicit, I broke <a id="_idIndexMarker188"/>the program into two <a id="_idIndexMarker189"/>parts. The first one asks for the CoT and <span class="No-Break">shows it:</span></p>
			<pre class="source-code">
    problem = """When I was 6 my sister was half my age. Now I'm 70. How old is my sister?"""
    pe_plugin = kernel.add_plugin(None, parent_directory="../../plugins", plugin_name="prompt_engineering")
    solve_steps = await kernel.invoke(pe_plugin["solve_math_problem_v2"], KernelArguments(problem = problem))
    print(f"\n\nSteps: {str(solve_steps)}\n\n")</pre>			<p>The second part shows the answer. If all you care about is the answer, you don’t need to print the steps, but you still need to use the LLM to calculate the steps because they are a required parameter to <span class="No-Break">the CoT:</span></p>
			<pre class="source-code">
    response = await kernel.invoke(pe_plugin["chain_of_thought"], KernelArguments(problem = problem, input = solve_steps))
    print(f"\n\nFinal answer: {str(response)}\n\n")</pre>			<h2 id="_idParaDest-66"><a id="_idTextAnchor065"/>Implementing CoT with C#</h2>
			<p>As <a id="_idIndexMarker190"/>we did <a id="_idIndexMarker191"/>in Python, we <a id="_idIndexMarker192"/>can break the C# program into two parts. Part one will show the reasoning steps elicited by the <span class="No-Break">CoT prompt:</span></p>
			<pre class="source-code">
var problem = "When I was 6 my sister was half my age. Now I'm 70. How old is my sister?";
var chatFunctionVariables1 = new KernelArguments()
{
    ["problem"] = problem,
};
var steps = await kernel.InvokeAsync(promptPlugin["solve_math_problem_v2"], chatFunctionVariables1);</pre>			<p>The <a id="_idIndexMarker193"/>second part <a id="_idIndexMarker194"/>parses and reports <a id="_idIndexMarker195"/><span class="No-Break">the answer:</span></p>
			<pre class="source-code">
var chatFunctionVariables2 = new KernelArguments()
{
    ["problem"] = problem,
    ["input"] = steps.ToString()
};
var result = await kernel.InvokeAsync(promptPlugin["chain_of_thought_v2"], chatFunctionVariables2);Console.WriteLine(steps);</pre>			<p>Let’s see <span class="No-Break">the results.</span></p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor066"/>Results for CoT</h2>
			<p>I ran <a id="_idIndexMarker196"/>the program five times, and it got the right answer every time. The answers aren’t deterministic, so it may be that you run it on your machine and get one or two wrong answers. In the paper, the authors claimed that this method achieves 78.7% success in problems of this type, where the usual accuracy of LLMs is <span class="No-Break">around 17.7%.</span></p>
			<p>Let’s look at two sample responses. Here’s <span class="No-Break">the first:</span></p>
			<pre class="console">
Steps: When you were 6, your sister was half your age, which means she was 6/2 = 3 years old at that time.
The age difference between you and your sister remains the same over time, so the difference in your ages is 6 - 3 = 3 years.
Now that you are 70, your sister would be 70 - 3 = 67 years old.
Final answer:
Your sister is 67 years old.</pre>			<p>Here’s <span class="No-Break">the second:</span></p>
			<pre class="console">
Steps: When you were 6, your sister was half your age, so she was 6/2 = &lt;&lt;6/2=3&gt;&gt;3 years old.
Since then, the age difference between you and your sister remains constant, so your sister is 3 years younger than you.
Therefore, if you are now 70, your sister would be 70 - 3 = &lt;&lt;70-3=67&gt;&gt;67 years old.</pre>			<p>Instead of running the program manually, we can automate it. Let's <span class="No-Break">see how.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor067"/>An ensemble of answers</h2>
			<p>While the CoT technique <a id="_idIndexMarker197"/>helps a lot, we still have a 78.7% average accuracy, and that may not be enough. To address this problem, one technique that is frequently used is to ask the model the same question several times and compile only the most frequently <span class="No-Break">given answer.</span></p>
			<p>To achieve this, we will make a minor modification to our CoT prompt and call it <strong class="source-inline">chain_of_thought_v2</strong>. We will simply ask the LLM to answer in Arabic numerals, to make it easier to compare the answers in a <span class="No-Break">later step:</span></p>
			<pre class="source-code">
{{$problem}}
Let's think step by step.
{{$input}}
Using arabic numerals only, the answer is…</pre>			<p>We also need <a id="_idIndexMarker198"/>to change the program and ask it to run several times. For the next example, I chose <em class="italic">N</em> = 7. We will collect the answers and choose the answer that appears more frequently. Note that each call using this method is <em class="italic">N</em> times more expensive and takes <em class="italic">N</em> times longer than a single call. Accuracy is <span class="No-Break">not free.</span></p>
			<h3>Automatically running an ensemble with Python</h3>
			<p>Let’s run CoT seven times. For each time we run it, we’ll add the result to a list. Then, we’ll <a id="_idIndexMarker199"/>take advantage of the <strong class="source-inline">set</strong> data structure to quickly get the most <span class="No-Break">common element:</span></p>
			<pre class="source-code">
    responses = []
    for i in range(7):
        solve_steps = await kernel.invoke(pe_plugin["solve_math_problem_v2"], KernelArguments(problem = problem))
        response = await kernel.invoke(pe_plugin["chain_of_thought_v2"], KernelArguments(problem = problem, input = solve_steps))
        responses.append(int(str(response)))
    print("Responses:")
    print(responses)
    final_answer = max(set(responses), key = responses.count)
    print(f"Final answer: {final_answer}")</pre>			<p>Let’s see how to implement this <span class="No-Break">in C#.</span></p>
			<h3>Automatically running an ensemble with C#</h3>
			<p>The C# code <a id="_idIndexMarker200"/>uses the same idea as the Python code: we run the model seven times and store the results. Then, we search the results for the most <span class="No-Break">frequent answer:</span></p>
			<pre class="source-code">
var results = new List&lt;int&gt;();
for (int i = 0; i &lt; 7; i++)
{
    var chatFunctionVariables1 = new KernelArguments()
    {
        ["problem"] = problem,
    };
    var steps = await kernel.InvokeAsync(promptPlugin["solve_math_problem_v2"], chatFunctionVariables1);
    var chatFunctionVariables2 = new KernelArguments()
    {
        ["problem"] = problem,
        ["input"] = steps.ToString()
    };
    var result = await kernel.InvokeAsync(promptPlugin["chain_of_thought_v2"], chatFunctionVariables2);
    var resultInt = int.Parse(result.ToString());
    results.Add(resultInt);
}
var mostCommonResult = results.GroupBy(x =&gt; x)
    .OrderByDescending(x =&gt; x.Count())
    .First()
    .Key;
Console.WriteLine($"Your sister's age is {mostCommonResult}");</pre>			<p>Combining CoT and the ensemble method substantially increases the likelihood of getting a <a id="_idIndexMarker201"/>correct response. In the paper, the authors obtained 99.8% correct results, at the expense of making 10 LLM calls <span class="No-Break">per question.</span></p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Summary</h1>
			<p>In this chapter, you learned several techniques you can utilize to improve your prompts to obtain better results. You learned to use longer prompts that ensure that the LLM has the necessary context to provide the desired response. You also learned how to add multiple parameters to a prompt. Then, you learned how to chain prompts and how to implement the CoT method to help the LLM provide more accurate results. Finally, you learned how to ensemble several responses to increase accuracy. This accuracy, however, comes at <span class="No-Break">a cost.</span></p>
			<p>Now that we have mastered prompts, in the next chapter, we will explore how to customize plugins and their native and <span class="No-Break">semantic functions.</span></p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>References</h1>
			<p class="Bibliography">[1] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large Language Models are Zero-Shot Reasoners.” arXiv, Jan. 29, 2023. Accessed: Jun. 06, 2023. [Online]. <span class="No-Break">Available: </span><a href="http://arxiv.org/abs/2205.11916"><span class="No-Break">http://arxiv.org/abs/2205.11916</span></a></p>
		</div>
	</div>
</div>


<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer021" class="Content">
			<h1 id="_idParaDest-71" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor070"/>Part 2: Creating AI Applications with Semantic Kernel</h1>
		</div>
		<div id="_idContainer022">
			<p>In this part, we will go deep inside Semantic Kernel and learn how to use it to solve problems. We start by adding functions to a kernel, and then we use functions to solve a problem. The real power comes next when we ask the kernel to solve a problem on its own. Finally, we learn how to keep history for our kernel <span class="No-Break">using memory.</span></p>
			<p>This part includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21826_03.xhtml#_idTextAnchor071"><em class="italic">Chapter 3</em></a><em class="italic">, Extending Semantic Kernel</em></li>
				<li><a href="B21826_04.xhtml#_idTextAnchor086"><em class="italic">Chapter 4</em></a><em class="italic">, Performing Complex Actions by Chaining Functions</em></li>
				<li><a href="B21826_05.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a><em class="italic">, Programming with Planners</em></li>
				<li><a href="B21826_06.xhtml#_idTextAnchor120"><em class="italic">Chapter 6</em></a><em class="italic">, Adding Memories to Your AI Application</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer023">
			</div>
		</div>
		<div>
			<div id="_idContainer024" class="Basic-Graphics-Frame">
			</div>
		</div>
	</div>
</div>
</body></html>