- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accelerate Data Engineering on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Code assistance options with AWS services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code assistance integration with AWS Glue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code assistance integration with Amazon EMR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code assistance integration with AWS Lambda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code assistance integration with Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code assistance integration with Amazon Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous part of the book, we explored auto-code generation techniques
    and the integration of a code companion with **integrated development environments**
    (**IDEs**) and provided examples using JetBrains PyCharm IDE with Amazon Q Developer
    for different languages that developers use very often. In this chapter, we will
    specifically focus on how Amazon is expanding in the area of assisting code developers
    by integrating with core AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: Code assistance options with AWS services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS users select diverse services, considering factors such as the unique requirements
    of their projects, use cases, developers’ technical needs, developer preferences,
    and the characteristics of AWS services. To cater to various developer personas,
    such as data engineers, data scientists, application developers, and so on, AWS
    has integrated code assistance with many of its code services. If you are an application
    builder, software developer, data engineer, or data scientist working with AWS
    services, you would frequently use builder-friendly tools such as Amazon SageMaker
    as a platform for building AI / **machine learning** (**ML**) projects, Amazon
    EMR as a platform for building big data processing projects, AWS Glue for building
    **extract, transform, and load** (**ETL**) pipelines, AWS Lambda as a serverless
    compute service for application development. All these services provide tools
    that help builders and developers write code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Code assistance options with AWS services](img/B21378_14_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Code assistance options with AWS services
  prefs: []
  type: TYPE_NORMAL
- en: As of the writing of this book, AWS has integrated Amazon Q Developer with AWS
    Glue, Amazon EMR, AWS Lambda, Amazon SageMaker, and Amazon Redshift. However,
    we anticipate that the list of services benefiting from code assistance, such
    as Amazon Q Developer, will continue to expand in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will dive deep into each of these services, examining
    their integration with Amazon Q in detail. We will provide examples that will
    be helpful for data engineers to accelerate development on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**Large language models** (**LLMs**), by nature, are non-deterministic, so
    you may not get the same code blocks shown in the code snapshots. However, logically,
    the generated code should meet the requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CodeWhisperer** is a legacy name from a service that merged with Amazon Q
    Developer. As of the time of writing this book, some of the integrations are still
    referred to as CodeWhisperer in the AWS console, which may change in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: Code assistance integration with AWS Glue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start diving deep into code assistance support for AWS Glue service,
    let’s quickly go through an overview of AWS Glue. **AWS Glue** is a serverless
    data integration service designed to simplify the process of discovering, preparing,
    moving, and integrating data from diverse sources, catering to analytics, ML,
    and application development needs. At the very high level, AWS Glue has the following
    major components, and each of them has multiple features to support data engineers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Glue Data Catalog**: It’s a centralized technical metadata repository. It
    stores metadata about data sources, transformations, and targets, providing a
    unified view of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glue Studio**: AWS Glue Studio offers a graphical interface that facilitates
    the seamless creation, execution, and monitoring of data integration jobs within
    AWS Glue. Additionally, it provides Jupyter notebooks for advanced developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Glue Studio is seamlessly integrated with Amazon Q Developer. Let’s explore
    the further functionality by considering a very common use case of data enrichment.
  prefs: []
  type: TYPE_NORMAL
- en: Use case for AWS Glue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Features and functionalities of any service or tool are best understood when
    we have a use case to solve. So, let’s start with one of the easy and widely used
    use cases of data enrichment using lookups.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data enrichment using lookup**: In a typical scenario, business analysts
    often require data enrichment by incorporating details associated with codes/IDs
    found in a column through a lookup table. The desired result is a comprehensive
    and denormalized record containing both the code and corresponding details in
    the same row. To address this specific use case, data engineers develop ETL jobs
    to join the tables, creating the final structure with a denormalized dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this use case, we will use yellow taxi trip records that encompass
    details such as the date and time of pick-up and drop-off, the locations for pick-up
    and drop-off, the trip distance, comprehensive fare breakdowns, various rate types,
    utilized payment methods, and passenger counts reported by the driver. Additionally,
    trip information incorporates passenger location codes for both pick-up and drop-off.
  prefs: []
  type: TYPE_NORMAL
- en: The business objective is to enhance the dataset with zone information based
    on the pick-up location code.
  prefs: []
  type: TYPE_NORMAL
- en: To meet this requirement, data engineers must develop a PySpark ETL script.
    This script should perform a lookup for zone information corresponding to the
    pick-up location code. Subsequently, the engineers create denormalized/enriched
    data by amalgamating yellow taxi trip data with detailed pick-up zone information
    and save the result as a file.
  prefs: []
  type: TYPE_NORMAL
- en: As a code developer / data engineer, you will need to convert the preceding
    business objectives into technical requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Solution blueprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Write a PySpark code to handle technical requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `yellow_tripdata_2023-01.parquet` file from the S3 location in a DataFrame
    and display a sample of 10 records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the `taxi+_zone_lookup.csv` file from the S3 location in a DataFrame and
    display a sample of 10 records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a left outer join on `yellow_tripdata_2023-01.parquet` and `taxi+_zone_lookup.csv`
    on `PULocationID = LocationID` to gather pick-up zone information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in
    a new `glue_notebook_yellow_pick_up_zone_output` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For verification, download and check the files from the `glue_notebook_yellow_pick_up_zone_output`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have a use case defined, let’s go through the step-by-step solution
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step will be to prepare the data. To illustrate its functionality,
    in the following sections, we will utilize the publicly available NY Taxi dataset
    from TLC Trip Record Data. [https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we will download the required files on a local machine and then upload
    them in one of Amazon’s S3 buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Yellow Taxi Trip Records data for the Jan 2023 Parquet file (`yellow_tripdata_2023-01.parquet`)
    on a local machine from [https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file](img/B21378_14_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file
  prefs: []
  type: TYPE_NORMAL
- en: Download the Taxi Zone Lookup Table CSV file (`taxi+_zone_lookup.csv`) on a
    local machine from [https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv](https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.3 – The Zone Lookup Table CSV file](img/B21378_14_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – The Zone Lookup Table CSV file
  prefs: []
  type: TYPE_NORMAL
- en: Create the two `yellow_taxi_trip_records` and `zone_lookup` folders in Amazon
    S3, which we can reference in our Glue notebook job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.4 – S3 folders structure](img/B21378_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – S3 folders structure
  prefs: []
  type: TYPE_NORMAL
- en: Upload the `yellow_tripdata_2023-01.parquet` file to the `yellow_taxi_trip_records`
    folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.5 – The yellow_taxi_tripdata_record file](img/B21378_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 – The yellow_taxi_tripdata_record file
  prefs: []
  type: TYPE_NORMAL
- en: Upload the `taxi+_zone_lookup.csv` file to the `zone_lookup` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.6 – The zone_lookup file](img/B21378_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 – The zone_lookup file
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will use the same dataset and use case to discover solutions using AWS Glue
    and Amazon EMR. For illustrative purposes, we have prepared the data manually.
    However, in a production environment, file transfers can be automated by leveraging
    various AWS services and/or third-party software.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s dive deep into a detailed exploration of the solution using the integration
    of Amazon Q Developer with an AWS Glue Studio notebook for the preceding use case.
  prefs: []
  type: TYPE_NORMAL
- en: Solution – Amazon Q Developer with an AWS Glue Studio notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first enable Amazon Q Developer with an AWS Glue Studio notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites to enable Amazon Q Developer with an AWS Glue Studio notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The developer is required to modify the **identity and access management** (**IAM**)
    policy associated with the IAM user or role to grant permissions for Amazon Q
    Developer to initiate recommendations in a Glue Studio notebook. Reference [*Chapter
    2*](B21378_02.xhtml#_idTextAnchor022) for the details to enable Amazon Q Developer
    with an AWS Glue Studio notebook.
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill the previously mentioned solution blueprint, we will use various
    auto-code generation techniques that were discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060).
    Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought
    prompts for auto-code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use Amazon Q Developer to auto-generate an end-to-end script in an AWS
    Glue Studio notebook. Here is the step-by-step solution walk-through for the previously
    defined solution blueprint.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, you need to write some PySpark code.
  prefs: []
  type: TYPE_NORMAL
- en: While creating a Glue Studio notebook, select the **Spark (Python)** engine
    and the role that has the Amazon Q Developer policy attached.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Create a Glue Studio notebook with PySpark](img/B21378_14_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 – Create a Glue Studio notebook with PySpark
  prefs: []
  type: TYPE_NORMAL
- en: Once you create the notebook, observe the kernel named `Glue PySpark`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel](img/B21378_14_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read the `yellow_tripdata_2023-01.parquet` file from the S3 location in a DataFrame
    and display a sample of 10 records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a chain-of-thought prompt technique with multiple single-line prompts
    in different cells to achieve the preceding requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using
    single-line prompts](img/B21378_14_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using single-line
    prompts
  prefs: []
  type: TYPE_NORMAL
- en: Observe that upon entering the Amazon Q Developer-enabled Glue Studio notebook
    prompt, it initiates code recommendations. Q Developer recognizes the file format
    as Parquet and suggests using the `spark.read.parquet` method. You can directly
    execute each cell/code from the notebook. Furthermore, as you move to the next
    cell, Q Developer utilizes “line-by-line recommendations” to suggest displaying
    the schema.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.10 – Line-by-line recommendations to display schema](img/B21378_14_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.10 – Line-by-line recommendations to display schema
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read the `taxi+_zone_lookup.csv` file from the S3 location in a DataFrame and
    display a sample of 10 records.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already explored the chain-of-thought prompt technique with multiple single-line
    prompts for *Requirement 2*. Now, let’s try with a multi-line prompt to achieve
    the preceding requirement and we will try to customize the code for the DataFrame
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line
    prompt](img/B21378_14_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line
    prompt
  prefs: []
  type: TYPE_NORMAL
- en: Observe that Amazon Q Developer understood the context behind the multi-line
    prompt and also the specific DataFrame name instructed in the prompt. It auto-generated
    multiple lines of code with the DataFrame name as `zone_df` and file format as
    CSV, suggesting the use of the `spark.read.csv` method to read CSV files. You
    can directly execute each cell/code from the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perform a left outer join on `yellow_tripdata_2023-01.parquet` and `taxi+_zone_lookup.csv`
    on `pulocationid = LocationID` to gather pick-up zone information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue using multi-line prompts and some code customization to achieve
    the preceding requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt](img/B21378_14_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s review the schema of the DataFrame returns by the code execution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.13 – Left outer join df and dataframe zone_df – display schema](img/B21378_14_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.13 – Left outer join df and dataframe zone_df – display schema
  prefs: []
  type: TYPE_NORMAL
- en: Observe that, as instructed in the multi-line prompt, Amazon Q Developer understood
    the context and auto-generated error-free code with the exact specifications we
    provided related to the DataFrame name of `yellow_pu_zone_df`. You can directly
    execute each cell/code from the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in
    a new folder called `glue_notebook_yellow_pick_up_zone_output`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the preceding requirement is straightforward and can be encapsulated
    in a single sentence, we will use a single-line prompt to generate the code, and
    we will also include a header to facilitate easy verification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.14 – Save the CSV file with enrichment pick-up location data](img/B21378_14_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.14 – Save the CSV file with enrichment pick-up location data
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For verification, download and check files from the `glue_notebook_yellow_pick_up_zone_output`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go to the Amazon S3 console to verify the files. Select one of the files
    and click **Download**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.15 – Save a CSV file with enrichment pick-up location data](img/B21378_14_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.15 – Save a CSV file with enrichment pick-up location data
  prefs: []
  type: TYPE_NORMAL
- en: After downloading the file, you can use any text editor to review the file contents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.16 – Verify the CSV file with enrichment pick-up location data](img/B21378_14_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.16 – Verify the CSV file with enrichment pick-up location data
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the CSV file has additional columns with zone information based
    on the pick-up location ID. In the next section, we will explore Amazon Q Developer
    integration with AWS Glue and use the chat assistant technique.
  prefs: []
  type: TYPE_NORMAL
- en: Think challenge
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill *Requirement 6*, if you are interested, attempt to utilize the same
    Glue Studio notebook for reading a CSV file, displaying sample records, and adding
    a header.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hint**: Use the multi-line prompt technique, similar to the one we used when
    reading the Zone Lookup file.'
  prefs: []
  type: TYPE_NORMAL
- en: Solution – Amazon Q Developer with AWS Glue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Q Developer provides a chat-style interface in the AWS Glue console.
    Now, let’s explore the integration between Amazon Q Developer and AWS Glue for
    the same use case and solution blueprint that we handled using Amazon Q Developer
    and an AWS Glue Studio notebook integration.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at the prerequisites to enable Amazon Q with AWS Glue.
  prefs: []
  type: TYPE_NORMAL
- en: To enable Amazon Q Developer integration with AWS Glue, we will need to update
    the IAM policy. Please refer to [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022)
    for additional details on initiating interaction with Amazon Q in AWS Glue.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s dive deep into a detailed exploration of the integration of Amazon
    Q Developer with AWS Glue Studio for the preceding use case.
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill the mentioned requirements, we will mainly use the chat companion
    that was discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a step-by-step solution walk-through that we’ll use as a prompt for
    all of the preceding requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part
    1](img/B21378_14_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part
    1
  prefs: []
  type: TYPE_NORMAL
- en: You can see that, based on the instruction provided to Amazon Q, it generated
    the skeleton on the ETL code. It generated code structure with Glue-PySpark libraries,
    a s3node with create dynamic dataframe to read parquet file, and a s3node with
    write dynamic dataframe to write CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2](img/B21378_14_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2
  prefs: []
  type: TYPE_NORMAL
- en: Observe that Amazon Q also provided technical details to explain the script
    flow. This can also be used to meet the in-script documentation needs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script
    summary](img/B21378_14_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script summary
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers with coding experience can easily reference the script summary
    and script skeleton to write end-to-end scripts to meet the solution blueprint.
    LLMs, by nature, are non-deterministic, so you may not get the same code blocks
    shown in the code snapshots.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the preceding use case illustration, AWS Glue integration with Amazon
    Q Developer with prompting techniques can be used by data engineers at a relatively
    lower experience level, while AWS Glue integration with Amazon Q Developer using
    the chat assistant can be utilized by ETL developers with relatively more experience.
  prefs: []
  type: TYPE_NORMAL
- en: Summary – Amazon Q Developer with an AWS Glue Studio notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As illustrated, we can automatically generate end-to-end, error-free, and executable
    code simply by providing prompts with specific requirements. Amazon Q Developer,
    integrated with an AWS Glue Studio notebook, comprehends the context and automatically
    generates PySpark code that can be run directly from the notebook without the
    need to provision any hardware upfront. This marks a significant advancement for
    many data engineers, relieving them from concerns about the technical intricacies
    associated with PySpark libraries, methods, and syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore code assistance integration with Amazon EMR.
  prefs: []
  type: TYPE_NORMAL
- en: Code assistance integration with Amazon EMR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive deep into the details of code assistance support for Amazon
    EMR, let’s quickly go through an overview of Amazon EMR. **Amazon EMR** is a cloud-based
    big data platform that simplifies the deployment, management, and scaling of various
    big data frameworks such as Apache Hadoop, Apache Spark, Apache Hive, and Apache
    HBase. At a high level, Amazon EMR comprises the following major components, each
    with multiple features to support data engineers and data scientists:'
  prefs: []
  type: TYPE_NORMAL
- en: '**EMR on EC2/EKS**: The Amazon EMR service provides two options, EMR on EC2
    and EMR on EKS, allowing customers to provision clusters. Amazon EMR streamlines
    the execution of batch jobs and interactive workloads for data analysts and engineers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EMR Serverless**: Amazon EMR Serverless is a serverless alternative within
    Amazon EMR. With Amazon EMR Serverless, users can access the full suite of features
    and advantages offered by Amazon EMR, all without requiring specialized expertise
    for cluster planning and management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EMR Studio**: EMR Studio supports data engineers and data scientists in developing,
    visualizing, and debugging applications within an IDE. It also provides a Jupyter
    Notebook environment for interactive coding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case for Amazon EMR Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity and ease of following Amazon Q Developer integration with Amazon
    EMR, we will use the same use case and data that we used in this chapter under
    the *Code assistance integration with AWS Glue* section. Refer to the *Use case
    for AWS Glue* section, which covers details related to the solution blueprint
    and data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Solution – Amazon Q Developer with Amazon EMR Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first enable Amazon Q Developer with Amazon EMR Studio. To enable Amazon
    Q Developer integration with Amazon EMR Studio, we will need to update the IAM
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisite to enable Amazon Q Developer with Amazon EMR Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The developer is required to modify the IAM policy associated with the role
    to grant permissions for Amazon Q Developer to initiate recommendations in EMR
    Studio. Please reference [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022) for additional
    details on initiating interaction with Amazon Q Developer in Amazon EMR Studio.
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill the mentioned requirements, we will use various auto-code generation
    techniques that were discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060).
    Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought
    prompts for auto-code generation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use Amazon Q Developer to auto-generate end-to-end scripts, which can
    achieve the following requirements in Amazon EMR Studio. Here is a step-by-step
    solution walk-through of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can observe lots of similarities between a Glue Studio notebook and an EMR
    Studio notebook when it comes to code recommended by Amazon Q Developer.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will need to write a PySpark code to handle technical requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Once you open Amazon EMR Studio, use **Launcher** to select **PySpark** from
    the **Notebook** section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.20 – Create an EMR Studio notebook with PySpark](img/B21378_14_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.20 – Create an EMR Studio notebook with PySpark
  prefs: []
  type: TYPE_NORMAL
- en: Once you create the notebook, you can see a kernel named `PySpark`. The kernel
    is a standalone process that runs in the background and executes the code you
    write in your notebooks. For more information, refer to the *References* section
    at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.21 – The EMR Studio notebook with a PySpark kernel](img/B21378_14_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.21 – The EMR Studio notebook with a PySpark kernel
  prefs: []
  type: TYPE_NORMAL
- en: I have already attached a cluster to my notebook, but you can explore different
    options to attach the compute to EMR studio in AWS documentation at [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html).
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read the `yellow_tripdata_2023-01.parquet` file from the S3 location in a DataFrame
    and display a sample of 10 records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use a chain-of-thought prompts technique with multiple single-line prompts
    in different cells to achieve this requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using
    single-line prompts](img/B21378_14_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using
    single-line prompts
  prefs: []
  type: TYPE_NORMAL
- en: Observe that upon entering the Amazon Q Developer-enabled EMR Studio notebook
    prompt, it initiates code recommendations. Amazon Q Developer recognizes the file
    format as Parquet and suggests using the `spark.read.parquet` method. You can
    directly execute each cell/code from the notebook. Furthermore, as you move to
    the next cell, Amazon Q Developer utilizes “line-by-line recommendations” to suggest
    displaying the schema.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Read the `taxi+_zone_lookup.csv` file from the S3 location in a DataFrame and
    display a sample of 10 records.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already explored the chain-of-thought prompts technique with multiple single-line
    prompts for *Requirement 2*. Now, let’s try a multi-line prompt to achieve this
    requirement and we will try to customize the code for the DataFrame name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line
    prompt](img/B21378_14_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line
    prompt
  prefs: []
  type: TYPE_NORMAL
- en: Observe that Amazon Q Developer understood the context behind the multi-line
    prompt and also the specific DataFrame name instructed in the prompt. It auto-generated
    multiple lines of code with the DataFrame name of `zone_df` and file format as
    CSV, suggesting the use of the `spark.read.csv` method to read CSV files. You
    can directly execute each cell/code from the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perform a left outer join on `yellow_tripdata_2023-01.parquet` and `taxi+_zone_lookup.csv`
    on `pulocationid = LocationID` to gather pick-up zone information.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue using multi-line prompts and some code customization to achieve
    the preceding requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt](img/B21378_14_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s review the schema of the DataFrame printed by the code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.25 – Left outer join df and dataframe zone_df – display schema](img/B21378_14_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.25 – Left outer join df and dataframe zone_df – display schema
  prefs: []
  type: TYPE_NORMAL
- en: Observe that, as instructed in the multi-line prompt, Amazon Q Developer understood
    the context and auto-generated error-free code with the exact specifications we
    provided related to the DataFrame named `yellow_pu_zone_df`. You can directly
    execute each cell/code from the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Save the preceding dataset as a CSV file in the previous Amazon S3 bucket in
    a new folder called `glue_notebook_yellow_pick_up_zone_output`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this requirement is straightforward and can be encapsulated in a single
    sentence, we will use a single-line prompt to generate the code, and we will also
    include a header to facilitate easy verification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.26 – Save the CSV file with enrichment pick-up location data](img/B21378_14_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.26 – Save the CSV file with enrichment pick-up location data
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For verification, download and check files from the `glue_notebook_yellow_pick_up_zone_output`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go to the Amazon S3 console to verify the files. Select one of the files
    and click **Download**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR
    Studio](img/B21378_14_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR
    Studio
  prefs: []
  type: TYPE_NORMAL
- en: After downloading, you can use a text editor to review the file contents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon
    EMR Studio](img/B21378_14_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon
    EMR Studio
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the CSV file has additional columns with zone information based
    on the pick-up location ID.
  prefs: []
  type: TYPE_NORMAL
- en: Summary – Amazon Q Developer with Amazon EMR Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As illustrated, we can automatically generate end-to-end, error-free, and executable
    code simply by providing prompts with specific requirements. Amazon Q Developer,
    integrated with an Amazon EMR Studio notebook, comprehends the context and automatically
    generates PySpark code that can be run directly from the notebook. This marks
    a significant advancement for many data engineers, relieving them from concerns
    about the technical intricacies associated with PySpark libraries, methods, and
    syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Think challenge
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill *Requirement 6*, if you are interested, attempt to utilize the same
    EMR Studio notebook for reading a CSV file, displaying sample records, and adding
    a header.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hint**: Use the multi-line prompt technique, similar to the one we used when
    reading the Zone Lookup file.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will consider an application developer persona to explore
    code assistance integration with AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Code assistance integration with AWS Lambda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start diving deep into code assistance support for the AWS Lambda
    service, let’s quickly go through an overview of AWS Lambda. **AWS Lambda** is
    a serverless computing service that allows users to run code without provisioning
    or managing servers. With Lambda, you can upload your code or use the available
    editor from the Lambda console. During the runtime of the code, based on the provided
    configurations, the service automatically takes care of the compute resources
    needed for execution. It is designed to be highly scalable, cost effective, and
    suitable for event-driven applications.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda supports multiple programming languages, including Node.js, Python,
    Java, Go, and .NET Core, allowing you to choose the language that best fits your
    application. Lambda can be easily integrated with other AWS services, enabling
    you to build complex and scalable architectures. It works seamlessly with services
    such as Amazon S3, DynamoDB, and API Gateway.
  prefs: []
  type: TYPE_NORMAL
- en: The AWS Lambda console is integrated with Amazon Q Developer to make it easy
    for developers to get coding assistance/recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Use case for AWS Lambda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with one of the easy and widely used use cases of converting file
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '**File format conversion**: In a typical scenario, once a file is received
    from an external team and/or source, it may not be in the target location and
    have the required name expected by the application. In that case, AWS Lambda can
    be used to quickly copy the file from the source location to the target location
    and rename the file at the target location.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this use case, let’s copy the NY Taxi Zone lookup file from the
    source location (`s3://<your-bucket-name>/zone_lookup/`) to the target location
    (`s3://<your-bucket-name>/source_lookup_file/`). Also, remove the special character
    (`+`) from the filename to save it as `taxi_zone_lookup.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: To meet this requirement, application developers must develop a Python script.
    This script should copy and rename the Zone Lookup file from the source to the
    target location.
  prefs: []
  type: TYPE_NORMAL
- en: As a code developer / data engineer, you will need to convert the preceding
    business objectives into the solution blueprint.
  prefs: []
  type: TYPE_NORMAL
- en: Solution blueprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Write a Python script to handle technical requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the `taxi+_zone_lookup.csv` file from S3 to the `zone_lookup` folder to
    the `source_lookup_file` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During copying, change `taxi+_zone_lookup.csv` to `taxi_zone_lookup.csv` in
    the target `source_lookup_file` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For verification, check the contents of the `source_lookup_file/taxi_zone_lookup.csv`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have a use case defined, let’s go through the step-by-step solution
    for it.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are using the same lookup file that we provisioned in this chapter under
    the *Code assistance integration with AWS Glue* section. Please refer to the *Use
    case for AWS Glue* section, which covers details related to data preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Solution – Amazon Q Developer with AWS Lambda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first enable Amazon Q Developer with the AWS Lambda console. To enable
    Amazon Q Developer integration with AWS Lambda, we will need to update the IAM
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisite to enable Amazon Q Developer with AWS Lambda
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The developer is required to modify the IAM policy associated with the IAM user
    or role to grant permissions for Amazon Q Developer to initiate recommendations
    in the AWS Lambda console. Please reference [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022)
    for additional details on initiating interaction with Amazon Q Developer in AWS
    Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: To let Amazon Q Developer start code suggestions, make sure to choose **Tools**
    | **Amazon CodeWhisperer** **Code Suggestions**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime](img/B21378_14_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill the mentioned requirements, we will use auto-code generation techniques
    that were discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060). Mainly,
    we will focus on the multi-line prompt for auto-code generation. Let’s use Amazon
    Q Developer to auto-generate an end-to-end script that can achieve the following
    requirements in AWS Lambda Console and EMR Studio. Here is a step-by-step solution
    walk-through of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You need to write a Python script to handle technical requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Once you open the AWS Lambda console, use the launcher to select a Python runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.30 – Create a Python runtime from AWS Lambda](img/B21378_14_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.30 – Create a Python runtime from AWS Lambda
  prefs: []
  type: TYPE_NORMAL
- en: Once you successfully create a Lambda function, observe that AWS Lambda creates
    a `lambda_function.py` file with some sample code. We can safely delete the sample
    code for this exercise, as we will use Amazon Q Developer to generate end-to-end
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python
    runtime](img/B21378_14_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python runtime
  prefs: []
  type: TYPE_NORMAL
- en: Let’s combine *Requirements 2* and *3*, as we are planning to use a multi-line
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements 2 and 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Copy the `taxi+_zone_lookup.csv` file from S3 to the `zone_lookup` folder to
    the `source_lookup_file` folder.
  prefs: []
  type: TYPE_NORMAL
- en: During copying, change the file name from `taxi+_zone_lookup.csv` to `taxi_zone_lookup.csv`
    in the target `source_lookup_file` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use multi-line prompts to auto-generate the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console](img/B21378_14_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console
  prefs: []
  type: TYPE_NORMAL
- en: Observe that Amazon Q Developer created a `lambda_handler` function and added
    `return code of 200` with a success message.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For verification, check the contents of the `source_lookup_file/taxi_zone_lookup.csv`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s deploy and use a test event to run Lambda code generated by Amazon Q Developer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.33 – Deploy AWS Lambda code](img/B21378_14_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.33 – Deploy AWS Lambda code
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s test the code by going to the **Test** tab and clicking the **Test**
    button. Since we are not passing any values to this Lambda function, the JSON
    event values from the **Test** tab do not matter in our case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.34 – Test AWS Lambda code](img/B21378_14_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.34 – Test AWS Lambda code
  prefs: []
  type: TYPE_NORMAL
- en: Once the Lambda code executes successfully, it will provide you with the details
    of the execution. Observe that the code is executed successfully and displays
    the returned code with a success message.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.35 – AWS Lambda code execution](img/B21378_14_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.35 – AWS Lambda code execution
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the Amazon S3 console to download and verify `s3://<your-bucket-name>/source_lookup_file/taxi_zone_lookup.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.36 – Target lookup file from Amazon S3](img/B21378_14_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.36 – Target lookup file from Amazon S3
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.37 – The Zone Lookup file](img/B21378_14_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.37 – The Zone Lookup file
  prefs: []
  type: TYPE_NORMAL
- en: Summary – Amazon Q Developer with AWS Lambda
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As illustrated, we can automatically generate end-to-end, error-free, and executable
    code simply by providing prompts with specific requirements. Amazon Q Developer,
    integrated with AWS Lambda, automatically generates the `lambda_handle``r` `()`
    function with return code based on the Lambda runtime environment selected. This
    integration can assist application developers with relatively limited coding experience
    in automatically generating Lambda functions with minor to no code changes.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with the application developer persona, next, we will explore the
    data scientist persona to investigate code assistance integration with Amazon
    SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Code assistance integration with Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start diving deep into code assistance support for the Amazon SageMaker
    service, let’s quickly go through an overview of Amazon SageMaker. **Amazon SageMaker**
    is a fully managed service that simplifies the process of building, training,
    and deploying ML models at scale. It is designed to make it easier for developers
    and data scientists to build, train, and deploy ML models without the need for
    extensive expertise in ML or deep learning. It has multiple features such as end-to-end
    workflow, built-in algorithms, custom model training, automatic model tuning,
    ground truth, edge manager, augmented AI, and managed notebooks, just to name
    a few. Amazon SageMaker integrates with other AWS services, such as Amazon S3
    for data storage, AWS Lambda for serverless inference, and Amazon CloudWatch for
    monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker Studio hosts the managed notebooks, which are integrated with
    Amazon Q Developer.
  prefs: []
  type: TYPE_NORMAL
- en: Use case for Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s use a very common business use case related to churn prediction for which
    data scientists use the XGBoost algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Churn prediction** in business involves utilizing data and algorithms to
    forecast which customers are at risk of discontinuing their usage of a product
    or service. The term “churn” commonly denotes customers ending subscriptions,
    discontinuing purchases, or ceasing service utilization. The primary objective
    of churn prediction is to identify these customers before they churn, enabling
    businesses to implement proactive measures for customer retention.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use publicly available direct marketing bank data to illustrate the
    support provided by Amazon Q Developer for milestone steps such as data collection,
    feature engineering, model training, and model deployment using Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, data scientists need to write a complex script to carry out all of
    the preceding milestone steps from an Amazon SageMaker Studio notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Solution blueprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Set up an environment with the required set of libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data collection**: Download and unzip direct marketing bank data from [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature engineering**: To demonstrate the functionality, we will carry out
    the following commonly used feature engineering steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manipulate column data using default values
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop extra columns
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Carry out one-hot encoding
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training**: Let’s use the XGBoost algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rearrange data to create training, validation, and test datasets/files
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use XGBoost algorithms to train the model using the training dataset
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: Deploy the model as an endpoint to allow inferences.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the preceding solution blueprint, we illustrate the integration of Amazon
    Q Developer with Amazon SageMaker by handling commonly used milestone steps. However,
    based on the complexity of your data and enterprise needs, there might be additional
    steps required.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will utilize an AWS dataset publicly hosted for direct marketing bank data.
    The complete dataset is available at [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip).
    All the data preparation steps will be conducted in the SageMaker Studio notebook
    as part of the data collection requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Solution – Amazon Q with Amazon SageMaker Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first enable Amazon Q Developer with Amazon SageMaker Studio. The following
    prerequisites are needed to allow Amazon Q Developer to auto-generate code inside
    Amazon SageMaker studio.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisite to enable Amazon Q Developer with Amazon SageMaker Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The developer is required to modify the IAM policy associated with the IAM user
    or role to grant permissions for Amazon Q Developer to initiate recommendations
    in for Amazon SageMaker Studio notebook. Refer [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022)
    for the details to enable Amazon Q Developer with Amazon SageMaker Studio notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Amazon Q Developer is activated for Amazon SageMaker Studio notebook,
    select **Create notebook** from the **Launcher** to verify that Amazon Q Developer
    is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio](img/B21378_14_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill the mentioned requirements, we will use auto-code generation techniques
    that were discussed in [*Chapter 4*](B21378_04.xhtml#_idTextAnchor081). Mainly,
    we will focus on single-line prompts, multi-line prompts, and chain-of-thought
    prompts for auto-code generation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Set up an environment with the required set of libs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use single-line prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment](img/B21378_14_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment
  prefs: []
  type: TYPE_NORMAL
- en: Observe that, based on our prompts, Amazon Q Developer generated code with a
    default set of libraries and variables. However, based on your needs, and account
    setup, you may need to update/add the code.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For data collection, download and unzip direct marketing bank data from [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on multi-line prompts to achieve this requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection](img/B21378_14_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate the functionality of feature engineering, we will carry out
    the following commonly used feature engineering steps, which will help us improve
    the model accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: Manipulate column data using default values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop extra columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Carry out one-hot encoding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will focus on multi-line prompts to achieve this requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We get the following screen.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering](img/B21378_14_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s proceed with the generated code for the model training, testing,
    and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering](img/B21378_14_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: Note that for both single-line and multi-line prompts, we needed to provide
    much more specific details to generate the code as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Model training**: Let’s use the XGBoost algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Rearrange data to create training, validation, and test datasets/files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use XGBOOST algorithms to train the model using a training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will focus on multi-line prompts to achieve this requirement to start the
    model training activity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.43 – Amazon Q Developer – SageMaker Studio model training](img/B21378_14_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.43 – Amazon Q Developer – SageMaker Studio model training
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deploy the model as an endpoint to allow inferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on single-line prompts to achieve this requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 14.44 – Amazon Q Developer -SageMaker studio model training](img/B21378_14_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.44 – Amazon Q Developer -SageMaker studio model training
  prefs: []
  type: TYPE_NORMAL
- en: Observe that Amazon Q Developer uses a default configuration for `instance_type`
    and `initial_instance_count`. You can check the hosted model from the Amazon SageMaker
    console by clicking the **Inference** dropdown and selecting the **Endpoints**
    option.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, we extensively used inline prompts with single-line
    prompting, multi-line prompting, and chain of thought prompting techniques. If
    you wish to use a chat-style interface, you can leverage the Amazon Q Developer
    chat-style interface, as shown in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface](img/B21378_14_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface
  prefs: []
  type: TYPE_NORMAL
- en: Summary – Amazon Q Developer with Amazon SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As demonstrated, Amazon Q Developer seamlessly integrated with the Amazon SageMaker
    Studio notebook IDE, enables the automatic generation of end-to-end, error-free,
    and executable code. By supplying prompts with specific requirements, Q Developer
    can auto-generate code for essential milestone steps, including data collection,
    feature engineering, model training, and model deployment within the SageMaker
    Studio notebook.
  prefs: []
  type: TYPE_NORMAL
- en: While data scientists can utilize this integration to produce code blocks, customization
    may be necessary. Specific details must be provided in prompts to tailor the code.
    In some instances, adjustments may be required to align with enterprise standards,
    business requirements, and configurations. Users should possess expertise in prompt
    engineering, familiarity with scripting, and conduct thorough testing to ensure
    the scripts meet business requirements before deploying them into production.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s dive deep to see how data analysts can use code assistance while
    working with Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Code assistance integration with Amazon Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start diving deep into code assistance support for the Amazon Redshift
    service, let’s quickly go through an overview of AWS Redshift. **Amazon Redshift**
    is an AI-powered, fully managed, cloud-based data warehouse service. It is designed
    for high-performance analysis and the processing of large datasets using standard
    SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift is optimized for data warehousing, providing a fast and scalable
    solution for processing and analyzing large volumes of structured data. It uses
    columnar storage and **massively parallel processing** (**MPP**) architecture,
    distributing data and queries across multiple nodes to deliver high performance
    for complex queries. This architecture allows it to easily scale from a few hundred
    gigabytes to petabytes of data, enabling organizations to grow their data warehouse
    as their needs evolve. It integrates with various data sources, allowing you to
    load data from multiple sources, including Amazon S3, Amazon DynamoDB, and Amazon
    EMR.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To query the data, Amazon Redshift also provides a query editor. The Redshift
    query editor v2 has two modes to interact with databases: **Editor** and **Notebook**.
    Code assistance is integrated with the Notebook mode of the Redshift query editor
    v2.'
  prefs: []
  type: TYPE_NORMAL
- en: Use case for Amazon Redshift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with one of the easy and widely used use cases of converting file
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying top performers**: In a typical business use case, analysts are
    interested in identifying top performers based on certain criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this use case, we will be using the publicly available `tickit`
    database, which is readily available with Amazon Redshift. For more information
    about the `tickit` database, refer to the *References* section at the end of the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Analysts want to identify the top state where most of the venues are.
  prefs: []
  type: TYPE_NORMAL
- en: To meet this requirement, analyst developers must develop SQL queries to interact
    with different tables from the `tickit` database.
  prefs: []
  type: TYPE_NORMAL
- en: Solution blueprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we are considering the data analyst persona and using code assistance to
    generate the code, we do not need to further break down the business ask into
    the solution blueprint. This makes it easy for analysts to interact with databases
    without getting involved in table structures and relationship details:'
  prefs: []
  type: TYPE_NORMAL
- en: Write SQL to identify the top state where most of the venues are
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using the publicly available `tickit` database, which comes with
    Amazon Redshift. Let’s import the data using Redshift query editor v2:'
  prefs: []
  type: TYPE_NORMAL
- en: Connect to your Amazon Redshift cluster or Serverless endpoint from Redshift
    query editor 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, choose `sample_data_dev` and click on `tickit`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.46 – Import the tickit database using Amazon Redshift](img/B21378_14_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.46 – Import the tickit database using Amazon Redshift
  prefs: []
  type: TYPE_NORMAL
- en: Solution – Amazon Q with Amazon Redshift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first enable Amazon Q with Amazon Redshift. To allow Amazon Q to generate
    SQL inside Amazon Redshift, the admin needs to enable the **Generative SQL** option
    inside **Notebook** of Redshift query editor v2\. Please reference [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060)
    for additional details on initiating interaction with Amazon Q in Amazon Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisite to enable Amazon Q with Amazon Redshift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s walk through the steps needed to enable the **Generative SQL** option
    inside **Notebook** of the Redshift query editor v2.
  prefs: []
  type: TYPE_NORMAL
- en: Log in with admin privileges to connect to your Amazon Redshift cluster or Serverless
    endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Notebook**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.47 – Notebook using Redshift query editor v2](img/B21378_14_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.47 – Notebook using Redshift query editor v2
  prefs: []
  type: TYPE_NORMAL
- en: Choose **Generative SQL**, then check the **Generative SQL** box, and click
    **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 14.48 – Enable Generative SQL using Redshift query editor v2](img/B21378_14_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.48 – Enable Generative SQL using Redshift query editor v2
  prefs: []
  type: TYPE_NORMAL
- en: To fulfill the mentioned requirements, we will use auto-code generation techniques
    that were discussed in [*Chapter 4*](B21378_04.xhtml#_idTextAnchor081). Mainly,
    we will focus on the chat companion for auto-code generation.
  prefs: []
  type: TYPE_NORMAL
- en: Requirement 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Write SQL to identify the top state where most of the venues are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use Amazon Q’s interactive session to ask the following question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Observe that we did not provide database details to the Amazon Q Developer,
    but it was still able to identify the required table, `tickit.venue`. It generated
    the fully executable end-to-end query with `Group by`, `Order by`, and `Limit`
    to meet the requirements. To make it easy for analysts to run the queries, code
    assistance is integrated with the notebook. Just by clicking **Add to notebook**,
    the SQL code will be available in a notebook cell that users can run directly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.49 – Interact with code assistance from Amazon Redshift](img/B21378_14_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.49 – Interact with code assistance from Amazon Redshift
  prefs: []
  type: TYPE_NORMAL
- en: Summary – Amazon Q with Amazon Redshift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As demonstrated, we can effortlessly generate end-to-end, error-free, and executable
    SQL by interacting with Amazon Q through a chat-style interface. Amazon Q seamlessly
    integrates with notebooks in the Amazon Redshift query editor v2\. Users are not
    required to provide database and/or table details to the code assistant. It autonomously
    identifies the necessary tables and generates SQL code to fulfill the specified
    requirements in the prompt. Furthermore, to facilitate analysts in running queries,
    it is directly integrated with the notebook. Amazon Q, in conjunction with Amazon
    Redshift, proves to be a valuable asset for data analysts. In many cases, data
    analysts do not need to translate business requirements into technical steps.
    They can leverage the auto-generate SQL feature, bypassing the need to delve deep
    into database and table details.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we initially covered the integration of different AWS services
    with code companions to assist users in auto-code generation. Then, we explored
    the integration of Amazon Q Developer with some of the core services, such as
    AWS Glue, Amazon EMR, AWS Lambda, Amazon Redshift, and Amazon SageMaker, commonly
    used by application developers, data engineers, and data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed, in the prerequisites, the in-depth integration with sample
    common use cases and corresponding solution walk-throughs for various integrations.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Glue integration with Amazon Q Developer, aiding data engineers in generating
    and executing ETL scripts using the AWS Glue Studio notebook environment. This
    includes a skeletal outline of a full end-to-end Glue ETL job using AWS Glue Studio.
  prefs: []
  type: TYPE_NORMAL
- en: AWS EMR integration with Amazon Q Developer to assist data engineers in generating
    and executing ETL scripts using the AWS EMR Studio notebook environment.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda console IDE integration with Amazon Q Developer, supporting application
    engineers in generating and executing end-to-end Python-based applications for
    file movement.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker studio notebook integration with Amazon Q Developer to help
    data scientists achieve major milestone steps in data collection, feature engineering,
    model training, and model deployment using different prompting techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift integration with Amazon Q to aid business analysts in generating
    SQL queries by simply providing business requirements. Users are not required
    to provide database and/or table details to the code assistant.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how you can use Amazon Q Developer to get
    AWS-specific guidance and recommendations, either from the AWS console or from
    the documentation on a variety of topics such as architecture and best practices
    support.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS Prescriptive Guidance - Data engineering: [https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html](https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jupyter kernel: [https://docs.jupyter.org/en/latest/projects/kernels.html](https://docs.jupyter.org/en/latest/projects/kernels.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon Q Developer with AWS Glue Studio: [https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TLC Trip Record Data: [https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting up Amazon Q data integration in AWS Glue: [https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html](https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Setting up Amazon Q Developer data integration in Amazon EMR: [https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Attach a compute to an EMR Studio Workspace: [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using Amazon Q Developer with AWS Lambda: [https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interacting with query editor v2 generative SQL: [https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon Redshift “tickit” database: [https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html](https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Direct marketing bank data: [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon SageMaker Studio: [https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
