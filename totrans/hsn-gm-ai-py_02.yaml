- en: Understanding Rewards-Based Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world is consumed with the machine learning revolution and, in particular,
    the search for a functional **artificial general intelligence** or **AGI**. Not
    to be confused with a conscious AI, AGI is a broader definition of machine intelligence
    that seeks to apply generalized methods of learning and knowledge to a broad range
    of tasks, much like the ability we have with our brains—or even small rodents
    have, for that matter. Rewards-based learning and, in particular, **reinforcement
    learning** (**RL**) are seen as the next steps to a more generalized intelligence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '"Short-term AGI is a serious possibility."'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: – OpenAI Co-founder and Chief Scientist, **Ilya Sutskever**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we start from the beginning of rewards-based learning and RL with
    its history to modern inception and its use in gaming and simulation. RL and,
    specifically, deep RL are gaining popularity in both research and use. In just
    a few years, the advances in RL have been dramatic, which have made it both impressive
    but, at the same time, difficult to keep up with and make sense of. With this
    book, we will unravel the abstract terminology that plagues this multi-branch
    and complicated topic in detail. By the end of this book, you should be able to
    consider yourself a confident practitioner of RL and deep RL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'For this first chapter, we will start with an overview of RL and look at the
    terminology, history, and basic concepts. In this chapter, the high-level topics
    we will cover are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Understanding rewards-based learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the Markov decision process
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using value learning with multi-armed bandits
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Q-learning with contextual bandits
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to mention some important technical requirements before continuing in
    the next section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is a hands-on one, which means there are plenty of code examples to
    work through and discover on your own. The code for this book can be found in
    the following GitHub repository: [https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games)[.](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: As such, be sure to have a working Python coding environment set up. Anaconda,
    which is a cross-platform wrapper framework for both Python and R, is the recommended
    platform to use for this book. We also recommend Visual Studio Code or Visual
    Studio Professional with the Python tools as good **Integrated development editors**,
    or **IDEs**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda, recommended for this book, can be downloaded from [https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: With that out of the way, we can move on to learning the basics of RL and, in
    the next section, look at why rewards-based learning works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Understanding rewards-based learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Machine learning is quickly becoming a broad and growing category, with many
    forms of learning systems addressed. We categorize learning based on the form
    of a problem and how we need to prepare it for a machine to process. In the case
    of supervised machine learning, data is first labeled before it is fed into the
    machine. Examples of this type of learning are simple image classification systems
    that are trained to recognize a cat or dog from a prelabeled set of cat and dog
    images. Supervised learning is the most popular and intuitive type of learning
    system. Other forms of learning that are becoming increasingly powerful are unsupervised
    and semi-supervised learning. Both of these methods eliminate the need for labels
    or, in the case of semi-supervised learning, require the labels to be defined
    more abstractly. The following diagram shows these learning methods and how they
    process data:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习正在迅速成为一个广泛且不断发展的类别，涵盖了多种学习系统。我们根据问题的形式以及我们如何为机器处理它进行分类。在监督机器学习的案例中，数据在输入机器之前首先被标记。这类学习的例子包括简单的图像分类系统，这些系统被训练从预先标记的猫和狗图像集中识别猫或狗。监督学习是最受欢迎且直观的学习系统类型。其他越来越强大的学习形式是无监督学习和半监督学习。这两种方法都消除了对标签的需求，或者在半监督学习的案例中，需要更抽象地定义标签。以下图示展示了这些学习方法和它们如何处理数据：
- en: '![](img/b9ba18f3-498c-4f72-89b7-88ff6d76e79a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9ba18f3-498c-4f72-89b7-88ff6d76e79a.png)'
- en: Variations of supervised learning
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的变体
- en: A couple of recent papers on [arXiv.org](http://arxiv.org) (pronounced archive.org)
    suggest the use of semi-supervised learning to solve RL tasks. While the papers
    suggest no use of external rewards, they do talk about internal updates or feedback
    signals. This suggests a method of using internal reward RL, which, as we mentioned
    before, is a thing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最近在 [arXiv.org](http://arxiv.org)（发音为archive.org）上的论文建议使用半监督学习来解决强化学习任务。虽然论文建议不使用外部奖励，但它们确实谈到了内部更新或反馈信号。这表明了一种使用内部奖励强化学习的方法，正如我们之前提到的，这是一个存在的事物。
- en: 'While this family of supervised learning methods has made impressive progress
    in just the last few years, they still lack the necessary planning and intelligence
    we expect from a truly intelligent machine. This is where RL picks up and differentiates
    itself. RL systems learn from interacting and making selections in the environment
    the agent resides in. The classic diagram of an RL system is shown here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个监督学习方法的家族在过去几年中取得了令人印象深刻的进步，但它们仍然缺乏我们从真正智能机器中期望的必要规划和智能。这就是强化学习发挥作用并区别于其他方法的地方。强化学习系统通过在与代理所在的环境中进行交互和选择来学习。一个典型的强化学习系统图示如下：
- en: '![](img/af027863-1265-4af3-947a-b74dad43a9ec.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af027863-1265-4af3-947a-b74dad43a9ec.png)'
- en: An RL system
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强化学习系统
- en: 'In the preceding diagram, you can identify the main components of an RL system:
    the **Agent** and **Environment**, where the **Agent** represents the RL system,
    and the **Environment** could be representative of a game board, game screen,
    and/or possibly streaming data. Connecting these components are three primary
    signals, the **State**, **Reward**, and **Action**. The **State** signal is essentially
    a snapshot of the current state of **Environment**. The **Reward** signal may
    be externally provided by the **Environment** and provides feedback to the agent,
    either bad or good. Finally, the **Action** signal is the action the **Agent**
    selects at each time step in the environment. An action could be as simple as
    *jump* or a more complex set of controls operating servos. Either way, another
    key difference in RL is the ability for the agent to interact with, and change,
    the **Environment**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，您可以识别出强化学习系统的主要组成部分：**代理**和**环境**，其中**代理**代表强化学习系统，而**环境**可能是游戏板、游戏屏幕，以及/或可能是流数据。连接这些组件的是三个主要信号：**状态**、**奖励**和**动作**。**状态**信号基本上是**环境**当前状态的快照。**奖励**信号可能由**环境**外部提供，并为代理提供反馈，无论是好是坏。最后，**动作**信号是代理在每个时间步在环境中选择的动作。一个动作可能像*跳跃*一样简单，或者是一组更复杂的控制伺服机构。无论如何，强化学习中的另一个关键区别是代理能够与**环境**交互并改变它。
- en: Now, don't worry if this all seems a little muddled still—early researchers
    often encountered trouble differentiating between supervised learning and RL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果这一切仍然显得有些混乱，请不要担心——早期研究人员经常在区分监督学习和强化学习之间遇到麻烦。
- en: In the next section, we look at more RL terminology and explore the basic elements
    of an RL agent.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨更多的强化学习术语，并探讨强化学习代理的基本要素。
- en: The elements of RL
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的要素
- en: 'Every RL agent is comprised of four main elements. These are **policy**, **reward
    function**, **value function**, and, optionally, **model**. Let''s now explore
    what each of these terms means in more detail:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每个强化学习代理都由四个主要元素组成。这些是**策略**、**奖励函数**、**价值函数**，以及可选的**模型**。现在让我们更详细地探讨这些术语的含义：
- en: '**The policy**: A policy represents the decision and planning process of the
    agent. The policy is what decides the actions the agent will take during a step.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：策略代表了代理的决策和规划过程。策略决定了代理在每一步将采取哪些动作。'
- en: '**The reward function**: The reward function determines what amount of reward
    an agent receives after completing a series of actions or an action. Generally,
    a reward is given to an agent externally but, as we will see, there are internal
    reward systems as well.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励函数**：奖励函数决定了代理在完成一系列动作或单个动作后所获得的奖励量。通常，奖励是由外部给予代理的，但正如我们将看到的，也存在内部奖励系统。'
- en: '**The value function**: A value function determines the value of a state over
    the long term. Determining the value of a state is fundamental to RL and our first
    exercise will be determining state values.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数**：价值函数决定了长期状态下状态的值。确定状态的值是强化学习的基础，我们的第一个练习将是确定状态值。'
- en: '**The model**: A model represents the environment in full. In the case of a
    game of tic-tac-toe, this may represent all possible game states. For more advanced
    RL algorithms, we use the concept of a partially observable state that allows
    us to do away with a full model of the environment. Some environments that we
    will tackle in this book have more states than the number of atoms in the universe.
    Yes, you read that right. In massive environments like that, we could never hope
    to model the entire environment state.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：模型代表了整个环境。在井字棋游戏中，这可能代表所有可能的游戏状态。对于更高级的强化学习算法，我们使用部分可观察状态的概念，这允许我们摆脱环境的完整模型。在这本书中我们将要解决的问题中，有些环境的状态数量比宇宙中的原子数量还要多。是的，您没有看错。在如此庞大的环境中，我们永远无法希望对整个环境状态进行建模。'
- en: We will spend the next several chapters covering each of these terms in excruciating
    detail, so don't worry if things feel a bit abstract still. In the next section,
    we will take a look at the history of RL.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几章中详细探讨这些术语，所以如果感觉有些抽象，请不要担心。在下一节中，我们将回顾强化学习的历史。
- en: The history of RL
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的历史
- en: '*An Introduction to RL*, by Sutton and Barto (1998), discusses the origins
    of modern RL being derived from two main threads with a later joining thread.
    The two main threads are trial and error-based learning and dynamic programming,
    with the third thread arriving later in the form of temporal difference learning.
    The primary thread founded by Sutton, trial and error, is based on animal psychology.
    As for the other methods, we will look at each in far more detail in their respective
    chapters. A diagram showing how these three threads converged to form modern RL
    is shown here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Sutton和Barto（1998年）的《强化学习导论》讨论了现代强化学习的起源，它源自两个主要线索，后来又加入了一个线索。这两个主要线索是基于试错的学习和动态规划，第三个线索以时间差分学习的形式在后来出现。Sutton创立的主要线索，即试错，基于动物心理学。至于其他方法，我们将在各自的章节中更详细地探讨。下面是一个展示这三个线索如何汇聚形成现代强化学习的图表：
- en: '![](img/34897072-41fb-41c9-bbfa-14d3712d417f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34897072-41fb-41c9-bbfa-14d3712d417f.png)'
- en: The history of modern RL
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现代强化学习的历史
- en: Dr. Richard S. Sutton, a distinguished research scientist for DeepMind and renowned
    professor from the University of Alberta, is considered the father of modern RL.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Dr. Richard S. Sutton，DeepMind的杰出研究科学家，同时也是阿尔伯塔大学的著名教授，被认为是现代强化学习（RL）之父。
- en: Lastly, before we jump in and start unraveling RL, let's look at why it makes
    sense to use this form of learning with games in the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们深入探讨强化学习之前，让我们在下一节中看看为什么使用这种学习形式与游戏相结合是有意义的。
- en: Why RL in games?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么是游戏中的强化学习？
- en: Various forms of machine learning systems have been used in gaming, with supervised
    learning being the primary choice. While these methods can be made to look intelligent,
    they are still limited by working on labeled or categorized data. While **generative
    adversarial networks** (**GANs**) show a particular promise in level and other
    asset generation, these families of algorithms cannot plan and make sense of long-term
    decision making. AI systems that replicate planning and interactive behavior in
    games are now typically done with hardcoded state machine systems such as finite
    state machines or behavior trees. Being able to develop agents that can learn
    for themselves the best moves or actions for an environment is literally game-changing,
    not only for the games industry, of course, but this should surely cause repercussions
    in every industry globally.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏领域，已经使用了各种形式的机器学习系统，其中监督学习是首选。虽然这些方法可以表现得像智能系统，但它们仍然受限于处理标记或分类的数据。尽管**生成对抗网络**（GANs）在关卡和其他资产生成方面显示出特别的潜力，但这些算法家族无法规划和理解长期决策。现在，在游戏中复制规划和交互行为的AI系统通常是通过硬编码的状态机系统，如有限状态机或行为树来实现的。能够开发出能够自己学习最佳移动或行为的智能体，这在实际上可以说是改变游戏规则，不仅对游戏产业如此，而且这肯定会在全球的每个产业中引起反响。
- en: In the next section, we take a look at the foundation of the RL system, the
    Markov decision process.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨强化学习系统的基础，即马尔可夫决策过程。
- en: Introducing the Markov decision process
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍马尔可夫决策过程
- en: In RL, the agent learns from the environment by interpreting the state signal.
    The state signal from the environment needs to define a discrete slice of the
    environment at that time. For example, if our agent was controlling a rocket,
    each state signal would define an exact position of the rocket in time. State,
    in that case, may be defined by the rocket's position and velocity. We define
    this state signal from the environment as a Markov state. The Markov state is
    not enough to make decisions from, and the agent needs to understand previous
    states, possible actions, and any future rewards. All of these additional properties
    may converge to form a Markov property, which we will discuss further in the next
    section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，智能体通过解释状态信号从环境中学习。环境的状态信号需要定义当时环境的离散切片。例如，如果我们的智能体正在控制一枚火箭，每个状态信号都会定义火箭在时间上的确切位置。在这种情况下，状态可能由火箭的位置和速度定义。我们将从环境中定义的这个状态信号称为马尔可夫状态。马尔可夫状态不足以做出决策，智能体需要理解先前状态、可能的行为以及任何未来的奖励。所有这些附加属性可能汇聚形成一个马尔可夫性质，我们将在下一节中进一步讨论。
- en: The Markov property and MDP
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫性质和马尔可夫决策过程
- en: 'An RL problem fulfills the Markov property if all Markov signals/states predict
    a future state. Subsequently, a Markov signal or state is considered a Markov
    property if it enables the agent to predict values from that state. Likewise,
    a learning task that is a Markov property and is finite is called a finite **Markov
    decision process**, or **MDP**. A very classic example of an MDP used to often
    explain RL is shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有马尔可夫信号/状态都预测未来状态，那么一个强化学习问题就满足了马尔可夫性质。随后，如果一个马尔可夫信号或状态能够使智能体从该状态预测值，那么它就被认为是具有马尔可夫性质的。同样，如果一个既是马尔可夫性质又是有限的学习任务，那么它被称为有限**马尔可夫决策过程**或**MDP**。这里展示了一个非常经典的MDP例子，经常用来解释强化学习：
- en: '![](img/de46c636-7f35-454c-8044-41e0fbce202a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/de46c636-7f35-454c-8044-41e0fbce202a.png)'
- en: The Markov decision process (Dr. David Silver)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（Dr. David Silver）
- en: The preceding diagram was taken from the excellent online lecture by Dr. David
    Silver on YouTube ([https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)).
    Dr. Silver, a former student of Dr. Sutton, has since gone on to great fame by
    being the brains that power most of DeepMind's early achievements in RL.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表取自YouTube上David Silver博士的优秀在线讲座（[https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)）。Silver博士是Sutton博士的前学生，后来因成为DeepMind在强化学习（RL）早期成就背后的智慧大脑而声名鹊起。
- en: The diagram is an example of a finite discrete MDP for a post-secondary student
    trying to optimize their actions for maximum reward. The student has the option
    of attending class, going to the gym, hanging out on Instagram or whatever, passing
    and/or sleeping. States are denoted by circles and the text defines the activity.
    In addition to this, the numbers next to each path from a circle denote the probability
    of using that path. Note how all of the values around a single circle sum to 1.0
    or 100% probability. The R= denotes the reward or output of the reward function
    when the student is in that state. To solidify this abstract concept further,
    let's build our own MDP in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Building an MDP
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this hands-on exercise, we will build an MDP using a task from your own
    daily life or experience. This should allow you to better apply this abstract
    concept to something more tangible. Let''s begin as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Think of a daily task you do that may encompass six or so states. Examples of
    this may be going to school, getting dressed, eating, showering, browsing Facebook,
    and traveling.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write each state within a circle on a full sheet of paper or perhaps some digital
    drawing app.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connect the states with the actions you feel most appropriate. For example,
    don't get dressed before you shower.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the probability you would use to take each action. For example, if you
    have two actions leaving a state, you could make them both 50/50 or 0.5/0.5, or
    some other combination that adds up to 1.0.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the reward. Decide what rewards you would receive for being within each
    state and mark those on your diagram.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare your completed diagram with the preceding example. How did you do?
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before we get to solving your MDP or others, we first need to understand some
    background on calculating values. We will uncover this in the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Using value learning with multi-armed bandits
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Solving a full MDP and, hence, the full RL problem first requires us to understand
    values and how we calculate the value of a state with a value function. Recall
    that the value function was a primary element of the RL system. Instead of using
    a full MDP to explain this, we instead rely on a simpler single-state problem
    known as the multi-armed bandit problem. This is named after the one-armed slot
    machines often referred to as bandits by their patrons but, in this case, the
    machine has multiple arms. That is, we now consider a single-state or stationary
    problem with multiple actions that lead to terminal states providing constant
    rewards. More simply, our agent is going to play a multi-arm slot machine that
    will give either a win or loss based on the arm pulled, with each arm always returning
    the same reward. An example of our agent playing this machine is shown here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a1eab02-d933-4189-9134-27881ca733af.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Illustration of an agent playing multi-armed bandits
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'We can consider the value for a single state to be dependent on the next action,
    provided we also understand the reward provided by that action. Mathematically,
    we can define a simple value equation for learning like so:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af99e6f2-9a84-45e7-b895-e27d70a13847.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, we have the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '*V(a)*: the value for a given action'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a*: action'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α*: alpha or the learning rate'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*r*: reward'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice the addition of a new variable called α (alpha) or the learning rate.
    This learning rate denotes how fast the agent needs to learn the value from pull
    to pull. The smaller the learning rate (0.1), the slower the agent learns. This
    method of action-value learning is fundamental to RL. Let's code up this simple
    example to solidify further in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Coding a value learner
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since this is our first example, make sure your Python environment is set to
    go. Again for simplicity, we prefer Anaconda. Make sure you are comfortable coding
    with your chosen IDE and open up the code example, `Chapter_1_1.py`, and follow
    along:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s examine the first section of the code, as shown here:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We first start by doing `import` of `random`. We will use `random` to randomly
    select an arm during each training episode.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we define a list of rewards, `reward`. This list defines the reward for
    each arm (action) and hence defines the number of arms/actions on the bandit.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we determine the number of arms using the `len()` function.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we set the number of training episodes our agent will use to evaluate
    the value of each arm.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `learning_rate` value to `.1`. This means the agent will learn slowly
    the value of each pull.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we initialize the value for each action in a list called `Value`, using
    the following code:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we print the `Value` list to the console, making sure all of the values
    are 0.0.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first section of code initialized our rewards, number of arms, learning
    rate, and value list. Now, we need to implement the training cycle where our agent/algorithm
    will learn the value of each pull. Let''s jump back into the code for `Chapter_1_1.py`
    and look to the next section:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section of code in the listing we want to focus on is entitled `agent
    learns` and is shown here for reference:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We start by first defining a `for` loop that loops through `0` to our number
    of episodes. For each episode, we let the agent pull an arm and use the reward
    from that pull to update its determination of value for that action or arm.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we want to determine the action or arm the agent pulls randomly using
    the following code:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code just selects a random arm/action number based on the total number of
    arms on the bandit (minus one to allow for proper indexing).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This then allows us to determine the value of the pull by using the next line
    of code, which mirrors very well our previous value equation:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That line of code clearly resembles the math for our previous `Value` equation.
    Now, think about how `learning_rate` is getting applied during each iteration
    of an episode. Notice that, with a rate of `.1`, our agent is learning or applying
    1/10^(th) of what `reward` the agent receives minus the `Value` function the agent
    previously equated. This little trick has the effect of averaging out the values
    across the episodes.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这行代码明显类似于我们之前的`Value`方程的数学公式。现在，思考一下`learning_rate`是如何在剧集的每次迭代中应用的。注意，以`.1`的速率，我们的智能体正在学习或应用智能体收到的`reward`与智能体之前等价的`Value`函数之差的1/10^(th)。这个小技巧的效果是在剧集之间平均化值。
- en: Finally, after the looping completes and all of the episodes are run, we print
    the updated `Value` function for each action.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在循环完成后，并运行了所有剧集后，我们打印出每个动作更新的`Value`函数。
- en: 'Run the code from the command line or your favorite Python editor. In Visual
    Studio, this is as simple as hitting the play button. After the code has completed
    running, you should see something similar to the following, but not the exact
    output:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过命令行或您喜欢的Python编辑器运行代码。在Visual Studio中，这就像按一下播放按钮那么简单。代码运行完成后，你应该会看到以下类似的内容，但不是确切的输出：
- en: '![](img/56ea32c7-778f-4fa6-b871-c537ea378028.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/56ea32c7-778f-4fa6-b871-c537ea378028.png)'
- en: Output from Chapter_1_1.py
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`Chapter_1_1.py`的输出
- en: You will most certainly see different output values since the random action
    selections on your computer will be different. Python has many ways to set static
    values for random seeds but that isn't something we want to worry about quite
    yet.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你肯定会看到不同的输出值，因为你在电脑上的随机动作选择将是不同的。Python有许多方法可以设置随机种子的静态值，但我们现在还不必担心这个问题。
- en: Now, think back and compare those output values to the rewards set for each
    arm. Are they the same or different and if so, by how much? Generally, the learned
    values after only 100 episodes should indicate a clear value but likely not the
    finite value. This means the values will be smaller than the final rewards but
    they should still indicate a preference.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回想一下，并将这些输出值与为每个杠杆设定的奖励进行比较。它们是否相同或不同？以及如果不同，差异有多大？一般来说，仅经过100个剧集的学习后，应该可以清楚地显示出价值，但可能不是最终的价值。这意味着这些值将小于最终奖励，但它们应该仍然显示出偏好。
- en: The solution we show here is an example of **trial and error** learning; it's
    that first thread we talked about back in the history of RL section. As you can
    see, the agent learns by randomly pulling an arm and determining the value. However,
    at no time does our agent learn to make better decisions based on those updated
    values. The agent always just pulls randomly. Our agent currently has no decision
    mechanism or what we call a **policy** in RL. We will look at how to implement
    a basic greedy policy in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示的解决方案是一个**试错学习**的例子；它就是我们在RL历史部分提到的那第一条线索。正如你所见，智能体通过随机拉动杠杆并确定其价值来学习。然而，我们的智能体从未学会根据那些更新的价值做出更好的决策。智能体总是随机拉动。我们的智能体目前还没有决策机制，或者我们称之为RL中的**策略**。我们将在下一节中探讨如何实现一个基本的贪婪策略。
- en: Implementing a greedy policy
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现贪婪策略
- en: 'Our current value learner is not really learning aside from finding the optimum
    calculated value or the reward for each action over several episodes. Since our
    agent is not learning, it also makes it a less efficient learner as well. After
    all, the agent is just randomly picking any arm each episode when it could be
    using its acquired knowledge, which is the `Value` function, to determine it''s
    next best choice. We can code this up in a very simple policy called a greedy
    policy in the next exercise:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的价值学习器除了在几个剧集内找到每个动作的最优计算值或奖励之外，并没有真正学习。由于我们的智能体没有学习，这也使得它成为一个效率较低的智能体。毕竟，智能体在每一期中只是随机选择任何杠杆，而它本可以使用其获得的知识，即`Value`函数，来确定其下一个最佳选择。我们可以在下一个练习中通过一个非常简单的策略，即贪婪策略来实现这一点：
- en: 'Open up the `Chapter_1_2.py` example. The code is basically the same as our
    last example except for the episode iteration and, in particular, the selection
    of action or arm. The full listing can be seen here—note the new highlighted sections:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_1_2.py`示例。代码基本上与我们的上一个示例相同，除了剧集迭代和特别地，动作或杠杆的选择。完整的列表可以在这里看到——注意新的高亮部分：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice the inclusion of a new `greedy()` function. This function will always
    select the action with the highest value and return the corresponding index/action
    index. This function is essentially our agent's policy.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意到新加入的`greedy()`函数。这个函数将始终选择具有最高价值的动作，并返回相应的索引/动作索引。这个函数本质上就是我们的智能体的策略。
- en: 'Scrolling down in the code, notice inside the training loop how we are now
    using the `greedy()` function to select our action, as shown here:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Again, run the code and look at the output. Is it what you expected? What went
    wrong?
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Looking at your output likely shows that the agent calculated the maximum reward
    arm correctly, but likely didn't determine the correct values for the other arms.
    The reason for this is that, as soon as the agent found the most valuable arm,
    it kept pulling that arm. Essentially the agent finds the best path and sticks
    with it, which is okay in this single step or stationary environment but certainly
    won't work over a many step problem requiring multiple decisions. Instead, we
    need to balance the agents need to explore and find new paths, versus maximizing
    the immediate optimum reward. This problem is called the **exploration versus
    exploitation** dilemma in RL and something we will explore in the next section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Exploration versus exploitation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen, having our agent always make the best choice limits their
    ability to learn the full values of a single state never mind multiple connected
    states. This also severely limits an agent''s ability to learn, especially in
    environments where multiple states converge and diverge. What we need, therefore,
    is a way for our agent to choose an action based on a policy that favors more
    equal action/value distribution. Essentially, we need a policy that allows our
    agent to explore as well as exploit its knowledge to maximize learning. There
    are multiple variations and ways of balancing the trade-off between exploration
    and exploitation. Much of this will depend on the particular environment as well
    as the specific RL implementation you are using. We would never use an absolute
    greedy policy but, instead, some variation of greedy or another method entirely.
    In our next exercise, we show how to implement an initial optimistic value method,
    which can be effective:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Chapter_1_3.py` and look at the highlighted lines shown here:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we have increased the number of `episodes` to `10000`. This will allow
    us to confirm that our new policy is converging to some appropriate solution.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we set the initial value of the `Value` list to `5.0`. Note that this
    value is well above the reward value maximum of `1.0`. Using a higher value than
    our reward forces our agent to always explore the most valuable path, which now
    becomes any path it hasn't explored, hence ensuring our agent will always explore
    each action or arm at least once.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are no more code changes and you can run the example as you normally
    would. The output of the example is shown here:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ab73e287-c06e-4544-8969-b6f6809d00fb.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Output from Chapter_1_3.py
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Your output may vary slightly but it likely will show very similar values. Notice
    how the calculated values are now more relative. That is, the value of `1.0` clearly
    indicates the best course of action, the arm with a reward of `1.0`, but the other
    values are less indicative of the actual reward. Initial option value methods
    are effective but will force an agent to explore all paths, which are not so efficient
    in larger environments. There are of course a multitude of other methods you can
    use to balance exploration versus exploitation and we will cover a new method
    in the next section, where we introduce solving the full RL problem with Q-learning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Q-learning with contextual bandits
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understand how to calculate values and the delicate balance of
    exploration and exploitation, we can move on to solving an entire MDP. As we will
    see, various solutions work better or worse depending on the RL problem and environment.
    That is actually the basis for the next several chapters. For now, though, we
    just want to introduce a method that is basic enough to solve the full RL problem.
    We describe the full RL problem as the non-stationary or contextual multi-armed
    bandit problem, that is, an agent that moves across a different bandit each episode
    and chooses a single arm from multiple arms. Each bandit now represents a different
    state and we no longer want to determine just the value of an action but the quality.
    We can calculate the quality of an action given a state using the Q-learning equation
    shown here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04ab42e6-5e3e-4b4e-a48c-14760d0e428f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we have the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d397c65-4e23-4b49-9c8f-6a0036816a5b.png): state'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/24f39736-665b-475e-ae52-d143b709d7f9.png): current state'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/66980f9a-9d74-4324-95f1-6787c2118fca.png): next action'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/696a9e64-bb73-4824-bf1d-83c7a5cf0e36.png): current action'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ϒ: gamma—reward discount'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'α: alpha—learning rate'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'r: reward'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/226fbd43-f55d-4b2c-96bb-12aa1fc6797e.png): next reward'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/989607da-b947-4a5f-b610-1ada3e3fe922.png): quality'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, don't get overly concerned if all of these terms are a little foreign and
    this equation appears overwhelming. This is the Q-learning equation developed
    by Chris Watkins in 1989 and is a method that simplifies the solving of a **Finite
    Markov Decision Process** or **FMDP**. The important thing to observe about the
    equation at this point is to understand the similarities it shares with the earlier
    action-value equation. In [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml),
    *Dynamic Programming and the Bellman Equation*, we will learn in more detail how
    this equation is derived and functions. For now, the important concept to grasp
    is that we are now calculating a quality-based value on previous states and rewards
    based on actions rather than just a single action-value. This, in turn, allows
    our agent to make better planning for multiple states. We will implement a Q-learning
    agent that can play several multi-armed bandits and be able to maximize rewards
    in the next section.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a Q-learning agent
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While that Q-learning equation may seem a lot more complex, actually implementing
    the equation is not unlike building our agent that just learned values earlier.
    To keep things simpler, we will use the same base of code but turn it into a Q-learning
    example. Open up the code example, `Chapter_1_4.py`, and follow the exercise here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full code listing for reference:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'All of the highlighted sections of code are new and worth paying closer attention
    to. Let''s take a look at each section in more detail here:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We start by initializing the `arms` variable to `7` then a new `bandits` variable
    to `7` as well. Recall that `arms` is analogous to `actions` and `bandits` likewise
    is to `state`. The last new variable, `gamma`, is a new learning parameter used
    to discount rewards. We will explore this discount factor concept in future chapters:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next section of code builds up the reward table matrix as a set of random
    values from -1 to 1\. We use a list of lists in this example to better represent
    the separate concepts:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The following section is very similar and this time sets up a Q table matrix
    to hold our calculated quality values. Notice how we initialize our starting Q
    value to 10.0\. We do this to account for subtle changes in the math, again something
    we will discuss later.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since our states and actions can be all mapped onto a matrix/table, we refer
    to our RL system as using a model. A model represents all actions and states of
    an environment:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We next define a new function called `learn`. This new function is just a straight
    implementation of the Q equation we observed earlier:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, the agent learning section is updated significantly with new code.
    This new code sets up the parameters we need for the new learn function we looked
    at earlier. Notice how the bandit or state is getting randomly selected each time.
    Essentially, this means our agent is just randomly walking from bandit to bandit.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code as you normally would and notice the new calculated Q values printed
    out at the end. Do they match the rewards for each of the arm pulls?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Likely, a few of your arms don't match up with their respective reward values.
    This is because the new Q-learning equation solves the entire MDP but our agent
    is NOT moving in an MDP. Instead, our agent is just randomly moving from state
    to state with no care on which state it saw before. Think back to our example
    and you will realize since our current state does not affect our future state,
    it fails to be a Markov property and hence is not an MDP. However, that doesn't
    mean we can't successfully solve this problem and we will look to do that in the
    next section.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Removing discounted rewards
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem with our current solution and using the full Q-learning equation
    is that the equation assumes any state our agent is in affects future states.
    Except, remember in our example, the agent just walked randomly from bandit to
    bandit. This means using any previous state information would be useless, as we
    saw. Fortunately, we can easily fix this by removing the concept of discounted
    rewards. Recall that new variable, gamma, that appeared in this complicated term: ![](img/3372b23b-e6dc-4ac3-b188-96f0350de6a6.png).
    Gamma and this term are a way of discounting future rewards and something we will
    discuss at length starting in [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml), *Dynamic
    Programming and the Bellman Equation*. For now, though, we can fix this sample
    up by just removing that term from our learn function. Let''s open up code example, `Chapter_1_5.py`,
    and follow the exercise here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'The only section of code we really need to focus on is the updated `learn`
    function, here:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first line of code in the function is responsible for discounting the future
    reward of the next state. Since none of the states in our example are connected,
    we can just comment out that line. We create a new initializer for `q = 0` in
    the next line.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code as you normally would. Now you should see very close values closely
    matching their respective rewards.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By omitting the discounted rewards part of the calculation, hopefully, you can
    appreciate that this would just revert to a value calculation problem. Alternatively,
    you may also realize that if our bandits were connected. That is, pulling an arm
    led to another one arm machine with more actions and so on. We could then use
    the Q-learning equation to solve the problem as well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: That concludes a very basic introduction to the primary components and elements
    of RL. Throughout the rest of this book, we will dig into the nuances of policies,
    values, actions, and rewards.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first introduced ourselves to the world of RL. We looked
    at what makes RL so unique and why it makes sense for games. After that, we explored
    the basic terminology and history of modern RL. From there, we looked to the foundations
    of RL and the Markov decision process, where we discovered what makes an RL problem.
    Then we looked to building our first learner a value learner that calculated the
    values of states on an action. This led us to uncover the need for exploration
    and exploitation and the dilemma that constantly challenges RL implementers. Next,
    we jumped in and discovered the full Q-learning equation and how to build a Q-learner,
    where we later realized that the full Q equation was beyond what we needed for
    our unconnected state environment. We then reverted our Q learned back into a
    value learner and watched it solve the contextual bandit problem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue from where we left off and look into how
    rewards are discounted with the Bellman equation, as well as look at the many
    other improvements dynamic programming has introduced to RL.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Use these questions and exercises to reinforce the material you just learned.
    The exercises may be fun to attempt, so be sure to try atleast two to four questions/exercises:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Questions:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: What are the names of the main components of an RL system? Hint, the first one
    is **Environment**.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the four elements of an RL system. Remember that one element is optional.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the three main threads that compose modern RL.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What makes a Markov state a Markov property?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a policy?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Exercises:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Using `Chapter_1_2.py`, alter the code so the agent pulls from a bandit with
    1,000 arms. What code changes do you need to make?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `Chapter_1_3.py`, alter the code so that the agent pulls from the average
    value, not greedy/max. How did this affect the agent's exploration?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `Chapter_1_3.py`, alter the `learning_rate` variable to determine how
    fast or slow you can make the agent learn. How few episodes are you required to
    run for the agent to solve the problem?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `Chapter_1_5.py`, alter the code so that the agent uses a different policy
    (either the greedy policy or something else). Take points off yourself if you
    look ahead in this book or online for solutions.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `Chapter_1_4.py`, alter the code so that the bandits are connected. Hence,
    when an agent pulls an arm, they receive a reward and are transported to another
    specific bandit, no longer at random. **Hint:** This likely will require a new
    destination table to be built and you will now need to include the discounted
    reward term we removed.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even completing a few of these questions and/or exercises will make a huge difference
    to your learning this material. This is a hands-on book after all.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
