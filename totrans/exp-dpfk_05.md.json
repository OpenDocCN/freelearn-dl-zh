["```py\ncd {Folder with video}\nmkdir frames\nffmpeg -i {Video Filename} frames\\video_frame_%05d.png\n```", "```py\ncd {Folder of the downloaded git repo}\\\npython C5-face_detection.py {Folder of frame images}\n```", "```py\n    import os\n    import torch\n    import cv2\n    import json_tricks\n    import numpy as np\n    from tqdm import tqdm\n    from argparse import ArgumentParser\n    from face_alignment.detection.sfd import FaceDetector\n    from face_alignment import FaceAlignment, LandmarksType\n    from skimage.transform._geometric import _umeyama as umeyama\n    from lib.bisenet import BiSeNet\n    ```", "```py\n    if __name__ == \"__main__\":\n    \"\"\" Process images in a directory into aligned face images\n    Example CLI:\n    ------------\n    python face_detection.py \"C:/media_files/\"\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument(\"path\",\n      help=\"folder of images to run detection on\")\n    parser.add_argument(\"--cpu\", action=\"store_true\",\n      help=\"Force CPU usage\")\n    parser.add_argument(\"--size\", default=256,\n      help=\"height and width to save the face images\")\n    parser.add_argument(\"--max_detection_size\", default=1024,\n      help=\"Maximum size of an image to run detection on.\n        (If you get memory errors, reduce this size)\")\n    parser.add_argument(\"--jpg\", action=\"store_true\",\n      help=\"use JPG instead of PNG for image saving\n        (not recommended due to artifacts in JPG images)\")\n    parser.add_argument(\"--min_size\", default=.1,\n      help=\"Minimum size of the face relative to image\")\n    parser.add_argument(\"--min_confidence\", default=.9,\n      help=\"Minimum confidence for the face detection\")\n    parser.add_argument(\"--export-path\",\n      default=\"$path/face_images\",\n      help=\"output folder (replaces $path with the input)\"\n    opt = parser.parse_args()\n    opt.export_path = opt.export_path.replace('$path',opt.path)\n    main(opt)\n    ```", "```py\n    def main(opt):\n    ```", "```py\n    if not os.path.exists(opt.export_path):\n      os.mkdir(opt.export_path)\n    ```", "```py\n    device = \"cuda\" if torch.cuda.is_available() and not opt.cpu else \"cpu\"\n    ```", "```py\n    face_detector = FaceDetector(device=device, verbose=False)\n    face_aligner = FaceAlignment(LandmarksType._2D, device=device, verbose=False)\n    ```", "```py\n    masker = BiSeNet(n_classes=19)\n    if device == \"cuda\":\n      masker.cuda()\n    model_path = os.path.join(\".\", \"binaries\",\n      \"BiSeNet.pth\")\n    masker.load_state_dict(torch.load(model_path))\n    masker.eval()\n    desired_segments = [1, 2, 3, 4, 5, 6, 10, 12, 13]\n    ```", "```py\n    alignment_data = {}\n    list_of_images_in_dir = [ file for file in os.listdir(opt.path) if os.path.isfile(os.path.join(opt.path,file)) ]\n    ```", "```py\n    for file in tqdm(list_of_images_in_dir):\n      filename, extension = os.path.splitext(file)\n    ```", "```py\n    image_bgr = cv2.imread(os.path.join(opt.path,file))\n    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    height, width, channels = image_rgb.shape\n    ```", "```py\n    adjustment = opt.max_detection_size / max(height, width)\n    if adjustment < 1.0:\n      resized_image = cv2.resize(image_rgb, None, fx=adjustment, fy=adjustment)\n    else:\n      resized_image = image_rgb\n      adjustment = 1.0\n    ```", "```py\n    faces = face_detector.detect_from_image(resized_image)\n    ```", "```py\n    for idx, face in enumerate(faces):\n      top,left,bottom,right = (face[0:4] /\n        adjustment).astype(int)\n      confidence = face[4]\n      if confidence < opt.min_confidence:\n        Continue\n    ```", "```py\n    face_height = bottom - top\n    face_width = right – left\n    face_size = face_height * face_width\n    if face_size/(height*width) < opt.min_size:\n      continue\n    ```", "```py\n    detected_face = image_bgr[y1:y2,x1:x2]\n    cv2.imwrite(os.path.join( opt.export_path,\n                  f\"face_bbox_{filename}_{fn}.png\"),\n                detected_face)\n    ```", "```py\n    landmarks = face_aligner.get_landmarks_from_image(\n      image_rgb, detected_faces = [face[0:4]/adjustment])\n    ```", "```py\n    landmark_image = image_bgr.copy()\n    landmark_image = cv2.rectangle(landmark_image,\n      (top, left), (bottom, right), thickness=10,\n      color=(0, 0, 0))\n    ```", "```py\n    right_eye = np.mean(landmarks[0][36:42],axis=0)\n    left_eye = np.mean(landmarks[0][42:48],axis=0)\n    nose_tip = landmarks[0][30]\n    right_mouth = landmarks[0][48]\n    left_mouth = landmarks[0][54]\n    limited_landmarks = np.stack(\n      (right_eye,\n       left_eye,\n       nose_tip,\n       right_mouth,\n       left_mouth))\n    ```", "```py\n    landmark_image = cv2.rectangle(landmark_image,\n      (x1,y1), (x2,y2), thickness=10, color=(0,0,0))\n    colors = [[255,0,0], # Blue\n              [0,255,0], # Green\n              [0,0,255], # Red\n              [255,255,0], # Cyan\n              [0,255,255]] # Yellow\n    for count, landmark in enumerate(limited_landmarks):\n      landmark_adjusted = landmark.astype(int)\n      landmark_image = cv2.circle(landmark_image,  tuple(landmark_adjusted), radius=10,\n      thickness=-1, color=colors[count])\n    cv2.imwrite(os.path.join(opt.export_path,\n                  f\"face_landmarks_{filename}_{idx}.png\",\n                landmark_image)\n    ```", "```py\n    MEAN_FACE = np.array([[0.25, 0.22],\n                          [0.75, 0.22],\n                          [0.50, 0.51],\n                          [0.26, 0.78],\n                          [0.74, 0.78]])\n    ```", "```py\n    warp_matrix = umeyama(limited_landmarks,\n      MEAN_FACE * (opt.size*.6)+(opt.size*.2), True)\n    ```", "```py\n    alignment_data[file] = {\"landmark\": landmarks,\n                            \"warp_matrix\": warp_matrix}\n    ```", "```py\n    aligned_face = image_bgr.copy()\n    aligned_face = cv2.warpAffine(aligned_face,\n      warp_matrix[:2], (opt.size,opt.size))\n    cv2.imwrite(os.path.join(opt.export_path,\n                  f\"face_aligned_{filename}_{idx}.png\"),\n                aligned_face)\n    ```", "```py\n    if file not in alignment_data.keys():\n      alignment_data[file] = {\"faces\": list()}\n      alignment_data[file]['faces'].append({\n        \"landmark\": landmarks,\"warp_matrix\": warp_matrix})\n    ```", "```py\n    mask_face = cv2.resize(aligned_face, (512, 512))\n    mask_face = torch.tensor(mask_face,\n      device=device).unsqueeze(0)\n    mask_face = mask_face.permute(0, 3, 1, 2) / 255\n    if device == \"cuda\":\n      mask_face.cuda()\n    segments = masker(mask_face)[0]\n    ```", "```py\n    segments = torch.softmax(segments, dim=1)\n    segments = torch.nn.functional.interpolate(segments,\n      size=(256, 256),\n      mode=\"bicubic\",\n      align_corners=False)\n    mask = torch.where( torch.sum(\n        segments[:,desired_segments,:,:], dim=1) > .7,\n      255, 0)[0]\n    mask = mask.cpu().numpy()\n    cv2.imwrite(os.path.join(opt.export_path,\n                  f\"face_mask_{filename}_{idx}.png\"),\n                mask)\n    ```", "```py\n    with open(os.path.join(opt.export_path,\n      f\"face_alignments.json\", \"w\") as alignment_file:\n      alignment_file.write(\n        json_tricks.dumps(alignment_data, indent=4))\n    ```", "```py\n    face_aligner = FaceAlignment(LandmarksType._2D,\n      device=device, verbose=False)\n    ```", "```py\nface_aligner = FaceAlignment(LandmarksType._3D,\n  device=device, verbose=False)\n```"]