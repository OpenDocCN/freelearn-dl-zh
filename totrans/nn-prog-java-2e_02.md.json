["```py\npublic abstract class LearningAlgorithm {\n    protected NeuralNet neuralNet;\n    public enum LearningMode {ONLINE,BATCH};\n    protected enum LearningParadigm {SUPERVISED,UNSUPERVISED};\n//…\n    protected int MaxEpochs=100;\n    protected int epoch=0;\n    protected double MinOverallError=0.001;\n    protected double LearningRate=0.1;\n    protected NeuralDataSet trainingDataSet;\n    protected NeuralDataSet testingDataSet;\n    protected NeuralDataSet validatingDataSet;\n    public boolean printTraining=false;\n    public abstract void train() throws NeuralException;\n    public abstract void forward() throws NeuralException;\n    public abstract void forward(int i) throws NeuralException;\n    public abstract Double calcNewWeight(int layer,int input,int neuron) throws NeuralException;\n    public abstract Double calcNewWeight(int layer,int input,int neuron,double error) throws NeuralException;\n//…\n}\n```", "```py\npublic class DeltaRule extends LearningAlgorithm {\n    public ArrayList<ArrayList<Double>> error;\n    public ArrayList<Double> generalError;\n    public ArrayList<Double> overallError;\n    public double overallGeneralError;\n    public double degreeGeneralError=2.0;\n    public double degreeOverallError=0.0;\n    public enum ErrorMeasurement {SimpleError, SquareError,NDegreeError,MSE}\n\n    public ErrorMeasurement generalErrorMeasurement=ErrorMeasurement.SquareError;\n    public ErrorMeasurement overallErrorMeasurement=ErrorMeasurement.MSE;\n    private int currentRecord=0;\n    private ArrayList<ArrayList<ArrayList<Double>>> newWeights;\n//…\n}\n```", "```py\n@Override\npublic void train() throws NeuralException{\n//…\n  switch(learningMode){\n    case BATCH: //this is the batch training mode\n      epoch=0;\n      forward(); //all data are presented to the neural network\n      while(epoch<MaxEpochs && overallGeneralError>MinOverallError){ //continue condition\n        epoch++; //new epoch                       \n        for(int j=0;j<neuralNet.getNumberOfOutputs();j++){\n          for(int i=0;i<=neuralNet.getNumberOfInputs();i++){\n            //here the new weights are calculated\n            newWeights.get(0).get(j).set(i,calcNewWeight(0,i,j));\n          }\n        }\n//only after all weights are calculated, they are applied\n        applyNewWeights();\n// the errors are updated with the new weights\n        forward();\n      }\n      break;\n    case ONLINE://this is the online training\n      epoch=0;\n      int k=0;\n      currentRecord=0; //this attribute is used in weight update\n      forward(k); //only the k-th record is presented\n      while(epoch<MaxEpochs && overallGeneralError>MinOverallError){\n        for(int j=0;j<neuralNet.getNumberOfOutputs();j++){\n          for(int i=0;i<=neuralNet.getNumberOfInputs();i++){\n            newWeights.get(0).get(j).set(i,calcNewWeight(0,i,j));\n          }\n        }\n//the new weights will be considered for the next record\n        applyNewWeights();\n        currentRecord=++k;\n        if(k>=trainingDataSet.numberOfRecords){\n          k=0; //if it was the last record, again the first\n          currentRecord=0;\n          epoch++; //epoch completes after presenting all records\n        }\n        forward(k); //presenting the next record\n      }\n    break;\n    }\n  }\n```", "```py\n@Override\npublic Double calcNewWeight(int layer,int input,int neuron)\n            throws NeuralException{\n//…\n  Double deltaWeight=LearningRate;\n  Neuron currNeuron=neuralNet.getOutputLayer().getNeuron(neuron);\n  switch(learningMode){\n    case BATCH: //Batch mode\n      ArrayList<Double> derivativeResult=currNeuron.derivativeBatch(trainingDataSet.getArrayInputData());\n      ArrayList<Double> _ithInput;\n      if(input<currNeuron.getNumberOfInputs()){ // weights\n        _ithInput=trainingDataSet.getIthInputArrayList(input);\n      }\n      else{ // bias\n        _ithInput=new ArrayList<>();\n        for(int i=0;i<trainingDataSet.numberOfRecords;i++){\n          _ithInput.add(1.0);\n        }\n      }\n      Double multDerivResultIthInput=0.0; // dot product\n      for(int i=0;i<trainingDataSet.numberOfRecords;i++){\n        multDerivResultIthInput+=error.get(i).get(neuron)*derivativeResult.get(i)*_ithInput.get(i);\n      }\n      deltaWeight*=multDerivResultIthInput;\n    break;\n    case ONLINE:\n      deltaWeight*=error.get(currentRecord).get(neuron);\n      deltaWeight*=currNeuron.derivative(neuralNet.getInputs());\n      if(input<currNeuron.getNumberOfInputs()){\n        deltaWeight*=neuralNet.getInput(input);\n      }\n      break;\n  }\nreturn currNeuron.getWeight(input)+deltaWeight;\n//…\n}\n```", "```py\npublic class Hebbian extends LearningAlgorithm {\n//…\n    private ArrayList<ArrayList<ArrayList<Double>>> newWeights;\n    private ArrayList<Double> currentOutputMean;\n    private ArrayList<Double> lastOutputMean;\n}\n```", "```py\n@Override\npublic Double calcNewWeight(int layer,int input,int neuron)\n         throws NeuralException{\n//…\n  Double deltaWeight=LearningRate;\n  Neuron currNeuron=neuralNet.getOutputLayer().getNeuron(neuron);\n  switch(learningMode){\n    case BATCH:\n//…\n//the batch case is analogous to the implementation in Delta Rule\n//but with the neuron's output instead of the error\n//we're suppressing here to save space\n      break;\n    case ONLINE:\n      deltaWeight*=currNeuron.getOutput();\n      if(input<currNeuron.getNumberOfInputs()){\n        deltaWeight*=neuralNet.getInput(input);\n      }\n      break;\n    }\n    return currNeuron.getWeight(input)+deltaWeight;\n  }\n```", "```py\n@Override\npublic Double calcNewWeight(int layer,int input,int neuron)\n            throws NeuralException{\n//…\n  Double deltaWeight=LearningRate;\n  Neuron currNeuron=neuralNet.getOutputLayer().getNeuron(neuron);\n  switch(learningMode){\n    case BATCH:\n//…\n    break;\n    case ONLINE:\n      deltaWeight*=error.get(currentRecord).get(neuron)\n        *currNeuron.getOutputBeforeActivation();\n      if(input<currNeuron.getNumberOfInputs()){\n        deltaWeight*=neuralNet.getInput(input);\n      }\n    break;\n  }\n  return currNeuron.getWeight(input)+deltaWeight;\n}\n```", "```py\nDouble[][] _neuralDataSet = {\n  {1.2 , fncTest(1.2)}\n ,   {0.3 , fncTest(0.3)}\n ,   {-0.5 , fncTest(-0.5)}\n ,   {-2.3 , fncTest(-2.3)}\n ,   {1.7 , fncTest(1.7)}\n ,   {-0.1 , fncTest(-0.1)}\n ,   {-2.7 , fncTest(-2.7)}  };\nint[] inputColumns = {0};\nint[] outputColumns = {1};\nNeuralDataSet neuralDataSet = newNeuralDataSet(_neuralDataSet,inputColumns,outputColumns);\n```", "```py\n    public static double fncTest(double x){\n        return 0.11*x;\n    }\n```", "```py\nint numberOfInputs=1;\nint numberOfOutputs=1;\nHyperTan htAcFnc = new HyperTan(0.85);\nNeuralNet nn = new NeuralNet(numberOfInputs,numberOfOutputs,\n                 htAcFnc);\n```", "```py\nDeltaRule deltaRule=new DeltaRule(nn,neuralDataSet.LearningAlgorithm.LearningMode.ONLINE);\ndeltaRule.printTraining=true;\ndeltaRule.setLearningRate(0.3);\ndeltaRule.setMaxEpochs(1000);\ndeltaRule.setMinOverallError(0.00001);\n```", "```py\ndeltaRule.forward();\nneuralDataSet.printNeuralOutput();\n```", "```py\nSystem.out.println(\"Beginning training\");\n  deltaRule.train();\nSystem.out.println(\"End of training\");\n  if(deltaRule.getMinOverallError()>=deltaRule.getOverallGeneralError()){\n  System.out.println(\"Training succesful!\");\n}\nelse{\n  System.out.println(\"Training was unsuccesful\");\n}\n```", "```py\nweight = nn.getOutputLayer().getWeight(0, 0);\nbias = nn.getOutputLayer().getWeight(1, 0);\nSystem.out.println(\"Weight found:\"+String.valueOf(weight));\nSystem.out.println(\"Bias found:\"+String.valueOf(bias));\n//Weight found:0.2668421011698528\n//Bias found:0.0011258204676042108\n```", "```py\nDouble[][] _testDataSet ={\n  {-1.7 , fncTest(-1.7) }\n, {-1.0 , fncTest(-1.0) }\n, {0.0 , fncTest(0.0) }\n, {0.8 , fncTest(0.8) }\n, {2.0 , fncTest(2.0) }\n};\nNeuralDataSet testDataSet = new NeuralDataSet(_testDataSet, ....inputColumns, outputColumns);\ndeltaRule.setTestingDataSet(testDataSet);\ndeltaRule.test();\ntestDataSet.printNeuralOutput();\n```"]