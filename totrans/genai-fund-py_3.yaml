- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracing the Foundations of Natural Language Processing and the Impact of the
    Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transformer architecture is a key advancement that underpins most modern
    generative language models. Since its introduction in 2017, it has become a fundamental
    part of **natural language processing** (**NLP**), enabling models such as **Generative
    Pre-trained Transformer 4** (**GPT-4**) and Claude to advance text generation
    capabilities significantly. A deep understanding of the transformer architecture
    is crucial for grasping the mechanics of modern **large language** **models**
    (**LLMs**).
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we explored generative modeling techniques, including
    **generative adversarial networks** (**GANs**), diffusion models, and **autoregressive**
    (**AR**) transformers. We discussed how Transformers can be leveraged to generate
    images from text. However, transformers are more than just one generative approach
    among many; they form the basis for nearly all state-of-the-art generative language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover the evolution of NLP that ultimately led to the
    advent of the transformer architecture. We cannot cover all the critical steps
    forward, but we will attempt to cover major milestones, starting with early linguistic
    analysis techniques and statistical language modeling, followed by advancements
    in **recurrent neural networks** (**RNNs**) and **convolutional neural networks**
    (**CNNs**) that highlight the potential of **deep learning** (**DL**) for NLP.
    Our main objective will be to introduce the transformer—its basis in DL, its self-attention
    architecture, and its rapid evolution, which has led to LLMs and this phenomenon
    we call **generative** **AI** (**GenAI**).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the origins and mechanics of the transformer architecture is important
    for recognizing its groundbreaking impact. The principles and modeling capabilities
    introduced by transformers are carried forward by all modern language models built
    upon this framework. We will build our intuition for Transformers through historical
    context and hands-on implementation, as this foundational understanding is key
    to understanding the future of GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Early approaches in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the widespread use of **neural networks** (**NNs**) in language processing,
    NLP was largely grounded in methods that counted words. Two particularly notable
    techniques were **count vectors** and **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**). In essence, count vectors tallied up how often each word appeared
    in a document. Building on this, Dadgar et al. applied the TF-IDF algorithm (historically
    used for information retrieval) to text classification in 2016\. This method assigned
    weights to words based on their significance in one document relative to their
    occurrence across a collection of documents. These count-based methods were successful
    for tasks such as searching and categorizing. However, they presented a key limitation
    in that they could not capture the semantic relationships between words, meaning
    they could not interpret the nuanced meanings of words in context. This challenge
    paved the way for exploring NNs, offering a deeper and more nuanced way to understand
    and represent text.
  prefs: []
  type: TYPE_NORMAL
- en: Advent of neural language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2003, Yoshua Bengio’s team at the University of Montreal introduced the **Neural
    Network Language Model** (**NNLM**), a novel approach to language technology.
    The NNLM was designed to predict the next word in a sequence based on prior words
    using a particular type of **neural network** (**NN**). The design prominently
    featured hidden layers that learned word embeddings, which are compact vector
    representations capturing the core semantic meanings of words. This aspect was
    absent in count-based approaches. However, the NNLM was still limited in its ability
    to interpret longer sequences and handle large vocabularies. Despite these limitations,
    the NNLM sparked widespread exploration of NNs in language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: The introduction of the NNLM highlighted the potential of NNs in language processing,
    particularly using word embeddings. Yet, its limitations with long sequences and
    large vocabulary signaled the need for further research.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following the inception of the NNLM, NLP research was propelled toward crafting
    high-quality word vector representations. These representations could be initially
    learned from extensive sets of unlabeled text data and later applied to downstream
    models for various tasks. The period saw the emergence of two prominent methods:
    **Word2Vec** (introduced by Mikolov et al., 2013) and **Global Vectors** (**GloVe**,
    introduced by Pennington et al., 2014). These methods applied **distributed representation**
    to craft high-quality word vector representations. Distributed representation
    portrays items such as words not as unique identifiers but as sets of continuous
    values or vectors. In these vectors, each value corresponds to a specific feature
    or characteristic of the item. Unlike traditional representations, where each
    item has a unique symbol, distributed representations allow these items to share
    features with others, enabling a more intelligent capture of underlying patterns
    in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us elucidate this concept a bit further. Suppose we represent words based
    on two features: **Formality** and **Positivity**. We might have vectors such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Formal: [1, 0]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Happy: [0, 1]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Cheerful: [0, 1]`'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, each element in the vector corresponds to one of these features.
    In the vector for `Formal`, the `1` element under *Formality* indicates that the
    word is formal, while the `0` element under *Positivity* indicates neutrality
    in terms of positivity. Similarly, for `Happy` and `Cheerful`, the 1 element under
    *Positivity* indicates that these words have a positive connotation. This way,
    distributed representation captures the essence of words through vectors, allowing
    for shared features among different words to understand underlying patterns in
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec employs a relatively straightforward approach where NNs are used to
    predict the surrounding words for each target word in a dataset. Through this
    process, the NN ascertains values or “weights” for each target word. These weights
    form a vector for each word in a **continuous vector space**—a mathematical space
    wherein each point represents a possible value a vector can take. In the context
    of NLP, each dimension of this space corresponds to a feature, and the position
    of a word in this space captures its semantic or linguistic relationships to other
    words.
  prefs: []
  type: TYPE_NORMAL
- en: These vectors form a **feature-based representation**—a type of representation
    where each dimension represents a different feature that contributes to the word’s
    meaning. Unlike a symbolic representation, where each word is represented as a
    unique symbol, a feature-based representation captures the semantic essence of
    words in terms of shared features.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, GloVe adopts a different approach. It analyzes the **global
    co-occurrence statistics**—a count of how often words appear together in a large
    text corpus. GloVe learns vector representations that capture the relationships
    between words by analyzing these counts across the entire corpus. This method
    also results in a distributed representation of words in a continuous vector space,
    capturing **semantic similarity**—a measure of the degree to which two words are
    similar in meaning. In a continuous vector space, we can think about semantic
    similarity as the simple geometric proximity of vectors representing words.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further illustrate, suppose we have a tiny corpus of text containing the
    following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: “Coffee is hot.”
  prefs: []
  type: TYPE_NORMAL
- en: “Ice cream is cold.”
  prefs: []
  type: TYPE_NORMAL
- en: 'From this corpus, GloVe would notice that “*coffee*” co-occurs with “*hot*”
    and “*ice* *cream*” co-occurs with “*cold*.” Through its optimization process,
    it would aim to create vectors for these words in a way that reflects these relationships.
    In this oversimplified example, GloVe might produce a vector such as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Coffee: [1, 0]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Hot: [0.9, 0]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Ice Cream: [0, 1]`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Cold: [0, 0.9]`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the closeness of the vectors for “*coffee*” and “*hot*” (and, similarly,
    “*ice* *cream*” and “*cold*”) in this space reflects the co-occurrence relationships
    observed in the corpus. The vector difference between “*coffee*” and “*hot*” might
    resemble the vector difference between “*ice* *cream*” and “*cold*,” capturing
    the contrasting temperature relationships in a geometric way within the vector
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Both Word2Vec and GloVe excel at encapsulating relevant semantic information
    about words to represent an efficient **encoding**—a compact way of representing
    information that captures the essential features necessary for a task while reducing
    the dimensionality and complexity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: These methodologies in creating meaningful vector representations served as
    a step toward the adoption of **transfer learning** in NLP. The vectors provide
    a shared semantic foundation that facilitates the transfer of learned relationships
    across varying tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GloVe and other methods of deriving distributed representations paved the way
    for transfer learning in NLP. By creating rich vector representations of words
    that encapsulate semantic relationships, these methods provided a foundational
    understanding of text. The vectors serve as a shared base of knowledge that can
    be applied to different tasks. When a model, initially trained on one task, is
    utilized for another, the pre-learned vector representations aid in preserving
    the semantic understanding, thereby reducing the data or training needed for the
    new task. This practice of transferring acquired knowledge has become fundamental
    for efficiently addressing a range of NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a model trained to understand sentiments (positive or negative) in
    movie reviews. Through training, this model has learned distributed representations
    of words, capturing sentiment-related nuances. Now, suppose there is a new task:
    understanding sentiments in product reviews. Instead of training a new model from
    the beginning, transfer learning allows us to use the distributed representations
    from the movie review task to initiate training for the product review task. This
    could lead to quicker training and better performance, especially with limited
    data for the product review task.'
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of transfer learning, bolstered by distributed representations
    from methods such as GloVe, highlighted the potential of leveraging pre-existing
    knowledge for new tasks. It was a precursor to the integration of NNs in NLP,
    highlighting the benefits of utilizing learned representations across tasks. The
    advent of NNs in NLP brought about models capable of learning even richer representations,
    further amplifying the impact and scope of transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Advent of NNs in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advent of NNs in NLP marked a monumental shift in the field’s capability
    to understand and process language. Building upon the groundwork laid by methodologies
    such as Word2Vec, GloVe, and the practice of transfer learning, NNs introduced
    a higher level of abstraction and learning capacity. Unlike previous methods that
    often relied on hand-crafted features, NNs could automatically learn intricate
    patterns and relationships from data. This ability to learn from data propelled
    NLP into a new era where models could achieve unprecedented levels of performance
    across a myriad of language-related tasks. The emergence of architectures such
    as CNNs and RNNs, followed by the revolutionary transformer architecture, showcased
    the remarkable versatility and efficacy of NNs in tackling complex NLP challenges.
    This transition not only accelerated the pace of innovation but also expanded
    the horizon of what could be achieved in understanding human language computationally.
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling with RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite how well these distributed word vectors excelled at encoding local semantic
    relationships, modeling long-range dependencies would require a more sophisticated
    network architecture. This led to the use of RNNs. RNNs (originally introduced
    by Elman in 1990) are a type of NN architecture that processes data sequences
    by iterating through each element of the sequence while maintaining a dynamic
    internal state that captures information about the previous elements. Unlike traditional
    **feedforward networks** (**FNNs**) that processed each input independently, RNNs
    introduced iterations that allowed information to be passed from one step in the
    sequence to the next, enabling them to capture temporal dependencies in data.
    The iterative processing and dynamic updating in NNs enable them to learn and
    represent relationships within the text. These networks can capture contextual
    connections and interdependencies across sentences or even entire documents.
  prefs: []
  type: TYPE_NORMAL
- en: However, standard RNNs had technical limitations when dealing with long sequences.
    This led to the development of **long short-term memory** (**LSTM**) networks.
    LSTMs were first introduced by Hochreiter and Schmidhuber in 1997\. They were
    a special class of RNNs designed to address the **vanishing gradient** problem,
    which is the challenge where the network cannot learn from earlier parts of a
    sequence as the sequence gets longer. LSTMs applied a unique gating architecture
    to control the flow of information within the network, enabling them to maintain
    and access information over long sequences without suffering from the vanishing
    gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The name “**long short-term memory**” refers to the network’s ability to keep
    track of information over both short and long sequences of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Short-term**: LSTMs can remember recent information, which is useful for
    understanding the current context. For example, in language modeling, knowing
    the last few words can be crucial for predicting the next word. Consider a phrase
    such as, “The cat, which already ate a lot, was not hungry.” As the LSTM processes
    the text, when it reaches the word “not,” the recent information that the cat
    “ate a lot” is crucial to predict the next word, “hungry,” accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term**: Unlike standard RNNs, LSTMs are also capable of retaining information
    from many steps back in the sequence, which is particularly useful for long-range
    dependencies, where a piece of information early in a sentence could be important
    for understanding a word much later in the sequence. In the same phrase, the information
    that “The cat” is the subject of the sentence is introduced early on. This information
    is crucial later to understand who “was not hungry” as it processes the later
    part of the sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **M** or **memory** in LSTMs is maintained through a unique architecture
    that employs three gating mechanisms—input, output, and forget gates. These gates
    control the flow of information within the network, deciding what information
    should be kept, discarded, or used at each step in the sequence, enabling LSTMs
    to maintain and access information over long sequences. Effectively, these gates
    and the network state allowed LTSMs to carry the “memory” across time steps, ensuring
    that valuable information was retained throughout the processing of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, LSTMs obtained state-of-the-art results on many language modeling
    and text classification benchmarks. They became the dominant NN architecture for
    NLP tasks due to their ability to capture short- and long-range contextual relationships.
  prefs: []
  type: TYPE_NORMAL
- en: The success of LSTMs demonstrated the potential of neural architectures in capturing
    the complex relationships inherent in language, significantly advancing the field
    of NLP. However, the continuous pursuit of more efficient and effective models
    led the community toward exploring other NN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Rise of CNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Around 2014, the NLP domain witnessed a rise in the popularity of CNNs for tackling
    NLP tasks, a notable shift led by Yoon Kim. CNNs (originally brought forward by
    LeCun et al. for image recognition) operate based on convolutional layers that
    scan the input by moving a filter (or kernel) across the input data, at each position
    calculating the dot product of the filter’s weights and the input data. In NLP,
    these layers work over local n-gram windows (consecutive sequences of *n* words)
    to identify patterns or features, such as specific sequences of words or characters
    in the text. Employing convolutional layers over local n-gram windows, CNNs scan
    and analyze the data to detect initial patterns or features. Following this, pooling
    layers are used to reduce the dimensionality of the data, which helps in both
    reducing computational complexity and focusing on the most salient features identified
    by the convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: Combining convolutional and pooling layers, CNNs can extract hierarchical features.
    These features represent information at different levels of abstraction by combining
    simpler, lower-level features to form more complex, higher-level features. In
    NLP, this process might start with detecting basic patterns such as common word
    pairs or phrases in the initial layers, progressing to recognizing more abstract
    concepts such as semantic relationships in the higher layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For comparison, we again consider a scenario where a CNN is employed to analyze
    and categorize customer reviews into positive, negative, or neutral sentiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lower-level features (initial layers)**: The CNN might identify basic patterns
    such as common word pairs or phrases in the initial layers. For instance, it might
    recognize phrases such as “great service,” “terrible experience,” or “not happy.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermediate-level features (middle layers)**: As data progresses through
    the network, middle layers might start recognizing more complex patterns, such
    as negations (“not good”) or contrasts (“good but expensive”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Higher-level features (later layers)**: The CNN could identify abstract concepts
    such as overall sentiment in the later layers. For instance, it might deduce a
    positive sentiment from phrases such as “excellent service” or “loved the ambiance”
    and a negative sentiment from phrases such as “worst experience” or “terrible
    food.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, CNNs inherently learn higher-level abstract representations of
    text. Although they lack the sequential processing characteristic of RNNs, they
    offer a computational advantage due to their inherent **parallelism** or ability
    to process multiple parts of the data simultaneously. Unlike RNNs, which process
    sequences iteratively and require the previous step to be completed before proceeding
    to the next, CNNs can process various parts of the input data in parallel, significantly
    speeding up training times.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs, while efficient, have a limitation in their convolution operation, which
    only processes local data from smaller or nearby regions, thereby missing relationships
    across more significant portions of the entire input data, referred to as global
    information. This gave rise to attention-augmented convolutional networks that
    integrate self-attention with convolutions to address this limitation. Self-attention,
    initially used in sequence and generative modeling, was adapted for visual tasks
    such as image classification, enabling the network to process and capture relationships
    across the entire input data. However, attention augmentation, which combines
    convolutions and self-attention, yielded the best results. This method retained
    the computational efficiency of CNNs and captured global information, marking
    an advancement in image classification and object detection tasks. We will discuss
    self-attention in detail later as it became a critical component of the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of CNNs to process multiple parts of data simultaneously marked
    a significant advancement in computational efficiency, paving the way for further
    innovations in NN architectures for NLP. As the field progressed, a pivotal shift
    occurred with the advent of attention-augmented NNs, introducing a new paradigm
    in how models handle sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: The emergence of the Transformer in advanced language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2017, inspired by the capabilities of CNNs and the innovative application
    of attention mechanisms, Vaswani et al. introduced the transformer architecture
    in the seminal paper *Attention is All You Need*. The original transformer applied
    several novel methods, particularly emphasizing the instrumental impact of attention.
    It employed a **self-attention mechanism**, allowing each element in the input
    sequence to focus on distinct parts of the sequence, capturing dependencies regardless
    of their positions in a structured manner. The term “self” in “self-attention”
    refers to how the attention mechanism is applied to the input sequence itself,
    meaning each element in the sequence is compared to every other element to determine
    its attention scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'To truly appreciate how the transformer architecture works, we can describe
    how the components in its architecture play a role in handling a particular task.
    Suppose we need our transformer to translate the English sentence “*Hello, how
    are you?*” into French: “*Bonjour, comment ça va?*” Let us walk through this step
    by step to examine and elucidate how the transformer might accomplish this task.
    For now, we will describe each step in detail and later implement the full architecture
    using Python.'
  prefs: []
  type: TYPE_NORMAL
- en: Components of the transformer architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving into how the transformer model fulfills our translation task,
    we need to understand the steps involved. The complete architecture is quite dense,
    so we will break it down into small, logical, and digestible components.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we discuss the two components central to the architectural design of
    the transformer model: the encoder and decoder stacks. We will also explain how
    data flows within these layer stacks, including the concept of tokens, and how
    relationships between tokens are captured and refined using critical techniques
    such as self-attention and FFNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we transition into the training process of the transformer model. Here,
    we review fundamental concepts such as batches, masking, the training loop, data
    preparation, optimizer selection, and strategies to improve performance. We will
    explain how the transformer optimizes performance using a loss function, which
    is crucial in shaping how the model learns to translate.
  prefs: []
  type: TYPE_NORMAL
- en: Following the training process, we discuss model inference, which is how our
    trained model generates translations. This section points out the order in which
    individual model components operate during translation and emphasizes the importance
    of each step.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, central to the transformer are two vital components, often called
    the encoder stack and the decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder and decoder stacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of the transformer model, **stacks** reference a hierarchical
    arrangement of **layers**. Each layer in this context is, in fact, an NN layer
    like the layers we come across in classical DL models. While a layer is a level
    in the model where specific computational operations occur, a stack refers to
    multiple such layers arranged consecutively.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder stack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider our example sentence “*Hello, how are you?*”. We first convert it
    into tokens. Each token typically represents a word. In the case of our example
    sentence, tokenization would break it down into separate tokens, resulting in
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[“Hello”, “,”, “how”, “are”, “``you”, “?”]`'
  prefs: []
  type: TYPE_NORMAL
- en: Here, each word or punctuation represents a distinct token. These tokens are
    then transformed into numerical representations, also known as **embeddings**.
    These embedding vectors capture the semantic meaning and context of the words,
    enabling the model to understand and process the input data effectively. The embeddings
    aid in capturing complex relationships and contexts from the original English
    input sentence through this series of transformations across layers.
  prefs: []
  type: TYPE_NORMAL
- en: This stack comprises multiple layers, where each layer applies self-attention
    and FFN computations on its input data (which we will describe in detail shortly).
    The embeddings iteratively capture complex relationships and context from the
    original English input sentence through this series of transformations across
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder stack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once the encoder completes its task, the output vectors—or the embeddings of
    the input sentence that hold its contextual information—are passed on to the decoder.
    Within the decoder stack, multiple layers work sequentially to generate a French
    translation from the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The process begins by converting the first embedding into the French phrase
    “*Bonjour*.” The subsequent layer uses the following embedding and context from
    the previously generated words to predict the next word in the French sentence.
    This process is repeated through all the layers in the stack, each using input
    embeddings and generated words to define and refine the translation.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder stack progressively builds (or decodes) the translated sentence
    through this iterative process, arriving at “*Bonjour, comment* *ça va?*”.
  prefs: []
  type: TYPE_NORMAL
- en: With an overall understanding of the encoder-decoder structure, our next step
    is unraveling the intricate operations within each stack. However, before delving
    into the self-attention mechanism and FFNs, there is one vital component we need
    to understand — positional encoding. Positional encoding is paramount to the transformer’s
    performance because it gives the transformer model a sense of the order of words,
    something subsequent operations in the stack lack.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every word in a sentence holds two types of information — its meaning and its
    role in the larger context of the sentence. The contextual role often stems from
    a word’s position in the arrangement of words. A sentence such as “*Hello, how
    are you?*” makes sense because the words are in a specific order. Change that
    to “*Are you, how hello?*” and the meaning becomes unclear.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, Vaswani et al. introduced **positional encoding** to ensure that
    the transformer encodes each word with additional data about its position in the
    sentence. Positional encodings are computed using a blend of sine and cosine functions
    across different frequencies, which generate a unique set of values for each position
    in a sequence. These values are then added to the original embeddings of the tokens,
    providing a way for the model to capture the order of words. These enriched embeddings
    are then ready to be processed by the self-attention mechanism in the subsequent
    layers of the transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Self-attention mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As each token of our input sentence “*Hello, how are you?*” passes through each
    layer of the encoder stack, it undergoes a transformation via the self-attention
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, the self-attention mechanism allows each token (word)
    to attend to (or focus on) other vital tokens to understand the full context within
    the sentence. Before encoding a particular word, this attention mechanism interprets
    the relationship between each word and the others in the sequence. It then assigns
    distinct attention scores to different words based on their relevance to the current
    word being processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider again our input sentence “*Hello, how are you?*”. When the self-attention
    mechanism is processing the last word, “*you*,” it does not just focus on “*you*.”
    Instead, it takes into consideration the entire sentence: it looks at “*Hello*,”
    glances over “*how*,” reflects on “*are*,” and, of course, focuses on “*you*.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'In doing so, it assigns various levels of attention to each word. You can visualize
    attention (*Figure 3**.1*) as lines connecting “*you*” to every other word. The
    line to “*Hello*” might be thick, indicating a lot of attention, representing
    the influence of “*Hello*” on the encoding of “*you*.” The line connecting “*you*”
    and “*how*” might be thinner, suggesting less attention given to “*how*.” The
    lines to “*are*” and “*you*” would have other thicknesses based on how they help
    in providing context to “*you*”:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Self-attention mechanism](img/B21773_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Self-attention mechanism'
  prefs: []
  type: TYPE_NORMAL
- en: This way, when encoding “*you*,” a weighted mix of the entire sentence is considered,
    not just the single word. And these weights defining the mix are what we refer
    to as attention scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'The self-attention mechanism is implemented through a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, each input word is represented as a vector, which we obtain from
    the word embedding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These vectors are then mapped to new vectors called query, key, and value vectors
    through learned transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An attention score for each word is then computed by taking the dot product
    of the query vector of the word with the key vector of every other word, followed
    by a SoftMax operation (which we will describe later).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These scores indicate how much focus to place on other parts of the input sentence
    for each word as it is encoded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, a weighted sum of the value vectors is computed based on these scores
    to give us our final output vectors, or the self-attention outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is important to note that this computation is done for each word in the sentence.
    This ensures a comprehensive understanding of the context in the sentence, considering
    multiple parts of the sentence at once. This concept set the transformer apart
    from nearly every model that came before it.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of running the self-attention mechanism once (or “single-head” attention),
    the transformer replicates the self-attention mechanism multiple times in parallel.
    Each replica or head operates on the same input but has its own independent set
    of learned parameters to compute the attention scores. This allows each head to
    learn different contextual relationships between words. This parallel process
    is known as **multi-head** **attention** (**MHA**).
  prefs: []
  type: TYPE_NORMAL
- en: Imagine our sentence “*Hello, how are you?*” again. One head might concentrate
    on how “*Hello*” relates to “*you*,” whereas another head might focus more on
    how “*how*” relates to “*you*.” Each head has its own set of query, key, and value
    weights, further enabling them to specialize and learn different things. The outputs
    of these multiple heads are then concatenated and transformed to produce final
    values passed onto the next layer in the stack.
  prefs: []
  type: TYPE_NORMAL
- en: This multi-head approach allows the model to capture a wider range of information
    from the same input words. It is like having several perspectives on the same
    sentence, each providing unique insights.
  prefs: []
  type: TYPE_NORMAL
- en: So far, for our input sentence “*Hello, how are you?*”, we have converted each
    word into token representations, which are then contextualized using the MHA mechanism.
    Through parallel self-attention, our transformer can consider the full range of
    interactions between each word and every other word in the sentence. We now have
    a set of diverse and context-enriched word representations, each containing a
    textured understanding of a word’s role in the sentence. However, this contextual
    understanding contained within the attention mechanism is just one component of
    the information processing in our transformer model. Next comes another layer
    of interpretation through position-wise FFNs. The FFN will add further nuances
    to these representations, making them more informative and valuable for our translation
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we discuss a vital aspect of the transformer’s training
    sequence: masking. Specifically, the transformer applies causal (or look-ahead)
    masking during the decoder self-attention to ensure that each output token prediction
    depends only on previously generated tokens, not future unknown tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Masking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The transformer applies two types of masking during training. The first is a
    preprocessing step to ensure input sentences are of the same length, which enables
    efficient batch computation. The second is look-ahead (or causal) masking, which
    allows the model to selectively ignore future tokens in a sequence. This type
    of masking occurs in the self-attention mechanism in the decoder and prevents
    the model from peeking ahead at future tokens in the sequence. For example, when
    translating the word “*Hello*” to French, look-ahead masking ensures that the
    model does not have access to the subsequent words “*how*,” “*are*,” or “*you*.”
    This way, the model learns to generate translations based on the current and preceding
    words, adhering to a natural progression in translation tasks, mimicking that
    of human translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a clearer understanding of how data is prepared and masked for training,
    we now transition to another significant aspect of the training process: hyperparameters.
    Unlike parameters learned from the data, hyperparameters are configurations set
    before training to control the model optimization process and guide the learning
    journey. The following section will explore various hyperparameters and their
    roles during training.'
  prefs: []
  type: TYPE_NORMAL
- en: SoftMax
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand the role of the FFN, we can describe its two primary components—linear
    transformations and an activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear transformations** are essentially matrix multiplications. Think of
    them as tools that reshape or tweak the input data. In the FFN, these transformations
    occur twice, where two different weights (or matrices) are used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **rectified linear unit** (**ReLU**) function is applied between these two
    transformations. The role of the ReLU function is to introduce non-linearity in
    the model. Simply put, the ReLU function allows the model to capture patterns
    within the input data that are not strictly proportional, i.e., non-linear, which
    is typical of **natural language** (**NL**) data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FFN is called **position-wise** because it treats each word in the sentence
    separately (position by position), regardless of the sequence. This contrasts
    with the self-attention mechanism, which considers the entire sequence at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let us attempt to visualize the process: Imagine our word “*Hello*” arriving
    here after going through the self-attention mechanism. It carries with it information
    about its own identity mixed with contextual references to “*how*,” “*are*,” and
    “*you*.” This integrated information resides within a vector that characterizes
    “*Hello*.”'
  prefs: []
  type: TYPE_NORMAL
- en: When “*Hello*” enters the FFN, picture it as a tunnel with two gates. At the
    first gate (or linear layer), “*Hello*” is transformed by a matrix multiplication
    operation, changing its representation. Afterward, it encounters the ReLU function—which
    makes the representation non-linear.
  prefs: []
  type: TYPE_NORMAL
- en: After this, “*Hello*” passes through a second gate (another linear layer), emerging
    on the other side transformed yet again. The core identity of “*Hello*” remains
    but is now imbued with even more context, carefully calibrated and adjusted by
    the FFN.
  prefs: []
  type: TYPE_NORMAL
- en: Once the input passes through the gates, there is one additional step. The transformed
    vector still must be converted into a form that can be interpreted as a prediction
    for our final translation task.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to using the SoftMax function, the final transformation within
    the transformer’s decoder. After the vectors pass through the FFN, they are further
    processed through a final linear layer. The result is then fed into a SoftMax
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'SoftMax serves as a mechanism for converting the output of our model into a
    form that can be interpreted as probabilities. In essence, the SoftMax function
    will take the output from our final linear layer (which could be any set of real
    numbers) and transform it into a distribution of probabilities, representing the
    likelihood of each word being the next word in our output sequence. For example,
    if our target vocabulary includes “*Bonjour*,” “*Hola*,” “*Hello*,” and “*Hallo*,”
    the SoftMax function will assign each of these words a probability, and the word
    with the highest probability will be chosen as the output translation for the
    word “*Hello*.” We can illustrate with this oversimplified representation of the
    output probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[ Bonjour: 0.4, Hola: 0.3, Hello: 0.2, Hallo:` `0.1 ]`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.2* shows a more complete (albeit oversimplified) view of the flow
    of information through the architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: A simplified illustration of the transformer](img/B21773_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: A simplified illustration of the transformer'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve introduced the architectural components of the transformer, we
    are poised to understand how its components work together.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The components of a transformer come together to learn from data using a mechanism
    known as **sequence-to-sequence** (**Seq2Seq**) learning, a subset of **supervised
    learning** (**SL**). Recall that SL is a technique that uses labeled data to train
    models to predict outcomes accurately. In Seq2Seq learning, we provide the transformer
    with training data that comprises examples of input and corresponding correct
    output, which, in this case, are correct translations. Seq2Seq learning is particularly
    well suited for tasks such as machine translation where both the input and output
    are sequences of words.
  prefs: []
  type: TYPE_NORMAL
- en: The very first step in the learning process is to convert each word in the phrase
    into tokens, which are then transformed into numerical embeddings. These embeddings
    carry the semantic essence of each word. Positional encodings are computed and
    added to these embeddings to imbue them with positional awareness.
  prefs: []
  type: TYPE_NORMAL
- en: As these enriched embeddings traverse through the encoder stack, within each
    layer, the self-attention mechanism refines the embeddings by aggregating contextual
    information from the entire phrase. Following self-attention, each word’s embedding
    undergoes further transformation in the position-wise FFNs, adjusting the embeddings
    to capture even more complex relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Upon exiting the encoder, the embeddings now hold a rich mixture of semantic
    and contextual information. They are passed onto the decoder stack, which aims
    to translate the phrase into another language (that is, the target sequence).
    As with the encoder, each layer in the decoder also employs self-attention and
    position-wise FFNs, but with an additional layer of cross-attention that interacts
    with the encoder’s outputs. This interaction helps align the input and output
    phrases, a crucial aspect of translation.
  prefs: []
  type: TYPE_NORMAL
- en: As the embeddings move through the decoder layers, they are progressively refined
    to represent the translated phrase that the model will predict. The final layer
    of the decoder processes the embeddings through a linear transformation and SoftMax
    function to produce a probability distribution over the target vocabulary. This
    distribution defines the model’s predicted likelihood for each potential next
    token at each step. The decoder then samples from this distribution to select
    the token with the highest predicted probability as its next output. By iteratively
    sampling the most likely next tokens according to the predicted distributions,
    the decoder can autoregressively generate the full translated output sequence
    token by token.
  prefs: []
  type: TYPE_NORMAL
- en: However, for the transformer to reliably sample from the predicted next-token
    distributions to generate high-quality translations, it must progressively learn
    by iterating over thousands of examples of input-output pairs. In the next section,
    we explore model training in further detail.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed, the primary goal of the training phase is to refine the model’s
    parameters to facilitate accurate translation from one language to another. But
    what does the refinement of parameters entail, and why is it pivotal?
  prefs: []
  type: TYPE_NORMAL
- en: Parameters are internal variables that the model utilizes to generate translations.
    Initially, these parameters are assigned random values, which are adjusted with
    each training iteration. Again, the model is provided with training data that
    comprises thousands of examples of input data and corresponding correct output,
    which, in this case, is the correct translation. It then compares its predicted
    output tokens to the correct (or actual) target sequences using an error (or loss)
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the loss, the model updates its parameters, gradually improving its
    ability to choose the correct item in the sequence at each step of decoding. This
    slowly refines the probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Over thousands of training iterations, the model learns associations between
    source and target languages. Eventually, it acquires enough knowledge to decode
    coherent, human-like translations from unseen inputs by relying on patterns discovered
    during training. Therefore, training drives the model’s ability to produce accurate
    target sequences from the predicted vocabulary distributions.
  prefs: []
  type: TYPE_NORMAL
- en: After training on sufficient translation pairs, the transformer reaches reliable
    translation performance. The trained model can then take in new input sequences
    and output translated sequences by generalizing to that new data.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, with our example sentence “*Hello, how are you?*” and its French
    translation “*Bonjour, comment ça va?*”, the English sentence serves as the input,
    and the French sentence serves as the target output. The training data comprises
    many translated pairs. Each time the model processes a batch of data, it generates
    predictions for the translation, compares them to the actual target translations,
    and then adjusts its parameters to reduce the discrepancy (or minimize the loss)
    between the predicted and actual translations. This is repeated with numerous
    batches of data until the model’s translations are sufficiently accurate.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, unlike parameters, which the model learns from the training data, hyperparameters
    are preset configurations that govern the training process and the structure of
    the model. They are a crucial part of setting up a successful training run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key hyperparameters in the context of transformer models include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate**: This value determines the step size at which the optimizer
    updates the model parameters. A higher learning rate could speed up the training
    but may overshoot the optimal solution. A lower learning rate may result in a
    more precise convergence to the optimal solution, albeit at the cost of longer
    training time. We will discuss optimizers in detail in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: The number of data examples processed in a single batch affects
    the computational accuracy and the memory requirements during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model dimensions**: The model’s size (for example, the number of layers in
    the encoder and decoder, the dimensionality of the embeddings, and so on) is a
    crucial hyperparameter that affects the model’s capacity to learn and generalize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer settings**: Choosing an optimizer and its settings, such as the
    initial learning rate, beta values in the Adam optimizer, and so on, are also
    considered hyperparameters. Again, we will explore optimizers further in the next
    section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization terms**: Regularization terms such as dropout rate are hyperparameters
    that help prevent overfitting by adding some form of randomness or constraint
    to the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the proper values for hyperparameters is crucial for the training
    process as it significantly impacts the model’s performance and efficiency. It
    often involves hyperparameter tuning, which involves experimentation and refining
    to find values for hyperparameters that yield reliable performance for a given
    task. Hyperparameter tuning can be somewhat of an art and a science. We will touch
    on this more in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: With a high-level grasp of hyperparameters, we will move on to the choice of
    optimizer, which is pivotal in controlling how efficiently the model learns from
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Choice of optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The optimizer is a fundamental component of the training process and is responsible
    for updating the model’s parameters to minimize error. Different optimizers have
    different strategies for navigating the parameter space to find a set of parameter
    values that yield low loss (or less error). The choice of optimizer can significantly
    impact the speed and quality of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of transformer models, the Adam optimizer is often the optimizer
    of choice due to its efficiency and empirical success in training deep networks.
    Adam adapts learning rates during training. For simplicity, we will not explore
    all the possible optimizers but instead describe their purpose.
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer’s primary task is to fine-tune the model’s parameters to reduce
    translation errors, progressively guiding the model toward the desired level of
    performance. However, an over-zealous optimization could lead the model to memorize
    the training data, failing to generalize well to unseen data. To mitigate this,
    we employ regularization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore regularization—a technique that works with
    optimization to ensure that while the model learns to minimize translation errors,
    it also remains adaptable to new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regularization techniques are employed to deter the model from memorizing the
    training data (a phenomenon known as overfitting) and to promote better performance
    on new, unseen data. Overfitting arises when the model, to minimize the error,
    learns the training data to such an extent that it captures useless patterns (or
    noise) along with the actual patterns. This over-precision in learning the training
    data leads to a decline in performance when the model is exposed to new data.
  prefs: []
  type: TYPE_NORMAL
- en: Let us revisit our simple scenario where we train a model to translate English
    greetings to French greetings using a dataset that includes the word “*Hello*”
    and its translation “*Bonjour*.” If the model is overfitting, it may memorize
    the exact phrases from the training data without understanding the broader translation
    pattern.
  prefs: []
  type: TYPE_NORMAL
- en: In an overfit scenario, suppose the model learns to translate “*Hello*” to “*Bonjour*”
    with a probability of 1.0 because that is what it encountered most often in the
    training data. When presented with new, unseen data, it may encounter variations
    it has not seen before, such as “*Hi*,” which should also translate to “*Bonjour*.”
    However, due to overfitting, the model might fail to generalize from “*Hello*”
    to “*Hi*” as it is overly focused on the exact mappings it saw during training.
  prefs: []
  type: TYPE_NORMAL
- en: Several regularization techniques can mitigate the overfitting problem. These
    techniques apply certain constraints on the model’s parameters during training,
    encouraging the model to learn a more generalized representation of the data rather
    than memorizing the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some standard regularization techniques used in the context of transformer
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout**: In the context of NN-based models such as the transformer, the
    term “neurons” refers to individual elements within the model that work together
    to learn from the data and make predictions. Each neuron learns specific aspects
    or features from the data, enabling the model to understand and translate text.
    During training, dropout randomly deactivates or “drops out” a fraction of these
    neurons, temporarily removing them from the network. This random deactivation
    encourages the model to spread its learning across many neurons rather than relying
    too heavily on a few. By doing so, dropout helps the model to better generalize
    its learning to unseen data rather than merely memorizing the training data (that
    is, overfitting).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer normalization**: Layer normalization is a technique that normalizes
    the activations of neurons in a layer for each training example rather than across
    a batch of examples. This normalization helps stabilize the training process and
    acts as a form of regularization, preventing overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L1 or L2 regularization**: L1 regularization, also known as Lasso, adds a
    penalty equal to the absolute magnitude of coefficients, promoting parameter sparsity.
    L2 regularization, or Ridge, adds a penalty based on the square of the coefficients,
    discouraging large values to prevent overfitting. Although these techniques help
    in controlling model complexity and enhancing generalization, they were not part
    of the transformer’s initial design.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By employing these regularization techniques, the model is guided toward learning
    more generalized patterns in the data, which improves its ability to perform well
    on unseen data, thus making the model more reliable and robust in translating
    new text inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the training process, we have mentioned the loss function and discussed
    how the optimizer leverages it to adjust the model’s parameters, aiming to minimize
    prediction error. The loss function quantifies the model’s performance. We discussed
    how regularization penalizes the loss function to prevent overfitting, encouraging
    the model to learn simpler, more generalizable patterns. In the next section,
    we look closer at the nuanced role of the loss function itself.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The loss function is vital in training the transformer model, quantifying the
    differences between the model’s predictions and the actual data. In language translation,
    this error is measured between generated and actual translations in the training
    dataset. A common choice for this task is cross-entropy loss, which measures the
    difference between the model’s predicted probability distribution across the target
    vocabulary and the actual distribution, where the truth has a probability of 1
    for the correct word and 0 for the rest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformer often employs a variant known as label-smoothed cross-entropy
    loss. Label smoothing adjusts the target probability distribution during training,
    slightly lowering the probability for the correct class and increasing the probability
    for all other classes, which helps prevent the model from becoming too confident
    in its predictions. For instance, with a target vocabulary comprising “*Bonjour*,”
    “*Hola*,” “*Hello*,” and “*Hallo*,” and assuming “*Bonjour*” is the correct translation,
    a standard cross-entropy loss would aim for the probability distribution of `Bonjour:
    1.0`, `Hola: 0.0`, `Hello: 0.0`, `Hallo: 0.0`. However, the label-smoothed cross-entropy
    loss would slightly adjust these probabilities, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[ “Bonjour”: 0.925, “Hola”: 0.025, “Hello”: 0.025, “Hallo”:` `0.025 ]`'
  prefs: []
  type: TYPE_NORMAL
- en: The smoothing reduces the model’s confidence and promotes better generalization
    to unseen data. With a clearer understanding of the loss function’s role, we can
    move on to the inference phase, where the trained model generates translations
    for new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having traversed the training landscape, our trained model is now adept with
    optimized parameters to tackle the translation task. In the inference stage, these
    learned parameters are employed to translate new, unseen text. We will continue
    with our example phrase “*Hello, how are you?*” to elucidate this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inference stage is the practical application of the trained model on new
    data. The trained parameters, refined after numerous iterations during training,
    are now used to translate text from one language to another. The inference steps
    can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input preparation**: Initially, our phrase “Hello, how are you?” is tokenized
    and encoded into a format that the model can process, akin to the preparation
    steps in the training phase.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Passing through the model**: The encoded input is then propagated through
    the model. As it navigates through the encoder and decoder stacks, the trained
    parameters guide the transformation of the input data, inching closer to accurate
    translations at each step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output generation**: At the culmination of the decoder stack, the model generates
    a probability distribution across the target vocabulary for each word in the input
    text. For the word “*Hello*,” a probability distribution is formed over the target
    vocabulary, which, in our case, comprises French words. The word with the highest
    probability is selected as the translation. This process is replicated for each
    word in the phrase, rendering the translated output “*Bonjour, comment* *ça va?*”.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we understand how the model produces the final output, we can implement
    a transformer model step by step to solidify the concepts we have discussed. However,
    before we dive into the code, we can briefly give a synopsis of the end-to-end
    architecture flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input tokenization**: The initial English phrase “*Hello, how are you?*”
    is tokenized into smaller units such as “*Hello*,” “*,*,” “*how*,” and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Embeddings**: These tokens are then mapped to continuous vector representations
    through an embedding layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Positional encoding**: To preserve the order of the sequence, positional
    encodings are added to the embeddings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encoder self-attention**: The embedded input sequence navigates through the
    encoder’s sequence of self-attention layers. Here, each word gauges the relevance
    of every other word to comprehend the full context.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**FFN**: The representations are subsequently refined by position-wise FFNs
    within each encoder layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encoder output**: The encoder renders contextual representations capturing
    the essence of the input sequence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decoder attention**: Incrementally, the decoder crafts the output sequence,
    employing self-attention solely on preceding words to maintain the sequence order.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encoder-decoder attention**: The decoder evaluates the encoder’s output,
    centering on pertinent input context while generating each word in the output
    sequence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output layers**: The decoder feeds its output to the linear and SoftMax layers
    to produce “*Bonjour, comment* *ça va?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of this chapter, we will adapt a best-in-class implementation of
    the original transformer (Huang et al., 2022) into a minimal example that could
    later be trained on various downstream tasks. This will serve as a theoretical
    exercise to further solidify our understanding. In practice, we would rely on
    pre-trained or foundation models, which we will learn to implement in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we begin our practice project, we can trace its impact on the
    current landscape of GenAI. We follow the trajectory of early applications of
    the architecture (for example, **Bidirectional Encoded Representations from Transformers**
    (**BERT**)) through to the first GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Evolving language models – the AR Transformer and its role in GenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B21773_02.xhtml#_idTextAnchor045), we reviewed some of the
    generative paradigms that apply a transformer-based approach. Here, we trace the
    evolution of Transformers more closely, outlining some of the most impactful transformer-based
    language models from the initial transformer in 2017 to more recent state-of-the-art
    models that demonstrate the scalability, versatility, and societal considerations
    involved in this fast-moving domain of AI (as illustrated in *Figure 3**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: From the original transformer to GPT-4](img/B21773_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: From the original transformer to GPT-4'
  prefs: []
  type: TYPE_NORMAL
- en: '**2017 – Transformer**: The transformer model, introduced by Vaswani et al.,
    was a paradigm shift in NLP, featuring self-attention layers that could process
    entire sequences of data in parallel. This architecture enabled the model to evaluate
    the importance of each word in a sentence relative to all other words, thereby
    enhancing the model’s ability to capture the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 – BERT**: Google’s BERT model innovated on the transformer architecture
    by utilizing a bidirectional context in its encoder layers during pre-training.
    It was one of the first models to understand the context of a word based on its
    entire sentence, both left and right, significantly improving performance on a
    wide range of NLP tasks, especially those requiring a deep understanding of context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 – GPT-1**: OpenAI’s GPT-1 model was a milestone in NLP, adopting a generative
    pre-trained approach with a transformer’s decoder-only model. It was pre-trained
    on a diverse corpus of text data and fine-tuned for various tasks, using a unidirectional
    approach that generated text sequentially from left to right, which was particularly
    suited for generative text applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2019 – GPT-2**: GPT-2 built upon the foundation laid by GPT-1, maintaining
    its decoder-only architecture but significantly expanding its scale in terms of
    dataset and model size. This allowed GPT-2 to generate text that was more coherent
    and contextually relevant across a broader range of topics, demonstrating the
    power of scaling up transformer models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2020 – GPT-3**: OpenAI’s GPT-3 pushed the boundaries of scale in transformer
    models to 175 billion parameters, enabling a wide range of tasks to be performed
    with minimal input, often with **zero-shot learning** (**ZSL**) or **few-shot
    learning** (**FSL**). This showed that Transformers could generalize across tasks
    and data types, often without the need for extensive task-specific data or fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2021 – InstructGPT**: An optimized variant of GPT-3, InstructGPT was fine-tuned
    specifically to follow user instructions and generate aligned responses, incorporating
    feedback loops that emphasized safety and relevance in its outputs. This represented
    a focus on creating AI models that could more accurately interpret and respond
    to human prompts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2023 – GPT-4**: GPT-4 was an evolution of OpenAI’s transformer models into
    the multimodal space, capable of understanding and generating content based on
    both text and images. This model aimed to produce safer and more contextually
    nuanced responses, showcasing a significant advancement in the model’s ability
    to handle complex tasks and generate creative content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2023 – LLaMA 2**: Meta AI’s LLaMA 2 was part of a suite of models that focused
    on efficiency and accessibility, allowing for high-performance language modeling
    while being more resource-efficient. This model was aimed at facilitating a broader
    range of research and application development within the AI community.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2023 – Claude 2**: Anthropic’s Claude 2 was an advancement over Claude 1,
    increasing its token context window and improving its reasoning and memory capabilities.
    It aimed to align more closely with human values, offering responsible and nuanced
    generative capabilities for open-domain question-answering and other conversational
    AI applications, marking progress in ethical AI development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The timeline presented highlights the remarkable progress in transformer-based
    language models over the past several years. What originated as an architecture
    that introduced the concept of self-attention has rapidly evolved into models
    with billions of parameters that can generate coherent text, answer questions,
    and perform a variety of intellectual tasks at high levels of performance. The
    increase in scale and accessibility of models such as GPT-4 has opened new possibilities
    for AI applications. At the same time, recent models have illustrated a focus
    on safety and ethics and providing more nuanced, helpful responses to users.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we accomplish a rite of passage for practitioners with
    an interest in the NL field. We implement the key components of the original transformer
    architecture using Python to more fully understand the mechanics that started
    it all.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the original Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following code demonstrates how to implement a minimal transformer model
    for a Seq2Seq translation task, mainly translating English text to French. The
    code is structured into multiple sections, handling various aspects from data
    loading to model training and translation.
  prefs: []
  type: TYPE_NORMAL
- en: Data loading and preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Initially, the code loads a dataset and prepares it for training. The data
    is loaded from a CSV file, which is then split into English and French text. The
    text is limited to 100 characters for demonstration purposes to reduce training
    time. The CSV file includes a few thousand example data points and can be found
    in the book’s GitHub repository ([https://github.com/PacktPublishing/Python-Generative-AI](https://github.com/PacktPublishing/Python-Generative-AI))
    along with the complete code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, a tokenizer is trained on the text data. The tokenizer is essential for
    converting text data into numerical data that can be fed into the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Data tensorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The text data is then tensorized, which involves converting the text data into
    tensor format. This step is crucial for preparing the data for training with `PyTorch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Dataset creation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A custom dataset class is created to handle the data. This class is essential
    for loading the data in batches during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Embeddings layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The embeddings layer maps each token to a continuous vector space. This layer
    is crucial for the model to understand and process the text data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Positional encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The positional encoding layer adds positional information to the embeddings,
    which helps the model understand the order of tokens in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Multi-head self-attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **multi-head self-attention** (**MHSA**) layer is a crucial part of the
    transformer architecture that allows the model to focus on different parts of
    the input sequence when producing an output sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: FFN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The FFN is a simple **fully connected NN** (**FCNN**) that operates independently
    on each position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Encoder layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The encoder layer consists of an MHSA mechanism and a simple FFNN. This structure
    is repeated in a stack to form the complete encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The encoder is a stack of identical layers with an MHSA mechanism and an FFN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Decoder layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similarly, the decoder layer consists of two MHA mechanisms—one self-attention
    and one cross-attention—followed by an FFN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The decoder is also a stack of identical layers. Each layer contains two MHA
    mechanisms and an FFN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This stacking layer pattern continues to build the transformer architecture.
    Each block has a specific role in processing the input data and generating output
    translations.
  prefs: []
  type: TYPE_NORMAL
- en: Complete transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The transformer model encapsulates the previously defined encoder and decoder
    structures. This is the primary class that will be used for training and translation
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Training function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `train` function iterates through the epochs and batches, calculates the
    loss, and updates the model parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Translation function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `translate` function uses the trained model to translate a source text
    into the target language. It generates a translation token by token and stops
    when an **end-of-sequence** (**EOS**) token is generated or when the maximum target
    length is reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Main execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the main block of the script, hyperparameters are defined, the tokenizer
    and model are instantiated, and training and translation processes are initiated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This script orchestrates a machine translation task from loading data to training
    a transformer model and eventually translating text from English to French. Initially,
    it loads a dataset, processes the text, and establishes tokenizers to convert
    text to numerical data. Following this, it defines the architecture of a transformer
    model in `PyTorch`, detailing each component from the embeddings’ self-attention
    mechanisms to the encoder and decoder stacks.
  prefs: []
  type: TYPE_NORMAL
- en: The script further organizes the data into batches, sets up a training loop,
    and defines a translation function. Training the model on the provided English
    and French sentences teaches it to map sequences from one language to another.
    Finally, it translates a sample sentence from English to French to demonstrate
    the model’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The advent of the transformer significantly propelled the field of NLP forward,
    serving as the foundation for today’s cutting-edge generative language models.
    This chapter delineated the progression of NLP that paved the way for this pivotal
    innovation. Initial statistical techniques such as count vectors and TF-IDF were
    adept at extracting rudimentary word patterns, yet they fell short in grasping
    semantic nuances.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating neural language models marked a stride toward more profound representations
    through word embeddings. Nevertheless, recurrent networks encountered hurdles
    in handling longer sequences. This inspired the emergence of CNNs, which introduced
    computational efficacy via parallelism, albeit at the expense of global contextual
    awareness.
  prefs: []
  type: TYPE_NORMAL
- en: The inception of attention mechanisms emerged as a cornerstone. In 2017, Vaswani
    et al. augmented these advancements, unveiling the transformer architecture. The
    hallmark self-attention mechanism of the transformer facilitates contextual modeling
    across extensive sequences in a parallelized manner. The layered encoder-decoder
    structure meticulously refines representations to discern relationships indispensable
    for endeavors such as translation.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer, with its parallelizable and scalable self-attention design,
    set new benchmarks in performance. Its core tenets are the architectural bedrock
    for contemporary high-achieving generative language models such as GPT.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how to apply pre-trained generative models
    from prototype to production.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  prefs: []
  type: TYPE_NORMAL
- en: Bahdanau, D., Cho, K., and Bengio, Y. (2014). *Neural machine translation by
    jointly learning to align and translate*. arXiv preprint arXiv:1409.0473.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio, Y., Ducharme, R., and Vincent, P. (2003). *A neural probabilistic language
    model. The Journal of Machine Learning Research*, 3, 1137-1155.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dadgar, S. M. H., Araghi, M. S., and Farahani, M. M. (2016). *Improving text
    classification performance based on TFIDF and LSI index*. 2016 IEEE International
    Conference on Engineering & Technology (ICETECH).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elman, J. L. (1990). *Finding structure in time. Cognitive science*, 14(2),
    179-211.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter, S., and Schmidhuber, J. (1997). *Long short-term memory. Neural
    computation*, 9(8), 1735-1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim, Y. (2014). *Convolutional neural networks for sentence classification*.
    arXiv preprint arXiv:1408.5882.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). *Distributed
    representations of words and phrases and their compositionality. Advances in neural
    information processing* *systems*, 26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington, J., Socher, R., and Manning, C. (2014). *GloVe: Global vectors
    for word representation. Proceedings of the 2014 conference on empirical methods
    in natural language processing (**EMNLP)*, 1532-1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    ... and Polosukhin, I. (2017). *Attention is all you need. Advances in neural
    information processing* *systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
