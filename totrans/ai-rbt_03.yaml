- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conceptualizing the Practical Robot Design Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter represents a *bridge* between the preceding chapters on general
    theory, introduction, and setup, and the following chapters, where we will apply
    problem-solving methods that use **artificial intelligence** (**AI**) techniques
    to robotics. The first step is to clearly state our problem, from the perspective
    of the use of the robot, which is different from our view as the designer/builder
    of the robot. Then, we need to decide how to approach each of the hardware- and
    software-based challenges that we and the robot will attempt. By the end of this
    chapter, you will be able to understand the process of how to design a robot systematically.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A systems engineering-based approach to robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding our task – cleaning up the playroom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to state the problem with the help of use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to approach solving problems with storyboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the scope of our use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying our hardware needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking down our software needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a specification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A systems engineering-based approach to robotics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you set out to create a complex robot with AI-based software, you can’t
    just jump in and start slinging code and throwing things together without some
    sort of game plan as to how the robot goes together and how all the parts communicate
    with one another. We will discuss a systematic approach to robot design based
    on **systems engineering** principles. We will be learning about use cases and
    will use storyboards as techniques to understand what we are building and what
    parts – hardware and software – are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding our task – cleaning up the playroom
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already talked a bit about our main task for Albert, our example robot
    for this book, which is to clean up the playroom in my house after my grandchildren
    come to visit. We need to provide a more formal definition of our problem, and
    then turn that into a list of tasks for the robot to perform along with a plan
    of action on how we might accomplish those tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why are we doing this? Well, consider this quote by Steve Maraboli:'
  prefs: []
  type: TYPE_NORMAL
- en: “If you don’t know where you are going, how do you know when you get there?”
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – It’s important to know what your robot does](img/B19846_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – It’s important to know what your robot does
  prefs: []
  type: TYPE_NORMAL
- en: 'The internet and various robot websites are littered with dozens of robots
    that share one fatal character flaw: the robot and its software were designed
    first and then they went out to look for a job for it. In the robot business,
    this is called the **ready, fire, aim problem**. The task, the customer, the purpose,
    the use, and the job of the robot comes first. Another way of saying this is:
    to create an effective tool, the first step is to decide what you do with it.'
  prefs: []
  type: TYPE_NORMAL
- en: I could have written this book as a set of theories and exercises that would
    have worked well in a classroom setting, which would have introduced you to a
    whole lot of new tools you would not know how to apply. However, this chapter
    is here to provide you with tools and methods to provide a path from having a
    good idea to having a good robot, with as little misdirection, pain, suffering,
    tears, and torn-out hair as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You are on your own with burns; please be careful with the soldering iron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process we will use is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to look at the robot from the user’s perspective and then
    describe what it does. We will call these descriptions **use cases** – examples
    of how the robot will be used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will break each use case down into **storyboards** (step-by-step illustrations),
    which can be word pictures or actual pictures. From the storyboards, we can extract
    tasks – a to-do list for our robot to accomplish.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final step for this part of the process is to separate the to-do list into
    things we can do with software and things we will need hardware for. This will
    give us detailed information for designing our robot and its AI-based software.
    Keep in mind that one of the robot’s uses is to be a good example for this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s start by looking at use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin our task with a statement of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Our robot’s task – part 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'About once or twice a month, my five delightful, intelligent, and playful grandchildren
    come to visit me and my wife. Like most grandparents, we keep a box full of toys
    in our upstairs playroom for them to play with during their visits. The first
    thing they do upon arrival – at least the older grandkids– is take every single
    toy out of the toy box and start playing. This results in the scene shown in the
    following photograph – toys randomly and uniformly distributed throughout the
    playroom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The playroom in the aftermath of the grandchildren](img/B19846_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The playroom in the aftermath of the grandchildren
  prefs: []
  type: TYPE_NORMAL
- en: Honestly, you could not get a better random distribution. They are really good
    at this. Since, as grandparents, our desire is to maximize the amount of time
    that our grandchildren have fun at our house and we want them to associate Granddad
    and Grandmother’s house with having fun, we don’t make them pick up the toys when
    they go home. You can see where this is heading.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, if you are a parent, let me apologize to you in advance; this is
    indeed an evil plot on our, the grandparents, part, and you’ll understand when
    you get grandkids of your own – and you will do this, too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where were we...? Yes, a room full of randomly and uniformly distributed foreign
    objects – toys – scattered about an otherwise serviceable playroom, which need
    to be removed. Normally, I’d just have to sigh heavily and pick up all this stuff
    myself, but I am a robot designer, so what I want to do is to make a robot that
    does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick up the toys – and not the furniture, lights, books, speakers, or other
    items in the room that are not toys.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Put them in the toy box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue to do this until there are no more toys to be found and then stop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a visual representation of this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Use case: pick up toys](img/B19846_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3 – Use case: pick up toys'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can ask some pertinent questions. I took journalism classes in school
    and I was taught the usefulness of the *5 Ws and an H* – *Who*, *What*, *When*,
    *Where*, *Why*, and *How*. These are just as useful for examining use cases. I’ve
    got one firm rule here in this section: no implementation details. Don’t worry
    about how you are going to do this. Just worry about defining the results. So
    we’ll leave out the *H* for now and focus on the *Ws*. Let’s give this a try:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Who**: The robot. That was easy. We want the robot to do something, as in
    the robot does this and not me. What do we want the robot to do?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What**: This question can be answered in two ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pick up toys and put them in the toy box**: What does this answer tell us?
    It says we are going to be grasping and lifting something – toys. What are toys?
    We could also rephrase this as a negative, which brings us to the second answer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pick up and put away in the toy box the items that were not previously in
    the room**: The toys were not in the room before the grandkids pulled them all
    out. So we either want to classify items as toys or as things that were not in
    the room before. *Not in the room* implies that the robot somehow knows what belongs
    in the room, possibly by making a survey prior to the children’s arrival. However,
    *toys* implies that the robot can classify objects at least as *toys* and *not
    toys*. Let’s stick with that for now. We may have some items in the room that
    are not toys but are out of place, and thus don’t belong in the toy box. You can
    already see these questions shaping what comes later in this process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When**: After the grandchildren have visited and they have left, continue
    to pick up toys until there are none left.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That gives us two conditions for *when*: a start and a stop. In this case,
    the start is defined as the grandkids have visited and they have left. Now, it
    is perfectly fair for me to state in the use case that I’ll tell the robot when
    these conditions are met, since that is not putting me out. I’ll be here, and
    I know when the room needs to be cleaned. Besides, I need to get the robot out
    and put it into the room. When not working, it stays on a bookshelf. So, let’s
    change our *when* statement to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*When I (the user) tell you to, and don’t stop until there are no more toys
    to* *be found.*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we could have decided that the robot needs to figure this out for itself
    and turn itself on after the grandchildren leave, but what is the return on investment
    for that? That would be a lot of work for not a lot of gain. The pain point for
    me, the user, is picking up toys, not deciding when to do it. This is a lot simpler.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note that my *when* statement has a start and an end. Anyone who watched Mickey
    Mouse in the *Sorcerer’s Apprentice* segment of *Fantasia* understands that when
    you have a robot, telling it when to stop can be important. Another important
    concept is defining the end condition. I did not say *stop when all of the toys
    are picked up* because that would imply the robot needed to know all of the toys,
    either by sight or number. It is easier as a task definition to say *stop when
    you see no more toys* instead, which accomplishes what we want without adding
    additional requirements to our robot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is perfectly normal to have to revisit use cases as the robot designer understands
    more about the problem – sometimes you can be working hard to solve a problem
    that is not relevant to solving the user’s task. You can imagine some robot engineer
    in a team being given the task of *pick up all the toys* as meaning all toys ever
    invented, in all cultures, in all parts of the world! Then, you get a request
    for a $500,000 database software license and a server farm to house it. We just
    want to pick up the toys found in the playroom.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Where**: The playroom upstairs. Now we have some tricky parts. The area to
    be cleaned is a specific area of the house, but it is not really bound by walls.
    And it is upstairs – there is a stairway going down in the playroom that we don’t
    want our robot tumbling down. How would you have known this? You won’t unless
    you ask these kinds of questions! The environment the robot operates in is just
    as important as what it does. In this case, let’s go back and ask the user. I’ll
    stick in a floor plan for you here to define what I mean by *playroom*. On the
    bright side, we don’t need to climb or descend stairs in this task. But we do
    need to look out for the staircase as a hazard:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.4 – The floor plan of my house, upstairs](img/B19846_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – The floor plan of my house, upstairs
  prefs: []
  type: TYPE_NORMAL
- en: '**Why**: So why is the robot picking up toys? I’m tempted to just write “Because
    *someone* has to do it.” However, the answer is that I don’t want the grandkids
    to pick up toys so that they have the maximum time to play, and I don’t want to
    do it, either. So we are making a robot for this task. One maxim in the robot
    world is that proper tasks for robots are *dirty*, *dull*, or *dangerous*. This
    one definitely falls into the *dull* category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our robot has more than one use case – it has more than one function to perform.
  prefs: []
  type: TYPE_NORMAL
- en: Our robot’s task – part 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The robot needs to interact with my grandchildren. Why is this important? As
    I told you in [*Chapter 1*](B19846_01.xhtml#_idTextAnchor015), the grandchildren
    were introduced to some of my other robots, and the oldest grandkid, William,
    always tries to talk to the robots. I have three grandchildren who are on the
    autistic spectrum, so this is not an idle desire – I’ve read the research, such
    as *Robots for Autism* ([https://www.robokind.com/](https://www.robokind.com/)),
    which states that robots can be helpful in such situations. While I’m not trying
    to do therapy, I’d like my robot to interact with my grandchildren verbally. I
    also have one specific request – the robot must be able to tell knock-knock jokes
    and respond to them, as they are a favorite of William. I want this robot to be
    verbally interactive.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here is a diagram of this use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Use case: interact with people](img/B19846_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5 – Use case: interact with people'
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s go through the same exercise with this use case. We ask the pertinent
    questions: *who*, *what*, *when*, *where*, and *why*? Let’s break these down:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Who**: The robot, the user (granddad), and the grandchildren.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, user interaction is part of the task. Who are we interacting with?
    I need to be able to command the robot to begin to interact. Then, we want the
    robot to both talk to and listen to the children.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**What**: Receive commands and verbally interact (hold a conversation) with
    children, which must include knock-knock jokes. We keep the two kinds of functions:
    receive commands from – let’s call me the **robot controller**, to make this more
    generic. The other function is to have a conversation with the children, including
    telling knock-knock jokes. We’ll define *conversation* further on in our breakdown.
    You can refer to [*Chapter 6*](B19846_06.xhtml#_idTextAnchor205) on using the
    robot as a **digital assistant**. We are going to use an open source digital assistant
    called *Mycroft* to act as a voice interface for the robot. We will add our own
    skills to the base Mycroft capability, which is actually quite versatile. The
    robot can get the weather, set timers, play music, look up information on Google
    (such as how many tablespoons in a quarter cup), and even tell you where the International
    Space Station is right now. But what it can’t do is tell knock-knock jokes – until
    now, as we are adding this feature to the robot. Fortunately for us, the knock-knock
    joke has a very structured form based on puns and a call-and-response format that
    goes like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Robot*: Knock knock.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Child*: Who’s there?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Robot*: Lettuce.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Child*: Lettuce who?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Robot*: Lettuce (let us) in, we’re freezing out here!'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: I’ll leave diagramming the opposite form – responding to a knock-knock joke
    – to you.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**When**: As requested by the robot controller, then when the child speaks
    to the robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I think this is fairly self-explanatory: the robot interacts when sent a command
    to do so. It then waits for someone to talk to it. One thing we can extrapolate
    from this information is that when we are picking up toys, we are not expecting
    the robot to talk – the two activities are exclusive. We only pick up toys after
    the kids are gone, ergo there is one to talk to.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Where**: In the playroom, within about six feet of the robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have to set some limits on how far we can hear – there is a limit on how
    sensitive our microphone can be. I’m suggesting six feet as a maximum distance.
    We may revisit this later. When you come to a requirement like this, you can ask
    the customer *Why six feet?* They may say, *Well, that sounds like a reasonable
    distance*. You can then ask, *Well, if it was five feet, would that be a failure
    of this function?* And the user might respond, *No, but it would not be as comfortable*.
    You can continue to ask questions on distances until you get a feeling for the
    required distance (how far away to not fail), which might be three feet in this
    case (so that the child does not have to bend over the robot to be heard), and
    the desired distance, which is how far the user wants the function to work. These
    are important distinctions when we get around to testing. Where is the pass-fail
    line for this requirement?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Why**: Because my grandchildren want to talk to the robot, and have it respond
    (i.e., the users have specifically requested this feature).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s delve deeper into our robot’s tasks.
  prefs: []
  type: TYPE_NORMAL
- en: What is our robot to do?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we are going to do some detailed analysis of what the robot needs to do
    by using the storyboard process. This works like this: We take each of our two
    tasks and break them down as completely as we can based on the answers to all
    of our *W* questions. Then we picturize each step. The pictures can be either
    a drawing or a word picture (a paragraph) describing what happens in that step.
    I like to start the decomposition process by describing the robot in terms of
    a **state machine**, which, for the first part of our problem, may be a good approach
    to understanding what is going on inside the robot at each step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You are probably familiar with **state machine diagrams**, but just in case,
    a state machine diagram describes the robot’s behavior as a series of discrete
    states or sets of conditions that define what actions are available to the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.6 – Robot state machine\uFEFF diagram](img/B19846_03_6.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Robot state machine diagram
  prefs: []
  type: TYPE_NORMAL
- en: Our first state is simply *Off* – the robot has no power turned on.
  prefs: []
  type: TYPE_NORMAL
- en: Each state is an event (or events) that causes the state to change. These are
    called **transitions**. To get from the *Off* state to whatever is next, some
    event has to occur – such as the human operator turning on the power. We’ll call
    that transition event *Power applied*. Now what state are we in? There is some
    amount of time to get the computer booted and the programs loaded (*Initializing*).
    Once everything boots up and initializes, the robot will be ready to accept commands.
    Let’s call this state *Standby*. The robot is just sitting waiting for instructions.
    Now we want to start cleaning the room. I send a *Begin cleaning* command to the
    robot, which changes the state to – what? What do we need to happen next? We could
    define a state called *Cleaning*, but that would encompass a lot of complex functions
    and we would not learn much from that. We need the robot to look for toys using
    its camera. If it does not find a toy, it needs to drive forward a short distance
    – avoiding obstacles – and then look again. In practice, we should be able to
    look for toys while driving without pausing constantly. We will need to make the
    *Look for toys* function interrupt driving when it sees a toy.
  prefs: []
  type: TYPE_NORMAL
- en: 'If it does find a toy, then the robot needs to position itself so that the
    toy is within reach of the robot arm. In the state machine diagram, we already
    added the transition *Begin cleaning*, which changes the state from *Standby*
    to *Look for toys*. Now we can add two more transitions: one called *Toy = no*
    and one called *Toy = yes*. The *Toy = no* branch goes to a state called *Drive
    ahead*, where the robot moves forward – while avoiding obstacles – and then goes
    back to the *Look for toys* state and tries again to find a toy. We will need
    some sort of means to tell the software how often to look for toys. We could use
    a simple timer – so many seconds elapsed. Or we could use some sort of distance
    function based on wheel motion.'
  prefs: []
  type: TYPE_NORMAL
- en: So, now we have found a toy, what do we do? We need to drive to the toy, which
    puts it in range of our robot arm. We try to grip the toy with the robot’s arm
    and hand. We may not be successful on the first try, in which case we want to
    try again. The loop transition, which is labeled *Grip unsuccessful*, says to
    go back and try again if you don’t succeed the first time. Where have I heard
    that before? You can see the same with *Pick up toy*. Why are there two parts?
    We need to first get a hold of the toy before we can lift it. So I thought it
    needed two states, since we may fail to get a grip – the toy falls out of the
    hand, separately from picking the toy up, where the toy is too heavy or awkward
    to lift.
  prefs: []
  type: TYPE_NORMAL
- en: OK, we found a toy and picked it up. What is next? We need to put it in the
    toy box. The next state is *Drive to toy box*. Don’t worry about *how* at this
    stage; this is just what we need to do. Later, we can further decompose this state
    into a more detailed version. We drive until we get to the event *Toy box found*.
    That means we see the toy box. Then we go to the *Position for drop* state, which
    moves the robot to a place where it can drop the toy in the box. The final state,
    *Drop toy*, is self-explanatory. We’ve dropped the toy, the robot has nothing
    in its gripper, and guess what? We start over by returning to the *Look for toys*
    state. If the robot decides that the drop was not successful (the toy is still
    in the gripper), then we have it try that step again, by repositioning the hand
    over the toy box and trying to drop the toy again by opening the hand. How do
    we know whether the gripper is empty? We try to close the grip and see what position
    the hand servo is in. If the gripper can close (go to a minimum state), then it
    is empty. If the toy falls outside the toy box (the robot misses the box entirely),
    then it is once again a toy on the floor, and will be treated in the normal manner
    – the robot will find it, pick it up, and try again.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all well and good, and our little robot goes around forever looking
    for toys, right? We’ve left out two important transitions. We need a *No more
    toys* event, and we need a way to get back to the *Off* state. Getting to *Off*
    is easy – the user turns off the power. I use the shorthand method of having a
    block labeled *Any state* since we can hit the off button at any time, no matter
    what else the robot is doing, and there is nothing the robot can or should do
    about it. It may be more proper to draw a line from each state back to *Off*,
    but that clutters the diagram, and this notation still gets the meaning across.
    The new state machine diagram looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – New state machine diagram](img/B19846_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – New state machine diagram
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a minute and talk about the concept of *No more toys*. How do we
    define this? This may take some experimentation, but for now, we’ll say if we
    have not found a toy after 10 minutes of trying, then we are satisfied that there
    are no more toys to be found. Later, we can adjust that time as necessary. It
    is possible that 5 minutes is totally adequate for a room our size. Note that
    the *No more toys* event can only come from the *Look for toys* state, which should
    make sense.
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned that the robot needs to avoid obstacles. But we don’t have a state
    called *Avoid obstacles*. Why is that? That is because several of the states include
    driving, and each of those includes avoiding obstacles. It would not be appropriate
    to have a state for avoiding obstacles, since it is not unique to one state. What
    we need is a separate state machine that describes the robot’s driving. As I mentioned
    in the *Introducing subsumption architecture* section in the last chapter, we
    can have more than one goal operational at a time.
  prefs: []
  type: TYPE_NORMAL
- en: The task of picking up toys is the mission, which is the overall goal of the
    robot. *Avoid obstacles* is a goal of the driving engine, the mid-level manager
    of our robot.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve discussed our use cases and drawn a state machine diagram, so now let’s
    move on to the next step, which is to create our storyboards.
  prefs: []
  type: TYPE_NORMAL
- en: Using storyboards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to decompose our use cases further in order to
    understand the various tasks our robot must undertake on our behalf in the course
    of its two missions. I’ve created some **storyboards** – quick little drawings
    – to illustrate each point.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of storyboards is borrowed from the movie industry, where a comic-strip-like
    narration is used to translate words on a page in the script into a series of
    pictures or cartoons that convey additional information not found in the script,
    such as framing, context, movement, props, sets, and camera moves. The practice
    of storyboarding goes all the way back to silent movies and is still used today.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use storyboards in robotics design for the same reasons: to convey additional
    information not found in the words of the use cases. Storyboards should be simple,
    quick, and just convey enough information to help you understand what is going
    on.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started. We are not going to create storyboards for *Power applied*,
    *Initializing*, or *Standby* because a storyboard is not really needed for those
    simple concepts. We will jump ahead to the *Begin cleaning* event in our state
    diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Storyboard – put away the toys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When our story begins, what is the robot doing? It has been turned on, and is
    in a standby state waiting to be told what to do. How does it receive a command?
    A nice, hands-free way would be to receive a voice command to *begin cleaning*,
    or some similar words that mean the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step in our process after *Begin cleaning* is *Look for toys*. This
    storyboard frame is *what the robot sees* as it is commanded to start cleaning.
    It sees the room, which has three kinds of objects visible – that is, toys, things
    that are not toys (the ottoman and the fireplace), and the room itself, including
    the walls, and the floor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Waiting for a voice command to begin cleaning](img/B19846_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Waiting for a voice command to begin cleaning
  prefs: []
  type: TYPE_NORMAL
- en: We could select any sort of sensor to detect our toys and direct our robot.
    We could have a LiDAR, thermal, or sonar scanner. Let’s hypothesize that the best
    sensor tool for this task is a regular USB camera. We have control of the lighting,
    the toys are not particularly warmer or cooler than the surroundings, and we need
    enough information to identify objects by type. So, video it is. We will determine
    later exactly what kind of camera we need, so add that to our *to-do list*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Look for toys](img/B19846_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Look for toys
  prefs: []
  type: TYPE_NORMAL
- en: Our next storyboard is to *look for toys*. We need to run some sort of algorithm
    or technique to classify objects by type. The results of that algorithm are to
    find the objects – separate them from the background of the floor – and then classify
    each object as a toy or not a toy. We don’t really care to have any more breakdown
    than that – we leave all *Not toy* objects alone, and pick up all *Toy* objects.
    Note that we draw circles around the objects that are toys, which is another way
    of saying that we must locate them in the camera frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'So what does this simple picture tell us we did not know before? It tells us
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to segment the camera image by objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to locate the objects in the camera frame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to classify the objects as either *Toy* or *Not toy*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to be able to store and remember this information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We only can pick up and move one toy at a time – we only have one hand, and
    nobody said in the use cases that we need to pick up more than one at a time.
    So, we only care about one toy – and let’s arbitrarily say we pick up the closest
    one to the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Select nearest toy](img/B19846_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Select nearest toy
  prefs: []
  type: TYPE_NORMAL
- en: We might also say that it’s the toy that is easiest to get to, which might be
    a slightly different process than choosing the closest one. We set that toy to
    be the target for our next action, which is what? If you said to drive to the
    toy, you would be correct. However, we must not just drive to the toy but put
    the robot’s body in a position to use the robot arm to grasp the toy. By the way,
    that means the robot arm must be able to reach the ground or very close to the
    ground, as we have some small toys.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our robot must plan a route from its current position to a spot where it can
    attempt to pick up the toy. We set a target goal an arms-length away from the
    center of the toy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Plan route to target](img/B19846_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Plan route to target
  prefs: []
  type: TYPE_NORMAL
- en: 'The robot needs to make sure that there are no obstacles en route. There are
    two ways of doing this. As illustrated, we can clear the path that the robot is
    traveling on by adding the width of the robot (plus a bit of extra) and see whether
    any obstacles are in that area, or we can add a border around obstacles and see
    whether our path goes into those boundaries. Regardless, we need to have a path
    free of obstacles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Look for obstacles on the route](img/B19846_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Look for obstacles on the route
  prefs: []
  type: TYPE_NORMAL
- en: 'The robot determines for itself the proper alignment to prepare to pick up
    the toy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Position robot hand](img/B19846_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Position robot hand
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the robot has completed its drive, the robot can move the robot hand
    to a position to pick up the toy. We need to put the robot hand over the center
    of mass of the toy, and then rotate the hand to match a narrow part of the toy
    so we can pick it up. One of our goals for this project is to not dictate how
    the robot does this, but rather to let it learn for itself. So, we can say for
    this storyboard panel that the robot uses its training and machine learning to
    use an appropriate hand pose to prepare to grasp the object. We can surmise that
    that includes lining the hand up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Pick up toy](img/B19846_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Pick up toy
  prefs: []
  type: TYPE_NORMAL
- en: Probably *storyboard 6* is the hard part (*Figure 3**.13*), and in *storyboard
    7*, the robot completes the grasp of the object and picks it up (*Figure 3**.14*).
    The robot has to be able to determine whether the pick-up was successful, and
    if not, try again. That was in the state machine diagram we did before. We have
    now picked up the toy. What’s next? Find the toy box!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Find toy box](img/B19846_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Find toy box
  prefs: []
  type: TYPE_NORMAL
- en: Now we need the robot to find the toy box. Again, we don’t care how at this
    point. We are still worried about *what* and not *how*. Somehow, the robot looks
    around and finds the toy box, which, in this case, is large, against the wall,
    and has a distinctive color. Regardless, the robot has to find the toy box on
    its own. The labels in the picture indicate that the robot can distinguish the
    toy box and that it considers all other objects it perceives as obstacles. We
    can see we don’t need to have the *Toy*/*Not toy* capability active at the same
    time, only the *Toy box*/*Not toy box* decision-making process. This does reduce
    some of the required processing and will make machine learning easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have found the toy box, we illustrate a slightly more complex task
    of navigating around an obstacle to get there. In this example, we show the purple
    outline of the robot’s base, compared to a red outline around the obstacle, which
    I labeled *Keep out zone*. This gives us more guidance on how to avoid obstacles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Plan path to toy box](img/B19846_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Plan path to toy box
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to keep the center of the robot out of the *Keep out zone*. We need
    to get close enough to the toy box to drop our toy into it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Align toy with box](img/B19846_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Align toy with box
  prefs: []
  type: TYPE_NORMAL
- en: In *storyboard 10*, we lift the toy high above the top of the toy box and position
    our toy to fall inside the toy box when we let go of it. Make a note that we have
    to have the toy lifted before the final few inches to the toy box. We put the
    robot hand over the top of the opening of the toy box, just as far forward as
    we can and in the middle of the toy box.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Drop toy in box](img/B19846_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – Drop toy in box
  prefs: []
  type: TYPE_NORMAL
- en: Our final step in the toy saga is to open the robot hand and let the toy hopefully
    fall into the toy box. I predict that we will have to spend some trial and error
    time getting this right. We may have to tilt the open hand right and left to get
    the toy to drop. If the toy falls outside of the box, then it is not put away
    and we have to start all over and try to put it away again. We don’t need a new
    state for this because it returns to being a toy on the floor, and we already
    have a state for that.
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you have seen in the storyboard process how this provides insight
    into visualizing the robot’s tasks. I would say the most important benefit is
    that it forces you to think about what the robot is doing and to break down each
    step into smaller and smaller parts. Don’t hesitate to take this storyboard and
    break an individual panel down into its own storyboard, if that is what you feel
    you need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Project goals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this is an AI/machine learning project, we must add to our project goals
    not just putting away toys but also using machine learning, adaptive systems,
    neural networks, and other tools to provide a new approach to solving these sorts
    of problems. You may think, “*Why bother? You can do this better with a standard
    programming approach.*” I would say from experience that these problems are difficult
    to solve that way, and you can do your own research to see where companies, large
    and small, have tried to solve this sort of problem and failed – or at least not
    succeeded. This problem is not easily solved by any means, and using an AI-based
    approach has a far greater chance of success than standard programming techniques.
    Now, I’m not saying we are going to succeed beyond our wildest dreams at this
    task in this book, but our objective is to learn a whole lot along the way!
  prefs: []
  type: TYPE_NORMAL
- en: So, we pause at this point in defining our project to say that we are deliberately
    choosing to use artificial intelligence and machine learning as an approach to
    solving a problem that has proven to be difficult with other means.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are going to be teaching the robot various tasks, it will be more effective
    if we can teleoperate the robot and drive it around like a radio-controlled car,
    in order to collect data and take pictures that we will use for object recognition
    later. We don’t need this for operations, we need this for training. We will add
    this required operation to our *to-do list.*
  prefs: []
  type: TYPE_NORMAL
- en: In our next step, we are going to extract from all of our hard work the **hardware**
    and **software** tasks that our robot will have to accomplish. But before we do
    this, let’s pause for a moment to discuss a common mistake made in defining the
    scope of the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the scope of our use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Desirements** (a word made up by combining *desire* and *requirements*) are
    functions that would be *nice to have* but not strictly necessary. For example,
    if we decided to add flashing lights to the robot because it looks cool, that
    would be a desirement. You may want to have it, but it does not contribute to
    the mission of the robot or the task it needs to perform.'
  prefs: []
  type: TYPE_NORMAL
- en: Another example would be if we added that the robot must operate in the dark.
    There is no reason for this in the current context, and nothing we’ve stated in
    the use cases said that the robot would operate in the dark – just in an indoor
    room. This would be an example of **scope creep**, or extending the operation
    conditions without a solid reason why. It’s important to work very hard to keep
    requirements and use cases to a minimum, and even to throw out use cases that
    are unnecessary or redundant. I might have added a requirement for sorting the
    toys by color, but sorting does not help with picking up the toys, and besides,
    I’ve only got one toy box. I might have added the task in the interest of education
    for you, my readers, but it does not help with that objective either, so color
    sorting is not included.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s proceed to identifying our hardware requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying our hardware needs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on our storyboards, I extracted or derived the following hardware tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Drive the robot base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carry the robot arm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lift toys
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put toys in the toy box (arm length)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sensors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arm location
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hand status (open/close)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Robot vision (camera) for obstacle avoidance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Provide power for all systems:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5V for Nvidia Nano
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 5V for Arduino
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Arm power – 7.2V
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Motor power – 7.2V
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Onboard computers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A computer that can receive commands remotely (Wi-Fi Nano):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs ROS 2
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs Python 3
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A computer that can interface with a camera
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A computer that can control motors (Arduino)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An interface that can drive servo motors for the robot arm (servo controller)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s take a look at the software requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down our software needs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This list of software tasks was composed by reviewing the state machine diagram,
    the use cases, and the storyboards. I’ve highlighted the steps that will require
    AI and will be covered in detail in the coming chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Power on** **self-test** (**POST**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start up robot programs.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Check that the Nano can talk to the Arduino and back.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to establish communications with the control station.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Report POST success or failure as appropriate and enter in the log.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Receive commands via Wi-Fi for teleoperation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drive motor base (right/left/forward/back)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Move hand up/down/right/left/in/out/twist
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Record video or record pictures as image files
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Send telemetry via Wi-Fi.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitor progress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate safely:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn to avoid obstacles
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to not fall down stairs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Find toys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect objects
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to classify objects (Toy/Not toy)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine which toy is closest
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pick up toys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move to the position where the arm can reach the toy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Devise a strategy for grasp
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Attempt grasp
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine whether grasping was successful
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If not, try again with a different strategy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reweight grasp technique score based on success
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Put toys in the toy box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn to identify the toy box
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the toy box
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Drive to the known toy box location using navigation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Move to the dump location:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid obstacles
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lift the toy above the toy box lid
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop the toy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Check to see whether the toy drop was successful
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If not, reposition and try again
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the toy misses the toy box, we treat it as a toy on the floor again
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine there are no more toys.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stand by for instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Teleoperate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move base forward/backward/left/right
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Move arm up/down/right/left
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Move hand in/out/twist/open/close
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Record video/take pictures
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simulate personality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Talk
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Listen/recognize words
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand some commands
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tell knock-knock jokes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand knock-knock jokes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voice commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clean-up room
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Put this away
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Come here
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Resume
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Go home
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Turn left/ right
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Forward/ back
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hand up/hand down
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hand left/hand right
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Open hand/close hand
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In this list, where did I get *teleoperate*? We don’t remember discussing that
    in the use cases and storyboards. We are going to need to teach the robot to navigate
    and find toys, and for that, we need to move the robot around and take pictures.
    One easy way to do that is by driving the robot around with **teleoperations**
    (remote control).
  prefs: []
  type: TYPE_NORMAL
- en: Writing a specification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our next task is to write specifications for our various components. I’ll go
    through an example here that we must do as part of our toy-grasping robot project:
    we need to *select a camera*. Just any old camera will not do – we need one that
    meets our needs. But what are those needs? We need to write a camera specification
    so that when we are looking at cameras to buy, we can tell which one will do the
    job.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve created our storyboard and our use cases, so we have the information
    we need to figure out what our camera needs to do. We can reverse engineer this
    process somewhat: let’s discuss what things make one camera different from another.
    First of all is the interface: this camera goes on board the robot, so it has
    to interface with the robot’s computer, which has USB, Ethernet, and a special
    camera bus. What other things about cameras do we care about? We certainly care
    about cost. We don’t want (or need) to use a $1,000 camera for our inexpensive
    robot. Cameras have resolution: the number of pixels in each image. That can vary
    from 320 x 240 to 4,000 x 2,000 (4K). Cameras also have a field of view, which
    is the number of angular degrees the camera can see. This can vary from 2.5 degrees
    (very narrow) to 180 degrees (very wide). There are also cameras that see in the
    dark or have various types of infrared sensitivity. Finally, there is size and
    weight; we need a small camera that fits on our robot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes the parameters that we need to decide the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Field of view**: [180 - > 2.5]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resolution**: [320 x 280 -> 4,000 x 2,000]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: (low to high) – cheaper is better'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sees in the** **dark**: Yes/no'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size and weight**: Smaller and lighter is much better; must fit on the robot'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interface**: USB, Ethernet, or camera bus; power >11V'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reason for listing these parameters like this is that we can now concentrate
    on those features that we can select, so we are not wasting time looking at other
    parameters that we don’t care about. Let’s see whether we can knock off some of
    the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: If we use USB as the **interface**, the power is provided by the connector,
    and we don’t need extra cables or routers. This is also the lowest cost method,
    so we choose USB as the interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also don’t have any requirements in our use cases to **see in the dark**,
    so we don’t need a special infrared camera.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next question is to determine the **field of view**. We need to see the
    entire area where the robot arm can move in as it picks up a toy. We also need
    enough field of view to see when we are driving to avoid obstacles. We can take
    some measurements from the robot, but we can quickly see that we mostly need to
    see close to the robot, and we can’t see past the tracks on either side. This
    sets the field of view required to be close to 90 degrees. More field of view
    than this is acceptable, less is not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our final problem is determining the **resolution** we need to perform our object
    recognition. For that, we need an additional data point – how many pixels do we
    need to recognize an object as a toy? That is what we will do with this camera
    – recognize toys and things that are not toys. We also have to pick a distance
    at which we can recognize the toy. We don’t have a firm requirement out of the
    use cases, so we have to make an educated guess. We know that our room is 17 feet
    long, and it has furniture in it. Let’s guess that we need 8 feet of distance.
    How do we know this is correct? We do a thought experiment. If we can identify
    a toy 8 feet away, can we accomplish our task? We can see the toy half a room
    away. That gives the robot plenty of space to go drive to the toy and it won’t
    spend much time looking for toys. As a check, if the robot had to be 4 feet away
    to recognize a toy, would that be unusable? The answer is probably not – the robot
    would work OK. How about 3 feet? Now we are getting to the point where the robot
    has to drive right up to the toy to determine what it is, and that might result
    in more complicated logic to examine toys. So, we say that 3 feet is not enough,
    4 feet is acceptable, and 8 feet would be great.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What resolution is required in the camera to recognize a toy at 8 feet with
    a 90-degree lens? I can tell you that the ImageNet database requires a sample
    35 pixels wide to recognize an object, so we can use that as a benchmark. We assume
    at this point that we need an image at least 35 pixels across. Let’s start with
    a camera with *1,024 x 768* pixels, which is 1,024 pixels wide. We divide by 90
    degrees to get that each degree has 11.3 pixels (*1,024/90*). How big is our smallest
    toy at 8 feet? Our smallest toy is a Hot Wheels toy, which is approximately 3
    inches long. At 8 feet, this is 1.79 degrees or 20.23 pixels (*1.79 degrees x
    11.3 pixels/degree*). That is not enough. Solving the distance equation for 3
    inches, we get a maximum distance of 4.77 feet for a camera with *1,024 x 768*
    pixels. That is just barely acceptable. What if we had an HD sensor with *1,900
    x 1200* pixels? Then, at 8 feet, I get 75 pixels – more than enough to give us
    the best possible distance. If we use a sensor 1,200 pixels wide, we have a recognition
    distance of 5.46 feet, which is adequate but not great.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: I walked you through this process to show you how to write a specification and
    the types of questions you should be asking yourself as you decide what sensors
    to acquire for your project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter outlined a suggested process for developing your to-do list as
    you develop your robot project. This process is called systems engineering. Our
    first step was to create use cases or descriptions of how the robot is to behave
    from a user’s perspective. Then, we created more detail behind the use cases by
    creating storyboards, where we went step by step through the use case. Our example
    followed the robot finding and recognizing toys, before picking them up and putting
    them in the toy box. We extracted our hardware and software needs, creating a
    to-do list of what the robot will be able to do. Finally, we wrote a specification
    for one of our critical sensors: the camera.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into our first robot task – teaching the robot
    to recognize toys using computer vision and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Describe some of the differences between a storyboard for a movie or cartoon
    and a storyboard for a software program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the five *W* questions? Can you think of any more questions that would
    be relevant to examine a use case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Complete this sentence: A use case shows what the robot does but not _____.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take *storyboard 9* in *Figure 3**.16*, where the robot is driving to the toy
    box, and break it down into more sequenced steps in your own storyboard. Think
    about all that must happen between *frames 9* *and 10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete the reply form of the knock-knock joke, where the robot answers the
    user telling the joke. What do you think is the last step?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the teleoperate operations. Would you add any more, or does this look
    like a good list?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a specification for a sensor that uses distance measurement to prevent
    the robot from driving downstairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the distance at which a camera with 320 x 200 pixels and a 30-degree
    field of view can see a 6-inch wide stuffed animal, still assuming we need 35
    pixels for recognition?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics in this chapter, you can refer to the following
    resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Practical Guide to SysML: The Systems Modeling Language*, by Sanford Friedenthal,
    Alan Moore, and Rick Steiner, published by Morgan Kaufman; this is the standard
    introduction to **Model-Based Systems** **Engineering** (**MBSE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Agile Developer’s Handbook* by Paul Flewelling, published by Packt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Adding Perception, Learning, and Interaction to Robotics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see, understand, and interact with the environment, robots need to have perception.
    AI is one approach that can be used for recognizing objects and navigation. This
    part empowers you with the essential skills to efficiently operate your robots
    using AI techniques. Our example in this book is creating a robot that picks up
    toys, so we start with recognizing toys with a **neural network**. Then we work
    with the robot arm to pick up toys using tools such as **reinforcement learning**
    and **genetic algorithms**. The next chapter covers the creation of a robot digital
    assistant that can listen and understand your commands, and even tell knock-knock
    jokes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B19846_04.xhtml#_idTextAnchor126), *Recognizing Objects Using
    Neural Networks and Supervised Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B19846_05.xhtml#_idTextAnchor159), *Picking Up and Putting Away
    Toys Using Reinforcement Learning and Genetic Algorithms*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19846_06.xhtml#_idTextAnchor205), *Teaching the Robot to Listen*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
