- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classifying Texts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be classifying texts using different methods. Classifying
    texts is a classic NLP problem. This NLP task involves assigning a value to a
    text, for example, a topic (such as sport or business) or a sentiment, such as
    negative or positive, and any such task needs evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After reading this chapter, you will be able to preprocess and classify texts
    using keywords, unsupervised clustering, and two supervised algorithms: **support
    vector machines** (**SVMs**) and a **convolutional neural network** (**CNN**)
    model trained within the spaCy framework. We will also use GPT-3.5 to classify
    texts.'
  prefs: []
  type: TYPE_NORMAL
- en: For theoretical background on some of the concepts discussed in this section,
    please refer to *Building Machine Learning Systems with Python* by Coelho et al.
    That book will explain the basics of building a machine learning project, such
    as training and test sets, as well as metrics used to evaluate such projects,
    including precision, recall, F1, and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of the recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the dataset and evaluation ready
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing rule-based text classification using keywords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering sentences using K-Means – unsupervised text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SVMs for supervised text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a spaCy model for supervised text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying texts using OpenAI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the `Chapter04` folder in the GitHub
    repository of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition)).
    As always, we will use the `poetry` environment to install the necessary packages.
    You can also install the required packages using the provided `requirements.txt`
    file. We will use the Hugging Face `datasets` package to get datasets that we
    will use throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the dataset and evaluation ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will load a dataset, prepare it for processing, and create
    an evaluation baseline. This recipe builds on some of the recipes from [*Chapter
    3*](B18411_03.xhtml#_idTextAnchor067), where we used different tools to represent
    text in a computer-readable form.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we will use the Rotten Tomatoes reviews dataset, available
    through Hugging Face. This dataset consists of user movie reviews that can be
    classified into positive and negative. We will prepare the dataset for machine
    learning classification. The preparation process in this case will involve loading
    the reviews, filtering out non-English language ones, tokenizing the text into
    words, and removing stopwords. Before the machine learning algorithm can run,
    the text reviews need to be transformed into vectors. This transformation process
    is described in detail in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067).
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will classify whether the input review is of negative or positive sentiment.
    We will first filter out non-English text, then tokenize it into words and remove
    stopwords and punctuation. Finally, we will look at the class distribution and
    review the most common words in each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import necessary classes. We import the **detect** function from **langdetect**,
    which will help us determine the language of the review. We also import the **word_tokenize**
    function, which we will use to split the reviews into words. The **FreqDist**
    class from NLTK will help us see the most frequent positive and negative words
    in the reviews. We will use the **stopwords** list, also from NLTK, to filter
    the stopwords from the text. Finally, the **punctuation** string from the **string**
    package will help us to filter punctuation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the training and test datasets using the function from the simple classifier
    file and print the two dataframes. We see that the data contains a **text** column
    and a **label** column, where the text column is lowercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we create a new column called **lang** in the dataframes that will contain
    the language of the review. We use the **detect** function to populate this column
    via the **apply** method. We then filter the dataframe to only contain English-language
    reviews. The final row counts of the training dataframe before and after the filtering
    show us that 178 rows were non-English. This step may take a minute to run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now the output should look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will do the same for the test dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will tokenize the text into words. If you get an error saying that the
    **english.pickle** tokenizer was not found, run the line **nltk.download(''punkt'')**
    before running the rest of the code. This code is also contained in the **lang_utils**
    **notebook** ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will remove stopwords and punctuation. First, we load the
    stopwords using the NLTK package. We then add **''s** and **``** to the list of
    stopwords. You can add other words that you think are also stopwords. We then
    define a function that will take a list of words as input and filter it, returning
    a new list that doesn’t contain stopwords or punctuation. Finally, we apply this
    function to the training and test data. From the printout, we can see that stopwords
    and punctuation were removed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will check the class balance over both datasets. It is important that
    the number of items in each class is approximately the same, since if one class
    dominates, the model can just learn to always assign this dominating class without
    being wrong much of the time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see that there are slightly, but not significantly, more negative reviews
    in the training data than positive, and the numbers are nearly equal in test data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s now save the cleaned data to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define a function that will take a list of words and the number
    of words as input and return a **FreqDist** object. It will also print out the
    top *n* most frequent words, where *n* is passed into the function and is **200**
    by default:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let’s use the preceding function and show the most common words in positive
    and negative reviews to see whether there are significant vocabulary differences
    between the two classes. We create two lists of words, one for positive and one
    for negative reviews. We first filter the dataframe by label and then use the
    **sum** function to get the words from all the reviews:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the output, we see that the words `film` and `movie` and some other words
    also act as stopwords in this case, as they are the most common words in both
    sets. We can add them to the stopwords list in step 7 and redo the cleaning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Performing rule-based text classification using keywords
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the vocabulary of the text to classify the Rotten
    Tomatoes reviews. We will create a simple classifier that will have a vectorizer
    for each class. That vectorizer will include the words characteristic to that
    class. The classification will simply be vectorizing the text using each of the
    vectorizers and then using the class that has more words.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `CountVectorizer` class and the `classification_report` function
    from `sklearn`, as well as the `word_tokenize` method from NLTK. All of these
    are included in the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will create a separate vectorizer for each class. We will
    then use those vectorizers to count the number of each class word in each review
    to classify it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Do the necessary imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the cleaned data from disk. If you receive a **FileNotFoundError** error
    at this step, you need to run the previous recipe, *Getting the dataset and evaluation
    ready*, first, since those files were created there after cleaning the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here we create a list of words unique to each class. We first concatenate all
    the words from the **text** column, filtering on the relevant **label** value
    (**0** for negative reviews and **1** for positive ones). We then get the words
    that appear in both of those lists in the **word_intersection** variable. Finally,
    we create filtered word lists, one for each class, that do not contain words that
    appear in both classes. Basically, we delete all the words that appear in both
    positive and negative reviews from the respective lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define a function to create vectorizers, one for each class. The input
    to this function is a list of lists, where each one is a list of words that only
    appear in that class; we created these in the previous step. For each of the word
    lists, we create a **CountVectorizer** object that takes the word list as the
    **vocabulary** parameter. Providing this ensures that we only count those words
    for the purpose of classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the vectorizers using the preceding function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create a **vectorize** function that takes in a list of words
    and a list of vectorizers. We first create a string from the word list, as the
    vectorizer expects a string. For each vectorizer in the list, we apply it to the
    text and then sum the total count of words in that vectorizer. Finally, we append
    that sum to a list of scores. This will count words in the input per class. We
    return this score list at the end of the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define the **classify** function, which takes a list of scores
    returned by the **vectorize** function. This function simply selects the maximum
    score from the list and returns the index of that score corresponding to the class
    label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we apply the preceding functions to the training data. We first vectorize
    the text and then classify it. We create a new column for the result called **prediction**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we measure the performance of the rule-based classifier by printing the
    classification report. We input the assigned label and the prediction columns.
    The result is an overall accuracy score of 87%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here we do the same for the test data, and we see a significant reduction in
    accuracy, down to 62%. This is because the vocabulary lists that we use to create
    the vectorizers only come from the training data and are not exhaustive. They
    will lead to errors in unseen data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Clustering sentences using K-Means – unsupervised text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will use the BBC news dataset. The dataset contains news
    pieces sorted by five topics: politics, tech, business, sport, and entertainment.
    We will apply the unsupervised K-Means algorithm to sort the data into unlabeled
    classes.'
  prefs: []
  type: TYPE_NORMAL
- en: After you read this recipe, you will be able to create your own unsupervised
    clustering model that will sort data into several classes. You can then later
    apply it to any text data without having to first label it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `KMeans` algorithm to create our unsupervised model. It is part
    of the `sklearn` package and is included in the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: The BBC news dataset as we use it here was uploaded by a Hugging Face user,
    and the link and the dataset might change in time. To avoid any potential issues,
    you can use the BBC dataset uploaded to the book’s GitHub repository by loading
    it from the CSV file provided in the `data` directory.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will preprocess the data, vectorize it, and then cluster
    it using K-Means. Since there are usually no right answers for unsupervised modeling,
    evaluating the models is more difficult, but we will be able to look at some statistics,
    as well as the most common words in all the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your steps should be formatted like so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classification file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the necessary functions and packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will load the BBC dataset. We use the **load_dataset** function from Hugging
    Face’s **datasets** package. This function was imported in the simple classifier
    file we ran in step 1\. In the Hugging Face repository, datasets are usually split
    into training and testing. We will load both, although in unsupervised learning,
    the test set is usually not used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will check the distribution of items per class for both training and
    test data. Class balance is important in classification, as a disproportionally
    larger class will influence the final classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see that the classes are pretty evenly split, but there are more examples
    in the `business` and `sport` categories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since there is almost as much data in the test set as in the training set,
    we will combine the data and create a better train/test split. We first concatenate
    the two dataframes. We then create a **StratifiedShuffleSplit** that will create
    a train/test split and will do it while preserving the class balance. We specify
    that we only need one split (**n_splits**) and that the test data needs to be
    20% of the whole dataset (**test_size**). The **sss** object’s **split** method
    returns a generator that contains the indices for the split. We can then use these
    indices to get new training and test dataframes. To do that, we filter on the
    relevant indices and then make a copy of the resulting dataframe slice. If we
    didn’t make a copy, then we would be working on the original dataframe. We then
    print out the class counts for both dataframes and see that there is more training
    and less testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now preprocess the data: tokenize it and remove stopwords and punctuation.
    The functions to do this (**tokenize**, **remove_stopword_punct**) are imported
    in the **language_utils** file we ran in step 1\. If you get an error that the
    **english.pickle** tokenizer was not found, run the line **nltk.download(''punkt'')**
    before running the rest of the code. This code is also contained in the **lang_utils
    notebook**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the vectorizer. To do that, we get all the words from
    the training news articles. First, we save the clean text in a separate column,
    **text_clean**, and then we save the two dataframes to disk. Then we create a
    TF-IDF vectorizer that will count unigrams, bigrams, and trigrams (the **ngram_range**
    parameter). We then fit the vectorizer on the training data only. The reason we
    fit it only on the training data is that if we fit it on both training and test
    data, it would lead to data leakage and we would get better test scores than actual
    performance on unseen data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can create the **Kmeans** classifier for five clusters and then fit
    it on the matrix produced using the vectorizer from the preceding code. We specify
    the number of clusters using the **n_clusters** parameter. We also specify that
    the number of times the algorithm should run is 10 using the **n_init** parameter.
    For higher-dimensional problems, it is recommended to do several runs. After initializing
    the classifier, we fit it on the matrix we created using the vectorizer in step
    7\. This will create the clustering of the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In real-life projects, you will not know the number of clusters in advance,
    as we do here. You will need to use the elbow method or other methods to estimate
    the optimal number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The **get_most_frequent_words** function will return a list of the most frequent
    words in a list. The most frequent words list will provide us with a clue as to
    which topic the text is about. We will use this function to print out the most
    frequent words in a cluster to understand which topic they refer to. The function
    takes in input text, tokenizes it, and then creates a **FreqDist** object. We
    get the top word frequency tuples by using its **most_common** function and finally
    get only the word without the frequencies and return this as a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define another function, **print_most_common_words_by_cluster**,
    which uses the **get_most_frequent_words** function we defined in the previous
    step. We take the dataframe, the **KMeans** model, and the number of clusters
    as input parameters. We then get the list of the clusters assigned to each data
    point and then create a column in the dataframe that specifies the assigned cluster.
    For each cluster, we then filter the dataframe to get the text just for that cluster.
    We use this text to then pass it into the **get_most_frequent_words** function
    to get the list of the most frequent words in that cluster. We print the cluster
    number and the list and return the input dataframe with the added cluster number
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we use the function we defined in the previous step on the training dataframe.
    We also pass in the fitted **KMeans** model and the number of clusters, **5**.
    The printout gives us an idea of which cluster is which topic. The cluster numbers
    might vary, but the cluster that has **labour**, **party**, **election** as the
    most frequent words is the **politics** cluster; the cluster with the words **music**,
    **award**, and **show** is the **entertainment** cluster; the cluster with the
    words **game**, **England**, **win**, **play**, and **cup** is the **sport** cluster;
    the cluster with the words **sales** and **growth** is the **business** cluster;
    and the cluster with the words **software**, **net**, and **search** is the **tech**
    cluster. We also note that the words **said** and **Mr** are clearly stopwords,
    as they appear in most clusters close to the top:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will vary each time you run the training, but they might look like
    this (output truncated):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we use the fitted model to predict the cluster for a test example.
    We use the text in row 1 of the test dataframe. It is a politics example. We use
    the vectorizer to turn the text into a vector and then use the K-Means model to
    predict the cluster. The prediction is cluster 0, which in this case is correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result might look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we save the model using the **joblib** package’s **dump** function
    and then load it again using the **load** function. We check the prediction of
    the loaded model, and it is the same as the prediction of the model in memory.
    This step will allow us to reuse the model in the future:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result might look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using SVMs for supervised text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will build a machine learning classifier that uses the SVM
    algorithm. By the end of this recipe, you will have a working classifier that
    you will be able to test on new inputs and evaluate using the same `classification_report`
    tools we used in the previous sections. We will use the same BBC news dataset
    we used with `KMeans` previously.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue working with the same packages that we already installed in
    the previous recipes. The packages needed are installed in the `poetry` environment
    or by installing the `requirements.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will load the cleaned training and test data that we had saved in the previous
    recipe. We will then create the SVM classifier and train it. We will use BERT
    encoding as our vectorizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your steps should be formatted like so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the necessary functions and packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we load the training and test data. If you get a **FileNotFoundError**
    error in this step, run steps 1-7 from the previous recipe, *Clustering sentences
    using K-Means – unsupervised text classification*. We then shuffle the training
    data using the **sample** function. Shuffling ensures that we do not have long
    sequences of data of the same class. Finally, we print out the number of counts
    of examples by class. We see that the classes are more or less balanced, which
    is important for training a classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we load the sentence transformer **all-MiniLM-L6-v2** model that will
    provide the vectors for us. To learn more about the model, please read the *Using
    BERT and OpenAI embeddings instead of word embeddings* recipe in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067).
    We then define the **get_sentence_vector** function, which returns the sentence
    embedding for the text input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will create an SVM object and train it given input data.
    It takes in the input vectors and the gold labels, creates an SVC object with
    the RBF kernel and a regularization parameter of **0.1**, and trains it on the
    training data. It then returns the trained classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we create the list of labels for the classifier and the **vectorize**
    method. We then create the training and test datasets using the **create_train_test_data**
    method, which is located in the simple classifier file. We then train the classifier
    using the **train_classifier** function and print the training and test metrics.
    We see that the test metrics are really good, all above 90%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we print out the confusion matrix to see where the classifier
    makes mistakes. The rows represent the correct labels, and the columns are the
    predicted labels. We see the most confusion (four examples) where the correct
    label is **business** but **tech** is predicted, and where **business** is the
    correct label and **politics** is predicted (two examples). We also see that **business**
    is predicted incorrectly for **tech**, **entertainment**, and **politics** once
    each. These errors are also reflected in the metrics, where we see that both recall
    and precision for **business** are affected. The only category with perfect scores
    is **sport** and it also has zeroes across the confusion matrix everywhere except
    the intersection of the correct row and predicted column. We can use the confusion
    matrix to see which categories have the most confusion between themselves and
    take measures to rectify that if needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will test the classifier on a new example. We first vectorize the text and
    then use the trained model to make a prediction and print the prediction. The
    new article is about tech, and the prediction is class **0**, which is indeed
    **tech**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many different machine learning algorithms that can be used instead
    of the SVM algorithm. Some of the others include regression, Naïve Bayes, and
    decision trees. You can experiment with them and see which ones perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Training a spaCy model for supervised text classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will train a spaCy model on the BBC dataset, the same dataset
    we used in the previous recipe, to will predict the text category.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the spaCy package to train our model. All the dependencies are taken
    care of by the `poetry` environment.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to download the config file from the book’s GitHub repository,
    located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg).
    This file should be located at the path `../data/spacy_config.cfg` with respect
    to the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can modify the training config, or generate your own at [https://spacy.io/usage/training](https://spacy.io/usage/training).
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The general structure of the training is similar to a plain machine learning
    model training, where we clean the data, create the dataset, and split it into
    training and testing datasets. We then train a model and test it on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the necessary functions and packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here we define the **preprocess_data_entry** function, which will take the
    input text, its label, and the list of all labels. It will then run the small
    spaCy model on the text. This model was imported by running the language utilities
    file in step 1\. It is not important which model we use in this step, since we
    just want to have a **Doc** object created from the text. That is why we run the
    smallest model, so it takes less time. We then create a one-hot encoding for the
    text class, setting the class label to **1** and the rest to **0**. We then create
    a label dictionary that maps the category name to its value. We set the **doc.cats**
    attribute to this dictionary and return the **Doc** object. spaCy requires this
    preprocessing of the data to train a classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we prepare the training and test datasets. We create the **DocBin** objects
    for both training and test data that is required by the spaCy algorithm. We then
    load the saved data from disk. This is the data we saved in the K-Means recipe.
    If you get a **FileNotFoundError** error here, you need to run steps 1-7 from
    the *Clustering sentences using K-Means – unsupervised text classification* recipe.
    We then shuffle the training dataframe. Then we preprocess each data point using
    the function we defined in the previous step. We then add each datapoint to the
    **DocBin** object. Finally, we save the two datasets to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model using the **train** command. In order for training to work,
    you will need to have the configuration file downloaded to the **data** folder.
    This is explained in the *Getting ready* section of this recipe. The training
    config specifies the location of the training and test datasets, so you need to
    run the previous step for the training to work. The **train** command saves the
    model in the **model_last** subdirectory of the directory we specify in the input
    (**../models/spacy_textcat_bbc/** in this case):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will differ but might look like this (truncated for easier reading).
    We see that the final accuracy of our trained model is 85%:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we test the model on an unseen example. We first load the model and then
    get an example from the test data. We then check the text and its category. We
    run the model on the input text and print the resulting probabilities. The model
    will give a dictionary of categories with their respective probability scores.
    These scores indicate the probability that the text belongs to the respective
    class. The class with the highest probability is the one we should assign to the
    text. The category dictionary is in the **doc.cats** attribute, just like when
    we were preparing the data, but in this case the model assigns it. In this case,
    the text is about politics and the model correctly classifies it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we define a **get_prediction** function, which takes text, a
    spaCy model, and the list of potential classes and outputs the category whose
    probability is the highest. We then apply this function to the **text** column
    of the test dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we print out the classification report based on the data from the test
    dataframe we generated in the previous step. The overall accuracy of the model
    is 87%, and the reason it is a bit low is because we do not have enough data to
    train a better model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we do the same evaluation using the spaCy **evaluate** command.
    This command takes in the path to the model and the path to the test dataset and
    outputs the scores in a slightly different format. We see that the scores from
    both steps are consistent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Classifying texts using OpenAI models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will ask an OpenAI model to provide the classification of
    an input text. We will use the same BBC dataset from previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run this recipe, you will need to have the `openai` package installed, provided
    as part of the `poetry` environment, and the `requirements.txt` file. You will
    also have to have an OpenAI API key. Paste it into the provided field in the file
    utilities notebook ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI frequently changes and retires existing models and introduces new ones.
    The model we use in this recipe, **gpt-3.5-turbo**, might be obsolete by the time
    you read this. In this case, please check the OpenAI documentation and select
    another suitable model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will query the OpenAI API and provide a request for classification
    as the prompt. We will then post-process the results and evaluate the Open AI
    model on this task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the simple classifier and the file utilities notebooks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the necessary functions and packages to create the OpenAI client using
    the API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the training and test datasets using Hugging Face without preprocessing
    them for the number of classes, as we will not be training a new model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load and print the first example in the dataset and its category:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the OpenAI model on this one example. In step 5, we query the OpenAI API
    asking it to classify this example. We create the prompt and append the example
    text to it. In the prompt, we specify to the model that it is to classify the
    input text as one of five classes and the output format. If we don’t include these
    output instructions, it might add other words to it and return text such as *The
    topic is entertainment*. We select the **gpt-3.5-turbo** model and specify the
    prompt, the temperature, and several other parameters. We set the temperature
    to **0** so that there is no or minimal variation in the model’s response. We
    then print the response returned by the API. The output might vary, but in most
    cases, it should return *entertainment*, which is correct:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result might vary, but should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function that will provide the classification of an input text and
    return the category. It takes input text and calls the OpenAI API with the same
    prompt we used previously. It then lowercases the response, strips it of extra
    white space, and returns it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we load test data. We take the test dataset from Hugging Face
    and convert it into a dataframe. We then shuffle the dataframe and select the
    first 200 examples. The reason is that we want to reduce the cost of testing this
    classifier through the OpenAI API. You can modify how much data you test this
    method on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In step 8, we use the **get_gpt_classification** function to create a new column
    in the test dataframe. Depending on the number of test examples you have, it might
    take a few minutes to run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Despite our instructions to OpenAI to only provide the category as the answer,
    it might add some other words, so we define a function, **get_one_word_match**,
    that cleans OpenAI’s output. In this function, we use a regular expression to
    match one of the class labels and return just that word from the original string.
    We then apply this function to the **gpt_prediction** column in the test dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we turn the label into numerical format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We print the resulting dataframe. We can see that we have all the information
    we need to perform an evaluation. We have both the correct labels (the **label**
    column) and the predicted labels (the **gpt_label** column):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result should look similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can print the classification report that evaluates the OpenAI classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results might vary. This is a sample output. We see that the overall accuracy
    is good, 90%:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
