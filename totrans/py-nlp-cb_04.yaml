- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Classifying Texts
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类
- en: In this chapter, we will be classifying texts using different methods. Classifying
    texts is a classic NLP problem. This NLP task involves assigning a value to a
    text, for example, a topic (such as sport or business) or a sentiment, such as
    negative or positive, and any such task needs evaluation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用不同的方法对文本进行分类。文本分类是经典的NLP问题。这个NLP任务包括为文本分配一个值，例如，一个主题（如体育或商业）或情感，如负面或正面，并且任何此类任务都需要评估。
- en: 'After reading this chapter, you will be able to preprocess and classify texts
    using keywords, unsupervised clustering, and two supervised algorithms: **support
    vector machines** (**SVMs**) and a **convolutional neural network** (**CNN**)
    model trained within the spaCy framework. We will also use GPT-3.5 to classify
    texts.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章后，您将能够使用关键词、无监督聚类和两种监督算法（**支持向量机**（**SVMs**）和spaCy框架内训练的**卷积神经网络**（**CNN**）模型）对文本进行预处理和分类。我们还将使用GPT-3.5对文本进行分类。
- en: For theoretical background on some of the concepts discussed in this section,
    please refer to *Building Machine Learning Systems with Python* by Coelho et al.
    That book will explain the basics of building a machine learning project, such
    as training and test sets, as well as metrics used to evaluate such projects,
    including precision, recall, F1, and accuracy.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本节中讨论的一些概念的理论背景，请参阅Coelho等人所著的《Building Machine Learning Systems with Python》。这本书将解释构建机器学习项目的基础，例如训练和测试集，以及用于评估此类项目的指标，包括精确度、召回率、F1和准确度。
- en: 'Here is a list of the recipes in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是本章中的食谱列表：
- en: Getting the dataset and evaluation ready
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据集和评估
- en: Performing rule-based text classification using keywords
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用关键词进行基于规则的文本分类
- en: Clustering sentences using K-Means – unsupervised text classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K-Means聚类句子 – 无监督文本分类
- en: Using SVMs for supervised text classification
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVMs进行监督文本分类
- en: Training a spaCy model for supervised text classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练spaCy模型进行监督文本分类
- en: Classifying texts using OpenAI models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenAI模型进行文本分类
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter can be found in the `Chapter04` folder in the GitHub
    repository of the book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition)).
    As always, we will use the `poetry` environment to install the necessary packages.
    You can also install the required packages using the provided `requirements.txt`
    file. We will use the Hugging Face `datasets` package to get datasets that we
    will use throughout the chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在书的GitHub仓库的`Chapter04`文件夹中找到([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition))。像往常一样，我们将使用`poetry`环境安装必要的包。您也可以使用提供的`requirements.txt`文件安装所需的包。我们将使用Hugging
    Face的`datasets`包来获取本章中我们将使用的所有数据集。
- en: Getting the dataset and evaluation ready
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集和评估
- en: In this recipe, we will load a dataset, prepare it for processing, and create
    an evaluation baseline. This recipe builds on some of the recipes from [*Chapter
    3*](B18411_03.xhtml#_idTextAnchor067), where we used different tools to represent
    text in a computer-readable form.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将加载一个数据集，为其准备处理，并创建一个评估基准。这个食谱基于[*第3章*](B18411_03.xhtml#_idTextAnchor067)中的一些食谱，其中我们使用了不同的工具将文本表示成计算机可读的形式。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we will use the Rotten Tomatoes reviews dataset, available
    through Hugging Face. This dataset consists of user movie reviews that can be
    classified into positive and negative. We will prepare the dataset for machine
    learning classification. The preparation process in this case will involve loading
    the reviews, filtering out non-English language ones, tokenizing the text into
    words, and removing stopwords. Before the machine learning algorithm can run,
    the text reviews need to be transformed into vectors. This transformation process
    is described in detail in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们将使用Rotten Tomatoes评论数据集，该数据集可通过Hugging Face获取。这个数据集包含用户电影评论，可以分类为正面和负面。我们将为机器学习分类准备数据集。在这种情况下，准备过程将涉及加载评论，过滤掉非英语语言评论，将文本分词成单词，并移除停用词。在机器学习算法运行之前，文本评论需要被转换成向量。这个转换过程在[*第3章*](B18411_03.xhtml#_idTextAnchor067)中有详细描述。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.1_data_preparation.ipynb)。
- en: How to do it…
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: We will classify whether the input review is of negative or positive sentiment.
    We will first filter out non-English text, then tokenize it into words and remove
    stopwords and punctuation. Finally, we will look at the class distribution and
    review the most common words in each class.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分类输入评论是负面还是正面情绪。我们首先过滤掉非英语文本，然后将其分词成单词并移除停用词和标点符号。最后，我们将查看类别分布并回顾每个类别中最常见的单词。
- en: 'Here are the steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是步骤：
- en: 'Run the simple classifier file:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单分类器文件：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import necessary classes. We import the **detect** function from **langdetect**,
    which will help us determine the language of the review. We also import the **word_tokenize**
    function, which we will use to split the reviews into words. The **FreqDist**
    class from NLTK will help us see the most frequent positive and negative words
    in the reviews. We will use the **stopwords** list, also from NLTK, to filter
    the stopwords from the text. Finally, the **punctuation** string from the **string**
    package will help us to filter punctuation:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的类。我们从**langdetect**导入**detect**函数，这将帮助我们确定评论的语言。我们还导入**word_tokenize**函数，我们将用它将评论拆分成单词。NLTK中的**FreqDist**类将帮助我们查看评论中最频繁出现的正面和负面单词。我们将使用来自NLTK的**stopwords**列表来过滤文本中的停用词。最后，来自**string**包的**punctuation**字符串将帮助我们过滤标点符号：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Load the training and test datasets using the function from the simple classifier
    file and print the two dataframes. We see that the data contains a **text** column
    and a **label** column, where the text column is lowercase:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用简单分类器文件中的函数加载训练和测试数据集，并打印出两个数据框。我们看到数据包含一个**text**列和一个**label**列，其中文本列是小写的：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output should look similar to this:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该看起来类似于以下内容：
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we create a new column called **lang** in the dataframes that will contain
    the language of the review. We use the **detect** function to populate this column
    via the **apply** method. We then filter the dataframe to only contain English-language
    reviews. The final row counts of the training dataframe before and after the filtering
    show us that 178 rows were non-English. This step may take a minute to run:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们在数据框中创建一个名为**lang**的新列，该列将包含评论的语言。我们使用**detect**函数通过**apply**方法填充此列。然后我们过滤数据框，只包含英语评论。过滤前后训练数据框的最终行数显示，有178行是非英语的。这一步可能需要一分钟才能运行：
- en: '[PRE4]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now the output should look something like this:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在的输出应该看起来像这样：
- en: '[PRE5]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we will do the same for the test dataframe:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将对测试数据框做同样的处理：
- en: '[PRE6]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we will tokenize the text into words. If you get an error saying that the
    **english.pickle** tokenizer was not found, run the line **nltk.download(''punkt'')**
    before running the rest of the code. This code is also contained in the **lang_utils**
    **notebook** ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb)):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将文本分词成单词。如果你收到一个错误信息说没有找到**english.pickle**分词器，请在运行其余代码之前运行**nltk.download('punkt')**这一行。此代码也包含在**lang_utils**笔记本中([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/lang_utils.ipynb))：
- en: '[PRE7]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The result will be similar to this:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将类似于以下内容：
- en: '[PRE8]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this step, we will remove stopwords and punctuation. First, we load the
    stopwords using the NLTK package. We then add **''s** and **``** to the list of
    stopwords. You can add other words that you think are also stopwords. We then
    define a function that will take a list of words as input and filter it, returning
    a new list that doesn’t contain stopwords or punctuation. Finally, we apply this
    function to the training and test data. From the printout, we can see that stopwords
    and punctuation were removed:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将删除停用词和标点符号。首先，我们使用NLTK包加载停用词。然后我们将**'s**和**``**添加到停用词列表中。你可以添加你认为也是停用词的其他单词。然后我们定义一个函数，该函数将接受一个单词列表作为输入并对其进行过滤，返回一个不包含停用词或标点的新的单词列表。最后，我们将此函数应用于训练数据和测试数据。从打印输出中，我们可以看到停用词和标点符号已被删除：
- en: '[PRE9]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result will look similar to this:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将类似于这个：
- en: '[PRE10]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we will check the class balance over both datasets. It is important that
    the number of items in each class is approximately the same, since if one class
    dominates, the model can just learn to always assign this dominating class without
    being wrong much of the time:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将检查两个数据集的类别平衡。每个类别中项目数量大致相同是很重要的，因为如果某个类别占主导地位，模型可以学会总是分配这个主导类别，而不会犯很多错误：
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We see that there are slightly, but not significantly, more negative reviews
    in the training data than positive, and the numbers are nearly equal in test data.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到在训练数据中负面评论略多于正面评论，但并不显著，而在测试数据中这两个数字几乎相等。
- en: '[PRE12]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s now save the cleaned data to disk:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将清理后的数据保存到磁盘：
- en: '[PRE13]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this step, we define a function that will take a list of words and the number
    of words as input and return a **FreqDist** object. It will also print out the
    top *n* most frequent words, where *n* is passed into the function and is **200**
    by default:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们定义一个函数，该函数将接受一个单词列表和单词数量作为输入，并返回一个**FreqDist**对象。它还将打印出前*n*个最频繁的单词，其中*n*是传递给函数的参数，默认值为**200**：
- en: '[PRE14]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now let’s use the preceding function and show the most common words in positive
    and negative reviews to see whether there are significant vocabulary differences
    between the two classes. We create two lists of words, one for positive and one
    for negative reviews. We first filter the dataframe by label and then use the
    **sum** function to get the words from all the reviews:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用前面的函数来展示正面和负面评论中最常见的单词，以查看这两个类别之间是否存在显著的词汇差异。我们创建了两个单词列表，一个用于正面评论，一个用于负面评论。我们首先通过标签过滤数据框，然后使用**sum**函数从所有评论中获取单词：
- en: '[PRE15]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the output, we see that the words `film` and `movie` and some other words
    also act as stopwords in this case, as they are the most common words in both
    sets. We can add them to the stopwords list in step 7 and redo the cleaning:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在输出中，我们看到单词`film`和`movie`以及一些其他单词在这种情况下也充当停用词，因为它们是两组中最常见的单词。我们可以在第7步将它们添加到停用词列表中，并重新进行清理：
- en: '[PRE16]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Performing rule-based text classification using keywords
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于规则的文本分类使用关键词
- en: In this recipe, we will use the vocabulary of the text to classify the Rotten
    Tomatoes reviews. We will create a simple classifier that will have a vectorizer
    for each class. That vectorizer will include the words characteristic to that
    class. The classification will simply be vectorizing the text using each of the
    vectorizers and then using the class that has more words.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用文本的词汇来对烂番茄评论进行分类。我们将创建一个简单的分类器，该分类器将为每个类别有一个向量器。该向量器将包括该类特有的单词。分类将简单地使用每个向量器对文本进行向量化，然后使用拥有更多单词的类别。
- en: Getting ready
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the `CountVectorizer` class and the `classification_report` function
    from `sklearn`, as well as the `word_tokenize` method from NLTK. All of these
    are included in the `poetry` environment.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`CountVectorizer`类和`sklearn`中的`classification_report`函数，以及NLTK中的`word_tokenize`方法。所有这些都包含在`poetry`环境中。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.2_rule_based.ipynb)。
- en: How to do it…
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'In this recipe, we will create a separate vectorizer for each class. We will
    then use those vectorizers to count the number of each class word in each review
    to classify it:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将为每个类别创建一个单独的向量器。然后我们将使用这些向量器来计算每个评论中每个类别的单词数量以进行分类：
- en: 'Run the simple classifier file:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类器文件：
- en: '[PRE17]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Do the necessary imports:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行必要的导入：
- en: '[PRE18]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load the cleaned data from disk. If you receive a **FileNotFoundError** error
    at this step, you need to run the previous recipe, *Getting the dataset and evaluation
    ready*, first, since those files were created there after cleaning the data:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从磁盘加载清洗后的数据。如果在这一步收到**FileNotFoundError**错误，你需要首先运行之前的食谱，*获取数据集和评估准备*，因为那些文件是在数据清洗后创建的：
- en: '[PRE19]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here we create a list of words unique to each class. We first concatenate all
    the words from the **text** column, filtering on the relevant **label** value
    (**0** for negative reviews and **1** for positive ones). We then get the words
    that appear in both of those lists in the **word_intersection** variable. Finally,
    we create filtered word lists, one for each class, that do not contain words that
    appear in both classes. Basically, we delete all the words that appear in both
    positive and negative reviews from the respective lists:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个包含每个类别独特单词的列表。我们首先将**text**列中的所有单词连接起来，根据相关的**label**值（**0**表示负面评论，**1**表示正面评论）进行过滤。然后我们在**word_intersection**变量中获取出现在这两个列表中的单词。最后，我们为每个类别创建过滤后的单词列表，这些列表不包含同时出现在两个类别中的单词。基本上，我们从相应的列表中删除了同时出现在正面和负面评论中的所有单词：
- en: '[PRE20]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we define a function to create vectorizers, one for each class. The input
    to this function is a list of lists, where each one is a list of words that only
    appear in that class; we created these in the previous step. For each of the word
    lists, we create a **CountVectorizer** object that takes the word list as the
    **vocabulary** parameter. Providing this ensures that we only count those words
    for the purpose of classification:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来创建向量器，每个类别一个。该函数的输入是一个列表的列表，其中每个列表是只出现在那个类中的单词列表；我们在上一步创建了这些列表。对于每个单词列表，我们创建一个**CountVectorizer**对象，将单词列表作为**vocabulary**参数。提供这个参数确保我们只为分类目的计算这些单词：
- en: '[PRE21]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create the vectorizers using the preceding function:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前面的函数创建向量器：
- en: '[PRE22]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In this step, we create a **vectorize** function that takes in a list of words
    and a list of vectorizers. We first create a string from the word list, as the
    vectorizer expects a string. For each vectorizer in the list, we apply it to the
    text and then sum the total count of words in that vectorizer. Finally, we append
    that sum to a list of scores. This will count words in the input per class. We
    return this score list at the end of the function:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们创建一个**vectorize**函数，该函数接受一个单词列表和一个向量器列表。我们首先从单词列表创建一个字符串，因为向量器期望一个字符串。对于列表中的每个向量器，我们将它应用于文本，然后计算该向量器中单词的总计数。最后，我们将这个总和追加到分数列表中。这将按类别计数输入中的单词。函数结束时返回这个分数列表：
- en: '[PRE23]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In this step, we define the **classify** function, which takes a list of scores
    returned by the **vectorize** function. This function simply selects the maximum
    score from the list and returns the index of that score corresponding to the class
    label:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们定义**classify**函数，该函数接受由**vectorize**函数返回的分数列表。这个函数简单地从列表中选择最大分数，并返回对应于类别标签的分数索引：
- en: '[PRE24]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, we apply the preceding functions to the training data. We first vectorize
    the text and then classify it. We create a new column for the result called **prediction**:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将前面的函数应用于训练数据。我们首先对文本进行向量化，然后进行分类。我们为结果创建一个名为**prediction**的新列：
- en: '[PRE25]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will look similar to this:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE26]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we measure the performance of the rule-based classifier by printing the
    classification report. We input the assigned label and the prediction columns.
    The result is an overall accuracy score of 87%:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过打印分类报告来衡量基于规则的分类器的性能。我们输入分配的标签和预测列。结果是整体准确率为87%：
- en: '[PRE27]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This results in the following:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致以下结果：
- en: '[PRE28]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here we do the same for the test data, and we see a significant reduction in
    accuracy, down to 62%. This is because the vocabulary lists that we use to create
    the vectorizers only come from the training data and are not exhaustive. They
    will lead to errors in unseen data:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们对测试数据做同样的处理，我们看到准确率显著下降，降至62%。这是因为我们用来创建向量器的词汇表只来自训练数据，并不全面。它们会导致未见数据中的错误：
- en: '[PRE29]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result will be as follows:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE30]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Clustering sentences using K-Means – unsupervised text classification
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用K-Means进行句子聚类——无监督文本分类
- en: 'In this recipe, we will use the BBC news dataset. The dataset contains news
    pieces sorted by five topics: politics, tech, business, sport, and entertainment.
    We will apply the unsupervised K-Means algorithm to sort the data into unlabeled
    classes.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用BBC新闻数据集。该数据集包含按五个主题排序的新闻文章：政治、科技、商业、体育和娱乐。我们将应用无监督的K-Means算法将数据分类到未标记的类别中。
- en: After you read this recipe, you will be able to create your own unsupervised
    clustering model that will sort data into several classes. You can then later
    apply it to any text data without having to first label it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读完这份食谱后，你将能够创建自己的无监督聚类模型，该模型能够将数据分类到几个类别中。之后，你可以将其应用于任何文本数据，而无需先对其进行标记。
- en: Getting ready
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the `KMeans` algorithm to create our unsupervised model. It is part
    of the `sklearn` package and is included in the `poetry` environment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`KMeans`算法创建我们的无监督模型。它是`sklearn`包的一部分，并包含在`poetry`环境中。
- en: The BBC news dataset as we use it here was uploaded by a Hugging Face user,
    and the link and the dataset might change in time. To avoid any potential issues,
    you can use the BBC dataset uploaded to the book’s GitHub repository by loading
    it from the CSV file provided in the `data` directory.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的BBC新闻数据集是由Hugging Face用户上传的，随着时间的推移，链接和数据集可能会发生变化。为了避免任何潜在问题，你可以使用GitHub仓库中提供的CSV文件加载的书籍的BBC数据集。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.3_unsupervised_classification.ipynb)。
- en: How to do it…
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this recipe, we will preprocess the data, vectorize it, and then cluster
    it using K-Means. Since there are usually no right answers for unsupervised modeling,
    evaluating the models is more difficult, but we will be able to look at some statistics,
    as well as the most common words in all the clusters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将对数据进行预处理，将其向量化，然后使用K-Means进行聚类。由于无监督建模通常没有正确答案，因此评估模型更困难，但我们将能够查看一些统计数据，以及所有聚类中最常见的单词。
- en: 'Your steps should be formatted like so:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你的步骤应该格式化为如下：
- en: 'Run the simple classification file:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类文件：
- en: '[PRE31]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Import the necessary functions and packages:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的函数和包：
- en: '[PRE32]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We will load the BBC dataset. We use the **load_dataset** function from Hugging
    Face’s **datasets** package. This function was imported in the simple classifier
    file we ran in step 1\. In the Hugging Face repository, datasets are usually split
    into training and testing. We will load both, although in unsupervised learning,
    the test set is usually not used:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载BBC数据集。我们使用Hugging Face的`datasets`包中的`**load_dataset**`函数。此函数在步骤1中运行的简单分类文件中已导入。在Hugging
    Face仓库中，数据集通常分为训练集和测试集。我们将加载两者，尽管在无监督学习中，测试集通常不使用：
- en: '[PRE33]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The result will look similar to this:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将类似于以下这样：
- en: '[PRE34]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we will check the distribution of items per class for both training and
    test data. Class balance is important in classification, as a disproportionally
    larger class will influence the final classifier:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将检查训练数据和测试数据中每个类别的项目分布。在分类中，类别平衡很重要，因为一个不成比例更大的类别将影响最终的分类器：
- en: '[PRE35]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We see that the classes are pretty evenly split, but there are more examples
    in the `business` and `sport` categories:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们看到类别分布相当均匀，但在`商业`和`体育`类别中示例更多：
- en: '[PRE36]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Since there is almost as much data in the test set as in the training set,
    we will combine the data and create a better train/test split. We first concatenate
    the two dataframes. We then create a **StratifiedShuffleSplit** that will create
    a train/test split and will do it while preserving the class balance. We specify
    that we only need one split (**n_splits**) and that the test data needs to be
    20% of the whole dataset (**test_size**). The **sss** object’s **split** method
    returns a generator that contains the indices for the split. We can then use these
    indices to get new training and test dataframes. To do that, we filter on the
    relevant indices and then make a copy of the resulting dataframe slice. If we
    didn’t make a copy, then we would be working on the original dataframe. We then
    print out the class counts for both dataframes and see that there is more training
    and less testing data:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于测试集和训练集中的数据量几乎一样多，我们将合并数据并创建一个更好的训练/测试分割。我们首先连接两个数据框。然后我们创建一个**StratifiedShuffleSplit**，它将创建一个训练/测试分割，并在保持类别平衡的同时进行。我们指定我们只需要一个分割（**n_splits**），并且测试数据需要占整个数据集的20%（**test_size**）。**sss**对象的**split**方法返回一个生成器，其中包含分割的索引。然后我们可以使用这些索引来获取新的训练和测试数据框。为此，我们根据相关索引进行筛选，然后复制结果数据框的切片。如果我们没有复制，那么我们就会在原始数据框上工作。然后我们打印出两个数据框的类别计数，并看到有更多的训练数据和较少的测试数据：
- en: '[PRE37]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The result should look like this:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像这样：
- en: '[PRE38]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We will now preprocess the data: tokenize it and remove stopwords and punctuation.
    The functions to do this (**tokenize**, **remove_stopword_punct**) are imported
    in the **language_utils** file we ran in step 1\. If you get an error that the
    **english.pickle** tokenizer was not found, run the line **nltk.download(''punkt'')**
    before running the rest of the code. This code is also contained in the **lang_utils
    notebook**:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将预处理数据：对其进行分词并去除停用词和标点符号。执行此操作的函数（**tokenize**，**remove_stopword_punct**）在步骤1中运行的**language_utils**文件中导入。如果你收到一个错误，表明找不到**english.pickle**分词器，请在运行其余代码之前运行**nltk.download('punkt')**这一行。此代码也包含在**lang_utils
    notebook**中：
- en: '[PRE39]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In this step, we create the vectorizer. To do that, we get all the words from
    the training news articles. First, we save the clean text in a separate column,
    **text_clean**, and then we save the two dataframes to disk. Then we create a
    TF-IDF vectorizer that will count unigrams, bigrams, and trigrams (the **ngram_range**
    parameter). We then fit the vectorizer on the training data only. The reason we
    fit it only on the training data is that if we fit it on both training and test
    data, it would lead to data leakage and we would get better test scores than actual
    performance on unseen data:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们创建向量器。为此，我们从训练新闻文章中获取所有单词。首先，我们将清洗后的文本保存在一个单独的列中，**text_clean**，然后我们将两个数据框保存到磁盘上。然后我们创建一个TF-IDF向量器，它将计算单语元、双语元和三元语（**ngram_range**参数）。然后我们仅在训练数据上拟合向量器。我们仅在训练数据上拟合它的原因是，如果我们同时在训练和测试数据上拟合它，就会导致数据泄露，我们会得到比实际在未见数据上的性能更好的测试分数：
- en: '[PRE40]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we can create the **Kmeans** classifier for five clusters and then fit
    it on the matrix produced using the vectorizer from the preceding code. We specify
    the number of clusters using the **n_clusters** parameter. We also specify that
    the number of times the algorithm should run is 10 using the **n_init** parameter.
    For higher-dimensional problems, it is recommended to do several runs. After initializing
    the classifier, we fit it on the matrix we created using the vectorizer in step
    7\. This will create the clustering of the training data:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以创建五个簇的**Kmeans**分类器，然后将其拟合到前面代码中使用的向量器生成的矩阵上。我们使用**n_clusters**参数指定簇的数量。我们还指定算法应该运行的次数为10，使用**n_init**参数。对于高维问题，建议进行多次运行。初始化分类器后，我们将其拟合到步骤7中使用的向量器创建的矩阵上。这将创建训练数据的聚类：
- en: Note
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In real-life projects, you will not know the number of clusters in advance,
    as we do here. You will need to use the elbow method or other methods to estimate
    the optimal number of classes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际项目中，你不会像我们这样事先知道簇的数量。你需要使用肘部方法或其他方法来估计最佳类别数量。
- en: '[PRE41]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The **get_most_frequent_words** function will return a list of the most frequent
    words in a list. The most frequent words list will provide us with a clue as to
    which topic the text is about. We will use this function to print out the most
    frequent words in a cluster to understand which topic they refer to. The function
    takes in input text, tokenizes it, and then creates a **FreqDist** object. We
    get the top word frequency tuples by using its **most_common** function and finally
    get only the word without the frequencies and return this as a list:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**get_most_frequent_words**函数将返回一个列表，其中包含列表中最频繁的单词。最频繁单词列表将为我们提供有关文本是关于哪个主题的线索。我们将使用此函数打印出聚类中最频繁的单词，以了解它们指的是哪个主题。该函数接受输入文本，对其进行分词，然后创建一个**FreqDist**对象。我们通过使用其**most_common**函数获取顶级单词频率元组，并最终仅获取没有频率的单词并作为列表返回：'
- en: '[PRE42]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In this step, we define another function, **print_most_common_words_by_cluster**,
    which uses the **get_most_frequent_words** function we defined in the previous
    step. We take the dataframe, the **KMeans** model, and the number of clusters
    as input parameters. We then get the list of the clusters assigned to each data
    point and then create a column in the dataframe that specifies the assigned cluster.
    For each cluster, we then filter the dataframe to get the text just for that cluster.
    We use this text to then pass it into the **get_most_frequent_words** function
    to get the list of the most frequent words in that cluster. We print the cluster
    number and the list and return the input dataframe with the added cluster number
    column:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们定义了另一个函数，**print_most_common_words_by_cluster**，它使用我们在上一步定义的**get_most_frequent_words**函数。我们以数据框、**KMeans**模型和聚类数量作为输入参数。然后我们获取分配给每个数据点的聚类列表，并在数据框中创建一个指定分配聚类的列。对于每个聚类，我们过滤数据框以获取仅针对该聚类的文本。我们使用此文本将其传递到**get_most_frequent_words**函数以获取该聚类中最频繁单词的列表。我们打印聚类编号和列表，并返回添加了聚类编号列的输入数据框：
- en: '[PRE43]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here, we use the function we defined in the previous step on the training dataframe.
    We also pass in the fitted **KMeans** model and the number of clusters, **5**.
    The printout gives us an idea of which cluster is which topic. The cluster numbers
    might vary, but the cluster that has **labour**, **party**, **election** as the
    most frequent words is the **politics** cluster; the cluster with the words **music**,
    **award**, and **show** is the **entertainment** cluster; the cluster with the
    words **game**, **England**, **win**, **play**, and **cup** is the **sport** cluster;
    the cluster with the words **sales** and **growth** is the **business** cluster;
    and the cluster with the words **software**, **net**, and **search** is the **tech**
    cluster. We also note that the words **said** and **Mr** are clearly stopwords,
    as they appear in most clusters close to the top:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将上一步定义的函数应用于训练数据框。我们还传递了拟合的**KMeans**模型和聚类数量，**5**。打印输出给我们一个关于哪个聚类对应哪个主题的想法。聚类编号可能不同，但包含**劳动**、**政党**、**选举**作为最频繁单词的聚类是**政治**聚类；包含单词**音乐**、**奖项**和**表演**的聚类是**娱乐**聚类；包含单词**游戏**、**英格兰**、**胜利**、**比赛**和**杯**的聚类是**体育**聚类；包含单词**销售**和**增长**的聚类是**商业**聚类；包含单词**软件**、**网络**和**搜索**的聚类是**技术**聚类。我们还注意到单词**说**和**先生**是明显的停用词，因为它们出现在大多数聚类中接近顶部：
- en: '[PRE44]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The results will vary each time you run the training, but they might look like
    this (output truncated):'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每次运行训练时结果都会有所不同，但它们可能看起来像这样（输出已截断）：
- en: '[PRE45]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In this step, we use the fitted model to predict the cluster for a test example.
    We use the text in row 1 of the test dataframe. It is a politics example. We use
    the vectorizer to turn the text into a vector and then use the K-Means model to
    predict the cluster. The prediction is cluster 0, which in this case is correct:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们使用拟合模型预测测试示例的聚类。我们使用测试数据框的第1行的文本。它是一个政治示例。我们使用向量器将文本转换为向量，然后使用K-Means模型预测聚类。预测是聚类0，在这种情况下是正确的：
- en: '[PRE46]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The result might look like this:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能看起来像这样：
- en: '[PRE47]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, we save the model using the **joblib** package’s **dump** function
    and then load it again using the **load** function. We check the prediction of
    the loaded model, and it is the same as the prediction of the model in memory.
    This step will allow us to reuse the model in the future:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用**joblib**包的**dump**函数保存模型，然后使用**load**函数再次加载它。我们检查加载模型的预测，它与内存中模型的预测相同。这一步将允许我们在未来重用该模型：
- en: '[PRE48]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The result might look like this:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能看起来像这样：
- en: '[PRE49]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Using SVMs for supervised text classification
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM进行监督文本分类
- en: In this recipe, we will build a machine learning classifier that uses the SVM
    algorithm. By the end of this recipe, you will have a working classifier that
    you will be able to test on new inputs and evaluate using the same `classification_report`
    tools we used in the previous sections. We will use the same BBC news dataset
    we used with `KMeans` previously.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将构建一个使用SVM算法的机器学习分类器。到这个菜谱结束时，你将拥有一个可以对新输入进行测试并使用我们在上一节中使用的相同`classification_report`工具进行评估的工作分类器。我们将使用与之前`KMeans`相同的BBC新闻数据集。
- en: Getting ready
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will continue working with the same packages that we already installed in
    the previous recipes. The packages needed are installed in the `poetry` environment
    or by installing the `requirements.txt` file.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用之前菜谱中已经安装的相同包。需要的包安装在了`poetry`环境中，或者通过安装`requirements.txt`文件。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.4-svm_classification.ipynb)。
- en: How to do it…
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: We will load the cleaned training and test data that we had saved in the previous
    recipe. We will then create the SVM classifier and train it. We will use BERT
    encoding as our vectorizer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载在之前菜谱中保存的清洗后的训练和测试数据。然后我们将创建SVM分类器并对其进行训练。我们将使用BERT编码作为我们的向量器。
- en: 'Your steps should be formatted like so:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你的步骤应该格式化如下：
- en: 'Run the simple classifier file:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单分类器文件：
- en: '[PRE50]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Import the necessary functions and packages:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的函数和包：
- en: '[PRE51]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Here, we load the training and test data. If you get a **FileNotFoundError**
    error in this step, run steps 1-7 from the previous recipe, *Clustering sentences
    using K-Means – unsupervised text classification*. We then shuffle the training
    data using the **sample** function. Shuffling ensures that we do not have long
    sequences of data of the same class. Finally, we print out the number of counts
    of examples by class. We see that the classes are more or less balanced, which
    is important for training a classifier:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们加载训练和测试数据。如果你在这个步骤中遇到**FileNotFoundError**错误，请运行之前菜谱中的步骤1-7，即*使用K-Means聚类句子
    – 无监督文本分类*。然后我们使用**sample**函数对训练数据进行洗牌。洗牌确保我们没有长序列的数据，这些数据属于同一类别。最后，我们打印出每个类别的示例数量。我们看到类别大致平衡，这对于训练分类器很重要：
- en: '[PRE52]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The result will look like this:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE53]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here, we load the sentence transformer **all-MiniLM-L6-v2** model that will
    provide the vectors for us. To learn more about the model, please read the *Using
    BERT and OpenAI embeddings instead of word embeddings* recipe in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067).
    We then define the **get_sentence_vector** function, which returns the sentence
    embedding for the text input:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们加载了为我们提供向量的句子转换器**all-MiniLM-L6-v2**模型。要了解更多关于该模型的信息，请阅读[*第3章*](B18411_03.xhtml#_idTextAnchor067)中的*使用BERT和OpenAI嵌入代替词嵌入*菜谱。然后我们定义**get_sentence_vector**函数，该函数返回文本输入的句子嵌入：
- en: '[PRE54]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define a function that will create an SVM object and train it given input data.
    It takes in the input vectors and the gold labels, creates an SVC object with
    the RBF kernel and a regularization parameter of **0.1**, and trains it on the
    training data. It then returns the trained classifier:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将创建一个SVM对象并在给定输入数据上对其进行训练。它接受输入向量和金标签，创建一个具有RBF核和正则化参数**0.1**的SVC对象，并在训练数据上对其进行训练。然后它返回训练好的分类器：
- en: '[PRE55]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In this step, we create the list of labels for the classifier and the **vectorize**
    method. We then create the training and test datasets using the **create_train_test_data**
    method, which is located in the simple classifier file. We then train the classifier
    using the **train_classifier** function and print the training and test metrics.
    We see that the test metrics are really good, all above 90%:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们为分类器和**vectorize**方法创建标签列表。然后我们使用位于简单分类器文件中的**create_train_test_data**方法创建训练和测试数据集。然后我们使用**train_classifier**函数训练分类器并打印训练和测试指标。我们看到测试指标非常好，所有指标都超过90%：
- en: '[PRE56]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output will be as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE57]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'In this step, we print out the confusion matrix to see where the classifier
    makes mistakes. The rows represent the correct labels, and the columns are the
    predicted labels. We see the most confusion (four examples) where the correct
    label is **business** but **tech** is predicted, and where **business** is the
    correct label and **politics** is predicted (two examples). We also see that **business**
    is predicted incorrectly for **tech**, **entertainment**, and **politics** once
    each. These errors are also reflected in the metrics, where we see that both recall
    and precision for **business** are affected. The only category with perfect scores
    is **sport** and it also has zeroes across the confusion matrix everywhere except
    the intersection of the correct row and predicted column. We can use the confusion
    matrix to see which categories have the most confusion between themselves and
    take measures to rectify that if needed:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们打印出混淆矩阵以查看分类器在哪些地方犯了错误。行代表正确的标签，列代表预测的标签。我们看到最多的混淆（四个例子）是正确的标签是**商业**但预测为**技术**，以及正确的标签是**商业**而预测为**政治**（两个例子）。我们还看到**商业**被错误地预测为**技术**、**娱乐**和**政治**各一次。这些错误也反映在指标中，我们看到**商业**的召回率和精确率都受到了影响。唯一得分完美的类别是**体育**，它在混淆矩阵的每个地方都是零，除了正确的行和预测的列的交叉点。我们可以使用混淆矩阵来查看哪些类别之间有最多的混淆，并在必要时采取措施纠正：
- en: '[PRE58]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We will test the classifier on a new example. We first vectorize the text and
    then use the trained model to make a prediction and print the prediction. The
    new article is about tech, and the prediction is class **0**, which is indeed
    **tech**:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在新的示例上测试分类器。我们首先将文本向量化，然后使用训练好的模型进行预测并打印预测结果。新文章是关于技术的，预测类别为 **0**，这确实是**技术**：
- en: '[PRE59]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The result will be as follows:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE60]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: There’s more…
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: There are many different machine learning algorithms that can be used instead
    of the SVM algorithm. Some of the others include regression, Naïve Bayes, and
    decision trees. You can experiment with them and see which ones perform better.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的机器学习算法可以用作 SVM 算法的替代。其中一些包括回归、朴素贝叶斯和决策树。你可以尝试它们，看看哪个表现更好。
- en: Training a spaCy model for supervised text classification
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 spaCy 模型进行监督文本分类
- en: In this recipe, we will train a spaCy model on the BBC dataset, the same dataset
    we used in the previous recipe, to will predict the text category.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用 BBC 数据集训练 spaCy 模型，与之前菜谱中使用的数据集相同，以预测文本类别。
- en: Getting ready
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the spaCy package to train our model. All the dependencies are taken
    care of by the `poetry` environment.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 spaCy 包来训练我们的模型。所有依赖项都由 `poetry` 环境处理。
- en: You will need to download the config file from the book’s GitHub repository,
    located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg).
    This file should be located at the path `../data/spacy_config.cfg` with respect
    to the notebook.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要从书籍的 GitHub 仓库下载配置文件，位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/spacy_config.cfg)。此文件应位于相对于笔记本的路径
    `../data/spacy_config.cfg`。
- en: Note
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can modify the training config, or generate your own at [https://spacy.io/usage/training](https://spacy.io/usage/training).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以修改训练配置，或在其 [https://spacy.io/usage/training](https://spacy.io/usage/training)
    上生成自己的配置。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于 [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.5-spacy_textcat.ipynb)。
- en: How to do it…
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'The general structure of the training is similar to a plain machine learning
    model training, where we clean the data, create the dataset, and split it into
    training and testing datasets. We then train a model and test it on unseen data:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的一般结构类似于普通机器学习模型训练，其中我们清理数据，创建数据集，并将其分为训练集和测试集。然后我们训练一个模型并在未见过的数据上测试它：
- en: 'Run the simple classifier file:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类器文件：
- en: '[PRE61]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Import the necessary functions and packages:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的函数和包：
- en: '[PRE62]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Here we define the **preprocess_data_entry** function, which will take the
    input text, its label, and the list of all labels. It will then run the small
    spaCy model on the text. This model was imported by running the language utilities
    file in step 1\. It is not important which model we use in this step, since we
    just want to have a **Doc** object created from the text. That is why we run the
    smallest model, so it takes less time. We then create a one-hot encoding for the
    text class, setting the class label to **1** and the rest to **0**. We then create
    a label dictionary that maps the category name to its value. We set the **doc.cats**
    attribute to this dictionary and return the **Doc** object. spaCy requires this
    preprocessing of the data to train a classification model:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们定义了**preprocess_data_entry**函数，它将接受输入文本、其标签以及所有标签的列表。然后它将在文本上运行小的spaCy模型。这个模型是通过在步骤1中运行语言实用工具文件导入的。在这个步骤中我们使用哪个模型并不重要，因为我们只是想要从文本中创建一个**Doc**对象。这就是为什么我们运行最小的模型，因为它花费的时间更少。然后我们为文本类别创建一个one-hot编码，将类别标签设置为**1**，其余设置为**0**。然后我们创建一个将类别名称映射到其值的标签字典。我们将**doc.cats**属性设置为这个字典，并返回**Doc**对象。spaCy需要对此数据进行预处理才能训练分类模型：
- en: '[PRE63]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now we prepare the training and test datasets. We create the **DocBin** objects
    for both training and test data that is required by the spaCy algorithm. We then
    load the saved data from disk. This is the data we saved in the K-Means recipe.
    If you get a **FileNotFoundError** error here, you need to run steps 1-7 from
    the *Clustering sentences using K-Means – unsupervised text classification* recipe.
    We then shuffle the training dataframe. Then we preprocess each data point using
    the function we defined in the previous step. We then add each datapoint to the
    **DocBin** object. Finally, we save the two datasets to disk:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们准备训练和测试数据集。我们为spaCy算法所需的训练和测试数据创建了**DocBin**对象。然后我们从磁盘加载保存的数据。这是我们保存到K-Means配方中的数据。如果你在这里遇到**FileNotFoundError**错误，你需要运行*使用K-Means聚类句子
    – 无监督文本分类*配方中的步骤1-7。然后我们随机打乱训练数据框。然后我们使用之前定义的函数预处理每个数据点。然后我们将每个数据点添加到**DocBin**对象中。最后，我们将两个数据集保存到磁盘：
- en: '[PRE64]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Train the model using the **train** command. In order for training to work,
    you will need to have the configuration file downloaded to the **data** folder.
    This is explained in the *Getting ready* section of this recipe. The training
    config specifies the location of the training and test datasets, so you need to
    run the previous step for the training to work. The **train** command saves the
    model in the **model_last** subdirectory of the directory we specify in the input
    (**../models/spacy_textcat_bbc/** in this case):'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**train**命令训练模型。为了使训练工作，你需要将配置文件下载到**data**文件夹中。这在本配方的*准备就绪*部分有解释。训练配置指定了训练和测试数据集的位置，因此你需要运行前面的步骤才能使训练工作。**train**命令将模型保存到我们在输入中指定的目录的**model_last**子目录中（在本例中为**../models/spacy_textcat_bbc/**）：
- en: '[PRE65]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output will differ but might look like this (truncated for easier reading).
    We see that the final accuracy of our trained model is 85%:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果可能会有所不同，但可能看起来像这样（为了便于阅读而截断）。我们可以看到，我们训练的模型的最终准确率是85%：
- en: '[PRE66]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now we test the model on an unseen example. We first load the model and then
    get an example from the test data. We then check the text and its category. We
    run the model on the input text and print the resulting probabilities. The model
    will give a dictionary of categories with their respective probability scores.
    These scores indicate the probability that the text belongs to the respective
    class. The class with the highest probability is the one we should assign to the
    text. The category dictionary is in the **doc.cats** attribute, just like when
    we were preparing the data, but in this case the model assigns it. In this case,
    the text is about politics and the model correctly classifies it:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们对一个未见过的例子进行模型测试。我们首先加载模型，然后从测试数据中获取一个例子。然后我们检查文本及其类别。我们在输入文本上运行模型并打印出结果概率。模型将给出一个包含各自概率得分的类别字典。这些得分表示文本属于相应类别的概率。概率最高的类别是我们应该分配给文本的类别。类别字典在**doc.cats**属性中，就像我们在准备数据时一样，但在这个情况下，模型分配它。在这种情况下，文本是关于政治的，模型正确地将其分类：
- en: '[PRE67]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output will look similar to this:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将看起来类似于这样：
- en: '[PRE68]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'In this step, we define a **get_prediction** function, which takes text, a
    spaCy model, and the list of potential classes and outputs the category whose
    probability is the highest. We then apply this function to the **text** column
    of the test dataframe:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们定义一个**get_prediction**函数，它接受文本、spaCy模型和潜在类别的列表，并输出概率最高的类别。然后我们将此函数应用于测试数据框的**text**列：
- en: '[PRE69]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now we print out the classification report based on the data from the test
    dataframe we generated in the previous step. The overall accuracy of the model
    is 87%, and the reason it is a bit low is because we do not have enough data to
    train a better model:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们根据之前步骤中生成的测试数据框中的数据打印出分类报告。模型的总体准确率为87%，它之所以有点低，是因为我们没有足够的数据来训练更好的模型：
- en: '[PRE70]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The result should look similar to this:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像这样：
- en: '[PRE71]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'In this step, we do the same evaluation using the spaCy **evaluate** command.
    This command takes in the path to the model and the path to the test dataset and
    outputs the scores in a slightly different format. We see that the scores from
    both steps are consistent:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们使用spaCy的**evaluate**命令进行相同的评估。此命令接受模型路径和测试数据集路径，并以略微不同的格式输出分数。我们看到这两个步骤的分数是一致的：
- en: '[PRE72]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The result should look similar to this:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像这样：
- en: '[PRE73]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Classifying texts using OpenAI models
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenAI模型进行文本分类
- en: In this recipe, we will ask an OpenAI model to provide the classification of
    an input text. We will use the same BBC dataset from previous recipes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将要求OpenAI模型对输入文本进行分类。我们将使用之前食谱中相同的BBC数据集。
- en: Getting ready
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To run this recipe, you will need to have the `openai` package installed, provided
    as part of the `poetry` environment, and the `requirements.txt` file. You will
    also have to have an OpenAI API key. Paste it into the provided field in the file
    utilities notebook ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此食谱，你需要安装`openai`包，该包作为`poetry`环境的一部分提供，以及`requirements.txt`文件。你还需要一个OpenAI
    API密钥。将其粘贴到文件实用工具笔记本中提供的字段（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb)）中。
- en: The notebook is located at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本位于[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter04/4.6_openai_classification.ipynb)。
- en: Note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: OpenAI frequently changes and retires existing models and introduces new ones.
    The model we use in this recipe, **gpt-3.5-turbo**, might be obsolete by the time
    you read this. In this case, please check the OpenAI documentation and select
    another suitable model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI经常更改和淘汰现有模型，并引入新的模型。我们在这个食谱中使用的**gpt-3.5-turbo**模型在你阅读本文时可能已经过时。在这种情况下，请检查OpenAI文档并选择另一个合适的模型。
- en: How to do it…
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In this recipe, we will query the OpenAI API and provide a request for classification
    as the prompt. We will then post-process the results and evaluate the Open AI
    model on this task:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将查询OpenAI API并提供一个作为提示的分类请求。然后我们将对结果进行后处理，并评估Open AI模型在此任务上的表现：
- en: 'Run the simple classifier and the file utilities notebooks:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行简单的分类器和文件实用工具笔记本：
- en: '[PRE74]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Import the necessary functions and packages to create the OpenAI client using
    the API key:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用API密钥导入必要的函数和包以创建OpenAI客户端：
- en: '[PRE75]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Load the training and test datasets using Hugging Face without preprocessing
    them for the number of classes, as we will not be training a new model:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Hugging Face加载训练和测试数据集，无需对类别数量进行预处理，因为我们不会训练新的模型：
- en: '[PRE76]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Load and print the first example in the dataset and its category:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并打印数据集中的第一个示例及其类别：
- en: '[PRE77]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The result should be as follows:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该是这样的：
- en: '[PRE78]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Run the OpenAI model on this one example. In step 5, we query the OpenAI API
    asking it to classify this example. We create the prompt and append the example
    text to it. In the prompt, we specify to the model that it is to classify the
    input text as one of five classes and the output format. If we don’t include these
    output instructions, it might add other words to it and return text such as *The
    topic is entertainment*. We select the **gpt-3.5-turbo** model and specify the
    prompt, the temperature, and several other parameters. We set the temperature
    to **0** so that there is no or minimal variation in the model’s response. We
    then print the response returned by the API. The output might vary, but in most
    cases, it should return *entertainment*, which is correct:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个示例上运行OpenAI模型。在第5步，我们查询OpenAI API，要求它对这个示例进行分类。我们创建提示并将示例文本附加到它。在提示中，我们指定模型将输入文本分类为五个类别之一，并指定输出格式。如果我们不包括这些输出指令，它可能会添加其他词语并返回类似“这个话题是娱乐”的文本。我们选择**gpt-3.5-turbo**模型并指定提示、温度和其他几个参数。我们将温度设置为**0**，以便模型响应没有或最小变化。然后我们打印API返回的响应。输出可能会有所不同，但在大多数情况下，它应该返回“娱乐”，这是正确的：
- en: '[PRE79]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The result might vary, but should look like this:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能会有所不同，但应该看起来像这样：
- en: '[PRE80]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Create a function that will provide the classification of an input text and
    return the category. It takes input text and calls the OpenAI API with the same
    prompt we used previously. It then lowercases the response, strips it of extra
    white space, and returns it:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，该函数将提供输入文本的分类并返回类别。它接受输入文本并调用我们之前使用的相同提示的OpenAI API。然后它将响应转换为小写，去除额外的空白，并返回它：
- en: '[PRE81]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'In this step, we load test data. We take the test dataset from Hugging Face
    and convert it into a dataframe. We then shuffle the dataframe and select the
    first 200 examples. The reason is that we want to reduce the cost of testing this
    classifier through the OpenAI API. You can modify how much data you test this
    method on:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们加载测试数据。我们从Hugging Face获取测试数据集并将其转换为数据框。然后我们打乱数据框并选择前200个示例。原因是我们要通过OpenAI
    API降低测试这个分类器的成本。你可以修改你测试此方法的数据量：
- en: '[PRE82]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'In step 8, we use the **get_gpt_classification** function to create a new column
    in the test dataframe. Depending on the number of test examples you have, it might
    take a few minutes to run:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第8步中，我们使用**get_gpt_classification**函数在测试数据框中创建一个新列。根据你拥有的测试示例数量，运行可能需要几分钟：
- en: '[PRE83]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Despite our instructions to OpenAI to only provide the category as the answer,
    it might add some other words, so we define a function, **get_one_word_match**,
    that cleans OpenAI’s output. In this function, we use a regular expression to
    match one of the class labels and return just that word from the original string.
    We then apply this function to the **gpt_prediction** column in the test dataframe:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尽管我们指示OpenAI只提供类别作为答案，但它可能还会添加一些其他词语，因此我们定义了一个函数**get_one_word_match**，用于清理OpenAI的输出。在这个函数中，我们使用正则表达式匹配其中一个类别标签，并从原始字符串中返回该单词。然后我们将此函数应用于测试数据框中的**gpt_prediction**列：
- en: '[PRE84]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Now we turn the label into numerical format:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将标签转换为数值格式：
- en: '[PRE85]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We print the resulting dataframe. We can see that we have all the information
    we need to perform an evaluation. We have both the correct labels (the **label**
    column) and the predicted labels (the **gpt_label** column):'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印出结果数据框。我们可以看到我们进行评估所需的所有信息。我们既有正确的标签（**标签**列）也有预测的标签（**gpt_label**列）：
- en: '[PRE86]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The result should look similar to this:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像这样：
- en: '[PRE87]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now we can print the classification report that evaluates the OpenAI classification:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以打印出评估OpenAI分类的分类报告：
- en: '[PRE88]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The results might vary. This is a sample output. We see that the overall accuracy
    is good, 90%:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能会有所不同。这是一个示例输出。我们看到整体准确率很好，达到90%：
- en: '[PRE89]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
