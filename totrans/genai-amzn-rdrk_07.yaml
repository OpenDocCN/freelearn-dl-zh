- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Question Answering Systems and Conversational Interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will delve into the realm of **question answering** (**QA**)
    and conversational interfaces, harnessing the power of Amazon Bedrock. The chapter
    begins with unveiling real-world use cases of QA with Amazon Bedrock, demonstrating
    the practical applications and benefits of this technology. Moving forward, the
    chapter will cover architectural patterns for QA on both small and large documents,
    providing a solid foundation to understand the underlying mechanics. Additionally,
    the concept of conversation memory will be explained, allowing for the storage
    and utilization of chat history, thereby enabling more contextually aware and
    coherent conversations.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will also dive into the concept of embeddings and their significance
    within the architectural flow of QA systems. Furthermore, we will learn about
    prompt engineering techniques for chatbots, equipping you with the skills to craft
    effective prompts and enhance the performance of their conversational interfaces.
    Contextual awareness will also be addressed, explaining how to develop chatbots
    that can seamlessly integrate and leverage external files and data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will conclude by exploring real-world use cases of conversational
    interfaces, showcasing the diverse applications and potential impact of this technology
    across various domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key topics that will be covered in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: QA overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document ingestion with Amazon Bedrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversational interfaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have access to an AWS account. If you don’t have
    it already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create an AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, you will need to install and configure the AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    after you create an account, which will be needed to access Amazon Bedrock FMs
    from your local machine. Since a major chunk of the code cells we will execute
    is based in Python, setting up the AWS SDK for Python (Boto3) ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))
    will be beneficial at this point. You can carry out the Python setup in the following
    ways – install it on your local machine, use AWS Cloud9, utilize AWS Lambda, or
    leverage Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There is a charge associated with the invocation and customization of the FMs
    of Amazon Bedrock. Refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: QA overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: QA systems are designed to understand natural language queries and provide relevant
    answers based on a given context or knowledge source. These systems leverage advanced
    NLP techniques and machine learning models to comprehend the intent behind a user’s
    question, extracting the most appropriate response from the available information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an example scenario of a typical QA system: suppose you are
    a content writer for a technology company and you need to explain the concept
    of **optical character recognition** (**OCR**) to your audience. A QA system could
    assist you in this task by providing relevant information from its knowledge base,
    or by analyzing a given text corpus related to OCR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how a QA system might handle a query such as `What is optical character
    recognition` `used for?`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query understanding**: The system first analyzes the query to understand
    the user’s intent and extract key entities and concepts. In this case, it recognizes
    that the query asks about the use cases or applications of optical character recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context retrieval**: The system then searches its knowledge base or the provided
    text corpus to find relevant information related to OCR and its applications.
    It may identify passages or paragraphs that discuss OCR’s purpose and practical
    uses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer extraction**: After retrieving the relevant context, the system employs
    advanced NLP techniques, such as named entity recognition, relation extraction,
    and semantic analysis, to identify the most relevant information that directly
    answers the query. It may extract specific use cases or applications of OCR from
    the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer generation**: Finally, the system composes a concise and natural-sounding
    answer based on the extracted information. For example, it might respond with
    something like the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The system may also provide additional context, examples, or relevant information
    to enhance the user’s understanding of the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretically speaking, all of this looks perfect and straightforward. However,
    let’s ponder over some challenges in this situation.
  prefs: []
  type: TYPE_NORMAL
- en: Since these QA systems are designed to automatically generate responses to inquiries
    by analyzing and extracting relevant information from a provided set of data or
    text sources, they may or may not explicitly contain the complete answer to the
    given query. In order words, a system’s ability to infer and combine disparate
    pieces of information from various contexts is crucial, as the complete answer
    may not be readily available in a single, self-contained statement within the
    provided data.
  prefs: []
  type: TYPE_NORMAL
- en: QA poses a significant challenge, as it necessitates models to develop a deep
    comprehension of the semantic meaning and intent behind a query, rather than merely
    relying on superficial keyword matching or pattern recognition. This elevated
    level of language understanding is crucial for accurately identifying the relevant
    information required to formulate a suitable response, even when the exact phrasing
    or terminology differs between the query and the available context.
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming these hurdles is essential for developing intelligent systems that
    can engage in fluid dialogue, provide accurate information, and enhance user experiences
    across a wide range of domains and applications.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, tons of generative AI use cases have spawned in a short
    period. Enterprises are scaling their conversational interfaces – chatbots and
    QA systems – with the goal of reducing manual labor and replacing existing frameworks
    with automated generative AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most promising applications of LLMs and generative AI technology
    is, in fact, QA. Being able to ask natural language questions and receive accurate,
    relevant answers could transform how we interact with information and computers.
  prefs: []
  type: TYPE_NORMAL
- en: Potential QA applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The applications for a robust QA system are far-reaching across many industries
    and domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer service**: Allow customers to ask questions and receive tailored
    help and troubleshooting in a natural language rather than search documentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Research and analytics**: Allow analysts and researchers to ask open-ended
    exploratory questions to discover insights across large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education**: Create intelligent tutoring systems where students can ask follow-up
    questions and receive explanations at their level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge management**: Make an organization’s data, documentation, and processes
    more accessible by allowing natural language queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, as with any generative AI system, there are concerns around factual
    accuracy, safety, and potential misuse that must be carefully addressed as QA
    systems are developed and deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, the ability to break down barriers between humans and information
    through natural language queries represents a key frontier in AI’s advancement.
    With FMs available on Amazon Bedrock, such QA systems powered by LLMs provide
    an exciting glimpse at that future.
  prefs: []
  type: TYPE_NORMAL
- en: QA systems with Amazon Bedrock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enterprise-grade QA systems are usually built on the foundation of cutting-edge
    NLP techniques, including transformer architectures and transfer learning. They
    should be designed to understand the nuances of human language, enabling it to
    comprehend complex queries and extract relevant information from various data
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of Amazon Bedrock is its ability to handle open-ended
    questions that require reasoning and inference. Unlike traditional QA systems
    that rely on predefined rules or patterns, Bedrock can understand the underlying
    context and provide thoughtful responses based on the information it has learned.
  prefs: []
  type: TYPE_NORMAL
- en: With a plethora of FMs available on Amazon Bedrock, developers, data scientists
    or generative AI enthusiasts can build applications or services that can potentially
    excel at dealing with ambiguity and uncertainty. If the available information
    is incomplete or contradictory, these engaging applications can provide responses
    that reflect their level of certainty, or they can request additional information,
    making the interaction more natural and human-like.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Amazon Bedrock is highly scalable and can be easily integrated into
    various applications and platforms, such as chatbots, virtual assistants, and
    knowledge management systems. Its cloud-based architecture and high availability
    nature ensure that it can handle high volumes of queries and adapt to changing
    data and user requirements.
  prefs: []
  type: TYPE_NORMAL
- en: QA without context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In scenarios where no additional context or supporting documents are provided,
    QA systems must rely solely on their pre-trained knowledge to generate responses.
    This type of open-domain **QA without context** presents several key challenges
    compared to scenarios where context is given. Some of these challenges are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge scope and completeness**: When no context is provided, the QA system’s
    knowledge comes entirely from what was present in its training data. This makes
    the scope and completeness of the training data extremely important. Ideally,
    the training data should cover a wide range of topics with factual accuracy. However,
    training datasets can have gaps, biases, or errors, which then get encoded into
    the model’s knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Querying the right knowledge**: Without context to ground the question, the
    QA system must accurately map the question to the relevant areas of knowledge
    in its parameters. This requires strong natural language understanding capabilities
    to correctly interpret the query, identify key entities/relations, and retrieve
    the appropriate factual knowledge to formulate a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hallucination**: A critical challenge is hallucination – when the model generates
    incorrect information that contradicts its training data. Without grounding context,
    there are fewer constraints on what a model may generate. Hallucinations can range
    from subtle mistakes to completely fabricated outputs presented with high confidence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt examples and templates for QA without context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When an LLM is asked a question without any additional context, it can be difficult
    for the LLM to understand the question and generate an accurate answer. It can
    be like providing them with a puzzle with missing pieces. Prompt engineering helps
    us provide the missing pieces, making it easier for LLMs to understand our questions
    and deliver accurate answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, careful prompt engineering is required to steer generation in the right
    direction and encourage well-calibrated, truthful responses. There are three main
    techniques for prompt engineering in QA without context:'
  prefs: []
  type: TYPE_NORMAL
- en: '`What is the capital of France?`, you could ask `What city is the capital of
    France?`. Let’s take another example. Instead of asking `What caused the extinction
    of dinosaurs?` (a broad question), the reformulated prompt would look like `What
    is the most widely accepted theory for the extinction of dinosaurs?` (which focuses
    on a specific aspect).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What are the Great Lakes?`, you could ask `Provide a list of the five Great
    Lakes of North America` (which specifies a desired answer format).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Who wrote Hamlet?` This seems like a straightforward question, but the LLM
    might be unsure whether it’s referring to the authorship of the original play
    or a modern adaptation. Instead, you could ask the model with attribution calibration
    in a certain manner, such as `Can you tell me definitively who wrote the original
    play Hamlet? Based on my understanding of literature, I am very likely (or less
    certainly) correct in my answer`. This version of the prompt offers a range of
    confidence levels (`very likely` or `less certain`) instead of just `confident`
    or `unsure`. This allows the LLM to express a more nuanced level of certainty
    based on the information it has processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the preceding techniques, you should leverage system prompts
    in order to shape the interpretation and response of LLMs when queried by the
    end users. Think of system prompts as carefully crafted instructions that are
    meant to guide the model’s behavior, directing it toward the desired outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, when crafting prompts for role-playing scenarios, system prompts
    can define the personality traits, communication style, and domain knowledge the
    AI should exhibit. Imagine you’re creating a virtual assistant. Through system
    prompts, you can specify a helpful, informative persona, ensuring that the FM
    uses language and knowledge appropriate for the role.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, system prompts can help maintain consistency in the model’s responses,
    especially during prolonged interactions. By outlining the persona and desired
    tone throughout the prompts, you ensure that the model stays true to its character,
    fostering trust and a more natural user experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For an example of system prompts with the Anthropic Claude model, we encourage
    you to peruse through [https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/](https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/).
    You should always keep in mind that the best prompts will depend on the specific
    question and the capabilities of the LLM you use. Experiment with different phrasing
    and templates to find what works best for your needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using prompt engineering, it is always possible to improve the accuracy and
    reliability of LLMs on QA tasks without context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple question prompts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most straightforward ways to prompt a generative model is to pose
    a direct question, formatted within triple quotes in the case of a multiline comprehensive
    prompt within the code. Let’s experiment with an example in the Amazon Bedrock
    chat playground.
  prefs: []
  type: TYPE_NORMAL
- en: In order to execute run simple QA prompts in Amazon Bedrock playground, let’s
    head back to the AWS console and navigate to the Amazon Bedrock landing page.
    Once you reach the landing page, scroll through the left pane and click on the
    **Chat** option under **Playgrounds**.
  prefs: []
  type: TYPE_NORMAL
- en: Select a particular model in the chat playground by navigating to **Select Model**.
    In our example, let’s select the **Jurassic-2 Ultra** FM and initiate the conversation
    with the following example in *Figure 7**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground](img/B22045_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in the preceding example, a simple prompt such as `What is Rabindranath
    Tagore's famous poem "Geetanjali" about?` was used without any context provided
    to the model. In order to further the chat with the model, a follow-up question
    was also asked, `What else are some of his famous poems?`, to which the model
    provided a decent response. (You can run this sample prompt in your Bedrock playground
    with other models and continue the conversation chain to witness any differences
    in the responses.)
  prefs: []
  type: TYPE_NORMAL
- en: You can also leverage **Compare mode** in **Chat Playground** by toggling the
    slider at the right side of the **Chat Playground** window, as shown in *Figure
    7**.2*, and execute a similar prompt against multiple FMs available on Amazon
    Bedrock. As visible in the following figure, three models are compared on a particular
    question. Note the third model was added by clicking on the **+** option on the
    right side.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock](img/B22045_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, by using Amazon Bedrock APIs, the models can be prompted in a QA
    context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding prompt, the FM available in Amazon Bedrock can be invoked;
    the model can then provide a particular response. You are encouraged to run this
    prompt with the Amazon Titan model and capture the response as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Model encouragement and constraints
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Optionally, you can encourage the model by framing the prompt in a motivational
    way. By combining model encouragement and constraints, you can create more effective
    prompts that guide the LLMs to generate high-quality responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Providing context and specific keywords can encourage the model to generate
    more accurate responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting length and format constraints can help the model generate responses
    that are concise and structured
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting the model to a specific domain or topic can help it generate responses
    that are more accurate and relevant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A prompt example can be formatted in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '`You are an expert in explaining complex scientific concepts in a clear and
    engaging manner. Your ability to break down intricate topics into understandable
    terms makes you an invaluable resource for` `educational purposes.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Constraints: Assume your audience consists of college students or professionals
    with a basic understanding of computer science and physics. Your explanation should
    be accessible yet informative, covering both theoretical and practical aspects
    of` `quantum computing.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is followed by the question:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Could you please provide a comprehensive overview of quantum computing, including
    its principles, potential applications, and the challenges` `it faces?`'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.3* illustrates the sample usage of model encouragement, along with
    constraints to invoke the Meta Llama model on Amazon Bedrock’s chat playground.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – A simple prompt example of using model encouragement and constraints
    on the Meta Llama 3 model in Amazon Bedrock’s chat playground](img/B22045_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – A simple prompt example of using model encouragement and constraints
    on the Meta Llama 3 model in Amazon Bedrock’s chat playground
  prefs: []
  type: TYPE_NORMAL
- en: You are encouraged to execute the prompt at your end and note the difference
    in responses with/without the constraints and model encouragement. You will notice
    that this type of prompt can help prime the model to provide a thoughtful, thorough
    response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example for you to execute, either in Amazon Bedrock’s chat
    playground or by using Amazon Bedrock APIs to invoke the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`You have an excellent grasp of complex machine learning concepts and can explain
    them in a clear and` `understandable way.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Could you please explain the concept of gradient descent in` `machine learning?`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Please keep your explanation concise and suitable for readers with a basic
    understanding of` `machine learning.`'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you invoke an FM with a hypothetical question without any relevant
    context. In some cases, it may end up hallucinating. *Figure 7**.4* illustrates
    a fascinating scenario where the model ends up hallucinating when queried about
    an imaginary BMX Subaru bike, which doesn’t really exist in real life!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat
    playground](img/B22045_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat playground
  prefs: []
  type: TYPE_NORMAL
- en: If proper prompt instructions are provided with context, the model will strive
    to find the relevant content within the context and then provide a reasonable
    desirable response.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that while QA without context is extremely challenging, strategies
    such as constitutional AI and iterative refinement techniques that leverage and
    re-combine the model’s internal knowledge in novel ways can help improve performance
    on open-domain QA.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**Constitutional AI** is an area of AI research concerned with developing AI
    systems that adhere to ethical principles and legal frameworks. It can involve
    designing AI systems that are fair, transparent, and accountable and respect human
    rights and privacy.'
  prefs: []
  type: TYPE_NORMAL
- en: QA with context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**QA with context** involves providing an input text and a question, and the
    language model must generate an answer based solely on the information contained
    within the given text. This task requires the model to comprehend the context,
    identify relevant details, and synthesize a coherent response that directly addresses
    the query while avoiding introducing external information.'
  prefs: []
  type: TYPE_NORMAL
- en: For this use case, it is beneficial to structure the prompt by presenting the
    input text first, followed by the question. This ordering allows the model to
    fully process the context before attempting to formulate an answer, potentially
    improving response quality and accuracy. As indicated in the previous section,
    incorporating techniques such as model encouragement can further enhance performance
    on QA tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ideal prompt will have the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_text: {{text}}`'
  prefs: []
  type: TYPE_NORMAL
- en: '`question: {{question}}`'
  prefs: []
  type: TYPE_NORMAL
- en: '`answer:`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see what the content of the prompt would be like:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input_text**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"""The Arctic fox (Vulpes lagopus) is a small fox native to the Arctic regions
    of the Northern Hemisphere and common throughout the Arctic tundra biome. It is
    well adapted to living in cold environments, with dense fur insulation and a compact
    body shape that limits exposure to the cold. Adults weigh 3–5 kg (6.6–11 lb) and
    have a body length of 46–68 cm (18–27 in). Their thick fur is an insulating blanket
    that keeps them warm even in the depths of an Arctic winter. The Arctic fox has
    a deep, thick underfur that insulates it from the cold and a dense, insulating
    guard hair coat` `on top."""`'
  prefs: []
  type: TYPE_NORMAL
- en: '`What are some key adaptations that allow the Arctic fox to survive in cold`
    `Arctic environments?`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example output**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example prompt showcases a scenario where an answer about the
    Arctic fox’s physical adaptations to cold environments is provided, and the question
    prompts the model to identify and summarize the relevant details from the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s walk through an example prompt of QA with context using Amazon
    Bedrock APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the preceding code, and try to invoke the Amazon Bedrock FM on your own
    to test the results. The generated output may look akin to *Figure 7**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Example output generated from an Amazon Bedrock FM](img/B22045_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Example output generated from an Amazon Bedrock FM
  prefs: []
  type: TYPE_NORMAL
- en: After executing the code to invoke the model, you will observe that the model
    can generate an appropriate response in most cases by leveraging the information
    provided as context.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered prompt engineering with QA use cases on Bedrock, let’s
    walk through document ingestion frameworks with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Document ingestion with Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architectural pattern for QA systems with context can be broadly divided
    into two categories – *QA on small documents* and *QA on large documents on knowledge
    bases*. While the core components remain similar, the approach and techniques
    employed may vary, depending on the size and complexity of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: QA on small documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For QA systems designed to handle small documents, such as paragraphs or short
    articles, the architectural pattern typically follows a pipeline approach consisting
    of the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query processing**: The natural language query is preprocessed by converting
    it to a vector representation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document retrieval**: Relevant documents or passages are retrieved from the
    corpus based on the query keywords or semantic similarity measures. For smaller
    documents, retrieval can be straightforward; you can directly embed and index
    the entire document or passage within your vector store. In another scenario,
    since the input documents are smaller in nature, there might not be a need to
    split them into smaller chunks as long as they can fit within the token size limit
    of the model. Once inspected, the document can be directly parsed in context within
    the model prompt template.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Passage ranking**: Once retrieved, the passages are ranked by their relevance
    to the query. This ranking can be done using techniques such as **term frequency-inverse
    document frequency** (**TF-IDF**) semantic similarity, or specialized neural ranking
    models. Automation of passage ranking can be made possible using an orchestrator
    or type or vector database. For instance, Amazon Kendra has a SOTA semantic searching
    mechanism built in to perform relevance ranking.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer extraction**: The top-ranked passages are analyzed to identify the
    most relevant spans or phrases that potentially answer the query. This stage often
    involves techniques such as named entity recognition, coreference resolution,
    and QA models. Hence, in the case of generative AI frameworks, relevant context
    extraction can be performed by these LLMs without the need to explicitly invoke
    complicated techniques.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer scoring and ranking**: The extracted answer candidates are scored
    and ranked based on their confidence or relevance to the query, using techniques
    such as answer verification models or scoring functions. There are some re-ranking
    models, such as Cohere Rerank, that can also be leveraged to improve recall performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer generation**: The top-ranked answer is generated, potentially involving
    post-processing steps such as formatting, rephrasing, or generating natural language
    responses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This pipeline approach is well-suited for QA on small documents, as it allows
    for efficient retrieval and ranking of relevant passages, followed by targeted
    answer extraction and scoring, without the need to split the document into chunks
    or process it in a different way.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through an example of small document ingestion with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: 'For small document ingestion with Amazon Bedrock and LangChain, you can use
    the `TextLoader` and `PDFLoader` are actually *Python* classes, not software components.
    Here’s a brief explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TextLoader` and `PDFLoader` are used to load and parse text and PDF documents,
    respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These classes are part of LangChain’s document loader functionality, which helps
    in preparing documents for further processing in AI applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here’s an example with TextLoader.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the previous chapters, ensure that the necessary libraries for LangChain
    are installed along with Chroma DB. We are using Chroma DB only for example purposes.
    You can leverage other vector databases, such as Chroma, Weaviate, Pinecone, and
    Faiss, based on their use case. If Chroma DB is not installed, execute `!pip install
    chromadb` before running the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: QA for large documents on knowledge bases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When dealing with large documents on knowledge bases, the architectural pattern
    may need to be adapted to handle the scale and complexity of the data. A common
    approach is to incorporate techniques from information retrieval and open-domain
    QA systems. The following steps highlight the process of ingesting large documents,
    creating a vector index, and creating an end-to-end QA pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge base construction**: The large corpus or knowledge base is preprocessed,
    indexed, and structured in a way that facilitates efficient retrieval and querying.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query processing**: Similar to the small document case, the natural language
    query is preprocessed by converting it to a vector representation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document or** **passage retrieval**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chunking**: For larger documents, directly embedding the entire document
    might not be ideal. You should consider chunking the document into smaller, more
    manageable segments such as paragraphs or sentences.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small-to-big retrieval**: In this case, the following process is followed:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Embed and search using smaller chunks during retrieval.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify relevant chunks based on their retrieved scores.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the retrieved chunk IDs to access and provide the corresponding larger document
    segment to the LLM for answer generation. This way, the LLM has access to the
    broader context, while retrieval leverages smaller, more focused units.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficiency**: Chunking and *small-to-big* retrieval can help improve efficiency
    by reducing the computational load of embedding and searching massive documents.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Passage re-ranking**: The retrieved passages or knowledge base entries may
    undergo further reranking or filtering based on their relevance to the query,
    using techniques such as neural re-rankers or semantic similarity measures.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer extraction and generation**: Depending on the nature of the query
    and the knowledge base, answer extraction and generation may involve techniques
    such as multi-hop reasoning, knowledge graph traversal, or generating natural
    language responses from structured data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer scoring and ranking**: Similar to the small document case, the extracted
    answer candidates are scored and ranked based on their confidence factor or relevance
    to the query.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer presentation**: The final answer or set of answers is presented to
    the user, potentially involving formatting, summarization, or generating natural
    language explanations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Additional points** **worth considering**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adaptive retrieval limits**: Depending on the complexity of the query and
    document collection, setting an adaptive limit on the number of retrieved documents
    can optimize performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compression**: Techniques such as summarization or information extraction
    can pre-process large documents to condense information without losing context,
    further aiding the LLM during answer generation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is particularly useful for QA systems operating on large, diverse,
    and potentially unstructured knowledge bases, as it leverages information retrieval
    techniques to efficiently retrieve and rank relevant information before answer
    extraction and generation.
  prefs: []
  type: TYPE_NORMAL
- en: For large document ingestion, it is recommended to use Amazon Bedrock’s knowledge
    bases to handle the ingestion workflow and store the embeddings in a vector database,
    as detailed in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090).
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the document size, modern QA systems tend to incorporate advanced
    techniques, such as transformer-based language models, graph neural networks,
    and multi-task learning. Additionally, techniques such as transfer learning, few-shot
    learning, and domain adaptation are commonly employed to adapt QA models to different
    domains or knowledge bases.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the specific implementation details and techniques
    employed may vary depending on the requirements, constraints, and resources available
    for a particular QA system. The architectural pattern serves as a general framework,
    providing a solid foundation to understand the underlying mechanics and guide
    the design and development of QA systems tailored to different use cases and domains.
  prefs: []
  type: TYPE_NORMAL
- en: QA implementation patterns with Amazon Bedrock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore different patterns pertaining to QA. First,
    we will look at how to ask queries to a model directly. Thereafter, another approach
    using RAG will be covered wherein we will add contextual information. Let us begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'The baseline approach: unbound exploration in the realm of knowledge'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this initial pattern, we embark on a journey where questions are posed directly
    to the model, unencumbered by external constraints. The responses we receive are
    rooted in the model’s foundational knowledge. However, as you clearly understand
    by now, this approach presents a formidable challenge – the outputs are broad
    and generic, devoid of the nuances and specifics that define a customer’s unique
    business landscape. *Figure 7**.6* depicts the journey of said user when interacting
    with Amazon Bedrock and using direct prompts, with small documents within the
    prompt to invoke the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input
    prompts](img/B22045_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input prompts
  prefs: []
  type: TYPE_NORMAL
- en: Note that we covered this approach in detail when we illustrated how to leverage
    the Amazon Bedrock Titan model to provide informative responses to user queries,
    as showcased in the *QA with* *context* section
  prefs: []
  type: TYPE_NORMAL
- en: As shown previously, the example demonstrated how the Bedrock Titan model can
    generate responses without any contextual information provided. Subsequently,
    we manually incorporated context into the model’s input to enhance the quality
    of its responses. It’s important to note that this approach does not involve any
    RAG to incorporate external knowledge into the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: While this straightforward approach can work well for short documents or singleton
    applications, it may not scale effectively for enterprise-level QA scenarios.
    In such cases, where large volumes of enterprise documents need to be considered,
    the entire context may not fit within the prompt sent to the model, necessitating
    more advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RAG approach: contextual illumination'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this second pattern, we will embark on a more refined journey, one that harnesses
    the power of RAG. Here, we artfully weave our questions with relevant contextual
    information, creating a tapestry that is more likely to contain the answers or
    insights that we seek. This approach is analogous to entering the library with
    a well-curated reading list, guiding us toward the shelves that hold the knowledge
    we desire.
  prefs: []
  type: TYPE_NORMAL
- en: However, even in this enhanced approach, a limitation persists – the amount
    of contextual information we can incorporate is bound by the context window imposed
    by the model. It’s akin to carrying a finite number of books in our metaphorical
    backpack, forcing us to carefully curate the contextual information we bring along,
    lest we exceed the weight limit and leave behind potentially crucial insights.
  prefs: []
  type: TYPE_NORMAL
- en: As you learned in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090), RAG combines
    the use of embeddings to index the corpus of documents, building a knowledge base,
    and the use of an LLM to perform the embeddings, with the goal of eventually extracting
    relevant information from a subset of documents within this knowledge base. In
    preparation for RAG, the documents comprising the knowledge base are split into
    chunks of a fixed or variable size. These chunks are then passed through the model
    to obtain their respective embedding vectors. Each embedding vector, along with
    its corresponding document chunk and additional metadata, is stored in a vector
    database, optimized for efficient similarity searches between vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.7* illustrates a RAG-based workflow using Amazon Bedrock in the
    context of a QA generation framework.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – QA with Amazon Bedrock using the RAG approach](img/B22045_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – QA with Amazon Bedrock using the RAG approach
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging this RAG approach, we can tap into a vast repository of contextual
    information, allowing our generative AI models to produce more informed and accurate
    outputs. However, we must remain mindful of the token limitations and carefully
    curate the contextual information we incorporate. Doing so would ensure that we
    strike a balance between the breadth and depth of the domain knowledge (being
    parsed to the model to provide a response instead of the model hallucinating),
    while staying within the model’s constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this approach, we will build upon the code discussed in the previous section
    on small document ingestion. However, you will find differentiating snippets in
    the code – specifically around identifying a similarity with the query from the
    source data and leveraging the pertinent information, augmented for the prompt
    in order to invoke the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Executing this code will give you an understanding of aptly structuring the
    prompt template and invoking the model to generate a desirable response.
  prefs: []
  type: TYPE_NORMAL
- en: You are further encouraged to execute the code on different documents and experiment
    with different vector DBs and FMs to gain a deeper understanding of this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Users should target finding relevant documents to provide accurate answers
    to their queries. Two of the key challenges that users experience when working
    on their generative AI use cases may include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing large documents that exceed the token limit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the most relevant documents for a given question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To tackle these challenges, the RAG approach proposes the following strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document preparation and embeddings**: Before answering questions, the documents
    must be processed and stored in a document store index, as shown in the *Document
    ingestion with Amazon Bedrock* section. The steps involved include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load the documents.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Process and split them into smaller, manageable chunks.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create numerical vector representations (embeddings) of each chunk using the
    Amazon Bedrock Titan Embeddings model or alternate embeddings models.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an index using the chunks and their corresponding embeddings.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question handling**: Once the document index is prepared, users can ask questions,
    and relevant document chunks will be fetched based on the query. The following
    steps will be executed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an embedding of the input question.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the question embedding with the embeddings in the index.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fetch the *Top K* relevant document chunks.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Add those chunks as part of the context in the prompt.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Send the prompt to the Amazon Bedrock FM.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Receive a contextual answer based on the retrieved documents.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By following this approach within the code, we can leverage the power of generative
    AI, embeddings, and vector datastores to provide accurate and context-aware responses
    to user queries, even when dealing with large document sets.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have uncovered QA answering systems in detail, it’s time to uncover
    the realm of its derivative – aka conversational interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Conversational interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Conversational interfaces**, such as virtual assistants or chatbots, have
    found widespread application across various domains, including customer service,
    sales, and e-commerce, offering swift and efficient responses to users. They can
    be accessed through diverse channels, such as websites, messaging applications,
    and social media platforms, thereby ensuring a seamless user experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbot using Amazon Bedrock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the realm of generative AI, Amazon Bedrock provides a robust platform for
    developing and deploying chatbots. *Figure 7**.8* highlights the overall conversational
    flow inculcated with Amazon Bedrock with chat history integration. The flow involves
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A given user asks a particular question via the interface to the appropriate
    Bedrock LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model stores the conversational history to a particular database, say DynamoDB.
    The chat history and the question are appended to form an augmented prompt. The
    conversational history is stored in a database, such as DynamoDB. This history,
    along with the current user query, is used to create an augmented prompt. This
    augmented prompt is then used to inform the generative AI model, which improves
    the chatbot’s responses in future interactions. By incorporating the conversational
    history, the chatbot can avoid prompting the user with questions they have already
    been asked. This fosters a more natural and engaging conversation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The augmented prompt is retrieved to get the relevant response from the LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The conversation continues in the form of feedback, wherein the output generated
    is then fed back in the form of a conversation chain to continue the ongoing interaction
    with the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.8 – A conversational flow with Amazon Bedrock](img/B22045_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – A conversational flow with Amazon Bedrock
  prefs: []
  type: TYPE_NORMAL
- en: 'The use cases for chatbots built on Amazon Bedrock are diverse and versatile,
    catering to a wide range of scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic chatbot – zero shot**: This use case involves the development of a
    basic chatbot that leverages a pre-trained FM to engage in conversational interactions
    without any additional context or prompting. For instance, the following prompt
    can be provided:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`You are a friendly and helpful conversational AI assistant. You should engage
    in natural language conversations on a wide range of topics, answering questions
    to the best of your knowledge and abilities. If you are unsure about something,
    you can respond politely that you don''t have enough information about that particular
    topic. Your main goal is to provide useful information to users in a conversational
    manner. You do not need any additional context or examples to` `start conversing.`'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt-based chatbot (LangChain)**: In this scenario, the chatbot is designed
    to operate within a specific context defined by a prompt template. Leveraging
    the LangChain library, developers can create chatbots that can engage in contextualized
    conversations, providing relevant and tailored responses. For instance, the following
    code snippet showcases how Langchain can be used with a prompt template and engage
    with the user in a conversational chain:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, users can create the LLM chain and provide a sample prompt, such as the
    following one, and invoke the model accordingly:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**A persona-driven chatbot**: This use case involves the creation of chatbots
    with well-defined personas or roles. For instance, a career coach chatbot can
    be developed to engage in dialogues with users, offering guidance and advice on
    career-related matters, while maintaining a consistent persona throughout the
    interaction. For example, a chatbot can be used as a teaching assistant, providing
    students with information and answering their questions. The chatbot can be designed
    to match the personality of a teacher, or it can take on a more playful persona
    to make learning more engaging. Yet another scenario can involve a persona-driven
    chatbot in a customer service or healthcare sector. Specifically, a chatbot in
    healthcare can be used to provide patients with information about their health
    conditions or to answer questions about medications. The chatbot can be designed
    to be empathetic and understanding, and it can use language that is easy for patients
    to understand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context-aware chatbot**: In this advanced use case, the chatbot is designed
    to understand and respond based on contextual information provided through external
    files. By generating embeddings from these files, the chatbot can comprehend and
    incorporate the provided context into its responses, delivering highly relevant
    and context-specific interactions. For instance, the examples provided in [*Chapter
    5*](B22045_05.xhtml#_idTextAnchor090) on RAG highlight a context-aware Chatbot
    use case, where a prompt is provided with the context extracted from ingested
    documents/external files to augment the prompt with the matched context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These use cases demonstrate the versatility and power of chatbots built on Amazon
    Bedrock, enabling developers to create conversational interfaces tailored to diverse
    user needs and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Empowering chatbot development with Amazon Bedrock and the LangChain framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the realm of conversational interfaces, such as chatbots, maintaining context
    and retaining memory across interactions is paramount. This is true not only for
    short-term exchanges but also for long-term conversations, where the ability to
    recall and build upon previous interactions is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous section on QA systems in greater detail (in addition
    to [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090)), LangChain provides memory
    components in two distinct forms to address this need. First, it offers a suite
    of helper utilities designed to manage and manipulate previous chat messages.
    These utilities are modular and highly versatile, enabling their integration into
    various workflows and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, LangChain streamlines the process of incorporating these memory utilities
    into chains, which are fundamental building blocks to create complex conversational
    systems. By leveraging LangChain’s abstractions and easy-to-use interfaces, developers
    can effortlessly define and interact with different types of memory components,
    enabling the creation of sophisticated and context-aware chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you’re building a simple QA bot or a complex, multi-turn conversational
    agent, LangChain’s memory management capabilities, combined with its integration
    with Amazon Bedrock, empower you to craft intelligent and engaging chatbot experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting context-aware conversational interfaces – the fundamental pillars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As detailed in the *QA systems with Amazon Bedrock* section, the cornerstone
    of developing a context-aware chatbot lies in the generation of contextual embeddings.
    As you are aware by now, this initial phase entails an ingestion process that
    feeds your data through an embedding model, wherein these embeddings are then
    meticulously stored in a specialized data structure, often referred to as a vector
    store, facilitating efficient retrieval and manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.9* depicts a process where documents or files are taken as input,
    processed, or transformed, and then converted into embeddings that are stored
    in a vector store.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Chunking large documents and storing embeddings in a vector
    store](img/B22045_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Chunking large documents and storing embeddings in a vector store
  prefs: []
  type: TYPE_NORMAL
- en: Identical to QA system implementation patterns with Bedrock, the second critical
    component in the orchestration of user interactions can be defined as the **request
    handling mechanism**. This intricate process involves receiving user input, interpreting
    the intent and context, invoking the appropriate models or services, and synthesizing
    the relevant responses. It acts as the central hub, choreographing the various
    components to deliver a seamless and contextually relevant conversational experience.
    In our scenario, this form or orchestrator or request handling hub can be executed
    with Langchain or Amazon Bedrock agents. *Figure 7**.10* illustrates the QA conversational
    interface workflow to retrieve a relevant response from the chunked documents,
    by extracting the desired information from the vector store.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – A QA conversational workflow with a similarity search and chunking
    the relevant information](img/B22045_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – A QA conversational workflow with a similarity search and chunking
    the relevant information
  prefs: []
  type: TYPE_NORMAL
- en: Within this request handling phase, the system leverages the previously generated
    embeddings, employing sophisticated algorithms to identify the most pertinent
    information from the vector store. This contextual retrieval enables the chatbot
    to provide responses that are tailored to the specific conversational thread,
    accounting for the user’s intents, previous utterances, and the overarching conversational
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s dive into a context-aware architectural workflow in the case of conversational
    interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: A context-aware chatbot architectural flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process flow for this architecture (also depicted in *Figure 7**.11*) is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the contextual documents are transformed into numerical embeddings
    using a powerful embeddings model, such as Amazon’s Titan Embeddings model. These
    embeddings are stored in a specialized vector database for efficient retrieval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user’s query is encoded into a numerical representation using an embeddings
    model, enabling the system to understand its semantic meaning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user’s query embeddings and the chat history are fed into an FM, specifically
    the text embeddings model, which searches the vector database for the most relevant
    contextual information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vector database returns the contextual embeddings that best match the query,
    allowing the LLM to generate a response that incorporates the relevant context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.11 – A conversational architectural flow for context-aware chatbots](img/B22045_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – A conversational architectural flow for context-aware chatbots
  prefs: []
  type: TYPE_NORMAL
- en: The code for this architectural flow using Amazon Titan is available at [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Additional examples with different FMs from Anthropic, AI21 Labs, and Meta are
    also available on the Amazon Bedrock samples page at the same link. More examples
    will continue to be added to Amazon Bedrock GitHub Samples over time for users
    to experiment and leverage for their enterprise use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, information on building a contextual chatbot application using
    Knowledge Bases with Amazon bedrock can be found here: [https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also encourage you to read about a well-defined QA bot built on an AWS solution,
    expanding your horizon of possibilities to build an enterprise-level conversational
    chatbot: [https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/).'
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of generative AI, QA patterns and conversational interfaces represent
    the ever-evolving journey of knowledge acquisition and dissemination. As we navigate
    these paths, we continually seek innovative ways to refine our queries, augment
    our context, and push the boundaries of what is possible, all in the pursuit of
    unlocking the treasure trove of knowledge that lies within these remarkable models.
  prefs: []
  type: TYPE_NORMAL
- en: As enterprises continue to embrace generative AI and seek more intelligent and
    automated solutions, Amazon Bedrock stands out as a powerful tool to build advanced
    QA systems that can enhance customer experiences, streamline operations, and unlock
    new possibilities in human-computer interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored architectural intricacies and key components that
    power modern QA interfaces and chatbots. We gained insights into prompt engineering
    techniques that facilitate natural and engaging conversations. We further illustrated
    how QA systems and conversational systems can be designed seamlessly with Amazon
    Bedrock, highlighting the architectural workflow for these patterns.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will uncover more potential use cases and applications
    of generative AI with Amazon Bedrock. We will gain a deeper understanding of entity
    extraction and code generation using Amazon Bedrock and its potential real-world
    use cases.
  prefs: []
  type: TYPE_NORMAL
