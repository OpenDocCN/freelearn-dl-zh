- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Building Question Answering Systems and Conversational Interfaces
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建问答系统和对话界面
- en: In this chapter, we will delve into the realm of **question answering** (**QA**)
    and conversational interfaces, harnessing the power of Amazon Bedrock. The chapter
    begins with unveiling real-world use cases of QA with Amazon Bedrock, demonstrating
    the practical applications and benefits of this technology. Moving forward, the
    chapter will cover architectural patterns for QA on both small and large documents,
    providing a solid foundation to understand the underlying mechanics. Additionally,
    the concept of conversation memory will be explained, allowing for the storage
    and utilization of chat history, thereby enabling more contextually aware and
    coherent conversations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨问答（QA）和对话界面的领域，利用 Amazon Bedrock 的力量。本章从展示 Amazon Bedrock 在问答方面的实际应用案例开始，演示这项技术的实际应用和好处。随后，本章将涵盖小型和大型文档上问答的架构模式，为理解其底层机制提供坚实的基础。此外，还将解释对话记忆的概念，允许存储和利用聊天历史，从而实现更具情境意识和连贯性的对话。
- en: The chapter will also dive into the concept of embeddings and their significance
    within the architectural flow of QA systems. Furthermore, we will learn about
    prompt engineering techniques for chatbots, equipping you with the skills to craft
    effective prompts and enhance the performance of their conversational interfaces.
    Contextual awareness will also be addressed, explaining how to develop chatbots
    that can seamlessly integrate and leverage external files and data sources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还将深入探讨嵌入的概念及其在问答系统架构流程中的重要性。此外，我们将学习关于聊天机器人提示工程技术的知识，为您提供制作有效提示并提升其对话界面性能的技能。同时，还将讨论情境意识，解释如何开发能够无缝集成和利用外部文件和数据源的聊天机器人。
- en: Finally, we will conclude by exploring real-world use cases of conversational
    interfaces, showcasing the diverse applications and potential impact of this technology
    across various domains.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过探讨对话界面的实际应用案例来结束本章节，展示这项技术在各个领域的多样应用和潜在影响。
- en: 'The key topics that will be covered in this chapter include the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下关键主题：
- en: QA overview
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答概述
- en: Document ingestion with Amazon Bedrock
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon Bedrock 进行文档摄取
- en: Conversational interfaces
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话界面
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires you to have access to an AWS account. If you don’t have
    it already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create an AWS account.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您拥有 AWS 账户访问权限。如果您还没有，可以访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    并创建一个 AWS 账户。
- en: Secondly, you will need to install and configure the AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))
    after you create an account, which will be needed to access Amazon Bedrock FMs
    from your local machine. Since a major chunk of the code cells we will execute
    is based in Python, setting up the AWS SDK for Python (Boto3) ([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))
    will be beneficial at this point. You can carry out the Python setup in the following
    ways – install it on your local machine, use AWS Cloud9, utilize AWS Lambda, or
    leverage Amazon SageMaker.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，您需要在创建账户后安装和配置 AWS CLI ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))，这将用于从您的本地机器访问
    Amazon Bedrock FMs。由于我们将执行的代码单元格大部分基于 Python，此时设置 Python 的 AWS SDK（Boto3）([https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html))将非常有用。您可以通过以下方式执行
    Python 设置 – 在您的本地机器上安装它，使用 AWS Cloud9，利用 AWS Lambda，或利用 Amazon SageMaker。
- en: Note
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There is a charge associated with the invocation and customization of the FMs
    of Amazon Bedrock. Refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Amazon Bedrock 的 FMs 调用和定制将产生费用。有关更多信息，请参阅 [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)。
- en: QA overview
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问答概述
- en: QA systems are designed to understand natural language queries and provide relevant
    answers based on a given context or knowledge source. These systems leverage advanced
    NLP techniques and machine learning models to comprehend the intent behind a user’s
    question, extracting the most appropriate response from the available information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 问答系统旨在理解自然语言查询并根据给定上下文或知识源提供相关答案。这些系统利用先进的NLP技术和机器学习模型来理解用户问题的意图，从可用信息中提取最合适的响应。
- en: 'Let’s consider an example scenario of a typical QA system: suppose you are
    a content writer for a technology company and you need to explain the concept
    of **optical character recognition** (**OCR**) to your audience. A QA system could
    assist you in this task by providing relevant information from its knowledge base,
    or by analyzing a given text corpus related to OCR.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个典型问答系统的示例场景：假设你是某科技公司的内容创作者，需要向你的听众解释**光学字符识别**（**OCR**）的概念。问答系统可以通过提供其知识库中的相关信息或分析与OCR相关的给定文本语料库来协助你完成这项任务。
- en: 'Here’s how a QA system might handle a query such as `What is optical character
    recognition` `used for?`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个问答系统如何处理诸如“什么是光学字符识别`用于什么`？”这样的查询的例子：
- en: '**Query understanding**: The system first analyzes the query to understand
    the user’s intent and extract key entities and concepts. In this case, it recognizes
    that the query asks about the use cases or applications of optical character recognition.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询理解**：系统首先分析查询以理解用户的意图并提取关键实体和概念。在这种情况下，它认识到查询询问的是光学字符识别的使用案例或应用。'
- en: '**Context retrieval**: The system then searches its knowledge base or the provided
    text corpus to find relevant information related to OCR and its applications.
    It may identify passages or paragraphs that discuss OCR’s purpose and practical
    uses.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文检索**：系统随后在其知识库或提供的文本语料库中搜索与OCR及其应用相关的相关信息。它可能识别出讨论OCR目的和实际用途的段落或段落。'
- en: '**Answer extraction**: After retrieving the relevant context, the system employs
    advanced NLP techniques, such as named entity recognition, relation extraction,
    and semantic analysis, to identify the most relevant information that directly
    answers the query. It may extract specific use cases or applications of OCR from
    the text.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案提取**：在检索到相关上下文后，系统采用先进的NLP技术，如命名实体识别、关系提取和语义分析，以识别直接回答查询的最相关信息。它可能从文本中提取OCR的具体使用案例或应用。'
- en: '**Answer generation**: Finally, the system composes a concise and natural-sounding
    answer based on the extracted information. For example, it might respond with
    something like the following:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**答案生成**：最后，系统根据提取的信息编写一个简洁且自然流畅的答案。例如，它可能会以以下方式回应：'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The system may also provide additional context, examples, or relevant information
    to enhance the user’s understanding of the topic.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 系统还可能提供额外的上下文、示例或相关信息，以增强用户对主题的理解。
- en: Theoretically speaking, all of this looks perfect and straightforward. However,
    let’s ponder over some challenges in this situation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，这一切看起来完美且简单直接。然而，让我们思考一下这种情况中的一些挑战。
- en: Since these QA systems are designed to automatically generate responses to inquiries
    by analyzing and extracting relevant information from a provided set of data or
    text sources, they may or may not explicitly contain the complete answer to the
    given query. In order words, a system’s ability to infer and combine disparate
    pieces of information from various contexts is crucial, as the complete answer
    may not be readily available in a single, self-contained statement within the
    provided data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些问答系统旨在通过分析和从提供的数据或文本源中提取相关信息来自动生成响应，它们可能或可能不明确包含给定查询的完整答案。换句话说，系统从各种上下文中推断和组合不同信息的能力至关重要，因为完整的答案可能不在提供的单一、自包含的陈述中
    readily available。
- en: QA poses a significant challenge, as it necessitates models to develop a deep
    comprehension of the semantic meaning and intent behind a query, rather than merely
    relying on superficial keyword matching or pattern recognition. This elevated
    level of language understanding is crucial for accurately identifying the relevant
    information required to formulate a suitable response, even when the exact phrasing
    or terminology differs between the query and the available context.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: QA提出了重大挑战，因为它要求模型深入理解查询背后的语义意义和意图，而不仅仅是依赖表面关键词匹配或模式识别。这种高级的语言理解能力对于准确识别构建合适响应所需的相关信息至关重要，即使查询和可用上下文之间的确切措辞或术语不同。
- en: Overcoming these hurdles is essential for developing intelligent systems that
    can engage in fluid dialogue, provide accurate information, and enhance user experiences
    across a wide range of domains and applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这些障碍对于开发能够进行流畅对话、提供准确信息并提升广泛领域和应用的用户体验的智能系统至关重要。
- en: At the time of writing, tons of generative AI use cases have spawned in a short
    period. Enterprises are scaling their conversational interfaces – chatbots and
    QA systems – with the goal of reducing manual labor and replacing existing frameworks
    with automated generative AI systems.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，短短一段时间内就产生了大量的生成式AI用例。企业正在扩大其对话界面——聊天机器人和QA系统——的规模，目标是减少人工劳动并用自动化的生成式AI系统替换现有框架。
- en: One of the most promising applications of LLMs and generative AI technology
    is, in fact, QA. Being able to ask natural language questions and receive accurate,
    relevant answers could transform how we interact with information and computers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs和生成式AI技术最有希望的应用之一实际上是QA。能够用自然语言提问并获得准确、相关的答案可能会改变我们与信息和计算机互动的方式。
- en: Potential QA applications
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在的QA应用
- en: 'The applications for a robust QA system are far-reaching across many industries
    and domains:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 强健的QA系统在许多行业和领域中的应用范围非常广泛：
- en: '**Customer service**: Allow customers to ask questions and receive tailored
    help and troubleshooting in a natural language rather than search documentation'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户服务**：允许客户用自然语言提问，并接收定制化的帮助和故障排除，而不是搜索文档'
- en: '**Research and analytics**: Allow analysts and researchers to ask open-ended
    exploratory questions to discover insights across large datasets'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**研究和分析**：允许分析师和研究人员提出开放式探索性问题，以发现大型数据集中的见解'
- en: '**Education**: Create intelligent tutoring systems where students can ask follow-up
    questions and receive explanations at their level'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**教育**：创建智能辅导系统，学生可以提出后续问题并接收符合其水平的解释'
- en: '**Knowledge management**: Make an organization’s data, documentation, and processes
    more accessible by allowing natural language queries'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识管理**：通过允许自然语言查询，使组织的数据、文档和流程更加易于访问'
- en: Of course, as with any generative AI system, there are concerns around factual
    accuracy, safety, and potential misuse that must be carefully addressed as QA
    systems are developed and deployed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，与任何生成式AI系统一样，围绕事实准确性、安全性和潜在误用的担忧必须得到妥善解决，因为QA系统在开发和部署过程中必须谨慎处理。
- en: Nonetheless, the ability to break down barriers between humans and information
    through natural language queries represents a key frontier in AI’s advancement.
    With FMs available on Amazon Bedrock, such QA systems powered by LLMs provide
    an exciting glimpse at that future.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，通过自然语言查询打破人与信息之间障碍的能力代表了人工智能进步的关键前沿。随着在Amazon Bedrock上可用的FMs，这些由LLMs驱动的QA系统为那个未来提供了一个令人兴奋的预览。
- en: QA systems with Amazon Bedrock
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于Amazon Bedrock的QA系统
- en: Enterprise-grade QA systems are usually built on the foundation of cutting-edge
    NLP techniques, including transformer architectures and transfer learning. They
    should be designed to understand the nuances of human language, enabling it to
    comprehend complex queries and extract relevant information from various data
    sources.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 企业级QA系统通常建立在最先进的NLP技术基础之上，包括transformer架构和迁移学习。它们应该被设计成理解人类语言的细微差别，使其能够理解复杂的查询并从各种数据源中提取相关信息。
- en: One of the key advantages of Amazon Bedrock is its ability to handle open-ended
    questions that require reasoning and inference. Unlike traditional QA systems
    that rely on predefined rules or patterns, Bedrock can understand the underlying
    context and provide thoughtful responses based on the information it has learned.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Bedrock 的一个关键优势是它能够处理需要推理和推理的开放性问题。与传统依赖于预定义规则或模式的 QA 系统不同，Bedrock 可以理解潜在上下文，并根据其学习到的信息提供深思熟虑的响应。
- en: With a plethora of FMs available on Amazon Bedrock, developers, data scientists
    or generative AI enthusiasts can build applications or services that can potentially
    excel at dealing with ambiguity and uncertainty. If the available information
    is incomplete or contradictory, these engaging applications can provide responses
    that reflect their level of certainty, or they can request additional information,
    making the interaction more natural and human-like.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Amazon Bedrock 上有大量的 FM（功能模块）可供选择，开发者、数据科学家或生成式 AI 爱好者可以构建能够潜在地擅长处理模糊性和不确定性的应用程序或服务。如果可用信息不完整或相互矛盾，这些引人入胜的应用程序可以提供反映其置信水平的响应，或者它们可以请求更多信息，使交互更加自然和人性化。
- en: Moreover, Amazon Bedrock is highly scalable and can be easily integrated into
    various applications and platforms, such as chatbots, virtual assistants, and
    knowledge management systems. Its cloud-based architecture and high availability
    nature ensure that it can handle high volumes of queries and adapt to changing
    data and user requirements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Amazon Bedrock 具有高度的扩展性，可以轻松集成到各种应用程序和平台中，例如聊天机器人、虚拟助手和知识管理系统。其基于云的架构和高可用性确保它可以处理大量查询并适应不断变化的数据和用户需求。
- en: QA without context
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无上下文 QA
- en: 'In scenarios where no additional context or supporting documents are provided,
    QA systems must rely solely on their pre-trained knowledge to generate responses.
    This type of open-domain **QA without context** presents several key challenges
    compared to scenarios where context is given. Some of these challenges are as
    follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有提供额外上下文或支持文档的情况下，QA 系统必须完全依赖其预训练的知识来生成响应。与提供上下文的情况相比，这种无上下文 **QA** 呈现出几个关键挑战。其中一些挑战如下：
- en: '**Knowledge scope and completeness**: When no context is provided, the QA system’s
    knowledge comes entirely from what was present in its training data. This makes
    the scope and completeness of the training data extremely important. Ideally,
    the training data should cover a wide range of topics with factual accuracy. However,
    training datasets can have gaps, biases, or errors, which then get encoded into
    the model’s knowledge.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识范围和完整性**：当没有提供上下文时，QA 系统的知识完全来自其训练数据中存在的部分。这使得训练数据的范围和完整性变得极其重要。理想情况下，训练数据应涵盖广泛的主题，并具有事实准确性。然而，训练数据集可能存在空白、偏差或错误，这些随后会被编码到模型的认知中。'
- en: '**Querying the right knowledge**: Without context to ground the question, the
    QA system must accurately map the question to the relevant areas of knowledge
    in its parameters. This requires strong natural language understanding capabilities
    to correctly interpret the query, identify key entities/relations, and retrieve
    the appropriate factual knowledge to formulate a response.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询正确知识**：没有上下文来定位问题，QA 系统必须准确地将问题映射到其参数中相关的知识领域。这需要强大的自然语言理解能力来正确解释查询，识别关键实体/关系，并检索适当的事实知识来形成响应。'
- en: '**Hallucination**: A critical challenge is hallucination – when the model generates
    incorrect information that contradicts its training data. Without grounding context,
    there are fewer constraints on what a model may generate. Hallucinations can range
    from subtle mistakes to completely fabricated outputs presented with high confidence.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉**：一个关键挑战是幻觉——当模型生成与训练数据相矛盾的错误信息时。没有基于上下文的限制，模型可能生成的信息种类就较少。幻觉可能从细微的错误到以高置信度呈现的完全虚构的输出都有可能。'
- en: Prompt examples and templates for QA without context
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无上下文 QA 的提示示例和模板
- en: When an LLM is asked a question without any additional context, it can be difficult
    for the LLM to understand the question and generate an accurate answer. It can
    be like providing them with a puzzle with missing pieces. Prompt engineering helps
    us provide the missing pieces, making it easier for LLMs to understand our questions
    and deliver accurate answers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLM被问到一个没有任何额外上下文的问题时，它可能很难理解问题并生成准确的答案。这就像给他们提供了一个缺少拼图的谜题。提示工程帮助我们提供缺失的拼图，使LLM更容易理解我们的问题并提供准确的答案。
- en: 'Thus, careful prompt engineering is required to steer generation in the right
    direction and encourage well-calibrated, truthful responses. There are three main
    techniques for prompt engineering in QA without context:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要仔细的提示工程来引导生成正确的方向，并鼓励准确、真实的回答。在无上下文的情况下，QA中的提示工程有三个主要技术：
- en: '`What is the capital of France?`, you could ask `What city is the capital of
    France?`. Let’s take another example. Instead of asking `What caused the extinction
    of dinosaurs?` (a broad question), the reformulated prompt would look like `What
    is the most widely accepted theory for the extinction of dinosaurs?` (which focuses
    on a specific aspect).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`法国的首都是什么？`，你可以问`法国的首府是哪个城市？`。让我们再举一个例子。与其问`什么导致了恐龙的灭绝？`（一个宽泛的问题），重新构架的提示应该像这样`关于恐龙灭绝最广泛接受的理论是什么？`（它关注一个特定的方面）。'
- en: '`What are the Great Lakes?`, you could ask `Provide a list of the five Great
    Lakes of North America` (which specifies a desired answer format).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`五大湖是什么？`，你可以问`提供北美五大湖的列表`（这指定了期望的答案格式）。'
- en: '`Who wrote Hamlet?` This seems like a straightforward question, but the LLM
    might be unsure whether it’s referring to the authorship of the original play
    or a modern adaptation. Instead, you could ask the model with attribution calibration
    in a certain manner, such as `Can you tell me definitively who wrote the original
    play Hamlet? Based on my understanding of literature, I am very likely (or less
    certainly) correct in my answer`. This version of the prompt offers a range of
    confidence levels (`very likely` or `less certain`) instead of just `confident`
    or `unsure`. This allows the LLM to express a more nuanced level of certainty
    based on the information it has processed.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`谁写了《哈姆雷特》？`这似乎是一个简单的问题，但LLM可能不确定它是指原始剧本的作者还是现代改编。相反，你可以用某种方式对模型进行归因校准来提问，例如`你能明确告诉我谁写了原始剧本《哈姆雷特》吗？根据我对文学的理解，我非常可能（或不太确定）是正确的`。这种提示版本的提示提供了一系列的信心水平（`非常可能`或`不太确定`），而不是仅仅`自信`或`不确定`。这允许LLM根据它处理的信息表达更细微的确定性水平。'
- en: In addition to the preceding techniques, you should leverage system prompts
    in order to shape the interpretation and response of LLMs when queried by the
    end users. Think of system prompts as carefully crafted instructions that are
    meant to guide the model’s behavior, directing it toward the desired outcome.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了上述技术之外，你还应该利用系统提示来塑造LLM在最终用户查询时的解释和响应。将系统提示视为精心设计的指令，旨在指导模型的行为，将其引导到期望的结果。
- en: For instance, when crafting prompts for role-playing scenarios, system prompts
    can define the personality traits, communication style, and domain knowledge the
    AI should exhibit. Imagine you’re creating a virtual assistant. Through system
    prompts, you can specify a helpful, informative persona, ensuring that the FM
    uses language and knowledge appropriate for the role.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，当为角色扮演场景制作提示时，系统提示可以定义AI应展现的性格特征、交流风格和领域知识。想象一下你正在创建一个虚拟助手。通过系统提示，你可以指定一个有帮助、信息丰富的角色，确保FM使用适合角色的语言和知识。
- en: Additionally, system prompts can help maintain consistency in the model’s responses,
    especially during prolonged interactions. By outlining the persona and desired
    tone throughout the prompts, you ensure that the model stays true to its character,
    fostering trust and a more natural user experience.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，系统提示可以帮助保持模型响应的一致性，尤其是在长时间交互期间。通过在提示中概述人物和期望的语气，你确保模型保持其性格，培养信任并提高用户体验的自然性。
- en: For an example of system prompts with the Anthropic Claude model, we encourage
    you to peruse through [https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/](https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/).
    You should always keep in mind that the best prompts will depend on the specific
    question and the capabilities of the LLM you use. Experiment with different phrasing
    and templates to find what works best for your needs.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是一个使用Anthropic Claude模型的系统提示示例，我们鼓励您浏览[https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/](https://promptengineering.org/claudes-system-prompt-a-prompt-engineering-case-study/)。您应该始终记住，最佳的提示将取决于具体问题和您使用的LLM的能力。尝试不同的措辞和模板，以找到最适合您需求的方法。
- en: By using prompt engineering, it is always possible to improve the accuracy and
    reliability of LLMs on QA tasks without context.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用提示工程，总是可以在没有上下文的情况下提高LLM在QA任务中的准确性和可靠性。
- en: Simple question prompts
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单的问题提示
- en: One of the most straightforward ways to prompt a generative model is to pose
    a direct question, formatted within triple quotes in the case of a multiline comprehensive
    prompt within the code. Let’s experiment with an example in the Amazon Bedrock
    chat playground.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 提示生成模型最直接的方法之一是提出一个直接的问题，在代码中多行综合提示的情况下，格式化为三引号。让我们在Amazon Bedrock聊天游乐场中尝试一个示例。
- en: In order to execute run simple QA prompts in Amazon Bedrock playground, let’s
    head back to the AWS console and navigate to the Amazon Bedrock landing page.
    Once you reach the landing page, scroll through the left pane and click on the
    **Chat** option under **Playgrounds**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Amazon Bedrock游乐场中执行简单的QA提示，让我们回到AWS控制台，导航到Amazon Bedrock登录页面。一旦到达登录页面，滚动左侧面板，然后在**游乐场**下的**聊天**选项上点击**Chat**。
- en: Select a particular model in the chat playground by navigating to **Select Model**.
    In our example, let’s select the **Jurassic-2 Ultra** FM and initiate the conversation
    with the following example in *Figure 7**.1*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导航到**选择模型**，在聊天游乐场中选择一个特定的模型。在我们的例子中，让我们选择**Jurassic-2 Ultra** FM，并使用*图7.1*中的以下示例来启动对话。
- en: '![Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground](img/B22045_07_01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 聊天游乐场中使用Amazon Bedrock模型的简单提示](img/B22045_07_01.jpg)'
- en: Figure 7.1 – A simple prompt with Amazon Bedrock models in the chat playground
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 聊天游乐场中使用Amazon Bedrock模型的简单提示
- en: As depicted in the preceding example, a simple prompt such as `What is Rabindranath
    Tagore's famous poem "Geetanjali" about?` was used without any context provided
    to the model. In order to further the chat with the model, a follow-up question
    was also asked, `What else are some of his famous poems?`, to which the model
    provided a decent response. (You can run this sample prompt in your Bedrock playground
    with other models and continue the conversation chain to witness any differences
    in the responses.)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，一个简单的提示，如`拉宾德拉纳特·泰戈尔著名的诗篇“Geetanjali”是关于什么的？`被使用，而没有向模型提供任何上下文。为了进一步与模型进行聊天，还问了一个后续问题，`他还有哪些著名诗作？`，模型提供了相当不错的回答。（您可以在Bedrock游乐场中使用其他模型运行此示例提示，并继续对话链以观察响应中的任何差异。）
- en: You can also leverage **Compare mode** in **Chat Playground** by toggling the
    slider at the right side of the **Chat Playground** window, as shown in *Figure
    7**.2*, and execute a similar prompt against multiple FMs available on Amazon
    Bedrock. As visible in the following figure, three models are compared on a particular
    question. Note the third model was added by clicking on the **+** option on the
    right side.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过在**聊天游乐场**窗口右侧切换滑块来利用**比较模式**，如图*图7.2*所示，并对Amazon Bedrock上可用的多个FM执行类似的提示。如图所示，在特定问题上比较了三个模型。注意第三个模型是通过点击右侧的**+**选项添加的。
- en: '![Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock](img/B22045_07_02.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 在Amazon Bedrock中使用比较模式进行简单QA提示](img/B22045_07_02.jpg)'
- en: Figure 7.2 – Simple QA prompting with Compare Mode in Amazon Bedrock
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 在Amazon Bedrock中使用比较模式进行简单QA提示
- en: 'Similarly, by using Amazon Bedrock APIs, the models can be prompted in a QA
    context:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，通过使用Amazon Bedrock API，可以在QA环境中提示模型：
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the preceding prompt, the FM available in Amazon Bedrock can be invoked;
    the model can then provide a particular response. You are encouraged to run this
    prompt with the Amazon Titan model and capture the response as an exercise.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的提示，可以在Amazon Bedrock中调用可用的FM；然后模型可以提供特定的响应。我们鼓励您使用Amazon Titan模型运行此提示，并将响应作为练习捕获。
- en: Model encouragement and constraints
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型鼓励和约束
- en: Optionally, you can encourage the model by framing the prompt in a motivational
    way. By combining model encouragement and constraints, you can create more effective
    prompts that guide the LLMs to generate high-quality responses.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`你可以选择以激励的方式构建提示来鼓励模型。通过结合模型鼓励和约束，你可以创建更有效的提示，引导LLMs生成高质量的响应。`'
- en: 'Here are some examples:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`以下是一些示例：`'
- en: Providing context and specific keywords can encourage the model to generate
    more accurate responses
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`提供上下文和特定关键词可以鼓励模型生成更准确的响应。`'
- en: Setting length and format constraints can help the model generate responses
    that are concise and structured
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`设置长度和格式约束可以帮助模型生成简洁、结构化的响应。`'
- en: Restricting the model to a specific domain or topic can help it generate responses
    that are more accurate and relevant
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`将模型限制在特定领域或主题可以帮助它生成更准确、更相关的响应。`'
- en: 'A prompt example can be formatted in the following order:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`一个提示示例可以按照以下顺序格式化：`'
- en: '`You are an expert in explaining complex scientific concepts in a clear and
    engaging manner. Your ability to break down intricate topics into understandable
    terms makes you an invaluable resource for` `educational purposes.`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`你是一位在清晰和吸引人的方式中解释复杂科学概念的专家。你将复杂主题分解成可理解术语的能力使你成为教育目的的无价资源。`'
- en: '`Constraints: Assume your audience consists of college students or professionals
    with a basic understanding of computer science and physics. Your explanation should
    be accessible yet informative, covering both theoretical and practical aspects
    of` `quantum computing.`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`约束条件：假设你的听众是具有计算机科学和物理基础知识的大学生或专业人士。你的解释应该是易于理解且信息丰富的，涵盖量子计算的理論和實踐方面。`'
- en: 'This is followed by the question:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`接着是这个问题：`'
- en: '`Could you please provide a comprehensive overview of quantum computing, including
    its principles, potential applications, and the challenges` `it faces?`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`请提供一个关于量子计算的全面概述，包括其原理、潜在应用以及它面临的挑战。`'
- en: '*Figure 7**.3* illustrates the sample usage of model encouragement, along with
    constraints to invoke the Meta Llama model on Amazon Bedrock’s chat playground.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.3* 展示了在Amazon Bedrock的聊天沙盒中使用模型鼓励和约束调用Meta Llama模型的示例用法。'
- en: '![Figure 7.3 – A simple prompt example of using model encouragement and constraints
    on the Meta Llama 3 model in Amazon Bedrock’s chat playground](img/B22045_07_03.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 在Amazon Bedrock的聊天沙盒中对Meta Llama 3模型使用模型鼓励和约束的简单提示示例](img/B22045_07_03.jpg)'
- en: Figure 7.3 – A simple prompt example of using model encouragement and constraints
    on the Meta Llama 3 model in Amazon Bedrock’s chat playground
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 在Amazon Bedrock的聊天沙盒中使用模型鼓励和约束对Meta Llama 3模型的简单提示示例
- en: You are encouraged to execute the prompt at your end and note the difference
    in responses with/without the constraints and model encouragement. You will notice
    that this type of prompt can help prime the model to provide a thoughtful, thorough
    response.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`鼓励你在自己的端执行这个提示，并注意有/没有约束条件和模型鼓励时的响应差异。你会发现这种类型的提示可以帮助引导模型提供深思熟虑、详尽的响应。`'
- en: 'Here is another example for you to execute, either in Amazon Bedrock’s chat
    playground or by using Amazon Bedrock APIs to invoke the model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`以下是一个供你执行的示例，你可以在Amazon Bedrock的聊天沙盒中执行，或者通过使用Amazon Bedrock API调用模型：`'
- en: '`You have an excellent grasp of complex machine learning concepts and can explain
    them in a clear and` `understandable way.`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`你对复杂的机器学习概念有很好的掌握，并且能够以清晰易懂的方式解释它们。`'
- en: '`Could you please explain the concept of gradient descent in` `machine learning?`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`请解释一下机器学习中梯度下降的概念。`'
- en: '`Please keep your explanation concise and suitable for readers with a basic
    understanding of` `machine learning.`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`请确保你的解释简洁，适合具有机器学习基础知识读者。`'
- en: Let’s say you invoke an FM with a hypothetical question without any relevant
    context. In some cases, it may end up hallucinating. *Figure 7**.4* illustrates
    a fascinating scenario where the model ends up hallucinating when queried about
    an imaginary BMX Subaru bike, which doesn’t really exist in real life!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`假设你用一个假设性的问题调用一个FM，没有任何相关上下文。在某些情况下，它可能会产生幻觉。*图7.4* 展示了一个有趣的场景，其中模型在查询一个虚构的BMX
    Subaru自行车时产生了幻觉，而这个自行车在现实生活中并不存在！`'
- en: '![Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat
    playground](img/B22045_07_04.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 在Amazon Bedrock的聊天沙盒中无上下文的一个QA提示示例](img/B22045_07_04.jpg)'
- en: Figure 7.4 – A QA prompt sample without context in Amazon Bedrock’s chat playground
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 在Amazon Bedrock的聊天沙盒中无上下文的QA提示示例
- en: If proper prompt instructions are provided with context, the model will strive
    to find the relevant content within the context and then provide a reasonable
    desirable response.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供了适当的提示指令和上下文，模型将努力在上下文中找到相关内容，然后提供合理的期望响应。
- en: Keep in mind that while QA without context is extremely challenging, strategies
    such as constitutional AI and iterative refinement techniques that leverage and
    re-combine the model’s internal knowledge in novel ways can help improve performance
    on open-domain QA.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，虽然无上下文的QA极具挑战性，但诸如宪法AI和利用模型内部知识以新颖方式利用和重新组合的迭代优化技术等策略可以帮助提高开放域QA的性能。
- en: Note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Constitutional AI** is an area of AI research concerned with developing AI
    systems that adhere to ethical principles and legal frameworks. It can involve
    designing AI systems that are fair, transparent, and accountable and respect human
    rights and privacy.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**宪法AI**是人工智能研究的一个领域，关注于开发遵守伦理原则和法律框架的人工智能系统。这可能包括设计公平、透明和可问责的人工智能系统，并尊重人权和隐私。'
- en: QA with context
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有上下文的QA
- en: '**QA with context** involves providing an input text and a question, and the
    language model must generate an answer based solely on the information contained
    within the given text. This task requires the model to comprehend the context,
    identify relevant details, and synthesize a coherent response that directly addresses
    the query while avoiding introducing external information.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**有上下文的QA**涉及提供输入文本和问题，语言模型必须基于给定文本中的信息生成答案。这项任务要求模型理解上下文，识别相关细节，并综合一个直接回答查询且不引入外部信息的连贯响应。'
- en: For this use case, it is beneficial to structure the prompt by presenting the
    input text first, followed by the question. This ordering allows the model to
    fully process the context before attempting to formulate an answer, potentially
    improving response quality and accuracy. As indicated in the previous section,
    incorporating techniques such as model encouragement can further enhance performance
    on QA tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '对于这个用例，通过首先展示输入文本，然后提出问题来结构化提示是有益的。这种顺序允许模型在尝试形成答案之前完全处理上下文，这可能会提高响应的质量和准确性。如前所述，结合诸如模型鼓励等技术可以进一步提高QA任务的表现。 '
- en: 'The ideal prompt will have the following structure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的提示应该具有以下结构：
- en: '`input_text: {{text}}`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_text: {{文本}}`'
- en: '`question: {{question}}`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`问题：{{问题}}`'
- en: '`answer:`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`answer:`'
- en: 'Now, let’s see what the content of the prompt would be like:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看提示的内容会是怎样的：
- en: '**Input_text**:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入文本**：'
- en: '`"""The Arctic fox (Vulpes lagopus) is a small fox native to the Arctic regions
    of the Northern Hemisphere and common throughout the Arctic tundra biome. It is
    well adapted to living in cold environments, with dense fur insulation and a compact
    body shape that limits exposure to the cold. Adults weigh 3–5 kg (6.6–11 lb) and
    have a body length of 46–68 cm (18–27 in). Their thick fur is an insulating blanket
    that keeps them warm even in the depths of an Arctic winter. The Arctic fox has
    a deep, thick underfur that insulates it from the cold and a dense, insulating
    guard hair coat` `on top."""`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`"""北极狐（Vulpes lagopus）是一种小型狐狸，原产于北半球的北极地区，在北极苔原生物群落中很常见。它非常适合生活在寒冷环境中，拥有密集的毛皮绝缘层和紧凑的体形，这有助于减少对寒冷的暴露。成年狐狸体重3-5公斤（6.6-11磅），体长46-68厘米（18-27英寸）。它们厚厚的毛皮像保温毯一样，即使在北极冬天的深处也能保持温暖。北极狐有一层厚厚的底毛，可以隔离寒冷，以及一层密集的绝缘保护毛皮覆盖在顶部。"""`'
- en: '`What are some key adaptations that allow the Arctic fox to survive in cold`
    `Arctic environments?`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`北极狐在寒冷的北极环境中有哪些关键的适应性特征？`'
- en: '**Example output**:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例输出**：'
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding example prompt showcases a scenario where an answer about the
    Arctic fox’s physical adaptations to cold environments is provided, and the question
    prompts the model to identify and summarize the relevant details from the text.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例提示展示了这样一个场景：提供了一个关于北极狐对寒冷环境物理适应性的答案，问题提示模型从文本中识别和总结相关细节。
- en: 'Next, let’s walk through an example prompt of QA with context using Amazon
    Bedrock APIs:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过一个使用Amazon Bedrock API的QA有上下文示例提示来进行分析：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run the preceding code, and try to invoke the Amazon Bedrock FM on your own
    to test the results. The generated output may look akin to *Figure 7**.5*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码，并尝试在自己的Amazon Bedrock FM上测试结果。生成的输出可能类似于*图7.5*：
- en: '![Figure 7.5 – Example output generated from an Amazon Bedrock FM](img/B22045_07_05.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 来自Amazon Bedrock FM的示例输出](img/B22045_07_05.jpg)'
- en: Figure 7.5 – Example output generated from an Amazon Bedrock FM
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 来自Amazon Bedrock FM的示例输出
- en: After executing the code to invoke the model, you will observe that the model
    can generate an appropriate response in most cases by leveraging the information
    provided as context.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行调用模型的代码后，您将观察到模型可以通过利用提供作为上下文的信息，在大多数情况下生成适当的响应。
- en: Now that we have covered prompt engineering with QA use cases on Bedrock, let’s
    walk through document ingestion frameworks with Amazon Bedrock.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了在Bedrock上的QA用例中的提示工程，让我们通过Amazon Bedrock的文档摄取框架来了解文档摄取。
- en: Document ingestion with Amazon Bedrock
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Amazon Bedrock进行文档摄取
- en: The architectural pattern for QA systems with context can be broadly divided
    into two categories – *QA on small documents* and *QA on large documents on knowledge
    bases*. While the core components remain similar, the approach and techniques
    employed may vary, depending on the size and complexity of the input data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 带有上下文的QA系统的架构模式可以大致分为两类 – *小型文档上的QA* 和 *知识库上的大型文档QA*。虽然核心组件保持相似，但所采用的方法和技术可能因输入数据的大小和复杂性而异。
- en: QA on small documents
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小型文档上的QA
- en: 'For QA systems designed to handle small documents, such as paragraphs or short
    articles, the architectural pattern typically follows a pipeline approach consisting
    of the following stages:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于旨在处理小型文档（如段落或短文章）的QA系统，其架构模式通常遵循以下阶段的管道方法：
- en: '**Query processing**: The natural language query is preprocessed by converting
    it to a vector representation.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询处理**：将自然语言查询预处理为向量表示。'
- en: '**Document retrieval**: Relevant documents or passages are retrieved from the
    corpus based on the query keywords or semantic similarity measures. For smaller
    documents, retrieval can be straightforward; you can directly embed and index
    the entire document or passage within your vector store. In another scenario,
    since the input documents are smaller in nature, there might not be a need to
    split them into smaller chunks as long as they can fit within the token size limit
    of the model. Once inspected, the document can be directly parsed in context within
    the model prompt template.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文档检索**：根据查询关键词或语义相似性度量从语料库中检索相关文档或段落。对于较小的文档，检索可以很直接；您可以直接嵌入和索引整个文档或段落到您的向量存储中。在另一种情况下，由于输入文档的性质较小，只要它们可以适应模型的令牌大小限制，可能就没有必要将它们分成更小的块。一旦检查，文档就可以在模型提示模板的上下文中直接解析。'
- en: '**Passage ranking**: Once retrieved, the passages are ranked by their relevance
    to the query. This ranking can be done using techniques such as **term frequency-inverse
    document frequency** (**TF-IDF**) semantic similarity, or specialized neural ranking
    models. Automation of passage ranking can be made possible using an orchestrator
    or type or vector database. For instance, Amazon Kendra has a SOTA semantic searching
    mechanism built in to perform relevance ranking.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**段落排名**：检索到段落后，根据其与查询的相关性进行排名。这种排名可以使用诸如**词频-逆文档频率（TF-IDF）**语义相似性或专门的神经排名模型等技术来完成。可以使用编排器或类型或向量数据库来实现段落排名的自动化。例如，Amazon
    Kendra内置了SOTA语义搜索机制，用于执行相关性排名。'
- en: '**Answer extraction**: The top-ranked passages are analyzed to identify the
    most relevant spans or phrases that potentially answer the query. This stage often
    involves techniques such as named entity recognition, coreference resolution,
    and QA models. Hence, in the case of generative AI frameworks, relevant context
    extraction can be performed by these LLMs without the need to explicitly invoke
    complicated techniques.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**答案提取**：分析排名靠前的段落，以识别最相关的范围或短语，这些范围或短语可能回答查询。这一阶段通常涉及诸如命名实体识别、指代消解和QA模型等技术。因此，在生成式AI框架的情况下，相关上下文提取可以由这些LLM执行，而无需显式调用复杂的技术。'
- en: '**Answer scoring and ranking**: The extracted answer candidates are scored
    and ranked based on their confidence or relevance to the query, using techniques
    such as answer verification models or scoring functions. There are some re-ranking
    models, such as Cohere Rerank, that can also be leveraged to improve recall performance.'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**答案评分和排名**：提取的答案候选者根据其置信度或与查询的相关性进行评分和排名，使用的技术包括答案验证模型或评分函数。还有一些重新排名模型，例如Cohere
    Rerank，也可以利用来提高召回性能。'
- en: '**Answer generation**: The top-ranked answer is generated, potentially involving
    post-processing steps such as formatting, rephrasing, or generating natural language
    responses.'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**答案生成**：生成排名最高的答案，可能涉及后处理步骤，如格式化、改写或生成自然语言响应。'
- en: This pipeline approach is well-suited for QA on small documents, as it allows
    for efficient retrieval and ranking of relevant passages, followed by targeted
    answer extraction and scoring, without the need to split the document into chunks
    or process it in a different way.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这种管道方法非常适合小型文档的问答，因为它允许高效地检索和排名相关段落，然后进行针对性的答案提取和评分，而无需将文档分成块或以不同的方式处理。
- en: Let’s walk through an example of small document ingestion with Amazon Bedrock.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用 Amazon Bedrock 的小型文档摄取示例来了解。
- en: 'For small document ingestion with Amazon Bedrock and LangChain, you can use
    the `TextLoader` and `PDFLoader` are actually *Python* classes, not software components.
    Here’s a brief explanation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 Amazon Bedrock 和 LangChain 的小型文档摄取，您可以使用 `TextLoader` 和 `PDFLoader`，实际上它们是
    *Python* 类，而不是软件组件。以下是一个简要的解释：
- en: '`TextLoader` and `PDFLoader` are used to load and parse text and PDF documents,
    respectively.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `TextLoader` 和 `PDFLoader` 分别加载和解析文本和 PDF 文档。
- en: These classes are part of LangChain’s document loader functionality, which helps
    in preparing documents for further processing in AI applications.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些类是 LangChain 文档加载功能的一部分，有助于在 AI 应用中准备文档以进行进一步处理。
- en: Here’s an example with TextLoader.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用 TextLoader 的示例。
- en: Note
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As shown in the previous chapters, ensure that the necessary libraries for LangChain
    are installed along with Chroma DB. We are using Chroma DB only for example purposes.
    You can leverage other vector databases, such as Chroma, Weaviate, Pinecone, and
    Faiss, based on their use case. If Chroma DB is not installed, execute `!pip install
    chromadb` before running the following code.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所示，请确保安装 LangChain 所需的库以及 Chroma DB。我们仅使用 Chroma DB 作为示例目的。根据用例，您可以使用其他向量数据库，如
    Chroma、Weaviate、Pinecone 和 Faiss。如果未安装 Chroma DB，请在运行以下代码之前执行 `!pip install chromadb`。
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: QA for large documents on knowledge bases
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识库中大型文档的问答
- en: 'When dealing with large documents on knowledge bases, the architectural pattern
    may need to be adapted to handle the scale and complexity of the data. A common
    approach is to incorporate techniques from information retrieval and open-domain
    QA systems. The following steps highlight the process of ingesting large documents,
    creating a vector index, and creating an end-to-end QA pipeline:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理知识库中的大型文档时，可能需要调整架构模式以处理数据的规模和复杂性。一种常见的方法是结合信息检索和开放域问答系统的技术。以下步骤突出了摄取大型文档、创建向量索引和创建端到端问答管道的过程：
- en: '**Knowledge base construction**: The large corpus or knowledge base is preprocessed,
    indexed, and structured in a way that facilitates efficient retrieval and querying.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**知识库构建**：大型语料库或知识库被预处理、索引和结构化，以便于高效检索和查询。'
- en: '**Query processing**: Similar to the small document case, the natural language
    query is preprocessed by converting it to a vector representation.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询处理**：类似于小型文档的情况，自然语言查询通过将其转换为向量表示进行预处理。'
- en: '**Document or** **passage retrieval**:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文档或段落检索**：'
- en: '**Chunking**: For larger documents, directly embedding the entire document
    might not be ideal. You should consider chunking the document into smaller, more
    manageable segments such as paragraphs or sentences.'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分块**：对于较大的文档，直接嵌入整个文档可能并不理想。您应该考虑将文档分成更小、更易于管理的部分，例如段落或句子。'
- en: '**Small-to-big retrieval**: In this case, the following process is followed:'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从小到大检索**：在这种情况下，遵循以下过程：'
- en: Embed and search using smaller chunks during retrieval.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检索期间使用较小的块进行嵌入和搜索。
- en: Identify relevant chunks based on their retrieved scores.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据检索到的分数识别相关的块。
- en: Use the retrieved chunk IDs to access and provide the corresponding larger document
    segment to the LLM for answer generation. This way, the LLM has access to the
    broader context, while retrieval leverages smaller, more focused units.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检索到的块 ID 访问并提供相应的较大文档段给 LLM 以生成答案。这样，LLM 可以访问更广泛的环境，而检索则利用更小、更专注的单位。
- en: '**Efficiency**: Chunking and *small-to-big* retrieval can help improve efficiency
    by reducing the computational load of embedding and searching massive documents.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：通过减少嵌入和搜索大量文档的计算负载，将文档分块和从小到大检索可以帮助提高效率。'
- en: '**Passage re-ranking**: The retrieved passages or knowledge base entries may
    undergo further reranking or filtering based on their relevance to the query,
    using techniques such as neural re-rankers or semantic similarity measures.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**段落重新排序**：检索到的段落或知识库条目可能根据其与查询的相关性进行进一步的重新排序或过滤，使用技术如神经重新排序器或语义相似度度量。'
- en: '**Answer extraction and generation**: Depending on the nature of the query
    and the knowledge base, answer extraction and generation may involve techniques
    such as multi-hop reasoning, knowledge graph traversal, or generating natural
    language responses from structured data.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**答案提取和生成**：根据查询和知识库的性质，答案提取和生成可能涉及多跳推理、知识图谱遍历或从结构化数据生成自然语言响应等技术。'
- en: '**Answer scoring and ranking**: Similar to the small document case, the extracted
    answer candidates are scored and ranked based on their confidence factor or relevance
    to the query.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**答案评分和排序**：类似于小文档的情况，提取的答案候选者将根据其置信度因子或与查询的相关性进行评分和排序。'
- en: '**Answer presentation**: The final answer or set of answers is presented to
    the user, potentially involving formatting, summarization, or generating natural
    language explanations.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**答案展示**：最终的答案或答案集将展示给用户，可能涉及格式化、摘要或生成自然语言解释。'
- en: '**Additional points** **worth considering**:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**额外考虑点** **值得考虑**：'
- en: '**Adaptive retrieval limits**: Depending on the complexity of the query and
    document collection, setting an adaptive limit on the number of retrieved documents
    can optimize performance.'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应检索限制**：根据查询和文档集合的复杂性，设置自适应的检索文档数量限制可以优化性能。'
- en: '**Compression**: Techniques such as summarization or information extraction
    can pre-process large documents to condense information without losing context,
    further aiding the LLM during answer generation.'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩**：诸如摘要或信息提取等技术可以预处理大量文档，在不丢失上下文的情况下压缩信息，从而进一步帮助 LLM 在答案生成过程中。'
- en: This approach is particularly useful for QA systems operating on large, diverse,
    and potentially unstructured knowledge bases, as it leverages information retrieval
    techniques to efficiently retrieve and rank relevant information before answer
    extraction and generation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法特别适用于在大型、多样化和可能未结构化的知识库上运行的问题回答系统，因为它利用信息检索技术，在答案提取和生成之前，有效地检索和排序相关信息。
- en: For large document ingestion, it is recommended to use Amazon Bedrock’s knowledge
    bases to handle the ingestion workflow and store the embeddings in a vector database,
    as detailed in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大量文档的摄取，建议使用 Amazon Bedrock 的知识库来处理摄取工作流程并将嵌入存储在向量数据库中，具体内容请参阅[*第5章*](B22045_05.xhtml#_idTextAnchor090)。
- en: Regardless of the document size, modern QA systems tend to incorporate advanced
    techniques, such as transformer-based language models, graph neural networks,
    and multi-task learning. Additionally, techniques such as transfer learning, few-shot
    learning, and domain adaptation are commonly employed to adapt QA models to different
    domains or knowledge bases.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 无论文档大小如何，现代 QA 系统往往都会采用高级技术，如基于 Transformer 的语言模型、图神经网络和多任务学习。此外，迁移学习、少样本学习和领域自适应等技术也常被采用，以适应不同领域或知识库的
    QA 模型。
- en: It’s important to note that the specific implementation details and techniques
    employed may vary depending on the requirements, constraints, and resources available
    for a particular QA system. The architectural pattern serves as a general framework,
    providing a solid foundation to understand the underlying mechanics and guide
    the design and development of QA systems tailored to different use cases and domains.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，具体实现细节和采用的技术可能因特定 QA 系统的要求、约束和可用资源而异。架构模式作为一个通用框架，提供了一个理解底层机制的基础，并指导针对不同用例和领域的
    QA 系统的设计和开发。
- en: QA implementation patterns with Amazon Bedrock
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Amazon Bedrock 的 QA 实现模式
- en: In this section, we will explore different patterns pertaining to QA. First,
    we will look at how to ask queries to a model directly. Thereafter, another approach
    using RAG will be covered wherein we will add contextual information. Let us begin!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨与 QA 相关的不同模式。首先，我们将探讨如何直接向模型提出查询。之后，我们将介绍另一种使用 RAG 的方法，其中我们将添加上下文信息。让我们开始吧！
- en: 'The baseline approach: unbound exploration in the realm of knowledge'
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基线方法：知识领域的无界探索
- en: In this initial pattern, we embark on a journey where questions are posed directly
    to the model, unencumbered by external constraints. The responses we receive are
    rooted in the model’s foundational knowledge. However, as you clearly understand
    by now, this approach presents a formidable challenge – the outputs are broad
    and generic, devoid of the nuances and specifics that define a customer’s unique
    business landscape. *Figure 7**.6* depicts the journey of said user when interacting
    with Amazon Bedrock and using direct prompts, with small documents within the
    prompt to invoke the model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个初始模式中，我们开始了一段旅程，在这个旅程中，问题直接向模型提出，不受外部约束的限制。我们收到的回答根植于模型的基础知识。然而，正如你现在所清楚理解的，这种方法提出了一个巨大的挑战——输出内容广泛且通用，缺乏定义客户独特商业景观的细微差别和具体细节。*图7.6*展示了用户与Amazon
    Bedrock交互并使用直接提示时的旅程，提示中的小文档用于调用模型。
- en: '![Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input
    prompts](img/B22045_07_06.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 使用直接输入提示对Bedrock LLM进行QA生成的提示](img/B22045_07_06.jpg)'
- en: Figure 7.6 – Prompting the Bedrock LLM for QA generation with direct input prompts
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 使用直接输入提示对Bedrock LLM进行QA生成的提示
- en: Note that we covered this approach in detail when we illustrated how to leverage
    the Amazon Bedrock Titan model to provide informative responses to user queries,
    as showcased in the *QA with* *context* section
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在展示如何利用Amazon Bedrock Titan模型向用户查询提供信息性回答时详细介绍了这种方法，如*带上下文的QA*部分所示。
- en: As shown previously, the example demonstrated how the Bedrock Titan model can
    generate responses without any contextual information provided. Subsequently,
    we manually incorporated context into the model’s input to enhance the quality
    of its responses. It’s important to note that this approach does not involve any
    RAG to incorporate external knowledge into the model’s output.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，示例演示了Bedrock Titan模型如何在没有任何上下文信息提供的情况下生成回答。随后，我们手动将上下文纳入模型输入中，以增强其回答的质量。重要的是要注意，这种方法不涉及任何RAG来将外部知识纳入模型的输出。
- en: While this straightforward approach can work well for short documents or singleton
    applications, it may not scale effectively for enterprise-level QA scenarios.
    In such cases, where large volumes of enterprise documents need to be considered,
    the entire context may not fit within the prompt sent to the model, necessitating
    more advanced techniques.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种直接的方法对于短文档或单一应用可能效果良好，但它可能无法有效地扩展到企业级QA场景。在这种情况下，需要考虑大量企业文档，整个上下文可能无法放入发送给模型的提示中，这需要更高级的技术。
- en: 'The RAG approach: contextual illumination'
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG方法：上下文照明
- en: In this second pattern, we will embark on a more refined journey, one that harnesses
    the power of RAG. Here, we artfully weave our questions with relevant contextual
    information, creating a tapestry that is more likely to contain the answers or
    insights that we seek. This approach is analogous to entering the library with
    a well-curated reading list, guiding us toward the shelves that hold the knowledge
    we desire.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第二个模式中，我们将开始一段更精细的旅程，这段旅程利用了RAG的力量。在这里，我们巧妙地将问题与相关的上下文信息交织在一起，创造出一个更有可能包含我们寻求的答案或见解的图案。这种方法类似于带着精心编制的阅读清单进入图书馆，引导我们走向存放我们渴望的知识的书架。
- en: However, even in this enhanced approach, a limitation persists – the amount
    of contextual information we can incorporate is bound by the context window imposed
    by the model. It’s akin to carrying a finite number of books in our metaphorical
    backpack, forcing us to carefully curate the contextual information we bring along,
    lest we exceed the weight limit and leave behind potentially crucial insights.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在改进后的这种方法中，仍然存在一个限制——我们可以整合的上下文信息量受到模型强加的上下文窗口的限制。这就像在我们的比喻背包中携带有限数量的书籍，迫使我们仔细挑选携带的上下文信息，以免超过重量限制并遗漏可能至关重要的见解。
- en: As you learned in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090), RAG combines
    the use of embeddings to index the corpus of documents, building a knowledge base,
    and the use of an LLM to perform the embeddings, with the goal of eventually extracting
    relevant information from a subset of documents within this knowledge base. In
    preparation for RAG, the documents comprising the knowledge base are split into
    chunks of a fixed or variable size. These chunks are then passed through the model
    to obtain their respective embedding vectors. Each embedding vector, along with
    its corresponding document chunk and additional metadata, is stored in a vector
    database, optimized for efficient similarity searches between vectors.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[*第五章*](B22045_05.xhtml#_idTextAnchor090)中学习到的，RAG结合了使用嵌入来索引文档语料库、构建知识库以及使用LLM来执行嵌入的方法，目的是最终从知识库中的一部分文档中提取相关信息。为了准备RAG，构成知识库的文档被分成固定或可变大小的块。然后，这些块通过模型传递以获得其各自的嵌入向量。每个嵌入向量及其对应的文档块和附加元数据都存储在向量数据库中，该数据库针对高效向量之间的相似性搜索进行了优化。
- en: '*Figure 7**.7* illustrates a RAG-based workflow using Amazon Bedrock in the
    context of a QA generation framework.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.7*展示了在QA生成框架的背景下使用Amazon Bedrock的基于RAG的工作流程。'
- en: '![Figure 7.7 – QA with Amazon Bedrock using the RAG approach](img/B22045_07_07.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 使用RAG方法与Amazon Bedrock进行QA](img/B22045_07_07.jpg)'
- en: Figure 7.7 – QA with Amazon Bedrock using the RAG approach
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 使用RAG方法与Amazon Bedrock进行QA
- en: By leveraging this RAG approach, we can tap into a vast repository of contextual
    information, allowing our generative AI models to produce more informed and accurate
    outputs. However, we must remain mindful of the token limitations and carefully
    curate the contextual information we incorporate. Doing so would ensure that we
    strike a balance between the breadth and depth of the domain knowledge (being
    parsed to the model to provide a response instead of the model hallucinating),
    while staying within the model’s constraints.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这种RAG方法，我们可以访问大量的上下文信息库，使我们的生成式AI模型能够产生更明智和准确的输出。然而，我们必须保持对令牌限制的警觉，并仔细挑选我们纳入的上下文信息。这样做将确保我们在领域知识的广度和深度之间取得平衡（将解析到模型中以提供响应，而不是让模型进行幻想），同时保持在模型的约束范围内。
- en: 'In this approach, we will build upon the code discussed in the previous section
    on small document ingestion. However, you will find differentiating snippets in
    the code – specifically around identifying a similarity with the query from the
    source data and leveraging the pertinent information, augmented for the prompt
    in order to invoke the LLM:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们将基于前一小节中讨论的小型文档摄取代码进行构建。但是，您将在代码中找到不同的片段 – 尤其是在识别与源数据中的查询的相似性以及利用相关信息，增强提示以调用LLM的部分：
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Executing this code will give you an understanding of aptly structuring the
    prompt template and invoking the model to generate a desirable response.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将帮助您了解恰当地构建提示模板并调用模型以生成期望的响应。
- en: You are further encouraged to execute the code on different documents and experiment
    with different vector DBs and FMs to gain a deeper understanding of this approach.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您还被鼓励在不同的文档上执行代码，并尝试不同的向量数据库和FM，以更深入地了解这种方法。
- en: 'Users should target finding relevant documents to provide accurate answers
    to their queries. Two of the key challenges that users experience when working
    on their generative AI use cases may include the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 用户应致力于找到相关文档，以提供对其查询的准确答案。用户在使用生成式AI用例时可能遇到的两个关键挑战可能包括以下内容：
- en: Managing large documents that exceed the token limit
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理超过令牌限制的大型文档
- en: Identifying the most relevant documents for a given question
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定给定问题的最相关文档
- en: 'To tackle these challenges, the RAG approach proposes the following strategy:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，RAG方法提出了以下策略：
- en: '**Document preparation and embeddings**: Before answering questions, the documents
    must be processed and stored in a document store index, as shown in the *Document
    ingestion with Amazon Bedrock* section. The steps involved include the following:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档准备和嵌入**：在回答问题之前，文档必须被处理并存储在文档存储索引中，如*使用Amazon Bedrock进行文档摄取*部分所示。涉及到的步骤包括以下内容：'
- en: Load the documents.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载文档。
- en: Process and split them into smaller, manageable chunks.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其处理并分割成更小、更易于管理的块。
- en: Create numerical vector representations (embeddings) of each chunk using the
    Amazon Bedrock Titan Embeddings model or alternate embeddings models.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Amazon Bedrock Titan Embeddings模型或替代嵌入模型为每个块创建数值向量表示（嵌入）。
- en: Create an index using the chunks and their corresponding embeddings.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用块及其相应的嵌入创建索引。
- en: '**Question handling**: Once the document index is prepared, users can ask questions,
    and relevant document chunks will be fetched based on the query. The following
    steps will be executed:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题处理**：一旦文档索引准备就绪，用户可以提出问题，并根据查询检索相关文档块。以下步骤将被执行：'
- en: Create an embedding of the input question.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入问题的嵌入。
- en: Compare the question embedding with the embeddings in the index.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将问题嵌入与索引中的嵌入进行比较。
- en: Fetch the *Top K* relevant document chunks.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取*Top K*个相关文档块。
- en: Add those chunks as part of the context in the prompt.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些块作为提示中的上下文部分添加。
- en: Send the prompt to the Amazon Bedrock FM.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将提示发送到Amazon Bedrock FM。
- en: Receive a contextual answer based on the retrieved documents.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据检索到的文档接收上下文答案。
- en: By following this approach within the code, we can leverage the power of generative
    AI, embeddings, and vector datastores to provide accurate and context-aware responses
    to user queries, even when dealing with large document sets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在代码中采用这种方法，我们可以利用生成式AI、嵌入和向量数据存储的力量，为用户查询提供准确和上下文感知的响应，即使在处理大量文档集时也是如此。
- en: Now that we have uncovered QA answering systems in detail, it’s time to uncover
    the realm of its derivative – aka conversational interfaces.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细了解了问答系统，是时候揭开其衍生领域——即对话界面的面纱了。
- en: Conversational interfaces
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对话界面
- en: '**Conversational interfaces**, such as virtual assistants or chatbots, have
    found widespread application across various domains, including customer service,
    sales, and e-commerce, offering swift and efficient responses to users. They can
    be accessed through diverse channels, such as websites, messaging applications,
    and social media platforms, thereby ensuring a seamless user experience.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**对话界面**，如虚拟助手或聊天机器人，在各种领域得到了广泛应用，包括客户服务、销售和电子商务，为用户提供快速高效的响应。它们可以通过多种渠道访问，如网站、消息应用和社交媒体平台，从而确保无缝的用户体验。'
- en: Chatbot using Amazon Bedrock
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon Bedrock的聊天机器人
- en: 'In the realm of generative AI, Amazon Bedrock provides a robust platform for
    developing and deploying chatbots. *Figure 7**.8* highlights the overall conversational
    flow inculcated with Amazon Bedrock with chat history integration. The flow involves
    the following steps:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI领域，Amazon Bedrock提供了一个强大的平台，用于开发和部署聊天机器人。*图7.8*突出了Amazon Bedrock中包含聊天历史集成的整体对话流程。该流程包括以下步骤：
- en: A given user asks a particular question via the interface to the appropriate
    Bedrock LLM.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 某个用户通过界面向适当的Bedrock LLM提出特定问题。
- en: The model stores the conversational history to a particular database, say DynamoDB.
    The chat history and the question are appended to form an augmented prompt. The
    conversational history is stored in a database, such as DynamoDB. This history,
    along with the current user query, is used to create an augmented prompt. This
    augmented prompt is then used to inform the generative AI model, which improves
    the chatbot’s responses in future interactions. By incorporating the conversational
    history, the chatbot can avoid prompting the user with questions they have already
    been asked. This fosters a more natural and engaging conversation.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型将对话历史存储到特定的数据库中，例如DynamoDB。聊天历史和问题被附加在一起形成增强提示。对话历史存储在数据库中，例如DynamoDB。这个历史记录，连同当前用户查询一起，用于创建增强提示。然后，这个增强提示被用来通知生成式AI模型，这有助于在未来的交互中改进聊天机器人的响应。通过整合对话历史，聊天机器人可以避免向用户提出他们已经回答过的问题。这促进了更自然和吸引人的对话。
- en: The augmented prompt is retrieved to get the relevant response from the LLM.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从LLM检索增强提示以获取相关响应。
- en: The conversation continues in the form of feedback, wherein the output generated
    is then fed back in the form of a conversation chain to continue the ongoing interaction
    with the user.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对话以反馈的形式继续，其中生成的输出随后以对话链的形式反馈，以继续与用户的持续交互。
- en: '![Figure 7.8 – A conversational flow with Amazon Bedrock](img/B22045_07_08.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 与Amazon Bedrock的对话流程](img/B22045_07_08.jpg)'
- en: Figure 7.8 – A conversational flow with Amazon Bedrock
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 与Amazon Bedrock的对话流程
- en: 'The use cases for chatbots built on Amazon Bedrock are diverse and versatile,
    catering to a wide range of scenarios:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon Bedrock上构建的聊天机器人的用例多种多样，适应广泛场景：
- en: '**Basic chatbot – zero shot**: This use case involves the development of a
    basic chatbot that leverages a pre-trained FM to engage in conversational interactions
    without any additional context or prompting. For instance, the following prompt
    can be provided:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基本聊天机器人 - 零样本**：这个用例涉及开发一个利用预训练的FM进行对话交互的基本聊天机器人，无需任何额外的上下文或提示。例如，可以提供以下提示：'
- en: '`You are a friendly and helpful conversational AI assistant. You should engage
    in natural language conversations on a wide range of topics, answering questions
    to the best of your knowledge and abilities. If you are unsure about something,
    you can respond politely that you don''t have enough information about that particular
    topic. Your main goal is to provide useful information to users in a conversational
    manner. You do not need any additional context or examples to` `start conversing.`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`您是一个友好且有帮助的对话式人工智能助手。您应该就广泛的话题进行自然语言对话，尽您所能回答问题。如果您对某事不确定，您可以礼貌地回应您对该特定主题的信息不足。您的主要目标是以对话方式向用户提供有用的信息。您不需要任何额外的上下文或示例来开始对话。`'
- en: '**Prompt-based chatbot (LangChain)**: In this scenario, the chatbot is designed
    to operate within a specific context defined by a prompt template. Leveraging
    the LangChain library, developers can create chatbots that can engage in contextualized
    conversations, providing relevant and tailored responses. For instance, the following
    code snippet showcases how Langchain can be used with a prompt template and engage
    with the user in a conversational chain:'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于提示的聊天机器人（LangChain）**：在这个场景中，聊天机器人被设计在由提示模板定义的特定上下文中运行。利用LangChain库，开发者可以创建能够进行情境化对话的聊天机器人，提供相关和定制的响应。例如，以下代码片段展示了如何使用提示模板与LangChain交互，并在对话链中与用户进行交互：'
- en: '[PRE6]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, users can create the LLM chain and provide a sample prompt, such as the
    following one, and invoke the model accordingly:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，用户可以创建LLM链并提供一个示例提示，如下所示，并相应地调用模型：
- en: '[PRE14]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**A persona-driven chatbot**: This use case involves the creation of chatbots
    with well-defined personas or roles. For instance, a career coach chatbot can
    be developed to engage in dialogues with users, offering guidance and advice on
    career-related matters, while maintaining a consistent persona throughout the
    interaction. For example, a chatbot can be used as a teaching assistant, providing
    students with information and answering their questions. The chatbot can be designed
    to match the personality of a teacher, or it can take on a more playful persona
    to make learning more engaging. Yet another scenario can involve a persona-driven
    chatbot in a customer service or healthcare sector. Specifically, a chatbot in
    healthcare can be used to provide patients with information about their health
    conditions or to answer questions about medications. The chatbot can be designed
    to be empathetic and understanding, and it can use language that is easy for patients
    to understand.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以角色驱动的聊天机器人**：这个用例涉及创建具有明确角色或身份的聊天机器人。例如，可以开发一个职业导师聊天机器人，与用户进行对话，提供与职业相关的事务指导和建议，同时在整个互动过程中保持一致的角色。例如，聊天机器人可以用作教学助理，为学生提供信息和解答他们的问题。聊天机器人可以被设计成符合教师的个性，或者采取更轻松的角色，使学习更加有趣。另一种场景可能涉及客户服务或医疗保健领域的角色驱动聊天机器人。具体来说，医疗保健领域的聊天机器人可以用来向患者提供有关其健康状况的信息，或者回答有关药物的问题。聊天机器人可以被设计成富有同情心和理解的，并且可以使用患者容易理解的语言。'
- en: '**Context-aware chatbot**: In this advanced use case, the chatbot is designed
    to understand and respond based on contextual information provided through external
    files. By generating embeddings from these files, the chatbot can comprehend and
    incorporate the provided context into its responses, delivering highly relevant
    and context-specific interactions. For instance, the examples provided in [*Chapter
    5*](B22045_05.xhtml#_idTextAnchor090) on RAG highlight a context-aware Chatbot
    use case, where a prompt is provided with the context extracted from ingested
    documents/external files to augment the prompt with the matched context.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文感知聊天机器人**：在这个高级用例中，聊天机器人被设计为根据通过外部文件提供的信息理解和响应。通过从这些文件生成嵌入，聊天机器人可以理解和整合提供的信息到其响应中，提供高度相关和特定上下文的交互。例如，[*第五章*](B22045_05.xhtml#_idTextAnchor090)中提供的例子强调了上下文感知聊天机器人的用例，其中提示与从摄取的文档/外部文件中提取的上下文相结合，以增强提示与匹配的上下文。'
- en: These use cases demonstrate the versatility and power of chatbots built on Amazon
    Bedrock, enabling developers to create conversational interfaces tailored to diverse
    user needs and scenarios.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这些用例展示了基于Amazon Bedrock构建的聊天机器人的多功能性和强大功能，使开发者能够创建满足不同用户需求和场景的对话接口。
- en: Empowering chatbot development with Amazon Bedrock and the LangChain framework
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Amazon Bedrock和LangChain框架赋能聊天机器人开发
- en: In the realm of conversational interfaces, such as chatbots, maintaining context
    and retaining memory across interactions is paramount. This is true not only for
    short-term exchanges but also for long-term conversations, where the ability to
    recall and build upon previous interactions is crucial.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话接口的领域，如聊天机器人，保持上下文和跨交互保留记忆至关重要。这不仅适用于短期交流，也适用于长期对话，其中回忆和基于先前交互构建的能力是至关重要的。
- en: As discussed in the previous section on QA systems in greater detail (in addition
    to [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090)), LangChain provides memory
    components in two distinct forms to address this need. First, it offers a suite
    of helper utilities designed to manage and manipulate previous chat messages.
    These utilities are modular and highly versatile, enabling their integration into
    various workflows and applications.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，在更详细地讨论问答系统（除了[*第五章*](B22045_05.xhtml#_idTextAnchor090)）的部分（以及[*第五章*](B22045_05.xhtml#_idTextAnchor090)中提到的[*第五章*](B22045_05.xhtml#_idTextAnchor090)），LangChain提供了两种不同形式的记忆组件来满足这一需求。首先，它提供了一套辅助工具，旨在管理和操作之前的聊天消息。这些工具是模块化的，并且非常灵活，可以集成到各种工作流程和应用中。
- en: Secondly, LangChain streamlines the process of incorporating these memory utilities
    into chains, which are fundamental building blocks to create complex conversational
    systems. By leveraging LangChain’s abstractions and easy-to-use interfaces, developers
    can effortlessly define and interact with different types of memory components,
    enabling the creation of sophisticated and context-aware chatbots.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，LangChain简化了将这些记忆工具集成到链中的过程，链是创建复杂对话系统的基本构建块。通过利用LangChain的抽象和易于使用的接口，开发者可以轻松定义和交互不同类型的记忆组件，从而创建复杂且上下文感知的聊天机器人。
- en: Whether you’re building a simple QA bot or a complex, multi-turn conversational
    agent, LangChain’s memory management capabilities, combined with its integration
    with Amazon Bedrock, empower you to craft intelligent and engaging chatbot experiences.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是在构建一个简单的问答机器人，还是一个复杂的多轮对话代理，LangChain的记忆管理功能，结合其与Amazon Bedrock的集成，使您能够打造智能且引人入胜的聊天机器人体验。
- en: Crafting context-aware conversational interfaces – the fundamental pillars
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打造上下文感知对话接口——基本支柱
- en: As detailed in the *QA systems with Amazon Bedrock* section, the cornerstone
    of developing a context-aware chatbot lies in the generation of contextual embeddings.
    As you are aware by now, this initial phase entails an ingestion process that
    feeds your data through an embedding model, wherein these embeddings are then
    meticulously stored in a specialized data structure, often referred to as a vector
    store, facilitating efficient retrieval and manipulation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如“使用Amazon Bedrock的问答系统”部分所述，开发上下文感知聊天机器人的基石在于生成上下文嵌入。如您现在所知，这一初始阶段涉及一个摄取过程，通过嵌入模型将数据喂入，其中这些嵌入随后被仔细存储在专门的数据结构中，通常称为向量存储，以促进高效的检索和处理。
- en: '*Figure 7**.9* depicts a process where documents or files are taken as input,
    processed, or transformed, and then converted into embeddings that are stored
    in a vector store.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.9*展示了将文档或文件作为输入，进行处理或转换，然后将其转换为存储在向量存储中的嵌入的过程。'
- en: '![Figure 7.9 – Chunking large documents and storing embeddings in a vector
    store](img/B22045_07_09.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 分块大型文档并将嵌入存储在向量存储中](img/B22045_07_09.jpg)'
- en: Figure 7.9 – Chunking large documents and storing embeddings in a vector store
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 分块大型文档并将嵌入存储在向量存储中
- en: Identical to QA system implementation patterns with Bedrock, the second critical
    component in the orchestration of user interactions can be defined as the **request
    handling mechanism**. This intricate process involves receiving user input, interpreting
    the intent and context, invoking the appropriate models or services, and synthesizing
    the relevant responses. It acts as the central hub, choreographing the various
    components to deliver a seamless and contextually relevant conversational experience.
    In our scenario, this form or orchestrator or request handling hub can be executed
    with Langchain or Amazon Bedrock agents. *Figure 7**.10* illustrates the QA conversational
    interface workflow to retrieve a relevant response from the chunked documents,
    by extracting the desired information from the vector store.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用Bedrock的QA系统实现模式相同，用户交互编排中的第二个关键组件可以定义为**请求处理机制**。这个复杂的过程包括接收用户输入，解释意图和上下文，调用适当的模型或服务，并综合相关响应。它作为中央枢纽，协调各种组件以提供无缝且上下文相关的对话体验。在我们的场景中，这种形式或编排器或请求处理中心可以使用Langchain或Amazon
    Bedrock代理执行。*图7.10*说明了QA对话界面工作流程，通过从向量存储中提取所需信息来从分块文档中检索相关响应。
- en: '![Figure 7.10 – A QA conversational workflow with a similarity search and chunking
    the relevant information](img/B22045_07_10.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 带有相似性搜索和相关信息分块的问题回答对话工作流程](img/B22045_07_10.jpg)'
- en: Figure 7.10 – A QA conversational workflow with a similarity search and chunking
    the relevant information
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 带有相似性搜索和相关信息分块的问题回答对话工作流程
- en: Within this request handling phase, the system leverages the previously generated
    embeddings, employing sophisticated algorithms to identify the most pertinent
    information from the vector store. This contextual retrieval enables the chatbot
    to provide responses that are tailored to the specific conversational thread,
    accounting for the user’s intents, previous utterances, and the overarching conversational
    context.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在请求处理阶段，系统利用先前生成的嵌入，采用复杂的算法从向量存储中识别最相关的信息。这种上下文检索使聊天机器人能够提供针对特定对话线程的响应，考虑到用户的意图、先前的话语和整体对话上下文。
- en: Now, let’s dive into a context-aware architectural workflow in the case of conversational
    interfaces.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨在对话界面情况下上下文感知的架构工作流程。
- en: A context-aware chatbot architectural flow
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文感知聊天机器人的架构流程
- en: 'The process flow for this architecture (also depicted in *Figure 7**.11*) is
    as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构的过程流程（也在*图7.11*中展示）如下：
- en: Initially, the contextual documents are transformed into numerical embeddings
    using a powerful embeddings model, such as Amazon’s Titan Embeddings model. These
    embeddings are stored in a specialized vector database for efficient retrieval.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始时，上下文文档使用强大的嵌入模型（如亚马逊的Titan Embeddings模型）转换为数值嵌入。这些嵌入存储在专门的向量数据库中，以便高效检索。
- en: The user’s query is encoded into a numerical representation using an embeddings
    model, enabling the system to understand its semantic meaning.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用嵌入模型将用户的查询编码成数值表示，使系统能够理解其语义含义。
- en: The user’s query embeddings and the chat history are fed into an FM, specifically
    the text embeddings model, which searches the vector database for the most relevant
    contextual information.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户的查询嵌入和聊天历史被输入到FM中，特别是文本嵌入模型，该模型在向量数据库中搜索最相关的上下文信息。
- en: The vector database returns the contextual embeddings that best match the query,
    allowing the LLM to generate a response that incorporates the relevant context.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量数据库返回与查询最佳匹配的上下文嵌入，使LLM能够生成包含相关上下文的响应。
- en: '![Figure 7.11 – A conversational architectural flow for context-aware chatbots](img/B22045_07_11.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 用于上下文感知聊天机器人的对话架构流程](img/B22045_07_11.jpg)'
- en: Figure 7.11 – A conversational architectural flow for context-aware chatbots
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 一种用于上下文感知聊天机器人的对话架构流程
- en: The code for this architectural flow using Amazon Titan is available at [https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Amazon Titan构建此架构流程的代码可在[https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb](https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/02_Langchain_Chatbot_examples/00_Chatbot_Titan.ipynb)找到。
- en: Additional examples with different FMs from Anthropic, AI21 Labs, and Meta are
    also available on the Amazon Bedrock samples page at the same link. More examples
    will continue to be added to Amazon Bedrock GitHub Samples over time for users
    to experiment and leverage for their enterprise use cases.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon Bedrock样本页上，还有来自Anthropic、AI21 Labs和Meta的不同FM的额外示例。随着时间的推移，Amazon Bedrock
    GitHub样本页上还将继续添加更多示例，供用户进行实验并用于他们的企业用例。
- en: 'Furthermore, information on building a contextual chatbot application using
    Knowledge Bases with Amazon bedrock can be found here: [https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有关使用Amazon Bedrock构建基于知识库的上下文聊天应用的信息，您可以在此找到：[https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/build-a-contextual-chatbot-application-using-knowledge-bases-for-amazon-bedrock/)。
- en: 'We also encourage you to read about a well-defined QA bot built on an AWS solution,
    expanding your horizon of possibilities to build an enterprise-level conversational
    chatbot: [https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还鼓励您阅读有关基于AWS解决方案构建的明确问答机器人的文章，以扩展您构建企业级对话聊天机器人的可能性：[https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/](https://aws.amazon.com/blogs/machine-learning/deploy-generative-ai-self-service-question-answering-using-the-qnabot-on-aws-solution-powered-by-amazon-lex-with-amazon-kendra-and-amazon-bedrock/)。
- en: In the realm of generative AI, QA patterns and conversational interfaces represent
    the ever-evolving journey of knowledge acquisition and dissemination. As we navigate
    these paths, we continually seek innovative ways to refine our queries, augment
    our context, and push the boundaries of what is possible, all in the pursuit of
    unlocking the treasure trove of knowledge that lies within these remarkable models.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI的领域内，问答模式和对话界面代表了知识获取和传播不断演进的旅程。在我们探索这些路径的过程中，我们不断寻求创新的方法来优化我们的查询，增强我们的上下文，并推动可能性的边界，所有这些都在追求解锁这些卓越模型中蕴藏的知识宝库。
- en: As enterprises continue to embrace generative AI and seek more intelligent and
    automated solutions, Amazon Bedrock stands out as a powerful tool to build advanced
    QA systems that can enhance customer experiences, streamline operations, and unlock
    new possibilities in human-computer interactions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 随着企业继续拥抱生成式AI并寻求更智能和自动化的解决方案，Amazon Bedrock作为一个强大的工具脱颖而出，可以构建先进的问答系统，从而提升客户体验，简化运营流程，并解锁人机交互中的新可能性。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored architectural intricacies and key components that
    power modern QA interfaces and chatbots. We gained insights into prompt engineering
    techniques that facilitate natural and engaging conversations. We further illustrated
    how QA systems and conversational systems can be designed seamlessly with Amazon
    Bedrock, highlighting the architectural workflow for these patterns.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了现代问答界面和聊天机器人的架构复杂性及其关键组件。我们了解了促进自然和吸引人对话的提示工程技巧。我们还展示了如何使用Amazon Bedrock无缝设计问答系统和对话系统，突出了这些模式的结构化工作流程。
- en: In the next chapter, we will uncover more potential use cases and applications
    of generative AI with Amazon Bedrock. We will gain a deeper understanding of entity
    extraction and code generation using Amazon Bedrock and its potential real-world
    use cases.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将揭示更多使用Amazon Bedrock的生成式AI的潜在用例和应用。我们将更深入地了解使用Amazon Bedrock进行实体提取和代码生成，以及其潜在的实际应用场景。
