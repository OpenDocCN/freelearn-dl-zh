- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Prompt Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we mentioned the term **prompt** several times while
    referring to user input in ChatGPT and **large language models** (**LLMs**) in
    general.
  prefs: []
  type: TYPE_NORMAL
- en: Since prompts have a massive impact on LLMs’ performance, prompt engineering
    is a crucial activity to get the most out of your GenAI tool. In fact, there are
    several techniques that can be implemented not only to refine your LLMs’ responses
    but also to reduce risks associated with hallucinations and biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the emerging techniques in the field
    of prompt engineering, starting from basic approaches up to advanced frameworks.
    More specifically, we will go through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring zero-, one-, and few-shot learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principles of prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at some advanced techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical considerations to avoid bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have the foundations to build functional
    and solid prompts to interact with ChatGPT and, more broadly, with GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ll need an OpenAI account. You can use the free ChatGPT version to run this
    chapter’s examples.
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before explaining what prompt engineering is, let’s start by defining a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **prompt** is text input that guides the behavior of an LLM to generate an
    output. For example, whenever we interact with ChatGPT, asking a question or giving
    an instruction, that input text is a prompt. In the context of LLMs and LLM-powered
    applications, we can distinguish two types of prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: The first type is a prompt that the user writes and sends to the LLM. For example,
    a prompt might be “Give me the recipe for Lasagna Bolognese,” or “Generate a workout
    plan to run a marathon.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A screenshot of a chat  Description automatically generated](img/B31559_03_01.png)![](img/B31559_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: An example of a user’s prompt'
  prefs: []
  type: TYPE_NORMAL
- en: You will hear this referred to simply as a **prompt**, a **query**, or **user
    input**.
  prefs: []
  type: TYPE_NORMAL
- en: The second type is a prompt that instructs the model to behave in a certain
    way regardless of the user’s query. This refers to the set of instructions in
    natural language that the model is provided with so that it behaves in a certain
    way when interacting with end users. You can think about that as a sort of “backend”
    of your LLM, something that will be handled by the application developers rather
    than the final users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31559_03_03.png)![](img/B31559_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: An example of a system message'
  prefs: []
  type: TYPE_NORMAL
- en: We refer to this type of prompt as a **system message**.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is the process of designing effective prompts that elicit
    high-quality and relevant outputs from LLMs. Prompt engineering requires creativity,
    an understanding of the LLM, and a clear understanding of the objective you want
    to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Example of prompt engineering to specialize LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: Over the last few years, prompt engineering has become a brand-new discipline
    in itself, and this is a demonstration of the fact that interacting with those
    models requires a new set of skills and capabilities that did not exist before.
  prefs: []
  type: TYPE_NORMAL
- en: The *art of prompting* has become a top skill when it comes to building GenAI
    applications in enterprise scenarios; however, it can also be extremely useful
    for individual users who use ChatGPT or similar AI assistants in daily tasks,
    as it dramatically improves the quality and accuracy of results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we are going to see some examples of how to build efficient,
    robust prompts leveraging ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding zero-, one-, and few-shot learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we mentioned how LLMs typically come in a pre-trained
    format. They have been trained on a huge amount of data and have had their (billions
    of) parameters configured accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: However, this doesn’t mean that those LLMs can’t learn anymore. In *Chapter
    2*, we learned the concept of fine-tuning. In the *Appendix*, too, we will see
    that one way to customize an OpenAI model and make it more capable of addressing
    specific tasks is by **fine-tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is a proper training process that requires a training dataset, compute
    power, and some training time (depending on the amount of data and compute instances).
  prefs: []
  type: TYPE_NORMAL
- en: 'That is why it is worth testing another method for our LLMs to become more
    skilled in specific tasks: **shot learning.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of LLMs, **shot learning** refers to the model’s ability to perform
    tasks with varying amounts of task-specific examples provided during inference.
    These shot-learning paradigms enable LLMs to adapt to new tasks with minimal to
    no additional training, enhancing their versatility and efficiency in natural
    language processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to let the model learn from simple examples rather than the entire
    dataset. Those examples are samples of the way we would like the model to respond
    so that the model not only learns the content but also the format, style, and
    taxonomy to use in its response.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, shot learning occurs directly via the prompt (as we will see in
    the following scenarios), so the whole experience is less time-consuming and easier
    to perform.
  prefs: []
  type: TYPE_NORMAL
- en: The number of examples provided determines the level of shot learning we are
    referring to. In other words, we refer to zero-shot if no example is provided,
    one-shot if one example is provided, and few-shot if more than two examples are
    provided.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on each of those scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this type of learning, the model is asked to perform a task for which it
    has not seen any training examples. The model must rely on prior knowledge or
    general information about the task to complete it. For example, a zero-shot-learning
    approach could be that of asking the model to generate a description, as defined
    in my prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Example of zero-shot learning'
  prefs: []
  type: TYPE_NORMAL
- en: One-shot learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this type of learning, the model is given a single example of each new task
    it is asked to perform. The model must use its prior knowledge to generalize from
    this single example to perform the task. If we consider the preceding example,
    I could provide my model with a prompt-completion example before asking it to
    generate a new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Example of one-shot learning'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the previous screenshot, the model was able to generate
    an answer that mirrors the style and template of the example provided. The same
    reasoning applies when we provide multiple examples, as described in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this type of learning, the model is given a small number of examples (typically
    between 2 and 5) of each new task it is asked to perform. The model must use its
    prior knowledge to generalize from these examples to perform the task. Let’s continue
    with our example and provide the model with further examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Example of few-shot learning with three examples'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, it is important to remember that these forms of learning
    are different from traditional supervised learning, as well as fine-tuning. In
    few-shot learning, the goal is to enable the model to learn from very few examples,
    and to generalize from those examples to new tasks. Plus, we are not modifying
    the architecture and knowledge of the model itself, meaning that the moment the
    user starts a new conversation, and the previous prompt is out of the context
    window, the model will “forget” about it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is a type of machine learning where a model is trained on
    a labeled dataset, meaning the input data is paired with corresponding correct
    outputs (labels). The goal is for the model to learn the relationship between
    inputs and outputs so it can accurately predict the output for new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned how to let OpenAI models learn from examples, let’s focus
    on how to properly define our prompt to make the model’s response as accurate
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Principles of prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditionally, in the context of computing and data processing, the expression
    *garbage in, garbage out* has been used, meaning that the quality of output is
    determined by the quality of the input. If incorrect or poor-quality input (garbage)
    is entered into a system, the output will also be flawed or nonsensical (garbage).
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to prompting, the story is similar: if we want accurate and relevant
    results from our LLMs, we need to provide high-quality input. However, building
    good prompts is not just about the quality of the response. In fact, we can construct
    good prompts to:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximize the relevancy of an LLM’s responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the type formatting and style of responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide conversational context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce inner LLMs’ biases and improve fairness and inclusivity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce hallucination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the context of LLMs, **hallucination** refers to the generation of text
    or responses that are factually incorrect, nonsensical, or not grounded in the
    training data. This occurs when an LLM produces confident-sounding but erroneous
    or fabricated information. For example, a user could ask an LLM: *“Who is the
    author of the book Invisible Cities?”* If the model responds with something like:
    *“Invisible Cities was written by Gabriel García Márquez.”,* this is a hallucination
    because the correct author is *Italo Calvino*. The model generated an answer that
    sounds plausible but is factually incorrect.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s see some basic prompt engineering techniques in the following sections
    to achieve these results.
  prefs: []
  type: TYPE_NORMAL
- en: Clear instructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The principle of giving clear instructions is to provide the model with enough
    information and guidance to perform the task correctly and efficiently. Clear
    instructions should include the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The goal or objective of the task, such as “write a poem” or “summarize an article.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The format or structure of the expected output, such as “use four lines with
    rhyming words” or “use bullet points with no more than 10 words each.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The constraints or limitations of the task, such as “do not use any profanity”
    or “do not copy any text from the source.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context or background of the task, such as “the poem is about autumn” or
    “the article is from a scientific journal.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s say, for example, that we want our model to fetch any kind of instructions
    from text and return to us a tutorial in a bullet list. If there are no instructions
    in the provided text, the model should inform us about that. Let’s see an example
    in ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Example of clear instructions in ChatGPT'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, if we pass the model other text that does not contain any instructions,
    it will be able to respond as we instructed it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Example of chat model following instructions'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous figure, we saw ChatGPT keeping in mind the instructions we
    prompted it with at the beginning of the conversation. This happens because ChatGPT
    has a so-called context window, which is equal to a single chat: everything we
    input in the chat session will be part of ChatGPT’s context and henceforth part
    of its knowledge; the moment we start a new session from scratch, ChatGPT will
    not remember any previous instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: By giving clear instructions, you can help the model understand what you want
    it to do and how you want it to do it. This can improve the quality and relevance
    of the model’s output, and reduce the need for further revisions or corrections.
  prefs: []
  type: TYPE_NORMAL
- en: However, sometimes there are scenarios where clarity is not enough. We might
    need to infer the way of thinking of our LLM to make it more robust with respect
    to its task. In the next subsection, we are going to examine a technique to do
    this – one that is very useful in cases of solving complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Split complex tasks into subtasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we interact with LLMs to let them solve some tasks, sometimes those tasks
    are too complex or ambiguous for a single prompt to handle, and it is better to
    split them into simpler subtasks that can be solved by different prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of splitting complex tasks into subtasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text summarization**: A complex task that involves generating a concise and
    accurate summary of a long text. This task can be split into subtasks such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting the main points or keywords from the text.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewriting the main points or keywords in a coherent way.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Trimming the summary to fit a desired length or format.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poem generation**: A creative task that involves producing a poem that follows
    a certain style, theme, or mood. This task can be split into subtasks such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a poetic form (such as sonnet, haiku, limerick, etc.) and a rhyme scheme
    (such as ABAB, AABB, ABCB, etc.) for the poem.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a title and a topic for the poem based on the user’s input or preference.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the lines or verses of the poem that match the chosen form, rhyme
    scheme, and topic.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Refining and polishing the poem to ensure coherence, fluency, and originality.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: A technical task that involves producing working code
    for a video game. This task can be split into subtasks such as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create basic movements and integrate their logic into the game engine’s loop.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add advanced movement features like printing or jumping logic with gravity.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure physics and collision handling are enabled.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable debugging and optimization by generating testing procedures.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate documentation for future reference.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider the following example. We will provide the model with a short
    article and ask it to summarize it following these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: You are an AI assistant that summarizes articles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To complete this task, do the following subtasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read the provided article context comprehensively and identify the main topic
    and key points.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a paragraph summary of the current article context that captures the
    essential information and conveys the main idea.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Print each step of the process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the short article we will provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how the model works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_11.png)Figure 3.9: Example of OpenAI GPT-4o splitting a task
    into subtasks to generate a summary'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting complex tasks into easier sub tasks is a powerful technique. Nevertheless,
    it does not address one of the main risks of LLM-generated content, that is, having
    an incorrect output. In the next two subsections, we are going to see some techniques
    that are mainly aimed at addressing this risk.
  prefs: []
  type: TYPE_NORMAL
- en: Ask for justification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In prompt engineering, requesting that a model provides justifications for its
    responses enhances transparency and reliability. This practice allows users to
    assess the reasoning behind the model’s answers, ensuring they are logical and
    grounded in relevant information ([https://arxiv.org/abs/2303.08769](https://arxiv.org/abs/2303.08769)).
    By understanding the model’s thought process, users can identify potential biases
    or inaccuracies, leading to more informed decisions and effective utilization
    of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, when an AI model suggests a medical diagnosis, asking for its
    reasoning can reveal whether the suggestion is based on pertinent symptoms and
    medical history or if it’s influenced by irrelevant data. Similarly, in legal
    contexts, if an AI system provides case recommendations, understanding its justification
    helps ensure the advice is based on appropriate legal precedents. This level of
    insight is crucial for building trust in AI applications and for refining prompts
    to elicit more accurate and contextually appropriate responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the following example. We want our LLM to solve riddles and
    we prompt it with the following set of instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Example of OpenAI’s GPT-4o providing justification after solving
    a riddle'
  prefs: []
  type: TYPE_NORMAL
- en: With a similar approach, we could also intervene at different prompt levels
    to improve our LLM’s performance. For example, we might discover that the model
    is systematically tackling a mathematical problem in the wrong way, hence we might
    want to suggest the right approach directly at the meta-prompt level.
  prefs: []
  type: TYPE_NORMAL
- en: Another example might be that of asking the model to generate multiple outputs
    – along with their justifications – to evaluate different reasoning techniques
    and prompt the best one in the meta-prompt. We’ll focus on this in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Generate many outputs, then use the model to pick the best one
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In prompt engineering, instructing a model to generate multiple responses to
    a single prompt is a technique known as self-consistency. This approach involves
    directing the model to produce several outputs for a given input, which are then
    evaluated to identify the most consistent or accurate response. By comparing these
    multiple outputs, users can discern common themes or solutions, enhancing the
    reliability of the LLM’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example, following up with the riddles examined in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: You are an AI assistant specialized in solving riddles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a riddle, you have to generate three answers to the riddle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each answer, be specific about the reasoning you made.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, among the three answers, select the one which is most plausible given
    the riddle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, I’ve prompted the model to generate three answers to the riddle,
    then to give me the most likely, justifying why. Let’s see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Example of GPT-4o generating three plausible answers and picking
    the most likely one, providing justification'
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, forcing the model to tackle a problem with different
    approaches is a way to collect multiple samples of reasonings, which might serve
    as further instructions in the meta-prompt. For example, if we want the model
    to always propose something that is not the most straightforward solution to a
    problem – in other words, if we want it to “think differently” – we might force
    it to solve a problem in N ways and then use the most creative reasoning as the
    framework in the meta-prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The last element we are going to examine is the overall structure we want to
    give to our meta-prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Use delimiters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last principle to be covered is related to the format we want to give to
    our meta prompt. This helps our LLM to better understand its intents as well as
    to make connections among sections and paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we can use delimiters within our prompt. A delimiter can be
    any sequence of characters or symbols that is clearly mapping a schema rather
    than a concept. For example, we can consider the following sequence delimiters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`>>>>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`====`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`------`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`####`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` ` ` ` ` ` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider, for example, a meta-prompt that aims at instructing the model
    to translate a user’s tasks into Python code, also providing an example of doing
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]def my_print(text):'
  prefs: []
  type: TYPE_NORMAL
- en: '#returning the printed text'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return print(text)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Sample output of a model using delimiters in the system message'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it also printed the code in backticks as shown within the system
    message.
  prefs: []
  type: TYPE_NORMAL
- en: All the principles examined up to this point are general rules that can make
    your interaction with ChatGPT and, more broadly, GenAI tools more meaningful to
    your goal. In the next section, we are going to see some advanced techniques for
    prompt engineering that address the way the model reasons and thinks about the
    answer, before providing it to the final user.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In prompt engineering, instructing a model to refine its own prompts – also
    known as meta-prompting ([https://arxiv.org/abs/2401.12954](https://arxiv.org/abs/2401.12954))
    – is an effective technique to enhance prompt quality and, consequently, the relevance
    of generated outputs. By engaging the model in the iterative process of prompt
    refinement, users can leverage the model’s language understanding to identify
    ambiguities or areas for improvement within the initial prompt. This self-improvement
    loop leads to more precise and contextually appropriate prompts, which in turn
    elicit more accurate and useful responses from the model.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let’s say we want to generate an elevator pitch for our new sustainable
    brand of running shoes. How would you ask the LLM to do that? Well, you might
    leverage some of the above techniques, like clear instructions or splitting the
    task into sub tasks; alternatively (or additionally), you could ask the LLM itself
    to refine your prompt to make it more relevant to your goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we can initially instruct the model to refine the prompt as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Example of a user asking ChatGPT to refine a prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s send our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Example of ChatGPT refining the user’s prompt'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, ChatGPT was able to refine our prompt and make it more tailored
    to our goal. Note that, in the above example, we only asked for one refinement;
    however, this can be an iterative process to not only enhance the clarity and
    precision of the prompt but also ensure that the model’s outputs are more aligned
    with the user’s specific requirements, making interactions more efficient and
    productive.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring some advanced techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous sections, we covered some basic techniques of prompt engineering
    that can improve your LLM’s response regardless of the type of task you are trying
    to accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, there are some advanced techniques that might be implemented
    for specific scenarios that we are going to cover in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Some advanced prompt engineering techniques like **chain-of-thought** (**CoT**)
    prompting are integrated into modern models such as OpenAI’s o1 series. These
    models are designed to internally process complex reasoning tasks by generating
    step-by-step logical sequences before arriving at a final answer, enhancing their
    problem-solving capabilities. This internal reasoning process allows o1 models
    to handle intricate queries more effectively without requiring explicit CoT prompts
    from users. However, employing CoT prompting can still be beneficial in guiding
    the model’s reasoning process for specific tasks and, more broadly, is a good
    practice whenever we interact with models of previous versions that do not exhibit
    advanced reasoning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Chain of thought
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced in the paper *Chain-of-Thought Prompting Elicits Reasoning in Large
    Language Models* by Wei et al., CoT is a technique that enables complex reasoning
    capabilities through intermediate reasoning steps. It also encourages the model
    to explain its reasoning, “forcing” it not to be too fast and risk giving the
    wrong response (as we saw in previous sections).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we want to prompt our LLM to solve first-degree equations. To
    do so, we are going to provide it with a generic reasoning list as a meta-prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_17.png)![](img/B31559_03_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: Output of the model solving an equation with the CoT approach'
  prefs: []
  type: TYPE_NORMAL
- en: This methodical approach mirrors human problem-solving by decomposing the task
    into manageable steps, enhancing clarity and reducing errors.
  prefs: []
  type: TYPE_NORMAL
- en: With CoT, we are prompting the model to generate intermediate reasoning steps.
    This is also a component of another reasoning technique that we are going to examine
    next.
  prefs: []
  type: TYPE_NORMAL
- en: ReAct
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introduced in the paper *ReAct: Synergizing Reasoning and Acting in Language
    Models* by Yao et al., **Reason and Act** (**ReAct**) is a general paradigm that
    combines reasoning and acting with LLMs. ReAct prompts the language model to generate
    verbal reasoning traces and actions for a task, and also receive observations
    from external sources such as web searches or databases. This allows the language
    model to perform dynamic reasoning and quickly adapt its action plan based on
    external information. For example, you can prompt the language model to answer
    a question by first reasoning about the question, then performing an action to
    send a query to the web, then receiving an observation from the search results,
    and then continuing with this thought, action, observation loop until it reaches
    a conclusion.'
  prefs: []
  type: TYPE_NORMAL
- en: The difference between CoT and ReAct approaches is that CoT prompts the language
    model to generate intermediate reasoning steps for a task, while ReAct prompts
    the language model to generate intermediate reasoning steps, actions, and observations
    for a task.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the “action” phase is generally related to the possibility of our
    LLM interacting with external tools, such as web search. However, in the following
    example, we won’t use tools but rather refer to the term “action” for any task
    we ask the model to do for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how the ReAct meta-prompt might look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how it works with a simple user query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Example of ReAct prompting'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, in this scenario, the model leveraged the web tool at the action
    input.
  prefs: []
  type: TYPE_NORMAL
- en: This is a great example of how prompting a model to think step by step and explicitly
    detail each step of the reasoning makes it “wiser” and more cautious before answering.
    It is also a great technique to prevent hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, prompt engineering is a powerful discipline, still in its emerging
    phase yet already widely adopted within LLM-powered applications. In the following
    chapters, we are going to see concrete applications of these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations to avoid bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we deal with AI systems like LLMs, we must be aware of their associated
    risk of **hidden bias**, which derives directly from the knowledge base the model
    has been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: Hidden bias, also known as implicit or unconscious bias, refers to the subtle
    and unintentional attitudes, stereotypes, or associations that influence a person’s
    perceptions and actions without their conscious awareness. These biases can shape
    behaviors and decisions in ways that reflect societal stereotypes, often leading
    to unintended discrimination. For example, someone might unknowingly associate
    leadership roles with men over women, which could impact hiring or promotion choices.
    In the context of LLM, hidden bias manifests in the model’s outputs when it reproduces
    or amplifies biases present in its training data, potentially leading to skewed
    or unfair responses. Addressing hidden bias is essential to fostering fairness
    and reducing systemic inequities.
  prefs: []
  type: TYPE_NORMAL
- en: For example, concerning the main chunk of training data of GPT-3, known as the
    **Common Crawl**, a 2012 study ([https://commoncrawl.org/blog/a-look-inside-common-crawls-210tb-2012-web-corpus](https://commoncrawl.org/blog/a-look-inside-common-crawls-210tb-2012-web-corpus))
    revealed that over 55% of the corpus originated from .*com* domains, with twelve
    top-level domains each representing more than 1% of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Given that *.com* domains are heavily utilized by Western entities, this concentration
    suggests a significant Western influence in the dataset. Additionally, the prevalence
    of English-language content within Common Crawl further indicates a Western-centric
    bias, as English is predominantly spoken in Western nations.
  prefs: []
  type: TYPE_NORMAL
- en: If this is the case, we are already facing a hidden bias of the model (more
    specifically, a racial and linguistic bias), which will inevitably mimic a limited
    and unrepresentative category of human beings.
  prefs: []
  type: TYPE_NORMAL
- en: In their paper *Language Models are Few-Shots Learners* ([https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)),
    OpenAI’s researchers Tom Brown et al. created an experimental setup to investigate
    racial bias in GPT-3\. The model was prompted with phrases containing racial categories
    and 800 samples were generated for each category. The sentiment of the generated
    text was measured using Senti WordNet based on word co-occurrences on a scale
    ranging from -100 to 100 (with positive scores indicating positive words and vice
    versa).
  prefs: []
  type: TYPE_NORMAL
- en: The results showed that the sentiment associated with each racial category varied
    across different models, with *Asian* consistently having a high sentiment (meaning
    a lot of positive words) and *Black* consistently having a low sentiment (meaning
    a lot of negative words). The authors caution that the results reflect the experimental
    setup and that socio-historical factors may influence the sentiment associated
    with different demographics.
  prefs: []
  type: TYPE_NORMAL
- en: This hidden bias could generate harmful responses not in line with responsible
    AI principles.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is worth noting how ChatGPT, as well as all OpenAI models, are subject
    to continuous improvements. This is also consistent with OpenAI’s AI alignment
    ([https://openai.com/index/our-approach-to-alignment-research/](https://openai.com/index/our-approach-to-alignment-research/)),
    whose research focuses on training AI systems to be helpful, truthful, and safe.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we ask GPT-4o to formulate guesses based on people’s gender
    and age, it will not accommodate the exact request, but rather provide us with
    a hypothetical function as well as a huge disclaimer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_03_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Example of GPT-4o improving over time since it gives an unbiased
    response'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, despite the continuous improvement in the domain of ethical principles,
    while using ChatGPT, we should always make sure that the output is in line with
    those principles. The concepts of bias and ethics within ChatGPT and OpenAI models
    have a wider connotation within the whole topic of responsible AI, which we are
    going to focus on in the last chapter of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have dived into the concept of prompt engineering since
    it’s a key component to control the output of ChatGPT and LLMs in general. We
    learned how to leverage different levels of shot learning to make LLMs more tailored
    to our objectives.
  prefs: []
  type: TYPE_NORMAL
- en: We started with an introduction to the concept of prompt engineering and why
    it is important, then moving toward the basic principles – including clear instructions,
    asking for justification, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we moved toward more advanced techniques, which are meant to shape the
    reasoning approach of our LLMs: few-shot learning, CoT, and ReAct.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is an emerging discipline that is paving the way for a new
    category of applications, infused with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the next chapter, we will explore different domains where ChatGPT
    can boost productivity and have a disruptive impact on the way we work today.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the references for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Language Models are Few-Shot Learners*: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?*: [https://dl.acm.org/doi/10.1145/3442188.3445922](https://dl.acm.org/doi/10.1145/3442188.3445922)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ReAct approach: [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chain-of-thought approach: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is prompt engineering?*: [https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering](https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt engineering principles: [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
