<html><head></head><body>
		<div id="_idContainer018" class="calibre2">
			<h1 id="_idParaDest-126" class="chapter-number"><a id="_idTextAnchor128" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">5</span></h1>
			<h1 id="_idParaDest-127" class="calibre7"><a id="_idTextAnchor129" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Getting Started with Information Extraction</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we will </span><a id="_idIndexMarker252" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.4.1">cover the basics of </span><strong class="bold"><span class="kobospan" id="kobo.5.1">information extraction</span></strong><span class="kobospan" id="kobo.6.1">. </span><span class="kobospan" id="kobo.6.2">Information extraction is the task of pulling very specific information from text. </span><span class="kobospan" id="kobo.6.3">For example, you might want to know the companies mentioned in a news article. </span><span class="kobospan" id="kobo.6.4">Instead of spending time reading the whole article, you can use information extraction techniques to access the companies </span><span><span class="kobospan" id="kobo.7.1">almost instantly.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.8.1">We will start with extracting emails addresses and URLs from job announcements. </span><span class="kobospan" id="kobo.8.2">Then, we will use an algorithm called </span><strong class="bold"><span class="kobospan" id="kobo.9.1">Levenshtein distance</span></strong><span class="kobospan" id="kobo.10.1"> to find similar strings. </span><span class="kobospan" id="kobo.10.2">Next, we will extract important</span><a id="_idIndexMarker253" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.11.1"> keywords from text. </span><span class="kobospan" id="kobo.11.2">After that, we will use </span><strong class="bold"><span class="kobospan" id="kobo.12.1">spaCy</span></strong><span class="kobospan" id="kobo.13.1"> to find named entities in text, and later, we will train our own named entity recognition model in spaCy. </span><span class="kobospan" id="kobo.13.2">We will then do basic sentiment analysis, and, finally, we will train two custom sentiment </span><span><span class="kobospan" id="kobo.14.1">analysis models.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.15.1">You will learn how to use existing tools and train your own models for information </span><span><span class="kobospan" id="kobo.16.1">extraction tasks.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.17.1">We will cover the following recipes in </span><span><span class="kobospan" id="kobo.18.1">this chapter:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.19.1">Using </span><span><span class="kobospan" id="kobo.20.1">regular expressions</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.21.1">Finding similar strings – </span><span><span class="kobospan" id="kobo.22.1">Levenshtein distance</span></span></li>
				<li class="calibre14"><span><span class="kobospan" id="kobo.23.1">Extracting keywords</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.24.1">Performing named entity recognition </span><span><span class="kobospan" id="kobo.25.1">using spaCy</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.26.1">Training your own NER model </span><span><span class="kobospan" id="kobo.27.1">with spaCy</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.28.1">Fine-tuning BERT </span><span><span class="kobospan" id="kobo.29.1">for NER</span></span></li>
			</ul>
			<h1 id="_idParaDest-128" class="calibre7"><a id="_idTextAnchor130" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.30.1">Technical requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.31.1">The code for this chapter is in a folder named </span><strong class="source-inline"><span class="kobospan" id="kobo.32.1">Chapter05</span></strong><span class="kobospan" id="kobo.33.1"> in the GitHub repository of the </span><span><span class="kobospan" id="kobo.34.1">book (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.35.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter05</span></span></a><span><span class="kobospan" id="kobo.36.1">).</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.37.1">As in previous chapters, the packages required for this chapter are part of the Poetry environment. </span><span class="kobospan" id="kobo.37.2">Alternatively, you can install all the packages using the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.38.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.39.1"> file.</span></span></p>
			<h1 id="_idParaDest-129" class="calibre7"><a id="_idTextAnchor131" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.40.1">Using regular expressions</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.41.1">In this recipe, we will use regular expressions to </span><a id="_idIndexMarker254" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.42.1">find email addresses and URLs in text. </span><span class="kobospan" id="kobo.42.2">Regular expressions are special character sequences that define search patterns and can be created and used via the Python </span><strong class="source-inline"><span class="kobospan" id="kobo.43.1">re</span></strong><span class="kobospan" id="kobo.44.1"> package. </span><span class="kobospan" id="kobo.44.2">We will use a job descriptions dataset and write two regular expressions, one for emails and one </span><span><span class="kobospan" id="kobo.45.1">for URLs.</span></span></p>
			<h2 id="_idParaDest-130" class="calibre5"><a id="_idTextAnchor132" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.46.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.47.1">Download the job descriptions dataset here: </span><a href="https://www.kaggle.com/andrewmvd/data-scientist-jobs" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.48.1">https://www.kaggle.com/andrewmvd/data-scientist-jobs</span></a><span class="kobospan" id="kobo.49.1">. </span><span class="kobospan" id="kobo.49.2">It is also available in the book’s GitHub repository at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.50.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/DataScientist.csv</span></a><span class="kobospan" id="kobo.51.1">. </span><span class="kobospan" id="kobo.51.2">Save it into the </span><strong class="source-inline"><span class="kobospan" id="kobo.52.1">/</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.53.1">data</span></strong></span><span><span class="kobospan" id="kobo.54.1"> folder.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.55.1">The notebook is located </span><span><span class="kobospan" id="kobo.56.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.57.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.1_regex.ipynb</span></span></a><span><span class="kobospan" id="kobo.58.1">.</span></span></p>
			<h2 id="_idParaDest-131" class="calibre5"><a id="_idTextAnchor133" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.59.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.60.1">We will read the data from the CSV file into a </span><strong class="source-inline"><span class="kobospan" id="kobo.61.1">pandas</span></strong><span class="kobospan" id="kobo.62.1"> DataFrame and will use the Python </span><strong class="source-inline"><span class="kobospan" id="kobo.63.1">re</span></strong><span class="kobospan" id="kobo.64.1"> package to create regular expressions and search the text. </span><span class="kobospan" id="kobo.64.2">The steps are </span><span><span class="kobospan" id="kobo.65.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.66.1">Import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.67.1">re</span></strong><span class="kobospan" id="kobo.68.1"> and </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.69.1">pandas</span></strong></span><span><span class="kobospan" id="kobo.70.1"> packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.71.1">
import re
import pandas as pd</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.72.1">Read in the data and check the contents </span><span><span class="kobospan" id="kobo.73.1">inside it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.74.1">
data_file = "../data/DataScientist.csv"
df = pd.read_csv(data_file, encoding='utf-8')
print(df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.75.1">The output will be long and should </span><a id="_idIndexMarker255" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.76.1">start </span><span><span class="kobospan" id="kobo.77.1">like this:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer013" class="img---figure">
					<span class="kobospan" id="kobo.78.1"><img src="image/B18411_05_1.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.79.1">Figure 5.1 – DataFrame output</span></p>
			<ol class="calibre13">
				<li value="3" class="calibre14"><span class="kobospan" id="kobo.80.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.81.1">get_list_of_items</span></strong><span class="kobospan" id="kobo.82.1"> helper function takes a DataFrame as input and turns one of its columns into a list. </span><span class="kobospan" id="kobo.82.2">It accepts the DataFrame and the column name as inputs. </span><span class="kobospan" id="kobo.82.3">First, it gets the column values, which is a list of lists, and then flattens that list. </span><span class="kobospan" id="kobo.82.4">It then removes duplicates by turning the list into a set and casts it back to </span><span><span class="kobospan" id="kobo.83.1">a list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.84.1">
def get_list_of_items(df, column_name):
    values = df[column_name].values
    values = [item for sublist in values for item in sublist]
    list_of_items = list(set(values))
    return list_of_items</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.85.1">In this step, we define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.86.1">get_emails</span></strong><span class="kobospan" id="kobo.87.1"> function to get all the emails that appear in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.88.1">Job Description</span></strong><span class="kobospan" id="kobo.89.1"> column. </span><span class="kobospan" id="kobo.89.2">The regular expression consists of three parts that appear in square brackets followed </span><span><span class="kobospan" id="kobo.90.1">by quantifiers:</span></span><ul class="calibre19"><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.91.1">[^\s:|()\']+</span></strong><span class="kobospan" id="kobo.92.1"> is the username part of the regular expression, followed by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.93.1">@</span></strong><span class="kobospan" id="kobo.94.1"> sign. </span><span class="kobospan" id="kobo.94.2">It consists of one group of characters, which is shown in square brackets. </span><span class="kobospan" id="kobo.94.3">Any characters from this group may appear in the username one or more times. </span><span class="kobospan" id="kobo.94.4">This is shown using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.95.1">+</span></strong><span class="kobospan" id="kobo.96.1"> quantifier. </span><span class="kobospan" id="kobo.96.2">The characters in the </span><a id="_idIndexMarker256" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.97.1">username can be anything but a space (</span><strong class="source-inline1"><span class="kobospan" id="kobo.98.1">\s</span></strong><span class="kobospan" id="kobo.99.1">), colon (</span><strong class="source-inline1"><span class="kobospan" id="kobo.100.1">:</span></strong><span class="kobospan" id="kobo.101.1">), pipe (</span><strong class="source-inline1"><span class="kobospan" id="kobo.102.1">|</span></strong><span class="kobospan" id="kobo.103.1">), and apostrophe (</span><strong class="source-inline1"><span class="kobospan" id="kobo.104.1">'</span></strong><span class="kobospan" id="kobo.105.1">). </span><span class="kobospan" id="kobo.105.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.106.1">^</span></strong><span class="kobospan" id="kobo.107.1"> character shows the negation of the character class. </span><span class="kobospan" id="kobo.107.2">An apostrophe is a special character in regular expressions and has to be escaped with a backward slash in order to invoke the regular meaning of </span><span><span class="kobospan" id="kobo.108.1">the character.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.109.1">[a-zA-Z0-9\.]+</span></strong><span class="kobospan" id="kobo.110.1"> is the first part of the domain name, followed by a dot. </span><span class="kobospan" id="kobo.110.2">This part is simply alphanumeric characters, lowercase or uppercase, and a dot appearing one or more times. </span><span class="kobospan" id="kobo.110.3">Since a dot is a special character, we escape it with a backward slash. </span><span class="kobospan" id="kobo.110.4">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.111.1">a-z</span></strong><span class="kobospan" id="kobo.112.1"> expression signifies a range of characters from </span><em class="italic"><span class="kobospan" id="kobo.113.1">a</span></em> <span><span class="kobospan" id="kobo.114.1">to </span></span><span><em class="italic"><span class="kobospan" id="kobo.115.1">z</span></em></span><span><span class="kobospan" id="kobo.116.1">.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.117.1">[a-zA-Z]+</span></strong><span class="kobospan" id="kobo.118.1"> is the last part of the domain name, which is the top-level domain, such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.119.1">.com</span></strong><span class="kobospan" id="kobo.120.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.121.1">.org</span></strong><span class="kobospan" id="kobo.122.1">, and so on. </span><span class="kobospan" id="kobo.122.2">Usually, no digits are allowed in these top-level domains, and the regular expression matches lowercase or uppercase characters that appear one or </span><span><span class="kobospan" id="kobo.123.1">more times.</span></span></li></ul><p class="calibre3"><span class="kobospan" id="kobo.124.1">This regular expression is sufficient to parse all emails in the dataset and not present any false positives. </span><span class="kobospan" id="kobo.124.2">You might find that, in your data, there are additional adjustments that need to be made to the </span><span><span class="kobospan" id="kobo.125.1">regular expression:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.126.1">
def get_emails(df):
    email_regex = '[^\s:|()\']+@[a-zA-Z0-9\.]+\.[a-zA-Z]+'
    df['emails'] = df['Job Description'].apply(
        lambda x: re.findall(email_regex, x))
    emails = get_list_of_items(df, 'emails')
    return emails</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.127.1">We will now get the emails from the DataFrame using the </span><span><span class="kobospan" id="kobo.128.1">previous functions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.129.1">
emails = get_emails(df)
print(emails)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.130.1">Part of the result will look</span><a id="_idIndexMarker257" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.131.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.132.1">['hrhelpdesk@phila.gov', 'talent@quartethealth.com', …, 'careers@edo.com', 'Talent.manager@techquarry.com', 'resumes@nextgentechinc.com', …, 'talent@ebay.com', …, 'info@springml.com',…]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.133.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.134.1">get_urls</span></strong><span class="kobospan" id="kobo.135.1"> helper function takes a DataFrame as input and turns one of its columns into </span><span><span class="kobospan" id="kobo.136.1">a list.</span></span><p class="calibre3"><span class="kobospan" id="kobo.137.1">URLs are significantly more complicated than emails. </span><span class="kobospan" id="kobo.137.2">Here is the breakdown of the </span><span><span class="kobospan" id="kobo.138.1">regular expression:</span></span></p><ul class="calibre19"><li class="calibre14"><span class="kobospan" id="kobo.139.1">http[s]?://: This is the </span><strong class="source-inline1"><span class="kobospan" id="kobo.140.1">http</span></strong><span class="kobospan" id="kobo.141.1"> part of the URL. </span><span class="kobospan" id="kobo.141.2">All URLs in this dataset had this part, but that might not be the case in your data and you will have to adjust the regular expression accordingly. </span><span class="kobospan" id="kobo.141.3">This part of the expression will match both </span><strong class="source-inline1"><span class="kobospan" id="kobo.142.1">http</span></strong><span class="kobospan" id="kobo.143.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.144.1">https</span></strong><span class="kobospan" id="kobo.145.1">, since </span><strong class="source-inline1"><span class="kobospan" id="kobo.146.1">s</span></strong><span class="kobospan" id="kobo.147.1"> is listed as appearing zero or one time, which is signified by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.148.1">?</span></strong> <span><span class="kobospan" id="kobo.149.1">quantifier.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.150.1">(www\.)?</span></strong><span class="kobospan" id="kobo.151.1">: Next, we have a </span><em class="italic"><span class="kobospan" id="kobo.152.1">group</span></em><span class="kobospan" id="kobo.153.1"> of characters, which are treated as a unit, but all have to appear in the order in which they are listed. </span><span class="kobospan" id="kobo.153.2">In this case, this is the </span><strong class="source-inline1"><span class="kobospan" id="kobo.154.1">www</span></strong><span class="kobospan" id="kobo.155.1"> part of the URL, followed by a dot, escaped with a backward slash. </span><span class="kobospan" id="kobo.155.2">The group of characters may appear zero or one time, signified by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.156.1">?</span></strong><span class="kobospan" id="kobo.157.1"> character at </span><span><span class="kobospan" id="kobo.158.1">the end.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.159.1">[A-Za-z0-9–_\.\-]+</span></strong><span class="kobospan" id="kobo.160.1">: This part is the domain name of the website, followed by the top-level domain. </span><span class="kobospan" id="kobo.160.2">Website names also include dashes, and the dot character appears before the top-level domain </span><span><span class="kobospan" id="kobo.161.1">and subdomains.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.162.1">/? </span><span class="kobospan" id="kobo.162.2">[A-Za-z0-9$\–_\-\/\.\?]*)</span></strong><span class="kobospan" id="kobo.163.1">: This part is whatever follows the domain name after the slash. </span><span class="kobospan" id="kobo.163.2">It could be a variety of characters that list files, parameters, and so on. </span><span class="kobospan" id="kobo.163.3">They could or could not be present, and that is why they are followed by the </span><strong class="source-inline1"><span class="kobospan" id="kobo.164.1">*</span></strong><span class="kobospan" id="kobo.165.1"> quantifier. </span><span class="kobospan" id="kobo.165.2">The bracket at the end signifies the end of the </span><span><span class="kobospan" id="kobo.166.1">matching group.</span></span></li><li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.167.1">[\.)\"]*</span></strong><span class="kobospan" id="kobo.168.1">: Many URLs in </span><a id="_idIndexMarker258" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.169.1">this dataset are followed by dots, brackets, and other characters, and this is the last part of the </span><span><span class="kobospan" id="kobo.170.1">regular expression.</span></span></li></ul><p class="calibre3"><span class="kobospan" id="kobo.171.1">In this function, we use the </span><strong class="source-inline"><span class="kobospan" id="kobo.172.1">finditer</span></strong><span class="kobospan" id="kobo.173.1"> function from the </span><strong class="source-inline"><span class="kobospan" id="kobo.174.1">re</span></strong><span class="kobospan" id="kobo.175.1"> package. </span><span class="kobospan" id="kobo.175.2">It finds all matches in a text and returns them as </span><strong class="source-inline"><span class="kobospan" id="kobo.176.1">Match</span></strong><span class="kobospan" id="kobo.177.1"> objects. </span><span class="kobospan" id="kobo.177.2">We can find the start and end of the match by using the </span><strong class="source-inline"><span class="kobospan" id="kobo.178.1">span()</span></strong><span class="kobospan" id="kobo.179.1">object method. </span><span class="kobospan" id="kobo.179.2">It returns a tuple, where the first element is the start and the second element is the end of </span><span><span class="kobospan" id="kobo.180.1">the m</span><a id="_idTextAnchor134" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.181.1">atch:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.182.1">
def get_urls(df):
    url_regex = '(http[s]?://(www\.)?[A-Za-z0-9–_\.\-]+\.[A-Za-z]+/?[A-Za-z0-9$\–_\-\/\.]*)[\.)\"]*'
    df['urls'] = df['Job Description'].apply(
        lambda x: [
            x[item.span()[0]:item.span()[1]] 
            for item in re.finditer(url_regex, x)
        ]
    )
    urls = get_list_of_items(df, 'urls')
    return urls</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.183.1">We will get the URLs in a </span><span><span class="kobospan" id="kobo.184.1">similar fashion:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.185.1">
urls = get_urls(df)
print(urls)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.186.1">Part of the result will look</span><a id="_idIndexMarker259" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.187.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.188.1">['</span><a href="https://youtu.be/c5TgbpE9UBI'" class="pcalibre pcalibre1 calibre20"><span class="kobospan1" id="kobo.189.1">https://youtu.be/c5TgbpE9UBI'</span></a><span class="kobospan1" id="kobo.190.1">, 'https://www.linkedin.com/in/emma-riley-72028917a/', 'https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm', 'https://www.naspovaluepoint.org/portfolio/mmis-provider-services-module-2018-2028/hhs-technology-group/).', 'https://www.instagram.com/gatestonebpo', 'http://jobs.sdsu.edu', 'http://www.colgatepalmolive.com.', 'http://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf', 'https://www.gofundme.com/2019https', 'https://www.decode-m.com/', 'https://bit.ly/2lCOcYS',…]</span></pre></li>			</ol>
			<h2 id="_idParaDest-132" class="calibre5"><a id="_idTextAnchor135" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.191.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.192.1">Writing regular expressions can quickly turn into a messy affair. </span><span class="kobospan" id="kobo.192.2">I use regular expression testing websites to enter the text in which I expect a match and the regular expression. </span><span class="kobospan" id="kobo.192.3">One example of such a site </span><span><span class="kobospan" id="kobo.193.1">is </span></span><a href="https://regex101.com/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.194.1">https://regex101.com/</span></span></a><span><span class="kobospan" id="kobo.195.1">.</span></span></p>
			<h1 id="_idParaDest-133" class="calibre7"><a id="_idTextAnchor136" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.196.1">Finding similar strings – Levenshtein distance</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.197.1">When doing information extraction, in many cases, we deal with misspellings, which can bring complications to the task. </span><span class="kobospan" id="kobo.197.2">To get around this problem, several methods are available, including Levenshtein distance. </span><span class="kobospan" id="kobo.197.3">This</span><a id="_idIndexMarker260" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.198.1"> algorithm finds the number of edits/additions/deletions needed to change one string into another. </span><span class="kobospan" id="kobo.198.2">For example, to change the word </span><em class="italic"><span class="kobospan" id="kobo.199.1">put</span></em><span class="kobospan" id="kobo.200.1"> into </span><em class="italic"><span class="kobospan" id="kobo.201.1">pat</span></em><span class="kobospan" id="kobo.202.1">, you need to substitute </span><em class="italic"><span class="kobospan" id="kobo.203.1">u</span></em><span class="kobospan" id="kobo.204.1"> for </span><em class="italic"><span class="kobospan" id="kobo.205.1">a</span></em><span class="kobospan" id="kobo.206.1">, and that is one change. </span><span class="kobospan" id="kobo.206.2">To change the word </span><em class="italic"><span class="kobospan" id="kobo.207.1">kitten</span></em><span class="kobospan" id="kobo.208.1"> into </span><em class="italic"><span class="kobospan" id="kobo.209.1">smitten</span></em><span class="kobospan" id="kobo.210.1">, you need to do two edits: change </span><em class="italic"><span class="kobospan" id="kobo.211.1">k</span></em><span class="kobospan" id="kobo.212.1"> into </span><em class="italic"><span class="kobospan" id="kobo.213.1">m</span></em><span class="kobospan" id="kobo.214.1"> and add an </span><em class="italic"><span class="kobospan" id="kobo.215.1">s</span></em><span class="kobospan" id="kobo.216.1"> at </span><span><span class="kobospan" id="kobo.217.1">the start.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.218.1">In this recipe, you will be able to use this technique to find a match to a </span><span><span class="kobospan" id="kobo.219.1">misspelled email.</span></span></p>
			<h2 id="_idParaDest-134" class="calibre5"><a id="_idTextAnchor137" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.220.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.221.1">We will use the same packages and the data scientist job description dataset that we used in the previous recipe, and the </span><strong class="source-inline"><span class="kobospan" id="kobo.222.1">python-Levenshtein</span></strong><span class="kobospan" id="kobo.223.1"> package, which is part of the Poetry environment and is included in the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.224.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.225.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.226.1">The notebook is located </span><span><span class="kobospan" id="kobo.227.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.228.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.2_similar_strings.ipynb</span></span></a><span><span class="kobospan" id="kobo.229.1">.</span></span></p>
			<h2 id="_idParaDest-135" class="calibre5"><a id="_idTextAnchor138" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.230.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.231.1">We will read the dataset into a </span><strong class="source-inline"><span class="kobospan" id="kobo.232.1">pandas</span></strong><span class="kobospan" id="kobo.233.1"> DataFrame and use the emails extracted from it to search for a misspelled</span><a id="_idIndexMarker261" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.234.1"> email. </span><span class="kobospan" id="kobo.234.2">Your steps should be formatted </span><span><span class="kobospan" id="kobo.235.1">like so:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.236.1">Run the language utilities file. </span><span class="kobospan" id="kobo.236.2">This file contains the </span><strong class="source-inline1"><span class="kobospan" id="kobo.237.1">get_emails</span></strong><span class="kobospan" id="kobo.238.1"> function we created in the </span><span><span class="kobospan" id="kobo.239.1">previous recipe:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.240.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.241.1">Do the </span><span><span class="kobospan" id="kobo.242.1">necessary imports:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.243.1">
import pandas as pd
import Levenshtein</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.244.1">Read the data into a </span><strong class="source-inline1"><span class="kobospan" id="kobo.245.1">pandas</span></strong> <span><span class="kobospan" id="kobo.246.1">DataFrame object:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.247.1">
data_file = "../data/DataScientist.csv"
df = pd.read_csv(data_file, encoding='utf-8')</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.248.1">Filter out all emails from the DataFrame using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.249.1">get_emails</span></strong><span class="kobospan" id="kobo.250.1"> function, which is explained in more detail in the previous recipe, </span><em class="italic"><span class="kobospan" id="kobo.251.1">Using </span></em><span><em class="italic"><span class="kobospan" id="kobo.252.1">regular expressions</span></em></span><span><span class="kobospan" id="kobo.253.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.254.1">
emails = get_emails(df)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.255.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.256.1">find_levenshtein</span></strong><span class="kobospan" id="kobo.257.1"> function takes in a DataFrame and an input string and computes the Levenshtein distance between it and each string in the emails column. </span><span class="kobospan" id="kobo.257.2">It takes in an input string and a DataFrame with emails and creates a new column in which the value is the Levenshtein distance between the input and the email address in the DataFrame. </span><span class="kobospan" id="kobo.257.3">The column name </span><span><span class="kobospan" id="kobo.258.1">is </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.259.1">distance_to_[input_string]</span></strong></span><span><span class="kobospan" id="kobo.260.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.261.1">
def find_levenshtein(input_string, df):
    df['distance_to_' + input_string] = \
        df['emails'].apply(lambda x: Levenshtein.distance(
            input_string, x))
    return df</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.262.1">In this step, we define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.263.1">get_closest_email_lev</span></strong><span class="kobospan" id="kobo.264.1"> function, which takes in a DataFrame with emails and an email to match and returns the email in the DataFrame that is closest to </span><a id="_idIndexMarker262" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.265.1">the input. </span><span class="kobospan" id="kobo.265.2">We accomplish this by using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.266.1">find_levenshtein</span></strong><span class="kobospan" id="kobo.267.1"> function to create a new column with distances to the input email and then using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.268.1">idxmin()</span></strong><span class="kobospan" id="kobo.269.1"> function from </span><strong class="source-inline1"><span class="kobospan" id="kobo.270.1">pandas</span></strong><span class="kobospan" id="kobo.271.1"> to find the index of the minimum value. </span><span class="kobospan" id="kobo.271.2">We use the minimum index to find the </span><span><span class="kobospan" id="kobo.272.1">closest email:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.273.1">
def get_closest_email_lev(df, email):
    df = find_levenshtein(email, df)
    column_name = 'distance_to_' + email
    minimum_value_email_index = df[column_name].idxmin()
    email = df.loc[minimum_value_email_index]['emails']
    return email</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.274.1">Next, we load the emails into a new DataFrame and use the misspelled email address </span><strong class="source-inline1"><span class="kobospan" id="kobo.275.1">rohitt.macdonald@prelim.com</span></strong><span class="kobospan" id="kobo.276.1"> to find a match in the new </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.277.1">email</span></strong></span><span><span class="kobospan" id="kobo.278.1"> DataFrame:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.279.1">
new_df = pd.DataFrame(emails,columns=['emails'])
input_string = "rohitt.macdonald@prelim.com"
email = get_closest_email_lev(new_df, input_string)
print(email)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.280.1">The function returns </span><strong class="source-inline"><span class="kobospan" id="kobo.281.1">rohit.mcdonald@prolim.com</span></strong><span class="kobospan" id="kobo.282.1">, the correct spelling of the </span><span><span class="kobospan" id="kobo.283.1">email address:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.284.1">rohit.mcdonald@prolim.com</span></pre></li>			</ol>
			<h2 id="_idParaDest-136" class="calibre5"><a id="_idTextAnchor139" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.285.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.286.1">The Levenshtein package includes other string similarity measuring methods, which you can explore at </span><a href="https://rapidfuzz.github.io/Levenshtein/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.287.1">https://rapidfuzz.github.io/Levenshtein/</span></a><span class="kobospan" id="kobo.288.1">. </span><span class="kobospan" id="kobo.288.2">In this </span><a id="_idIndexMarker263" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.289.1">section, we look at the </span><span><strong class="bold"><span class="kobospan" id="kobo.290.1">Jaro distance</span></strong></span><span><span class="kobospan" id="kobo.291.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.292.1">We can use another function, the Jaro</span><a id="_idIndexMarker264" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.293.1"> similarity, which outputs similarity between two strings as a number between </span><strong class="source-inline"><span class="kobospan" id="kobo.294.1">0</span></strong><span class="kobospan" id="kobo.295.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.296.1">1</span></strong><span class="kobospan" id="kobo.297.1">, where </span><strong class="source-inline"><span class="kobospan" id="kobo.298.1">1</span></strong><span class="kobospan" id="kobo.299.1"> means that two strings are the same. </span><span class="kobospan" id="kobo.299.2">The process is similar, but we need the index with the maximum value instead of the minimum since the Jaro similarity function returns a higher value for more similar strings. </span><span class="kobospan" id="kobo.299.3">Let’s go through </span><span><span class="kobospan" id="kobo.300.1">the steps:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.301.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.302.1">find_jaro</span></strong><span class="kobospan" id="kobo.303.1"> function takes in a DataFrame and an input string and computes the Jaro similarity between it and each string in the </span><span><span class="kobospan" id="kobo.304.1">email column:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.305.1">
def find_jaro(input_string, df):
    df['distance_to_' + input_string] = df['emails'].apply(
        lambda x: Levenshtein.jaro(input_string, x)
    )
    return df</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.306.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.307.1">get_closest_email_jaro</span></strong><span class="kobospan" id="kobo.308.1"> function uses the function we defined in the previous step to find the email address that is closest to the </span><span><span class="kobospan" id="kobo.309.1">one input:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.310.1">
def get_closest_email_jaro(df, email):
    df = find_jaro(email, df)
    column_name = 'distance_to_' + email
    maximum_value_email_index = df[column_name].idxmax()
    email = df.loc[maximum_value_email_index]['emails']
    return email</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.311.1">Next, we use the misspelled email address </span><strong class="source-inline1"><span class="kobospan" id="kobo.312.1">rohitt.macdonald@prelim.com</span></strong><span class="kobospan" id="kobo.313.1"> to find a match in the new </span><span><span class="kobospan" id="kobo.314.1">email DataFrame:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.315.1">
email = get_closest_email_jaro(new_df, input_string)
print(email)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.316.1">The output is </span><span><span class="kobospan" id="kobo.317.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.318.1">rohit.mcdonald@prolim.com</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.319.1">An extension of the</span><a id="_idIndexMarker265" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.320.1"> Jaro similarity function is the </span><strong class="bold"><span class="kobospan" id="kobo.321.1">Jaro-Winkler function</span></strong><span class="kobospan" id="kobo.322.1">, which attaches a weight to the</span><a id="_idIndexMarker266" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.323.1"> end of the word, and that weight lowers the importance of misspellings toward the end. </span><span class="kobospan" id="kobo.323.2">For example, let’s look at the </span><span><span class="kobospan" id="kobo.324.1">following function:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.325.1">
print(Levenshtein.jaro_winkler("rohit.mcdonald@prolim.com",
    "rohit.mcdonald@prolim.org"))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.326.1">This outputs </span><span><span class="kobospan" id="kobo.327.1">the following:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.328.1">1.0</span></pre></li>			</ol>
			<h1 id="_idParaDest-137" class="calibre7"><a id="_idTextAnchor140" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.329.1">Extracting keywords</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.330.1">In this recipe, we will extract </span><a id="_idIndexMarker267" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.331.1">keywords from a text. </span><span class="kobospan" id="kobo.331.2">We will be working with the BBC news dataset that contains news articles. </span><span class="kobospan" id="kobo.331.3">You can learn more about the dataset in </span><a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.332.1">Chapter 4</span></em></span></a><span class="kobospan" id="kobo.333.1">, in the recipe titled </span><em class="italic"><span class="kobospan" id="kobo.334.1">Clustering sentences using K-Means: unsupervised </span></em><span><em class="italic"><span class="kobospan" id="kobo.335.1">text classification</span></em></span><span><span class="kobospan" id="kobo.336.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.337.1">Extracting keywords from text can give us a quick idea about what the article is about and can also serve as a basis for a tagging system, for example, on </span><span><span class="kobospan" id="kobo.338.1">a website.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.339.1">For the extraction to work correctly, we need to train a TF-IDF vectorizer that we will use during the </span><span><span class="kobospan" id="kobo.340.1">extraction phase.</span></span></p>
			<h2 id="_idParaDest-138" class="calibre5"><a id="_idTextAnchor141" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.341.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.342.1">In this recipe, we will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.343.1">sklearn</span></strong><span class="kobospan" id="kobo.344.1"> package. </span><span class="kobospan" id="kobo.344.2">It is part of the Poetry environment. </span><span class="kobospan" id="kobo.344.3">You can also install it together with other packages by installing the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.345.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.346.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.347.1">The BBC news dataset is available on Hugging Face at  </span><a href="https://huggingface.co/datasets/SetFit/bbc-news" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.348.1">https://huggingface.co/datasets/SetFit/bbc-news</span></span></a><span><span class="kobospan" id="kobo.349.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.350.1">The notebook is located </span><span><span class="kobospan" id="kobo.351.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.352.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.3_keyword_extraction.ipynb</span></span></a><span><span class="kobospan" id="kobo.353.1">.</span></span></p>
			<h2 id="_idParaDest-139" class="calibre5"><a id="_idTextAnchor142" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.354.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.355.1">To extract keywords from a given text, we first need a corpus of text that we will fit the vectorizer on. </span><span class="kobospan" id="kobo.355.2">Once that is done, we</span><a id="_idIndexMarker268" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.356.1"> can use it to extract keywords from a text that is similar to the processed corpus. </span><span class="kobospan" id="kobo.356.2">Here are </span><span><span class="kobospan" id="kobo.357.1">the steps:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.358.1">Run the language </span><span><span class="kobospan" id="kobo.359.1">utilities notebook:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.360.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.361.1">Import the necessary packages </span><span><span class="kobospan" id="kobo.362.1">and functions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.363.1">
from datasets import load_dataset
from nltk import word_tokenize
from math import ceil
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.364.1">Load the training and testing datasets, convert them to </span><strong class="source-inline1"><span class="kobospan" id="kobo.365.1">pandas</span></strong><span class="kobospan" id="kobo.366.1"> DataFrame objects, and print out the training DataFrame to discover how it looks. </span><span class="kobospan" id="kobo.366.2">The DataFrame has three columns, one for the news article text, one for the label in numeric format, and one for the </span><span><span class="kobospan" id="kobo.367.1">label text:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.368.1">
train_dataset = load_dataset("SetFit/bbc-news", split="train")
test_dataset = load_dataset("SetFit/bbc-news", split="test")
train_df = train_dataset.to_pandas()
test_df = test_dataset.to_pandas()
print(train_df)
print(test_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.369.1">The result should look</span><a id="_idIndexMarker269" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.370.1"> similar </span><span><span class="kobospan" id="kobo.371.1">to this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.372.1">     text  label     label_text
0  wales want rugby league training wales could f... </span><span class="kobospan1" id="kobo.372.2">2  sport
1     china aviation seeks rescue deal scandal-hit j...  business
...     ...    ...            ...
</span><span class="kobospan1" id="kobo.372.3">1223  why few targets are better than many the econo... </span><span class="kobospan1" id="kobo.372.4">1  business
1224  boothroyd calls for lords speaker betty boothr... </span><span class="kobospan1" id="kobo.372.5">4  politics
[1225 rows x 3 columns]
     text  label     label_text
0  carry on star patsy rowlands dies actress pats... </span><span class="kobospan1" id="kobo.372.6">3  entertainment
1    sydney to host north v south game sydney will ... </span><span class="kobospan1" id="kobo.372.7">2  sport
..     ...    ...            ...
</span><span class="kobospan1" id="kobo.372.8">998  stormy year for property insurers a string of ... </span><span class="kobospan1" id="kobo.372.9">1  business
999  what the election should really be about  a ge... </span><span class="kobospan1" id="kobo.372.10">4  politics
[1000 rows x 3 columns]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.373.1">Create the vectorizer and fit it on the training data text. </span><span class="kobospan" id="kobo.373.2">To learn more about vectorizers, see </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.374.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.375.1">. </span><span class="kobospan" id="kobo.375.2">The TF-IDF </span><a id="_idIndexMarker270" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.376.1">vectorizer is discussed in the </span><em class="italic"><span class="kobospan" id="kobo.377.1">Representing texts with TF-IDF</span></em><span class="kobospan" id="kobo.378.1"> recipe. </span><span class="kobospan" id="kobo.378.2">We use English stopwords, a minimum document frequency of </span><strong class="source-inline1"><span class="kobospan" id="kobo.379.1">2</span></strong><span class="kobospan" id="kobo.380.1">, and a maximum document frequency of 95% (to learn more about stopwords, see the </span><em class="italic"><span class="kobospan" id="kobo.381.1">Removing stopwords</span></em><span class="kobospan" id="kobo.382.1"> recipe in </span><a href="B18411_01.xhtml#_idTextAnchor013" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.383.1">Chapter 1</span></em></span></a><span><span class="kobospan" id="kobo.384.1">):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.385.1">
vectorizer = TfidfVectorizer(stop_words='english', 
    min_df=2, max_df=0.95)
vectorizer.fit(train_df["text"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.386.1">Now, we will define a few helper functions. </span><span class="kobospan" id="kobo.386.2">The first one will sort a coordinate matrix by the TF-IDF score. </span><span class="kobospan" id="kobo.386.3">It takes the coordinate matrix that is converted from the vector created by the vectorizer. </span><span class="kobospan" id="kobo.386.4">This coordinate matrix’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.387.1">col</span></strong><span class="kobospan" id="kobo.388.1"> attribute provides the word indices and the </span><strong class="source-inline1"><span class="kobospan" id="kobo.389.1">data</span></strong><span class="kobospan" id="kobo.390.1"> attribute provides the TF-IDF scores for each word. </span><span class="kobospan" id="kobo.390.2">The function creates a list of tuples from this data, where the first value in the tuple is the index and the second value is the TF-IDF score. </span><span class="kobospan" id="kobo.390.3">It then sorts the tuple list by the TF-IDF score and returns the sorted result.  </span><span class="kobospan" id="kobo.390.4">This will give us words that have the maximum TF-IDF score or the ones that are most characteristic of this particular </span><span><span class="kobospan" id="kobo.391.1">news piece:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.392.1">
def sort_data_tfidf_score(coord_matrix):
    tuples = zip(coord_matrix.col, coord_matrix.data)
    return sorted(tuples, key=lambda x: (x[1], x[0]), 
        reverse=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.393.1">The next function, </span><strong class="source-inline1"><span class="kobospan" id="kobo.394.1">get_keyword_strings</span></strong><span class="kobospan" id="kobo.395.1">, will get the keywords for a given vector. </span><span class="kobospan" id="kobo.395.2">It returns the extracted keywords for a given vector. </span><span class="kobospan" id="kobo.395.3">It takes as input the fitted vectorizer, the number of keywords to extract, and the sorted vector of the input text. </span><span class="kobospan" id="kobo.395.4">The function first defines the </span><strong class="source-inline1"><span class="kobospan" id="kobo.396.1">index_dict</span></strong><span class="kobospan" id="kobo.397.1"> variable as the dictionary with word indices as keys and corresponding words as values. </span><span class="kobospan" id="kobo.397.2">It then iterates through the sorted vector and appends the words from the dictionary to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.398.1">words</span></strong><span class="kobospan" id="kobo.399.1"> list variable. </span><span class="kobospan" id="kobo.399.2">It </span><a id="_idIndexMarker271" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.400.1">stops when it reaches the desired number of words. </span><span class="kobospan" id="kobo.400.2">Since the function iterates through the sorted vector, it will give us the words with the highest TF-IDF scores. </span><span class="kobospan" id="kobo.400.3">These words will be the ones most used in this document but not used in other documents, thus giving us an idea about the topic of </span><span><span class="kobospan" id="kobo.401.1">the article:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.402.1">
def get_keyword_strings(vectorizer, num_words, sorted_vector):
    words = []
    index_dict = vectorizer.get_feature_names_out()
    for (item_index, score) in sorted_vector[0:num_words]:
        word = index_dict[item_index]
        words.append(word)
    return words</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.403.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.404.1">get_keywords_simple</span></strong><span class="kobospan" id="kobo.405.1"> function will return a list of keywords for a given text. </span><span class="kobospan" id="kobo.405.2">It takes in the input text, the fitted vectorizer, and the desired number of words. </span><span class="kobospan" id="kobo.405.3">It creates a vector for the input text by using the vectorizer, then sorts the vector by using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.406.1">sort_data_tfidf_score</span></strong><span class="kobospan" id="kobo.407.1"> function, and finally, gets the top words using the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.408.1">get_keyword_strings</span></strong></span><span><span class="kobospan" id="kobo.409.1"> function:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.410.1">
def get_keywords_simple(vectorizer, input_text,
    num_output_words=10):
    vector = vectorizer.transform([input_text])
    sorted = sort_data_tfidf_score(vector.tocoo())
    words = get_keyword_strings(vectorizer, num_output_words, 
        sorted)
    return words</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.411.1">We use the previous function on the first text from the test DataFrame. </span><span class="kobospan" id="kobo.411.2">We take the first article text in the test data and</span><a id="_idIndexMarker272" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.412.1"> create a list of keywords using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.413.1">get_keywords_simple</span></strong><span class="kobospan" id="kobo.414.1"> function. </span><span class="kobospan" id="kobo.414.2">We see that some of the keywords fit the summary, and some are </span><span><span class="kobospan" id="kobo.415.1">less suitable:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.416.1">
print(test_df.iloc[0]["text"])
keywords = get_keywords_simple(vectorizer,
    test_df.iloc[0]["text"])
print(keywords)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.417.1">The result will be </span><span><span class="kobospan" id="kobo.418.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.419.1">carry on star patsy rowlands dies actress patsy rowlands  known to millions for her roles in the carry on films  has died at the age of 71.  rowlands starred in nine of the popular carry on films  alongside fellow regulars sid james  kenneth williams and barbara windsor...
</span><span class="kobospan1" id="kobo.419.2">['carry', 'theatre', 'scholarship', 'appeared', 'films', 'mrs', 'agent', 'drama', 'died', 'school']</span></pre></li>			</ol>
			<h2 id="_idParaDest-140" class="calibre5"><a id="_idTextAnchor143" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.420.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.421.1">Now, we will use a more sophisticated approach to extracting keywords from news summaries. </span><span class="kobospan" id="kobo.421.2">We will use a vectorizer that scores not just individual words but also bigrams and trigrams. </span><span class="kobospan" id="kobo.421.3">We will also use the spaCy noun chunks to make sure that the bigrams and trigrams that are output make sense. </span><span class="kobospan" id="kobo.421.4">To learn more about noun chunks, see the </span><em class="italic"><span class="kobospan" id="kobo.422.1">Extracting noun chunks</span></em><span class="kobospan" id="kobo.423.1"> recipe in </span><a href="B18411_02.xhtml#_idTextAnchor042" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.424.1">Chapter 2</span></em></span></a><span class="kobospan" id="kobo.425.1">. </span><span class="kobospan" id="kobo.425.2">The advantage of this method is that we get not only individual words as output but also phrases, such as </span><em class="italic"><span class="kobospan" id="kobo.426.1">Saturday morning</span></em><span class="kobospan" id="kobo.427.1"> instead of just </span><em class="italic"><span class="kobospan" id="kobo.428.1">Saturday</span></em><span class="kobospan" id="kobo.429.1"> and </span><span><em class="italic"><span class="kobospan" id="kobo.430.1">morning</span></em></span><span><span class="kobospan" id="kobo.431.1"> individually.</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.432.1">Create the new vectorizer </span><a id="_idIndexMarker273" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.433.1">and fit it on the training summaries. </span><span class="kobospan" id="kobo.433.2">We exclude the word </span><strong class="source-inline1"><span class="kobospan" id="kobo.434.1">the</span></strong><span class="kobospan" id="kobo.435.1"> from the stopwords list since spaCy entities might </span><span><span class="kobospan" id="kobo.436.1">contain it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.437.1">
stop_words = list(stopwords.words('english'))
stop_words.remove("the")
trigram_vectorizer = TfidfVectorizer(
    stop_words=stop_words, min_df=2,
    ngram_range=(1,3), max_df=0.95)
trigram_vectorizer.fit(train_df["summary"])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.438.1">Now, define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.439.1">get_keyword_strings_all</span></strong><span class="kobospan" id="kobo.440.1"> function. </span><span class="kobospan" id="kobo.440.2">It will get all the keywords from the sorted vector. </span><span class="kobospan" id="kobo.440.3">It has no restriction on how many words </span><span><span class="kobospan" id="kobo.441.1">it gets:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.442.1">
def get_keyword_strings_all(vectorizer, sorted_vector):
    words = []
    index_dict = vectorizer.get_feature_names_out()
    for (item_index, score) in sorted_vector:
        word = index_dict[item_index]
        words.append(word)
    return words</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.443.1">Next, we define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.444.1">get_keywords_complex</span></strong><span class="kobospan" id="kobo.445.1"> function that outputs main keywords and phrases up to three </span><span><span class="kobospan" id="kobo.446.1">words long:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.447.1">
def get_keywords_complex(
    vectorizer, input_text, spacy_model, num_words=70
):
    keywords = []
    doc = spacy_model(input_text)
    vector = vectorizer.transform([input_text])
    sorted = sort_coo(vector.tocoo())
    ngrams = get_keyword_strings_all(vectorizer, sorted)
    ents = [ent.text.lower() for ent in doc.noun_chunks]
    for i in range(0, num_words):
        keyword = ngrams[i]
        if keyword.lower() in ents and not
        keyword.isdigit() and keyword not in keywords:
            keywords.append(keyword)
    return keywords</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.448.1">Now, we use the previous </span><a id="_idIndexMarker274" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.449.1">function on the first </span><span><span class="kobospan" id="kobo.450.1">test summary:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.451.1">
keywords = get_keywords_complex(trigram_vectorizer,
    test_df.iloc[0]["summary"], small_model)
print(keywords)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.452.1">The result will look </span><span><span class="kobospan" id="kobo.453.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.454.1">['the gop', 'the 50 states', 'npr', '11 states', 'state', 'republican governors', 'the dems', 'reelection', 'the helm', 'grabs']</span></pre></li>			</ol>
			<h1 id="_idParaDest-141" class="calibre7"><a id="_idTextAnchor144" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.455.1">Performing named entity recognition using spaCy</span></h1>
			<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.456.1">Named entity recognition</span></strong><span class="kobospan" id="kobo.457.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.458.1">NER</span></strong><span class="kobospan" id="kobo.459.1">) is the task of parsing the names of places, people, organizations, and so on, out of text. </span><span class="kobospan" id="kobo.459.2">This can be useful in many downstream tasks. </span><span class="kobospan" id="kobo.459.3">For example, you could imagine a situation</span><a id="_idIndexMarker275" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.460.1"> where you would like to sort</span><a id="_idIndexMarker276" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.461.1"> an article set by the people that are mentioned in it, for example, when carrying out research about a </span><span><span class="kobospan" id="kobo.462.1">certain person.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.463.1">In this recipe, we will use NER to parse out named entities from article texts in the BBC dataset. </span><span class="kobospan" id="kobo.463.2">We will load the package and the parsing engine and loop through the </span><span><span class="kobospan" id="kobo.464.1">NER results.</span></span></p>
			<h2 id="_idParaDest-142" class="calibre5"><a id="_idTextAnchor145" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.465.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.466.1">In this recipe, we will use spaCy. </span><span class="kobospan" id="kobo.466.2">To run it correctly, you will need to download a language model. </span><span class="kobospan" id="kobo.466.3">We will download the small and large models. </span><span class="kobospan" id="kobo.466.4">These models take up significant </span><span><span class="kobospan" id="kobo.467.1">disk space:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.468.1">
python -m spacy download en_core_web_sm
python -m spacy download en_core_web_lg</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.469.1">The notebook is located </span><span><span class="kobospan" id="kobo.470.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.471.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.4_named_entity_extraction.ipynb</span></span></a><span><span class="kobospan" id="kobo.472.1">.</span></span></p>
			<h2 id="_idParaDest-143" class="calibre5"><a id="_idTextAnchor146" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.473.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.474.1">NER happens automatically with the processing that spaCy does for an input text. </span><span class="kobospan" id="kobo.474.2">Accessing the entities happens through the </span><strong class="source-inline"><span class="kobospan" id="kobo.475.1">doc.ents</span></strong><span class="kobospan" id="kobo.476.1"> variable. </span><span class="kobospan" id="kobo.476.2">We will input an article about Apple’s iPhone and see which entities will get parsed from it. </span><span class="kobospan" id="kobo.476.3">Let’s see </span><span><span class="kobospan" id="kobo.477.1">the steps:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.478.1">Run the language utilities file. </span><span class="kobospan" id="kobo.478.2">This will import the necessary packages and functions and initialize the </span><span><span class="kobospan" id="kobo.479.1">spaCy engine:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.480.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.481.1">Initialize the article text. </span><span class="kobospan" id="kobo.481.2">This is an article </span><span><span class="kobospan" id="kobo.482.1">from </span></span><a href="https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.483.1">https://www.globalsmt.net/social-media-news/iphone-12-apple-makes-jump-to-5g/:</span></span></a><pre class="source-code"><span class="kobospan1" id="kobo.484.1">
article = """iPhone 12: Apple makes jump to 5G
Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks.
</span><span class="kobospan1" id="kobo.484.2">The company has also extended the range to include a new "Mini" model that has a smaller 5.4in screen.
</span><span class="kobospan1" id="kobo.484.3">The US firm bucked a wider industry downturn by increasing its handset sales over the past year.
</span><span class="kobospan1" id="kobo.484.4">But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6.
</span><span class="kobospan1" id="kobo.484.5">…
"Networks are going to have to offer eye-wateringly attractive deals, and the way they're going to do that is on great tariffs and attractive trade-in deals,"
predicted Ben Wood from the consultancy CCS Insight. </span><span class="kobospan1" id="kobo.484.6">Apple typically unveils its new iPhones in September, but opted for a later date this year.
</span><span class="kobospan1" id="kobo.484.7">It has not said why, but it was widely speculated to be related to disruption caused by the coronavirus pandemic. </span><span class="kobospan1" id="kobo.484.8">The firm's shares ended the day 2.7% lower.
</span><span class="kobospan1" id="kobo.484.9">This has been linked to reports that several Chinese internet platforms opted not to carry the livestream,
although it was still widely viewed and commented on via the social media network Sina Weibo."""</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.485.1">Here, we create the spaCy </span><strong class="source-inline1"><span class="kobospan" id="kobo.486.1">Doc</span></strong><span class="kobospan" id="kobo.487.1"> object and use it to extract the entities. </span><span class="kobospan" id="kobo.487.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.488.1">Doc</span></strong><span class="kobospan" id="kobo.489.1"> object is created by </span><a id="_idIndexMarker277" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.490.1">using the small spaCy model on</span><a id="_idIndexMarker278" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.491.1"> the text. </span><span class="kobospan" id="kobo.491.2">The model extracts different attributes, including named entities. </span><span class="kobospan" id="kobo.491.3">We print the length of the parsed entities and the entities themselves, together with start and end character information and the entity type (the meaning of the named entity labels can be found in the</span><a id="_idIndexMarker279" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.492.1"> spaCy documentation </span><span><span class="kobospan" id="kobo.493.1">at </span></span><a href="https://spacy.io/models/en" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.494.1">https://spacy.io/models/en</span></span></a><span><span class="kobospan" id="kobo.495.1">):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.496.1">
doc = small_model(article)
print(len(doc.ents))
small_model_ents = doc.ents
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.497.1">When we print out the </span><a id="_idIndexMarker280" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.498.1">result, we see different types of entities, including cardinal numbers, percentages, names of </span><a id="_idIndexMarker281" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.499.1">people, dates, organizations, and a </span><strong class="source-inline"><span class="kobospan" id="kobo.500.1">NORP</span></strong><span class="kobospan" id="kobo.501.1"> entity, which stands for </span><strong class="bold"><span class="kobospan" id="kobo.502.1">Nationalities or Religious or </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.503.1">Political groups</span></strong></span><span><span class="kobospan" id="kobo.504.1">:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.505.1">44
12 7 9 CARDINAL
Apple 11 16 ORG
5 31 32 CARDINAL
…
a later date this year 2423 2445 DATE
2.7% 2594 2598 PERCENT
Chinese 2652 2659 NORP
Sina Weibo 2797 2807 PERSON</span></pre></li>			</ol>
			<h2 id="_idParaDest-144" class="calibre5"><a id="_idTextAnchor147" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.506.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.507.1">We can compare the performance of the small and large models with the </span><span><span class="kobospan" id="kobo.508.1">following steps:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.509.1">Run the same step as </span><em class="italic"><span class="kobospan" id="kobo.510.1">step 3</span></em><span class="kobospan" id="kobo.511.1"> from the </span><em class="italic"><span class="kobospan" id="kobo.512.1">How to do it…</span></em><span class="kobospan" id="kobo.513.1"> section but with the </span><span><span class="kobospan" id="kobo.514.1">large model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.515.1">
doc = large_model(article)
print(len(doc.ents))
large_model_ents = doc.ents
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.516.1">The result will be</span><a id="_idIndexMarker282" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.517.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.518.1">46
12 7 9 CARDINAL
Apple 11 16 ORG
5 31 32 CARDINAL
…
the day 2586 2593 DATE
2.7% 2594 2598 PERCENT
Chinese 2652 2659 NORP
Sina Weibo 2797 2807 PERSON</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.519.1">There are more entities parsed by the large model, and we can take a look at the differences. </span><span class="kobospan" id="kobo.519.2">We</span><a id="_idIndexMarker283" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.520.1"> print out two lists; one list contains entities that the small model recognizes, and the other list contains entities that the large model recognizes but not </span><span><span class="kobospan" id="kobo.521.1">the small:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.522.1">
small_model_ents = [str(ent) for ent in small_model_ents]
large_model_ents = [str(ent) for ent in large_model_ents]
in_small_not_in_large = set(small_model_ents) \ 
    - set(large_model_ents)
in_large_not_in_small = set(large_model_ents) \ 
    - set(small_model_ents)
print(in_small_not_in_large)
print(in_large_not_in_small)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.523.1">The result will be </span><span><span class="kobospan" id="kobo.524.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.525.1">{'iPhone 11', 'iPhone', 'iPhones'}
{'6', 'the day', 'IDC', '11', 'Pro', 'G\nApple', 'SE'}</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.526.1">You can see that there are some differences between the results provided by the </span><span><span class="kobospan" id="kobo.527.1">two models.</span></span></p>
			<h1 id="_idParaDest-145" class="calibre7"><a id="_idTextAnchor148" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.528.1">Training your own NER model with spaCy</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.529.1">In the previous recipe, we used the pretrained spaCy model to extract named entities. </span><span class="kobospan" id="kobo.529.2">This NER model can suffice in many cases. </span><span class="kobospan" id="kobo.529.3">There might be other times, however, when we would like to create a new one from</span><a id="_idIndexMarker284" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.530.1"> scratch. </span><span class="kobospan" id="kobo.530.2">In this recipe, we will train a</span><a id="_idIndexMarker285" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.531.1"> new NER model to parse out the names of musicians and their works </span><span><span class="kobospan" id="kobo.532.1">of art.</span></span></p>
			<h2 id="_idParaDest-146" class="calibre5"><a id="_idTextAnchor149" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.533.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.534.1">We will use the spaCy package to train a new NER model. </span><span class="kobospan" id="kobo.534.2">You do not need any other packages other than </span><strong class="source-inline"><span class="kobospan" id="kobo.535.1">spacy</span></strong><span class="kobospan" id="kobo.536.1">. </span><span class="kobospan" id="kobo.536.2">The data we are going to use is from </span><a href="https://github.com/deezer/music-ner-eacl2023" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.537.1">https://github.com/deezer/music-ner-eacl2023</span></a><span class="kobospan" id="kobo.538.1">. </span><span class="kobospan" id="kobo.538.2">The data file is preloaded in the data folder (</span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.539.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/music_ner.csv</span></a><span class="kobospan" id="kobo.540.1">) and you will need to download it from the book’s GitHub repository into the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.541.1">data</span></strong></span><span><span class="kobospan" id="kobo.542.1"> directory.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.543.1">The notebook is located </span><span><span class="kobospan" id="kobo.544.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.545.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.5_training_own_spacy_model.ipynb</span></span></a><span><span class="kobospan" id="kobo.546.1">.</span></span></p>
			<h2 id="_idParaDest-147" class="calibre5"><a id="_idTextAnchor150" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.547.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.548.1">We will define our training data and then use it to train a new model. </span><span class="kobospan" id="kobo.548.2">We will then test the model and save it to disk. </span><span class="kobospan" id="kobo.548.3">The </span><a id="_idIndexMarker286" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.549.1">steps are </span><span><span class="kobospan" id="kobo.550.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.551.1">Run the language </span><span><span class="kobospan" id="kobo.552.1">utilities file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.553.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.554.1">Import other functions </span><span><span class="kobospan" id="kobo.555.1">and</span></span><span><a id="_idIndexMarker287" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.556.1"> packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.557.1">
import pandas as pd
from spacy.cli.train import train
from spacy.cli.evaluate import evaluate
from spacy.tokens import DocBin
from sklearn.model_selection import train_test_split</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.558.1">In this step, </span><a id="_idTextAnchor151" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.559.1">we load the data and print </span><span><span class="kobospan" id="kobo.560.1">it out:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.561.1">
music_ner_df = pd.read_csv('../data/music_ner.csv')
print(music_ner_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.562.1">The data has five columns: </span><strong class="source-inline"><span class="kobospan" id="kobo.563.1">id</span></strong><span class="kobospan" id="kobo.564.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.565.1">start offset</span></strong><span class="kobospan" id="kobo.566.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.567.1">end offset</span></strong><span class="kobospan" id="kobo.568.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.569.1">text</span></strong><span class="kobospan" id="kobo.570.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.571.1">label</span></strong><span class="kobospan" id="kobo.572.1">. </span><span class="kobospan" id="kobo.572.2">Sentences repeat if there is more than one entity per sentence, as there is one row per named entity. </span><span class="kobospan" id="kobo.572.3">There are 428 entries in </span><span><span class="kobospan" id="kobo.573.1">the data.</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer014" class="img---figure">
					<span class="kobospan" id="kobo.574.1"><img src="image/B18411_05_2.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.575.1">Figure 5.2 – DataFrame output</span></p>
			<ol class="calibre13">
				<li value="4" class="calibre14"><span class="kobospan" id="kobo.576.1">Here, we remove </span><strong class="source-inline1"><span class="kobospan" id="kobo.577.1">_deduced</span></strong><span class="kobospan" id="kobo.578.1"> from</span><a id="_idIndexMarker288" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.579.1"> the labels so the</span><a id="_idIndexMarker289" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.580.1"> labels are now </span><strong class="source-inline1"><span class="kobospan" id="kobo.581.1">Artist</span></strong><span class="kobospan" id="kobo.582.1">, </span><strong class="source-inline1"><span class="kobospan" id="kobo.583.1">WoA (work of </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.584.1">art)</span></strong></span><span><span class="kobospan" id="kobo.585.1">, </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.586.1">Artist_or_WoA</span></strong></span><span><span class="kobospan" id="kobo.587.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.588.1">
def change_label(input_label):
    input_label = input_label.replace("_deduced", "")
    return input_label
music_ner_df["label"] = music_ner_df["label"].apply(change_label)
print(music_ner_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.589.1">The result will look </span><span><span class="kobospan" id="kobo.590.1">like this:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer015" class="img---figure">
					<span class="kobospan" id="kobo.591.1"><img src="image/B18411_05_3.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.592.1">Figure 5.3 – DataFrame output</span></p>
			<ol class="calibre13">
				<li value="5" class="calibre14"><span class="kobospan" id="kobo.593.1">In this step, we create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.594.1">DocBin</span></strong><span class="kobospan" id="kobo.595.1"> objects that </span><a id="_idIndexMarker290" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.596.1">will store the processed data. </span><strong class="source-inline1"><span class="kobospan" id="kobo.597.1">DocBin</span></strong><span class="kobospan" id="kobo.598.1"> objects are</span><a id="_idIndexMarker291" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.599.1"> required for input data for spaCy models (to learn more about them, see the </span><em class="italic"><span class="kobospan" id="kobo.600.1">Training a spaCy textcat model</span></em><span class="kobospan" id="kobo.601.1"> recipe in </span><a href="B18411_04.xhtml#_idTextAnchor106" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.602.1">Chapter 4</span></em></span></a><span><span class="kobospan" id="kobo.603.1">):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.604.1">
train_db = DocBin()
test_db = DocBin()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.605.1">Here, we create a list of unique IDs and split it into training and test data. </span><span class="kobospan" id="kobo.605.2">The reason we would like to get the unique IDs is because sentences repeat through the dataset. </span><span class="kobospan" id="kobo.605.3">There are 227 unique IDs (or sentences), and there are 170 sentences in the </span><a id="_idIndexMarker292" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.606.1">training data and 57 sentences in the </span><span><span class="kobospan" id="kobo.607.1">test</span></span><span><a id="_idIndexMarker293" class="calibre6 pcalibre pcalibre1"/></span><span><span class="kobospan" id="kobo.608.1"> data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.609.1">
# Get a unique list of unique ids
ids = list(set(music_ner_df["id"].values))
print(len(ids))
# Split ids into training and test
train_ids, test_ids = train_test_split(ids)
print(len(train_ids))
print(len(test_ids))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.610.1">The result will be </span><span><span class="kobospan" id="kobo.611.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.612.1">227
170
57</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.613.1">Here, we create and save training and test data in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.614.1">DocBin</span></strong><span class="kobospan" id="kobo.615.1"> objects. </span><span class="kobospan" id="kobo.615.2">We loop through IDs, and for each ID, we get the sentence. </span><span class="kobospan" id="kobo.615.3">We process the sentence using the small model and then have a spaCy </span><strong class="source-inline1"><span class="kobospan" id="kobo.616.1">Doc</span></strong><span class="kobospan" id="kobo.617.1"> object. </span><span class="kobospan" id="kobo.617.2">Then, we loop through the entities in the sentence and add them to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.618.1">ents</span></strong><span class="kobospan" id="kobo.619.1"> attribute of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.620.1">Doc</span></strong><span class="kobospan" id="kobo.621.1"> object. </span><span class="kobospan" id="kobo.621.2">The processed </span><strong class="source-inline1"><span class="kobospan" id="kobo.622.1">Doc</span></strong><span class="kobospan" id="kobo.623.1"> object then goes into one of the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.624.1">DocBin</span></strong></span><span><span class="kobospan" id="kobo.625.1"> objects:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.626.1">
for id in ids:
    entity_rows = music_ner_df.loc[music_ner_df['id'] == id]
    text = entity_rows.head(1)["text"].values[0]
    doc = small_model(text)
    ents = []
    for index, row in entity_rows.iterrows():
        label = row["label"]
        start = row["start_offset"]
        end = row["end_offset"]
        span = doc.char_span(start, end, label=label, 
            alignment_mode="contract")
        ents.append(span)
    doc.ents = ents
    if id in train_ids:
        train_db.add(doc)
    else:
        test_db.add(doc)
train_db.to_disk('../data/music_ner_train.spacy')
test_db.to_disk('../data/music_ner_test.spacy')</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.627.1">In this step, we train the model. </span><span class="kobospan" id="kobo.627.2">We</span><a id="_idIndexMarker294" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.628.1"> use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.629.1">spacy_config_ner.cfg</span></strong><span class="kobospan" id="kobo.630.1"> configuration file in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.631.1">data</span></strong><span class="kobospan" id="kobo.632.1"> folder. </span><span class="kobospan" id="kobo.632.2">You can create your own customized configuration</span><a id="_idIndexMarker295" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.633.1"> file at </span><a href="https://spacy.io/usage/training/#quickstart" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.634.1">https://spacy.io/usage/training/#quickstart</span></a><span class="kobospan" id="kobo.635.1">. </span><span class="kobospan" id="kobo.635.2">The output shows the loss, accuracy, precision, recall, F1 score, and other metrics for every epoch. </span><span class="kobospan" id="kobo.635.3">Finally, it saves the model to the </span><span><span class="kobospan" id="kobo.636.1">specified directory:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.637.1">
train("../data/spacy_config_ner.cfg", output_path="../models/spacy_music_ner")</span></pre><p class="calibre3"><span class="kobospan" id="kobo.638.1">The output will look </span><span><span class="kobospan" id="kobo.639.1">like this:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer016" class="img---figure">
					<span class="kobospan" id="kobo.640.1"><img src="image/B18411_05_4.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.641.1">Figure 5.4 – Model training output</span></p>
			<ol class="calibre13">
				<li value="9" class="calibre14"><span class="kobospan" id="kobo.642.1">In this step, we load the trained model and use it on data not seen during training. </span><span class="kobospan" id="kobo.642.2">We get an ID from the</span><a id="_idIndexMarker296" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.643.1"> test set, get all the rows from the test data with that ID, and load the </span><a id="_idIndexMarker297" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.644.1">sentence. </span><span class="kobospan" id="kobo.644.2">We then print out the sentence and the annotated entities. </span><span class="kobospan" id="kobo.644.3">Then, we process the sentence using our model (in exactly the same way as other pretrained spaCy models) and print out the entities </span><span><span class="kobospan" id="kobo.645.1">it parsed:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.646.1">
nlp = spacy.load("../models/spacy_music_ner/model-last")
first_test_id = test_ids[0]
test_rows = music_ner_df.loc[music_ner_df['id'] 
    == first_test_id]
input_text = entity_rows.head(1)["text"].values[0]
print(input_text)
print("Gold entities:")
for index, row in entity_rows.iterrows():
    label = row["label"]
    start = row["start_offset"]
    end = row["end_offset"]
    span = doc.char_span(start, end, label=label,
        alignment_mode="contract")
    print(span)
doc = nlp(input_text)
print("Predicted entities: ")
for entity in doc.ents:
    print(entity)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.647.1">We see that the resulting </span><a id="_idIndexMarker298" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.648.1">entities are quite good (output results</span><a id="_idIndexMarker299" class="calibre6 pcalibre pcalibre1"/> <span><span class="kobospan" id="kobo.649.1">may vary):</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.650.1">songs with themes of being unable to settle | ex hoziers someone new elle kings exes and ohs
Gold entities:
hoziers
someone new
elle kings
exes and ohs
Predicted entities:
hoziers
someone new
elle kings
exes and</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.651.1">Here, we evaluate the model using spaCy’s </span><strong class="source-inline1"><span class="kobospan" id="kobo.652.1">evaluate</span></strong><span class="kobospan" id="kobo.653.1"> function. </span><span class="kobospan" id="kobo.653.2">We see that the </span><strong class="source-inline1"><span class="kobospan" id="kobo.654.1">WoA</span></strong><span class="kobospan" id="kobo.655.1"> and </span><strong class="source-inline1"><span class="kobospan" id="kobo.656.1">Artist</span></strong><span class="kobospan" id="kobo.657.1"> tags have metrics that are low but in the double digits, while the </span><strong class="source-inline1"><span class="kobospan" id="kobo.658.1">Artist_or_WoA</span></strong><span class="kobospan" id="kobo.659.1"> tag has an F1 score of about 10%. </span><span class="kobospan" id="kobo.659.2">This is due to the fact that it has much less data than the</span><a id="_idIndexMarker300" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.660.1"> other two tags. </span><span class="kobospan" id="kobo.660.2">Overall, the performance of the model </span><a id="_idIndexMarker301" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.661.1">according to the statistics is not very good, and that is because we have a very small amount of </span><span><span class="kobospan" id="kobo.662.1">data overall:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.663.1">
evaluate('../models/spacy_music_ner/model-last', '../data/music_ner_tes t.spacy')</span></pre><p class="calibre3"><span class="kobospan" id="kobo.664.1">The statistics might vary, but here is the output I got (</span><span><span class="kobospan" id="kobo.665.1">output condensed):</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.666.1">{'token_acc': 1.0,
 'token_p': 1.0,
 'token_r': 1.0,
 'token_f': 1.0,
 'tag_acc': 0.800658978583196,
…
 'ents_p': 0.4421052631578947,
 'ents_r': 0.42,
 'ents_f': 0.4307692307692308,
 'ents_per_type': {'WoA': {'p': 0.4358974358974359,
   'r': 0.425,
   'f': 0.43037974683544306},
  'Artist_or_WoA': {'p': 0.1,
   'r': 0.09090909090909091,
   'f': 0.09523809523809525},
  'Artist': {'p': 0.5217391304347826,
   'r': 0.4897959183673469,
   'f': 0.5052631578947369}},
 'speed': 3835.591242612551}</span></pre></li>			</ol>
			<h2 id="_idParaDest-148" class="calibre5"><a id="_idTextAnchor152" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.667.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.668.1">The spaCy NER model is a </span><a id="_idIndexMarker302" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.669.1">neural network model. </span><span class="kobospan" id="kobo.669.2">You can learn more </span><a id="_idIndexMarker303" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.670.1">about its architecture from the spaCy </span><span><span class="kobospan" id="kobo.671.1">documentation: </span></span><a href="https://spacy.io/models#architecture" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.672.1">https://spacy.io/models#architecture</span></span></a><span><span class="kobospan" id="kobo.673.1">.</span></span></p>
			<h1 id="_idParaDest-149" class="calibre7"><a id="_idTextAnchor153" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.674.1">Fine-tuning BERT for NER</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.675.1">In this recipe, we will fine-tune the pretrained BERT model for the NER task. </span><span class="kobospan" id="kobo.675.2">The difference between training a model from scratch and fine-tuning it is as follows. </span><span class="kobospan" id="kobo.675.3">Fine-tuning an NLP model, such as BERT, involves</span><a id="_idIndexMarker304" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.676.1"> taking a pretrained model and modifying it for your specific task, such as NER in this case. </span><span class="kobospan" id="kobo.676.2">The pretrained model already has lots of knowledge stored in it and the results are likely to be better than when training a model </span><span><span class="kobospan" id="kobo.677.1">from scratch.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.678.1">We will use similar data as in the previous recipe, creating a model that can tag entities as </span><strong class="source-inline"><span class="kobospan" id="kobo.679.1">Artist</span></strong><span class="kobospan" id="kobo.680.1"> or </span><strong class="source-inline"><span class="kobospan" id="kobo.681.1">WoA</span></strong><span class="kobospan" id="kobo.682.1">. </span><span class="kobospan" id="kobo.682.2">The data comes from the same dataset but it is labeled using the IOB format, which is required for the </span><strong class="source-inline"><span class="kobospan" id="kobo.683.1">transformers</span></strong><span class="kobospan" id="kobo.684.1"> packages we are going to use. </span><span class="kobospan" id="kobo.684.2">We also only use the </span><strong class="source-inline"><span class="kobospan" id="kobo.685.1">Artist</span></strong><span class="kobospan" id="kobo.686.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.687.1">WoA</span></strong><span class="kobospan" id="kobo.688.1"> tags, removing the </span><strong class="source-inline"><span class="kobospan" id="kobo.689.1">Artist_or_WoA</span></strong><span class="kobospan" id="kobo.690.1"> tag, since there is not enough data for </span><span><span class="kobospan" id="kobo.691.1">that tag.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.692.1">For this recipe, we will use the Hugging Face </span><strong class="source-inline"><span class="kobospan" id="kobo.693.1">Trainer</span></strong><span class="kobospan" id="kobo.694.1"> class, although it is also possible to train Hugging Face models using PyTorch or Tensorflow. </span><span class="kobospan" id="kobo.694.2">See more </span><span><span class="kobospan" id="kobo.695.1">at </span></span><a href="https://huggingface.co/docs/transformers/training" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.696.1">https://huggingface.co/docs/transformers/training</span></span></a><span><span class="kobospan" id="kobo.697.1">.</span></span></p>
			<h2 id="_idParaDest-150" class="calibre5"><a id="_idTextAnchor154" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.698.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.699.1">We will use the </span><strong class="source-inline"><span class="kobospan" id="kobo.700.1">transformers</span></strong><span class="kobospan" id="kobo.701.1"> package from Hugging Face. </span><span class="kobospan" id="kobo.701.2">It is preloaded in the Poetry environment. </span><span class="kobospan" id="kobo.701.3">You can also install the package from the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.702.1">requirements.txt</span></strong></span><span><span class="kobospan" id="kobo.703.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.704.1">The notebook is located </span><span><span class="kobospan" id="kobo.705.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.706.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter05/5.6_fine_tune_bert.ipynb</span></span></a><span><span class="kobospan" id="kobo.707.1">.</span></span></p>
			<h2 id="_idParaDest-151" class="calibre5"><a id="_idTextAnchor155" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.708.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.709.1">We will load and preprocess the data, train the model, and evaluate it, and then we will use it on unseen data. </span><span class="kobospan" id="kobo.709.2">Your </span><a id="_idIndexMarker305" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.710.1">steps should be formatted </span><span><span class="kobospan" id="kobo.711.1">like so:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.712.1">Run the language </span><span><span class="kobospan" id="kobo.713.1">utilities notebook:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.714.1">
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.715.1">Import other packages </span><span><span class="kobospan" id="kobo.716.1">and functions:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.717.1">
from datasets import (
    load_dataset, Dataset, Features, Value,
    ClassLabel, Sequence, DatasetDict)
import pandas as pd
from transformers import AutoTokenizer, AutoModel
from transformers import DataCollatorForTokenClassification
from transformers import (
    AutoModelForTokenClassification,
    TrainingArguments, Trainer)
import numpy as np
from sklearn.model_selection import train_test_split
from evaluate import load</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.718.1">In this step, we load the music NER dataset using the pandas </span><strong class="source-inline1"><span class="kobospan" id="kobo.719.1">read_csv</span></strong><span class="kobospan" id="kobo.720.1"> function. </span><span class="kobospan" id="kobo.720.2">We then define a function that takes a label, splits it on the underscore, and removes </span><a id="_idIndexMarker306" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.721.1">the last part (</span><strong class="source-inline1"><span class="kobospan" id="kobo.722.1">_deduced</span></strong><span class="kobospan" id="kobo.723.1">). </span><span class="kobospan" id="kobo.723.2">We then apply this function to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.724.1">label</span></strong><span class="kobospan" id="kobo.725.1"> column. </span><span class="kobospan" id="kobo.725.2">We also substitute the </span><strong class="source-inline1"><span class="kobospan" id="kobo.726.1">|</span></strong><span class="kobospan" id="kobo.727.1"> character in case it could interfere with </span><span><span class="kobospan" id="kobo.728.1">our code:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.729.1">
music_ner_df = pd.read_csv('../data/music_ner.csv')
def change_label(input_label):
    input_label = input_label.replace("_deduced", "")
    return input_label
music_ner_df["label"] = music_ner_df["label"].apply(
    change_label)
music_ner_df["text"] = music_ner_df["text"].apply(
    lambda x: x.replace("|", ","))
print(music_ner_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.730.1">The output will look similar </span><span><span class="kobospan" id="kobo.731.1">to this:</span></span></p></li>			</ol>
			<div class="calibre2">
				<div id="_idContainer017" class="img---figure">
					<span class="kobospan" id="kobo.732.1"><img src="image/B18411_05_5.jpg" alt="" role="presentation" class="calibre4"/></span>
				</div>
			</div>
			<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.733.1">Figure 5.5 – Dataset DataFrame</span></p>
			<ol class="calibre13">
				<li value="4" class="calibre14"><span class="kobospan" id="kobo.734.1">Here, we start our data preprocessing. </span><span class="kobospan" id="kobo.734.2">We get the list of unique IDs from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.735.1">id</span></strong><span class="kobospan" id="kobo.736.1"> column. </span><span class="kobospan" id="kobo.736.2">We loop through this list and get the sentence that corresponds to the given ID. </span><span class="kobospan" id="kobo.736.3">We then</span><a id="_idIndexMarker307" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.737.1"> process the text using the spaCy small model and add the entities from the DataFrame into the </span><strong class="source-inline1"><span class="kobospan" id="kobo.738.1">Doc</span></strong><span class="kobospan" id="kobo.739.1"> object. </span><span class="kobospan" id="kobo.739.2">We then store each of these sentences in a dictionary in which the keys are the sentence text strings and the values are the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.740.1">Doc</span></strong></span><span><span class="kobospan" id="kobo.741.1"> objects:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.742.1">
ids = list(set(music_ner_df["id"].values))
docs = {}
for id in ids:
    entity_rows = music_ner_df.loc[music_ner_df['id'] == id]
    text = entity_rows.head(1)["text"].values[0]
    doc = small_model(text)
    ents = []
    for index, row in entity_rows.iterrows():
        label = row["label"]
        start = row["start_offset"]
        end = row["end_offset"]
        span = doc.char_span(start, end, label=label,
            alignment_mode="contract")
        ents.append(span)
    doc.ents = ents
    docs[doc.text] = doc</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.743.1">Now, we load the data in IOB format. </span><span class="kobospan" id="kobo.743.2">This format is required to fine-tune BERT, as opposed to the format used by spaCy. </span><span class="kobospan" id="kobo.743.3">For that, we load a separate data file, </span><strong class="source-inline1"><span class="kobospan" id="kobo.744.1">../data/music_ner_bio.bio</span></strong><span class="kobospan" id="kobo.745.1">. </span><span class="kobospan" id="kobo.745.2">We create a dictionary of tag mappings and initialize empty lists for tokens, NER tags, and spans. </span><span class="kobospan" id="kobo.745.3">We then loop through the sentence data we read from</span><a id="_idIndexMarker308" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.746.1"> the data file. </span><span class="kobospan" id="kobo.746.2">For each sentence, each line is a pair of a word and its label. </span><span class="kobospan" id="kobo.746.3">We append the words to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.747.1">words</span></strong><span class="kobospan" id="kobo.748.1"> list and the numbers corresponding to the labels to the </span><strong class="source-inline1"><span class="kobospan" id="kobo.749.1">tags</span></strong><span class="kobospan" id="kobo.750.1"> list. </span><span class="kobospan" id="kobo.750.2">We also get the spans from the dictionary of </span><strong class="source-inline1"><span class="kobospan" id="kobo.751.1">Doc</span></strong><span class="kobospan" id="kobo.752.1"> objects we created in the previous step and append those to the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.753.1">spans</span></strong></span><span><span class="kobospan" id="kobo.754.1"> list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.755.1">
data_file = "../data/music_ner_bio.bio"
tag_mapping = {"O": 0, "B-Artist": 1, "I-Artist": 2, 
    "B-WoA": 3, "I-WoA": 4}
with open(data_file) as f:
    data = f.read()
tokens = []
ner_tags = []
spans = []
sentences = data.split("\n\n")
for sentence in sentences:
    words = []
    tags = []
    this_sentence_spans = []
    word_tag_pairs = sentence.split("\n")
    for pair in word_tag_pairs:
        (word, tag) = pair.split("\t")
        words.append(word)
        tags.append(tag_mapping[tag])
    sentence_text = " ".join(words)
    try:
        doc = docs[sentence_text]
    except:
        pass
    ent_dict = {}
    for ent in doc.ents:
        this_sentence_spans.append(f"{ent.label_}: {ent.text}")
    tokens.append(words)
    ner_tags.append(tags)
    spans.append(this_sentence_spans)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.756.1">Here, we split the data into training and testing. </span><span class="kobospan" id="kobo.756.2">For that, we split the indices of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.757.1">spans</span></strong><span class="kobospan" id="kobo.758.1"> list. </span><span class="kobospan" id="kobo.758.2">Then, we create separate</span><a id="_idIndexMarker309" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.759.1"> tokens, NER tags, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.760.1">spans</span></strong><span class="kobospan" id="kobo.761.1"> lists for training and </span><span><span class="kobospan" id="kobo.762.1">test data:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.763.1">
indices = range(0, len(spans))
train, test = train_test_split(indices, test_size=0.1)
train_tokens = []
test_tokens = []
train_ner_tags = []
test_ner_tags = []
train_spans = []
test_spans = []
for i, (token, ner_tag, span) in enumerate(
    zip(tokens, ner_tags, spans)
):
    if i in train:
        train_tokens.append(token)
        train_ner_tags.append(ner_tag)
        train_spans.append(span)
    else:
        test_tokens.append(token)
        test_ner_tags.append(ner_tag)
        test_spans.append(span)
print(len(train_spans))
print(len(test_spans))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.764.1">The output will be </span><span><span class="kobospan" id="kobo.765.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.766.1">539
60</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.767.1"> In this step, we create new DataFrames from the training and test lists we compiled in </span><em class="italic"><span class="kobospan" id="kobo.768.1">step 6</span></em><span class="kobospan" id="kobo.769.1">. </span><span class="kobospan" id="kobo.769.2">We then join the contents of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.770.1">tokens</span></strong><span class="kobospan" id="kobo.771.1"> column with spaces to get a sentence string</span><a id="_idIndexMarker310" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.772.1"> instead of a list of words. </span><span class="kobospan" id="kobo.772.2">We then drop empty data using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.773.1">dropna()</span></strong><span class="kobospan" id="kobo.774.1"> function and print the contents of the </span><span><span class="kobospan" id="kobo.775.1">test DataFrame:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.776.1">
training_df = pd.DataFrame({"tokens":train_tokens,
    "ner_tags": train_ner_tags, "spans": train_spans})
test_df = pd.DataFrame({"tokens": test_tokens,
    "ner_tags": test_ner_tags, "spans": test_spans})
training_df["text"] = training_df["tokens"].apply(
    lambda x: " ".join(x))
test_df["text"] = test_df["tokens"].apply(lambda x: " ".join(x))
training_df.dropna()
test_df.dropna()
print(test_df)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.777.1">The result will look </span><span><span class="kobospan" id="kobo.778.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.779.1">                                               tokens  \
0   [i, love, radioheads, kid, a, something, simil...
</span><span class="kobospan1" id="kobo.779.2">1   [bluesy, songs, kinda, like, evil, woman, by, ...
</span><span class="kobospan1" id="kobo.779.3">...
</span><span class="kobospan1" id="kobo.779.4">58  [looking, for, like, electronic, music, with, ...
</span><span class="kobospan1" id="kobo.779.5">59  [looking, for, pop, songs, about, the, end, of...
</span><span class="kobospan1" id="kobo.779.6">                                             ner_tags  \
0       [0, 0, 1, 3, 4, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0]
1                         [0, 0, 0, 0, 3, 4, 0, 1, 2]
...
</span><span class="kobospan1" id="kobo.779.7">58      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
59                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
                                                spans  \
0     [Artist: radioheads, Artist_or_WoA: aphex twin]
1            [WoA: evil woman, Artist: black sabbath]
...
</span><span class="kobospan1" id="kobo.779.8">58  [WoA: the piper at the gates of dawn, Artist: ...
</span><span class="kobospan1" id="kobo.779.9">59  [WoA: the piper at the gates of dawn, Artist: ...
</span><span class="kobospan1" id="kobo.779.10">                                                 text
0   i love radioheads kid a something similar , ki...
</span><span class="kobospan1" id="kobo.779.11">1   bluesy songs kinda like evil woman by black sa...
</span><span class="kobospan1" id="kobo.779.12">...
</span><span class="kobospan1" id="kobo.779.13">58  looking for like electronic music with a depre...
</span><span class="kobospan1" id="kobo.779.14">59   looking for pop songs about the end of the world</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.780.1">Here, we load the pretrained model and tokenizer and initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.781.1">Dataset</span></strong><span class="kobospan" id="kobo.782.1"> objects. </span><span class="kobospan" id="kobo.782.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.783.1">Features</span></strong><span class="kobospan" id="kobo.784.1"> object describes the data and its properties. </span><span class="kobospan" id="kobo.784.2">We create one training and one test </span><strong class="source-inline1"><span class="kobospan" id="kobo.785.1">Dataset</span></strong><span class="kobospan" id="kobo.786.1"> object. </span><span class="kobospan" id="kobo.786.2">We use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.787.1">Features</span></strong><span class="kobospan" id="kobo.788.1"> object we created, and the </span><a id="_idIndexMarker311" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.789.1">DataFrames we initialized in the previous step. </span><span class="kobospan" id="kobo.789.2">We then add these newly created </span><strong class="source-inline1"><span class="kobospan" id="kobo.790.1">Dataset</span></strong><span class="kobospan" id="kobo.791.1"> objects to </span><strong class="source-inline1"><span class="kobospan" id="kobo.792.1">DatasetDict</span></strong><span class="kobospan" id="kobo.793.1">, with one entry for the training dataset and one for the test data. </span><span class="kobospan" id="kobo.793.2">We then print out the </span><span><span class="kobospan" id="kobo.794.1">resulting object:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.795.1">
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
features = Features(
    {'tokens': Sequence(feature=Value(dtype='string',
            id=None),
        length=-1, id=None),
            'ner_tags': Sequence(feature=ClassLabel(
                names=['O', 'B-Artist', 'I-Artist',
                'B-WoA', 'I-WoA'], id=None),
                length=-1, id=None),
            'spans': Sequence(
                feature=Value(dtype='string',id=None),
                length=-1, id=None),
            'text': Value(dtype='string', id=None)
                    })
training_dataset = Dataset.from_pandas(
    training_df, features=features)
test_dataset = Dataset.from_pandas(test_df, features=features)
dataset = DatasetDict({"train":training_dataset, 
    "test":test_dataset})
print(dataset["train"].features)
label_names = \
    dataset["train"].features["ner_tags"].feature.names
print(dataset)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.796.1">The result will be </span><a id="_idIndexMarker312" class="calibre6 pcalibre pcalibre1"/><span><span class="kobospan" id="kobo.797.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.798.1">{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-Artist', 'I-Artist', 'B-WoA', 'I-WoA'], id=None), length=-1, id=None), 'spans': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'text': Value(dtype='string', id=None)}
DatasetDict({
    train: Dataset({
        features: ['tokens', 'ner_tags', 'spans', 'text'],
        num_rows: 539
    })
    test: Dataset({
        features: ['tokens', 'ner_tags', 'spans', 'text'],
        num_rows: 60
    })
})</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.799.1">In this step, we create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.800.1">tokenize_adjust_labels</span></strong><span class="kobospan" id="kobo.801.1"> function that will assign the correct labels to word parts. </span><span class="kobospan" id="kobo.801.2">We define the </span><strong class="source-inline1"><span class="kobospan" id="kobo.802.1">tokenize_adjust_labels</span></strong><span class="kobospan" id="kobo.803.1"> function. </span><span class="kobospan" id="kobo.803.2">The BERT tokenizer splits some words into components, and we need to make sure that the same label is assigned to each word part. </span><span class="kobospan" id="kobo.803.3">The</span><a id="_idIndexMarker313" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.804.1"> function first tokenizes all the text samples using the preloaded tokenizer. </span><span class="kobospan" id="kobo.804.2">It then loops through the input IDs of the tokenized samples and adjusts the labels according to the </span><span><span class="kobospan" id="kobo.805.1">word parts:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.806.1">
def tokenize_adjust_labels(all_samples_per_split):
    tokenized_samples = tokenizer.batch_encode_plus(
    all_samples_per_split["text"])
    total_adjusted_labels = []
    for k in range(0, len(tokenized_samples["input_ids"])):
        prev_wid = -1
        word_ids_list = tokenized_samples.word_ids(
            batch_index=k)
        existing_label_ids = all_samples_per_split[
            "ner_tags"][k]
        i = -1
        adjusted_label_ids = []
        for wid in word_ids_list:
            if (wid is None):
                adjusted_label_ids.append(-100)
            elif (wid != prev_wid):
                i = i + 1
                adjusted_label_ids.append(existing_label_ids[i])
                prev_wid = wid
            else:
                label_name =label_names[existing_label_ids[i]]
                adjusted_label_ids.append(existing_label_ids[i])
        total_adjusted_labels.append(adjusted_label_ids)
    tokenized_samples["labels"] = total_adjusted_labels
    return tokenized_samples</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.807.1">Use the previous function on </span><span><span class="kobospan" id="kobo.808.1">the dataset:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.809.1">
tokenized_dataset = dataset.map(tokenize_adjust_labels, 
    batched=True)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.810.1">Here, we initialize the data </span><a id="_idIndexMarker314" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.811.1">collator object. </span><span class="kobospan" id="kobo.811.2">Data collators simplify the handling of data for training, for example, padding and truncating the input for all inputs to be of the </span><span><span class="kobospan" id="kobo.812.1">same length:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.813.1">
data_collator = DataCollatorForTokenClassification(tokenizer)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.814.1">Now, we create the </span><strong class="source-inline1"><span class="kobospan" id="kobo.815.1">compute_metrics</span></strong><span class="kobospan" id="kobo.816.1"> function, which calculates evaluation metrics, including precision, recall, F1 score, and accuracy. </span><span class="kobospan" id="kobo.816.2">In the function, we delete all the tokens that </span><a id="_idIndexMarker315" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.817.1">have the label </span><strong class="source-inline1"><span class="kobospan" id="kobo.818.1">-100</span></strong><span class="kobospan" id="kobo.819.1">, which are the special tokens. </span><span class="kobospan" id="kobo.819.2">This function uses the </span><strong class="source-inline1"><span class="kobospan" id="kobo.820.1">seqeval</span></strong><span class="kobospan" id="kobo.821.1"> evaluation method commonly used to evaluate </span><span><span class="kobospan" id="kobo.822.1">NER tasks:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.823.1">
metric = load("seqeval")
def compute_metrics(data):
    predictions, labels = data
    predictions = np.argmax(predictions, axis=2)
    data = zip(predictions, labels)
    data = [
        [(p, l) for (p, l) in zip(prediction, label) 
            if l != -100]
        for prediction, label in data
    ]
    true_predictions = [
        [label_names[p] for (p, l) in data_point]
        for data_point in data
    ]
    true_labels = [
        [label_names[l] for (p, l) in data_point]
        for data_point in data
    ]
    results = metric.compute(predictions=true_predictions, 
        references=true_labels)
    flat_results = {
        "overall_precision": results["overall_precision"],
        "overall_recall": results["overall_recall"],
        "overall_f1": results["overall_f1"],
        "overall_accuracy": results["overall_accuracy"],
    }
    for k in results.keys():
      if (k not in flat_results.keys()):
        flat_results[k + "_f1"] = results[k]["f1"]
    return flat_results</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.824.1">Here, we load the pretrained BERT model (the uncased version since our input is in lowercase). </span><span class="kobospan" id="kobo.824.2">We then specify the training arguments by initializing the </span><strong class="source-inline1"><span class="kobospan" id="kobo.825.1">TrainingArguments</span></strong><span class="kobospan" id="kobo.826.1"> object. </span><span class="kobospan" id="kobo.826.2">This object contains the model hyperparameters. </span><span class="kobospan" id="kobo.826.3">We then initialize </span><a id="_idIndexMarker316" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.827.1">the </span><strong class="source-inline1"><span class="kobospan" id="kobo.828.1">Trainer</span></strong><span class="kobospan" id="kobo.829.1"> object by providing the training arguments, dataset, tokenizer, data collator, and </span><strong class="source-inline1"><span class="kobospan" id="kobo.830.1">metrics </span></strong><span class="kobospan" id="kobo.831.1">function. </span><span class="kobospan" id="kobo.831.2">We then start the </span><span><span class="kobospan" id="kobo.832.1">training process:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.833.1">
model = AutoModelForTokenClassification.from_pretrained(
    'bert-base-uncased', num_labels=len(label_names))
training_args = TrainingArguments(
    output_dir="./fine_tune_bert_output",
    evaluation_strategy="steps",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=7,
    weight_decay=0.01,
    logging_steps = 1000,
    run_name = "ep_10_tokenized_11",
    save_strategy='no'
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()</span></pre><p class="calibre3"><span class="kobospan" id="kobo.834.1">The output will include different information, including </span><span><span class="kobospan" id="kobo.835.1">the following:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.836.1">TrainOutput(global_step=238, training_loss=0.25769581514246326, metrics={'train_runtime': 25.8951, 'train_samples_per_second': 145.703, 'train_steps_per_second': 9.191, 'total_flos': 49438483110900.0, 'train_loss': 0.25769581514246326, 'epoch': 7.0})</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.837.1">In this step, we evaluate the </span><span><span class="kobospan" id="kobo.838.1">fine-tuned model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.839.1">
trainer.evaluate()</span></pre><p class="calibre3"><span class="kobospan" id="kobo.840.1">For the </span><strong class="source-inline"><span class="kobospan" id="kobo.841.1">Artist</span></strong><span class="kobospan" id="kobo.842.1"> label, it</span><a id="_idIndexMarker317" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.843.1"> achieves an F1 score of 76%, and for the </span><strong class="source-inline"><span class="kobospan" id="kobo.844.1">WoA</span></strong><span class="kobospan" id="kobo.845.1"> label, it achieves an F1 score </span><span><span class="kobospan" id="kobo.846.1">of 52%:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.847.1">{'eval_loss': 0.28670933842658997,
 'eval_overall_precision': 0.6470588235294118,
 'eval_overall_recall': 0.7096774193548387,
 'eval_overall_f1': 0.6769230769230768,
 'eval_overall_accuracy': 0.9153605015673981,
 'eval_Artist_f1': 0.761904761904762,
 'eval_WoA_f1': 0.5217391304347826,
 'eval_runtime': 0.3239,
 'eval_samples_per_second': 185.262,
 'eval_steps_per_second': 12.351,
 'epoch': 7.0}</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.848.1">Save </span><span><span class="kobospan" id="kobo.849.1">the model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.850.1">
trainer.save_model("../models/bert_fine_tuned")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.851.1">Now, load the </span><span><span class="kobospan" id="kobo.852.1">trained model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.853.1">
model = AutoModelForTokenClassification.from_pretrained("../models/bert_fine_tuned")
tokenizer = AutoTokenizer.from_pretrained(
    "../models/bert_fine_tuned")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.854.1">Here, we test the fine-tuned model on an unseen text. </span><span class="kobospan" id="kobo.854.2">We initialize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.855.1">text</span></strong><span class="kobospan" id="kobo.856.1"> variable. </span><span class="kobospan" id="kobo.856.2">We then load the </span><strong class="source-inline1"><span class="kobospan" id="kobo.857.1">pipeline</span></strong><span class="kobospan" id="kobo.858.1"> package to create a pipeline we will use. </span><span class="kobospan" id="kobo.858.2">A text-processing pipeline takes the text to its final output value processed by the model. </span><span class="kobospan" id="kobo.858.3">This particular pipeline specifies the task as </span><strong class="source-inline1"><span class="kobospan" id="kobo.859.1">token-classification</span></strong><span class="kobospan" id="kobo.860.1">, which fine-tuned model to use, the corresponding tokenizer, and the aggregation </span><a id="_idIndexMarker318" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.861.1">strategy. </span><span class="kobospan" id="kobo.861.2">The aggregation strategy parameter specifies how to combine the results of several models when several models are used. </span><span class="kobospan" id="kobo.861.3">We then run the pipeline on </span><span><span class="kobospan" id="kobo.862.1">the text:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.863.1">
text = "music similar to morphine robocobra quartet | featuring elements like saxophone prominent bass"
from transformers import pipeline
pipe = pipeline(task="token-classification",
    model=model.to("cpu"), tokenizer=tokenizer,
    aggregation_strategy="simple")
pipe(text)
# tag_mapping = {"O": 0, "B-Artist": 1, "I-Artist": 2, "B-WoA": 3, "I-WoA": 4}</span></pre><p class="calibre3"><span class="kobospan" id="kobo.864.1">The output will vary. </span><span class="kobospan" id="kobo.864.2">The sample output identifies the music artist, </span><em class="italic"><span class="kobospan" id="kobo.865.1">Morphine </span></em><span><em class="italic"><span class="kobospan" id="kobo.866.1">Robocobra Quartet</span></em></span><span><span class="kobospan" id="kobo.867.1">:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.868.1">[{'entity_group': 'LABEL_0',
  'score': 0.9991929,
  'word': 'music similar to',
  'start': 0,
  'end': 16},
 {'entity_group': 'LABEL_1',
  'score': 0.8970744,
  'word': 'morphine robocobra',
  'start': 17,
  'end': 35},
 {'entity_group': 'LABEL_2',
  'score': 0.5060059,
  'word': 'quartet',
  'start': 36,
  'end': 43},
 {'entity_group': 'LABEL_0',
  'score': 0.9988042,
  'word': '| featuring elements like saxophone prominent bass',
  'start': 44,
  'end': 94}]</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.869.1">We can see that the labels </span><a id="_idIndexMarker319" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.870.1">assigned by the model </span><span><span class="kobospan" id="kobo.871.1">are correct.</span></span></p>
		</div>
	</body></html>