["```py\nfrom abc import ABC, abstractmethod\nfrom langchain.prompts import PromptTemplate\nfrom pydantic import BaseModel\nclass PromptTemplateFactory(ABC, BaseModel):\n    @abstractmethod\n    def create_template(self) -> PromptTemplate:\n        pass \n```", "```py\nfrom typing import Any\nfrom llm_engineering.domain.queries import Query\nclass RAGStep(ABC):\n    def __init__(self, mock: bool = False) -> None:\n        self._mock = mock\n    @abstractmethod\n    def generate(self, query: Query, *args, **kwargs) -> Any:\n        pass \n```", "```py\nfrom pydantic import UUID4, Field\nfrom llm_engineering.domain.base import VectorBaseDocument\nfrom llm_engineering.domain.types import DataCategory \n```", "```py\nclass Query(VectorBaseDocument):\n    content: str\n    author_id: UUID4 | None = None\n    author_full_name: str | None = None\n    metadata: dict = Field(default_factory=dict)\nclass Config:\n        category = DataCategory.QUERIES \n```", "```py\n @classmethod\n    def from_str(cls, query: str) -> \"Query\":\n        return Query(content=query.strip(\"\\n \")) \n```", "```py\n def replace_content(self, new_content: str) -> \"Query\":\n        return Query(\n            id=self.id,\n            content=new_content,\n            author_id=self.author_id,\n            author_full_name=self.author_full_name,\n            metadata=self.metadata,\n        ) \n```", "```py\nclass EmbeddedQuery(Query):\n    embedding: list[float]\n    class Config:\n        category = DataCategory.QUERIES \n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom .base import RAGStep\nfrom .prompt_templates import QueryExpansionTemplate \n```", "```py\nclass QueryExpansion(RAGStep):\n    def generate(self, query: Query, expand_to_n: int) -> list[Query]:\n        assert expand_to_n > 0, f\"'expand_to_n' should be greater than 0\\. Got {expand_to_n}.\"\n        if self._mock:\n            return [query for _ in range(expand_to_n)] \n```", "```py\n query_expansion_template = QueryExpansionTemplate()\n        prompt = query_expansion_template.create_template(expand_to_n - 1)\n        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=0) \n```", "```py\n chain = prompt | model\n        response = chain.invoke({\"question\": query})\n        result = response.content \n```", "```py\n queries_content = result.strip().split(query_expansion_template.separator)\n        queries = [query]\n        queries += [\n            query.replace_content(stripped_content)\n            for content in queries_content\n            if (stripped_content := content.strip())\n        ]\n        return queries \n```", "```py\nfrom langchain.prompts import PromptTemplate\nfrom .base import PromptTemplateFactory\nclass QueryExpansionTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is to generate {expand_to_n}\n    different versions of the given user question to retrieve relevant documents from a vector\n    database. By generating multiple perspectives on the user question, your goal is to help\n    the user overcome some of the limitations of the distance-based similarity search.\n    Provide these alternative questions separated by '{separator}'.\n    Original question: {question}\"\"\"\n    @property\n    def separator(self) -> str:\n        return \"#next-question#\"\n    def create_template(self, expand_to_n: int) -> PromptTemplate:\n        return PromptTemplate(\n            template=self.prompt,\n            input_variables=[\"question\"],\n            partial_variables={\n                \"separator\": self.separator,\n                \"expand_to_n\": expand_to_n,\n            },\n        ) \n```", "```py\nquery = Query.from_str(\"Write an article about the best types of advanced RAG methods.\")\n    query_expander = QueryExpansion()\n    expanded_queries = query_expander.generate(query, expand_to_n=3)\n    for expanded_query in expanded_queries:\n        logger.info(expanded_query.content) \n```", "```py\n2024-09-18 17:51:33.529 | INFO  - Write an article about the best types of advanced RAG methods.\n2024-09-18 17:51:33.529 | INFO  - What are the most effective advanced RAG methods, and how can they be applied?\n2024-09-18 17:51:33.529 | INFO  - Can you provide an overview of the top advanced retrieval-augmented generation techniques? \n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom llm_engineering.application import utils\nfrom llm_engineering.domain.documents import UserDocument\nfrom llm_engineering.domain.queries import Query\nfrom llm_engineering.settings import settings\nfrom .base import RAGStep\nfrom .prompt_templates import SelfQueryTemplate \n```", "```py\nclass SelfQuery(RAGStep):\n    def generate(self, query: Query) -> Query:\n        if self._mock:\n            return query \n```", "```py\n prompt = SelfQueryTemplate().create_template()\n        model = ChatOpenAI(model=settings.OPENAI_MODEL_ID, api_key=settings.OPENAI_API_KEY, temperature=0) \n```", "```py\n chain = prompt | model\n        response = chain.invoke({\"question\": query})\n        user_full_name = response.content.strip(\"\\n \") \n```", "```py\n if user_full_name == \"none\":\n            return query \n```", "```py\n first_name, last_name = utils.split_user_full_name(user_full_name)\n        user = UserDocument.get_or_create(first_name=first_name, last_name=last_name) \n```", "```py\n query.author_id = user.id\n        query.author_full_name = user.full_name\n        return query \n```", "```py\nfrom langchain.prompts import PromptTemplate\nfrom .base import PromptTemplateFactory\nclass SelfQueryTemplate(PromptTemplateFactory):\n    prompt: str = \"\"\"You are an AI language model assistant. Your task is to extract information from a user question.\n    The required information that needs to be extracted is the user name or user id.\n    Your response should consist of only the extracted user name (e.g., John Doe) or id (e.g. 1345256), nothing else.\n    If the user question does not contain any user name or id, you should return the following token: none.\n\n    For example:\n    QUESTION 1:\n    My name is Paul Iusztin and I want a post about...\n    RESPONSE 1:\n    Paul Iusztin\n\n    QUESTION 2:\n    I want to write a post about...\n    RESPONSE 2:\n    none\n\n    QUESTION 3:\n    My user id is 1345256 and I want to write a post about...\n    RESPONSE 3:\n    1345256\n\n    User question: {question}\"\"\"\n    def create_template(self) -> PromptTemplate:\n        return PromptTemplate(template=self.prompt, input_variables=[\"question\"]) \n```", "```py\n query = Query.from_str(\"I am Paul Iusztin. Write an article about the best types of advanced RAG methods.\")\n    self_query = SelfQuery()\n    query = self_query.generate(query)\n    logger.info(f\"Extracted author_id: {query.author_id}\")\n    logger.info(f\"Extracted author_full_name: {query.author_full_name}\") \n```", "```py\n2024-09-18 18:02:10.362 | INFO - Extracted author_id: 900fec95-d621-4315-84c6-52e5229e0b96\n2024-09-18 18:02:10.362 | INFO - Extracted author_full_name: Paul Iusztin \n```", "```py\nfrom qdrant_client.models import FieldCondition, Filter, MatchValue\nrecords = qdrant_connection.search(\n            collection_name=\"articles\",\n            query_vector=query_embedding,\n            limit=3,\n            with_payload=True,\n            query_filter= Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"author_id\",\n                            match=MatchValue(\n                                value=str(\"1234\"),\n                            ),\n                        )\n                    ]\n                ),\n        ) \n```", "```py\nfrom llm_engineering.application.networks import CrossEncoderModelSingleton\nfrom llm_engineering.domain.embedded_chunks import EmbeddedChunk\nfrom llm_engineering.domain.queries import Query\nfrom .base import RAGStep \n```", "```py\nclass Reranker(RAGStep):\n    def __init__(self, mock: bool = False) -> None:\n        super().__init__(mock=mock)\n        self._model = CrossEncoderModelSingleton() \n```", "```py\n def generate(self, query: Query, chunks: list[EmbeddedChunk], keep_top_k: int) -> list[EmbeddedChunk]:\n        if self._mock:\n            return chunks\n        query_doc_tuples = [(query.content, chunk.content) for chunk in chunks]\n        scores = self._model(query_doc_tuples)\n        scored_query_doc_tuples = list(zip(scores, chunks, strict=False))\n        scored_query_doc_tuples.sort(key=lambda x: x[0], reverse=True)\n        reranked_documents = scored_query_doc_tuples[:keep_top_k]\n        reranked_documents = [doc for _, doc in reranked_documents]\n        return reranked_documents \n```", "```py\nfrom sentence_transformers.cross_encoder import CrossEncoder\nfrom .base import SingletonMeta \n```", "```py\nclass CrossEncoderModelSingleton(metaclass=SingletonMeta):\n    def __init__(\n        self,\n        model_id: str = settings.RERANKING_CROSS_ENCODER_MODEL_ID,\n        device: str = settings.RAG_MODEL_DEVICE,\n    ) -> None:\n        \"\"\"\n        A singleton class that provides a pre-trained cross-encoder model for scoring pairs of input text.\n        \"\"\"\n        self._model_id = model_id\n        self._device = device\n        self._model = CrossEncoder(\n            model_name=self._model_id,\n            device=self._device,\n        )\n        self._model.model.eval() \n```", "```py\n def __call__(self, pairs: list[tuple[str, str]], to_list: bool = True) -> NDArray[np.float32] | list[float]:\n        scores = self._model.predict(pairs)\n        if to_list:\n            scores = scores.tolist()\n        return scores \n```", "```py\nclass ContextRetriever:\n    def __init__(self, mock: bool = False) -> None:\n        self._query_expander = QueryExpansion(mock=mock)\n        self._metadata_extractor = SelfQuery(mock=mock)\n        self._reranker = Reranker(mock=mock) \n```", "```py\n def search(\n        self,\n        query: str,\n        k: int = 3,\n        expand_to_n_queries: int = 3,\n    ) -> list:\n        query_model = Query.from_str(query)\n        query_model = self._metadata_extractor.generate(query_model)\n        logger.info(\n            \"Successfully extracted the author_id from the query.\",\n            author_id=query_model.author_id,\n        ) \n```", "```py\n n_generated_queries = self._query_expander.generate(query_model, expand_to_n=expand_to_n_queries)\n        logger.info(\n            \"Successfully generated queries for search.\",\n            num_queries=len(n_generated_queries),\n        ) \n```", "```py\n with concurrent.futures.ThreadPoolExecutor() as executor:\n            search_tasks = [executor.submit(self._search, _query_model, k) for _query_model in n_generated_queries]\n            n_k_documents = [task.result() for task in concurrent.futures.as_completed(search_tasks)]\n            n_k_documents = utils.misc.flatten(n_k_documents)\n            n_k_documents = list(set(n_k_documents))\n        logger.info(\"All documents retrieved successfully.\", num_documents=len(n_k_documents)) \n```", "```py\n if len(n_k_documents) > 0:\n            k_documents = self.rerank(query, chunks=n_k_documents, keep_top_k=k)\n        else:\n            k_documents = []\n        return k_documents \n```", "```py\n def _search(self, query: Query, k: int = 3) -> list[EmbeddedChunk]:\n        assert k >= 3, \"k should be >= 3\"\n        def _search_data_category(\n            data_category_odm: type[EmbeddedChunk], embedded_query: EmbeddedQuery\n        ) -> list[EmbeddedChunk]:\n            if embedded_query.author_id:\n                query_filter = Filter(\n                    must=[\n                        FieldCondition(\n                            key=\"author_id\",\n                            match=MatchValue(\n                                value=str(embedded_query.author_id),\n                            ),\n                        )\n                    ]\n                )\n            else:\n                query_filter = None\n            return data_category_odm.search(\n                query_vector=embedded_query.embedding,\n                limit=k // 3,\n                query_filter=query_filter,\n            )\n        embedded_query: EmbeddedQuery = EmbeddingDispatcher.dispatch(query) \n```", "```py\n post_chunks = _search_data_category(EmbeddedPostChunk, embedded_query)\n        articles_chunks = _search_data_category(EmbeddedArticleChunk, embedded_query)\n        repositories_chunks = _search_data_category(EmbeddedRepositoryChunk, embedded_query)\n        retrieved_chunks = post_chunks + articles_chunks + repositories_chunks\n        return retrieved_chunks \n```", "```py\n def rerank(self, query: str | Query, chunks: list[EmbeddedChunk], keep_top_k: int) -> list[EmbeddedChunk]:\n        if isinstance(query, str):\n            query = Query.from_str(query)\n        reranked_documents = self._reranker.generate(query=query, chunks=chunks, keep_top_k=keep_top_k)\n        logger.info(\"Documents reranked successfully.\", num_documents=len(reranked_documents))\n        return reranked_documents \nsearch() method:\n```", "```py\nfrom loguru import logger\nfrom llm_engineering.application.rag.retriever import ContextRetriever\nquery = \"\"\"\n        My name is Paul Iusztin.\n\n        Could you draft a LinkedIn post discussing RAG systems?\n        I'm particularly interested in:\n            - how RAG works\n            - how it is integrated with vector DBs and large language models (LLMs).\n        \"\"\"\nretriever = ContextRetriever(mock=False)\ndocuments = retriever.search(query, k=3)\nlogger.info(\"Retrieved documents:\")\nfor rank, document in enumerate(documents):\n    logger.info(f\"{rank + 1}: {document}\") \n```", "```py\n2024-09-18 19:01:50.588 | INFO - Retrieved documents:\n2024-09-18 19:01:50.588 | INFO - 1: id=UUID('541d6c22-d15a-4e6a-924a-68b7b1e0a330') content='4 Advanced RAG Algorithms You Must Know by Paul Iusztin Implement 4 advanced RAG retrieval techniques to optimize your vector DB searches. Integrate the RAG retrieval module into a production LLM system…\" platform='decodingml.substack.com' document_id=UUID('32648f33-87e6-435c-b2d7-861a03e72392') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://decodingml.substack.com/p/the-4-advanced-rag-algorithms-you?r=1ttoeh'\n2024-09-18 19:01:50.588 | INFO - 2: id=UUID('5ce78438-1314-4874-8a5a-04f5fcf0cb21') content='Overview of advanced RAG optimization techniquesA production RAG system is split into 3 main components ingestion clean, chunk, embed, and load your data to a vector DBretrieval query your vector DB for …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium.com/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2'\n2024-09-18 19:02:45.729 | INFO  - 3: id=UUID('0405a5da-4686-428a-91ca-446b8e0446ff') content='Every Medium article will be its own lesson An End to End Framework for Production Ready LLM Systems by Building Your LLM TwinThe Importance of Data Pipelines in the Era of Generative AIChange Data Capture Enabling Event Driven …\" platform='medium' document_id=UUID('bd9021c9-a693-46da-97e7-0d06760ee6bf') author_id=UUID('900fec95-d621-4315-84c6-52e5229e0b96') author_full_name='Paul Iusztin' metadata={'embedding_model_id': 'sentence-transformers/all-MiniLM-L6-v2', 'embedding_size': 384, 'max_input_length': 256} link='https://medium.\ncom/decodingml/the-4-advanced-rag-algorithms-you-must-know-to-implement-5d0c7f1199d2' \n```", "```py\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=None\n    )\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer \n```", "```py\nprompt = f\"\"\"\nYou are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\nUser query: {query}\nContext: {context}\n          \"\"\" \n```", "```py\ndef rag(query: str) -> str:\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer \n```", "```py\nfrom superlinked.framework.common.schema.id_schema_object import IdField\nfrom superlinked.framework.common.schema.schema import schema\nfrom superlinked.framework.common.schema.schema_object import String\n… # Other Superlinked imports. \n@schema\nclass ArticleSchema:\n    id: IdField\n    platform: String\n    content: String\narticle = ArticleSchema()\narticles_space_content = TextSimilaritySpace(\n    text=chunk(article.content, chunk_size=500, chunk_overlap=50),\n    model=settings.EMBEDDING_MODEL_ID,\n)\narticles_space_plaform = CategoricalSimilaritySpace(\n    category_input=article.platform,\n    categories=[\"medium\", \"substack\", \"wordpress\"],\n    negative_filter=-5.0,\n)\narticle_index = Index(\n    [articles_space_content, articles_space_plaform],\n    fields=[article.author_id],\n) \n```"]