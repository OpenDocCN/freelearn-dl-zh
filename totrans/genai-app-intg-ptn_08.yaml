- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: 'Integration Pattern: Real-Time Retrieval Augmented Generation'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成模式：实时检索增强生成
- en: In this chapter, we’ll explore another integration pattern that combines the
    power of **Retrieval Augmented Generation** (**RAG**) and generative AI models
    to build a chatbot capable of answering questions based on the content of PDF
    files. This approach combines the strengths of both retrieval systems and generative
    models, allowing us to leverage existing knowledge sources while generating relevant
    and contextual responses.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨另一种集成模式，该模式结合了**检索增强生成**（**RAG**）和生成式AI模型的力量，以构建一个能够根据PDF文件内容回答问题的聊天机器人。这种方法结合了检索系统和生成模型的优势，使我们能够利用现有的知识源，同时生成相关和上下文化的响应。
- en: One of the key advantages of the RAG approach is its ability to prevent hallucinations
    and provide better context for the generated responses. Generative AI models,
    trained on broad data, can sometimes produce responses that are factually incorrect
    or outdated due to their training data being limited to up to a point in time
    or they might lack proper context at inference time. By grounding the model’s
    generation process in relevant information retrieved from a document corpus, the
    RAG approach mitigates the risk of hallucinations and ensures that the responses
    are accurate and contextually relevant.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: RAG方法的一个关键优势是其防止幻觉并提高生成响应上下文的能力。基于广泛数据训练的生成式AI模型有时可能会产生事实错误或过时的响应，因为它们的训练数据仅限于某个时间点或它们在推理时可能缺乏适当的上下文。通过将模型的生成过程建立在从文档语料库检索到的相关信息之上，RAG方法减轻了幻觉的风险，并确保响应准确且上下文相关。
- en: For example, the term *refund* can have different meanings and implications
    in different contexts. A refund in the context of retail banking may refer to
    a customer requesting a refund for a fee or charge, while a refund in the context
    of taxation may refer to a tax refund from the government. By retrieving the relevant
    context from the document corpus, the RAG-powered chatbot can generate responses
    that accurately reflect the intended meaning of *refund* based on the specific
    context of the user’s query.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，术语*退款*在不同的上下文中可能有不同的含义和影响。在零售银行业的上下文中，退款可能指客户请求退还费用或收费，而在税收的上下文中，退款可能指政府提供的税收退款。通过从文档语料库中检索相关上下文，RAG驱动的聊天机器人可以生成准确反映*退款*意图含义的响应，基于用户查询的具体上下文。
- en: 'The following image illustrates a simple RAG pipeline:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了简单的RAG流程：
- en: '![](img/B22175_08_01.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22175_08_01.png)'
- en: 'Figure 8.1: A simple RAG pipeline'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：简单的RAG流程
- en: Continuing our examples regarding financial services, these companies often
    deal with a vast amount of documentation, including legal contracts, regulatory
    filings, product disclosures, and internal policies and procedures. These document
    repositories can easily run into the tens of thousands or even hundreds of thousands
    of pages, making it challenging for employees and customers to quickly find relevant
    information when needed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们关于金融服务公司的例子，这些公司经常处理大量的文件，包括法律合同、监管文件、产品披露以及内部政策和程序。这些文档存储库可能包含数万甚至数十万页，这使得员工和客户在需要时难以快速找到相关信息。
- en: By implementing a RAG-based chatbot system, financial services companies can
    provide a user-friendly interface for employees, customers, and other stakeholders
    to ask natural language questions and receive concise, relevant answers derived
    from the vast collection of documents. The RAG approach allows the system to efficiently
    retrieve relevant information from the document corpus and then generate contextualized
    responses using a powerful generative AI model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施基于RAG的聊天机器人系统，金融服务公司可以为员工、客户和其他利益相关者提供一个用户友好的界面，让他们能够提出自然语言问题并接收来自大量文档的简洁、相关的答案。RAG方法允许系统从文档语料库中高效地检索相关信息，然后使用强大的生成式AI模型生成上下文化的响应。
- en: For example, a customer service representative could ask the chatbot a question
    about a specific clause in a loan agreement, and the system would retrieve the
    relevant section from the document corpus and generate a concise explanation tailored
    to the user’s query. Similarly, an investment advisor could ask about specific
    regulations or guidelines related to a financial product, and the chatbot would
    provide the necessary information from the relevant documents.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，客户服务代表可以向聊天机器人询问关于贷款协议中特定条款的问题，系统将检索文档语料库中的相关部分，并生成针对用户查询的简洁解释。同样，投资顾问可以询问与金融产品相关的特定法规或指南，聊天机器人将提供相关文档中的必要信息。
- en: By leveraging the RAG approach, financial services companies can greatly improve
    the accessibility and usability of their document repositories, enabling faster
    and more accurate information retrieval and reducing the time and effort required
    to manually search through thousands of pages of documentation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 RAG 方法，金融服务公司可以极大地提高其文档存储库的可访问性和可用性，实现更快、更准确的信息检索，并减少手动搜索数千页文档所需的时间和精力。
- en: 'In this chapter, we are going to cover:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Use case definition (for a financial services company)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例定义（针对金融服务公司）
- en: 'Architecture (overview of a RAG-based chatbot system):'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构（基于 RAG 的聊天机器人系统概述）：
- en: Ingestion layer
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取层
- en: Document corpus management
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档语料库管理
- en: AI processing layer
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI 处理层
- en: Monitoring and logging
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: Entry point (a design for handling various input modalities)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入口点（处理各种输入模式的设计）
- en: Prompt pre-processing and vector database integration
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示预处理和向量数据库集成
- en: Inference process using Vertex AI’s Gemini 1.5 Flash model
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Vertex AI 的 Gemini 1.5 Flash 模型进行推理过程
- en: Result post-processing and presentation using Markdown
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Markdown 进行结果后处理和展示
- en: A demo implementation (using Gradio)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示实现（使用 Gradio）
- en: The full code example of the RAG pipeline
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG 流程的完整代码示例
- en: Use case definition
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例定义
- en: Let’s consider a scenario where we’re working with a large financial institution
    that deals with a vast number of legal contracts, regulatory filings, product
    disclosures, and internal policies and procedures. These documents individually
    can be into the tens or even hundreds of pages, making it challenging for employees,
    customers, and other stakeholders to quickly find relevant information when needed.
    These documents also do not have a consistent format in the way the information
    is reported, disqualifying non-AI-powered text extractor solutions like regex
    statements or plain business rules.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，我们正在与一家处理大量法律合同、监管文件、产品披露、内部政策和程序的大型金融机构合作。这些文件单独可能长达数十页甚至数百页，使得员工、客户和其他利益相关者在需要时难以快速找到相关信息。这些文件在信息报告的方式上也没有一致的格式，这使得像正则表达式语句或普通业务规则这样的非
    AI 驱动的文本提取解决方案无效。
- en: The institution wants to implement a chatbot system that can provide a user-friendly
    interface for users to ask natural language questions and receive concise, relevant
    answers derived from the organization’s document corpus. This system should leverage
    the power of RAG to ensure that the generated responses are accurate, contextual,
    and grounded in the relevant information from the document corpus.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该机构希望实施一个聊天机器人系统，可以为用户提供友好的界面，让他们提出自然语言问题，并接收来自组织文档语料库的简洁、相关答案。该系统应利用 RAG 的力量，确保生成的回答准确、上下文相关，并基于文档语料库中的相关信息。
- en: Architecture
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建筑学
- en: 'To build our RAG-based chatbot system, we’ll leverage a serverless, event-driven
    architecture built on Google Cloud. This approach aligns with the cloud-native
    principles we have used in previous examples and allows for seamless integration
    with other cloud services. You can dive deep into a Google Cloud example in this
    sample architecture: [https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai](https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的基于 RAG 的聊天机器人系统，我们将利用建立在 Google Cloud 之上的无服务器、事件驱动的架构。这种方法与我们之前示例中使用的云原生原则相一致，并允许与其他云服务无缝集成。您可以在以下示例架构中深入了解
    Google Cloud 的示例：[https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai](https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai)。
- en: 'For the purpose of this example, the architecture consists of the following
    key components:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，架构包括以下关键组件：
- en: '**Ingestion layer**: This layer is responsible for accepting incoming user
    queries from various channels, such as web forms, chat interfaces, or API endpoints.
    We’ll use Google Cloud Functions as the entry point for our system, which can
    be triggered by events from services like Cloud Storage, Pub/Sub, or Cloud Run.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄取层**：这一层负责接受来自各种渠道的用户查询，例如网页表单、聊天界面或API端点。我们将使用Google Cloud Functions作为我们系统的入口点，它可以由云存储、Pub/Sub或Cloud
    Run等服务的事件触发。'
- en: '**Document corpus management**: In this layer, we’ll store embeddings representing
    the content of the documents. In this case, we can use a wide range of solutions
    from purpose-built vector databases such as Chroma DB, Pinecone, or Weaviate,
    to well-known industry standards such as Elastic, MongoDB, Redis, or even databases
    known for other capabilities such as PostgreSQL, SingleStore, Google AlloyDB,
    or Google BigQuery.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档语料库管理**：在这一层，我们将存储代表文档内容的嵌入表示。在这种情况下，我们可以使用从专门构建的向量数据库（如Chroma DB、Pinecone或Weaviate）到知名行业标准（如Elastic、MongoDB、Redis）的各种解决方案，甚至包括以其他能力著称的数据库，如PostgreSQL、SingleStore、Google
    AlloyDB或Google BigQuery。'
- en: '**AI processing layer**: In this layer, we’ll integrate Google Gemini through
    Vertex AI. Once the results are retrieved from the vector database, they will
    be exposed to Google Gemini as context along with the prompt. This process can
    be handled by a Cloud function.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI处理层**：在这一层，我们将通过Vertex AI集成Google Gemini。一旦从向量数据库检索到结果，它们将作为上下文与提示一起暴露给Google
    Gemini。这个过程可以通过云函数处理。'
- en: '**Monitoring and logging**: To ensure the reliability and performance of our
    system, you should implement robust monitoring and logging mechanisms. We’ll leverage
    services like Cloud Logging, Cloud Monitoring, and Cloud operations to gain visibility
    into our system’s behavior and quickly identify and resolve any issues.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和日志记录**：为确保系统的可靠性和性能，您应实施强大的监控和日志记录机制。我们将利用云日志、云监控和云操作等服务来了解系统行为，并快速识别和解决任何问题。'
- en: Entry point
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入口点
- en: The entry point for a RAG-based chatbot system is designed to be user-friendly,
    allowing users to submit their natural language queries through various interfaces,
    such as web forms, chat applications, or API endpoints. However, the entry point
    should not be limited to accepting text-based inputs only; it should also handle
    different modalities, such as audio files or images, depending on the capabilities
    of the underlying language model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于RAG的聊天机器人系统，入口点被设计成用户友好，允许用户通过各种接口提交他们的自然语言查询，例如网页表单、聊天应用或API端点。然而，入口点不应仅限于接受基于文本的输入；它还应处理不同的模态，如音频文件或图像，具体取决于底层语言模型的能力。
- en: In the case of models like Google Gemini (which support multimodal inputs),
    the entry point can directly accept and process text, audio, images, or even videos.
    This versatility enables users to interact with the chatbot system in a more natural
    and intuitive manner, aligning with the way humans communicate in real-world scenarios.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像Google Gemini（支持多模态输入）这样的模型，入口点可以直接接受和处理文本、音频、图像，甚至视频。这种多功能性使用户能够以更自然和直观的方式与聊天机器人系统交互，与人类在现实世界场景中的沟通方式相一致。
- en: In cases where the language model does not natively support multimodal inputs,
    the entry point can still accommodate various input modalities by pre-processing
    the data and extracting the textual content. This approach ensures that the chatbot
    system remains accessible and user-friendly, catering to a diverse range of input
    formats while leveraging the capabilities of the underlying language model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型不原生支持多模态输入的情况下，入口点仍然可以通过预处理数据和提取文本内容来适应各种输入模态。这种方法确保了聊天机器人系统易于访问和用户友好，同时利用底层语言模型的能力，满足多样化的输入格式。
- en: For text inputs, the entry point can simply pass the query directly to the subsequent
    phases of the RAG pipeline. However, when dealing with audio or image inputs,
    the entry point needs to perform additional processing to extract the textual
    content from these modalities.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本输入，入口点可以直接将查询传递给RAG管道的后续阶段。然而，当处理音频或图像输入时，入口点需要执行额外的处理来从这些模态中提取文本内容。
- en: For audio inputs, the entry point can leverage speech recognition technologies,
    such as Google Chirp, Amazon Transcribe, OpenAI Whisper, or open-source libraries
    like CMU Sphinx, to transcribe the audio data into text format. This process involves
    converting the audio signals into a sequence of words or phrases that can be understood
    by the language model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于音频输入，入口点可以利用语音识别技术，如Google Chirp、Amazon Transcribe、OpenAI Whisper或CMU Sphinx等开源库，将音频数据转录成文本格式。这个过程涉及将音频信号转换为语言模型可以理解的单词或短语序列。
- en: Similarly, for image inputs, the entry point can employ **optical character
    recognition** (**OCR**) techniques to extract text from the provided images. This
    can be achieved by integrating with services like Google Cloud Vision API, Amazon
    Textract, or open-source tools like Tesseract OCR. These technologies leverage
    computer vision and machine learning algorithms to accurately identify and extract
    textual content from images, enabling the chatbot system to understand and process
    information presented in visual form.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于图像输入，入口点可以采用**光学字符识别**（**OCR**）技术从提供的图像中提取文本。这可以通过集成Google Cloud Vision
    API、Amazon Textract或Tesseract OCR等开源工具来实现。这些技术利用计算机视觉和机器学习算法来准确识别和从图像中提取文本内容，使聊天机器人系统能够理解和处理以视觉形式呈现的信息。
- en: 'In this example, we will leverage text; the Python code will look like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将利用文本；Python代码将如下所示：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Regardless of the input type, the entry point should be designed to handle a
    wide range of scenarios and input formats. It may need to perform additional pre-processing
    steps, such as noise removal, format conversion, or data cleaning, to ensure that
    the input data is in a suitable format for the subsequent phases of the RAG pipeline.
    It is also best practice to run the raw request through a rigorous security monitoring
    pipeline to prevent data leakage or model intoxication such as prompt ingestion.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 无论输入类型如何，入口点都应该设计成能够处理各种场景和输入格式。它可能需要执行额外的预处理步骤，例如噪声去除、格式转换或数据清理，以确保输入数据适合RAG管道后续阶段的格式。此外，运行原始请求通过严格的安全监控管道也是最佳实践，以防止数据泄露或模型中毒，如提示摄入。
- en: 'The following website presents a very interesting point of view regarding the
    challenges posed by prompt injection in multimodal scenarios: [https://protectai.com/blog/hiding-in-plain-sight-prompt](https://protectai.com/blog/hiding-in-plain-sight-prompt).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下网站提出了关于多模态场景中提示注入带来的挑战的非常有趣的观点：[https://protectai.com/blog/hiding-in-plain-sight-prompt](https://protectai.com/blog/hiding-in-plain-sight-prompt)。
- en: Prompt pre-processing
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示预处理
- en: For our example, we will need to construct our prompt in real time along with
    its necessary context and instructions. In this step of our RAG pipeline, we will
    utilize a vector database for efficient vector similarity search. Vector databases
    are specialized data stores designed to store and retrieve high-dimensional vectors,
    enabling fast and accurate similarity searches. Although there are numerous vector
    database providers available, we will use Chroma DB for this specific example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，我们需要实时构建提示及其必要的上下文和指令。在我们RAG管道的这个步骤中，我们将利用向量数据库进行高效的向量相似度搜索。向量数据库是专门的数据存储，用于存储和检索高维向量，能够实现快速和准确的相似度搜索。尽管有众多向量数据库提供商，但我们将在这个特定例子中使用Chroma
    DB。
- en: The retrieval process in a RAG pipeline is relatively straightforward. First,
    we generate embeddings from the user’s query using a pretrained language model
    or embedding technique. These embeddings are numerical representations of the
    query that capture its semantic meaning. Next, we perform a similarity search
    on the vector database using the generated query embeddings. The vector database
    will return vectors that are most similar to the query embeddings, along with
    their corresponding textual information or context. These vectors are associated
    with previously ingested text passages.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG管道中的检索过程相对简单。首先，我们使用预训练的语言模型或嵌入技术从用户的查询中生成嵌入。这些嵌入是查询的数值表示，能够捕捉其语义意义。接下来，我们使用生成的查询嵌入在向量数据库上执行相似度搜索。向量数据库将返回与查询嵌入最相似的向量，以及它们对应的文本信息或上下文。这些向量与之前摄取的文本段落相关联。
- en: Different filtering strategies can be applied to refine the search results further.
    The specific parameters and techniques available may vary depending on the vector
    database provider being used. For instance, some vector databases provide a similarity
    score that measures the closeness between the query embeddings and the retrieved
    vectors. This score can be leveraged to identify and filter out vectors that fall
    below a certain similarity threshold.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可以应用不同的过滤策略来进一步细化搜索结果。具体参数和技术可能因所使用的向量数据库提供商而异。例如，一些向量数据库提供相似度分数，该分数衡量查询嵌入与检索向量之间的接近程度。可以利用此分数来识别和过滤掉低于一定相似度阈值的向量。
- en: Another common filtering strategy is to limit the number of results obtained
    from the vector database. This approach can be particularly useful when there
    are constraints on the maximum number of tokens that can be passed to the language
    model, either due to token limits imposed by the model or for cost optimization
    purposes. By limiting the number of results, we can control the amount of context
    information provided to the language model, ensuring efficient processing and
    cost-effective operation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的过滤策略是限制从向量数据库中获取的结果数量。这种方法在存在对可以传递给语言模型的标记最大数量的限制时特别有用，无论是由于模型施加的标记限制还是为了成本优化目的。通过限制结果数量，我们可以控制提供给语言模型的上下文信息量，确保高效处理和成本效益的运行。
- en: 'Once the results are filtered and the relevant textual information is obtained,
    it is used to construct the prompt that will be passed to the language model.
    In this example, we use the following prompt template:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦结果被过滤，并获取到相关的文本信息，它就被用来构建将传递给语言模型的提示。在这个例子中，我们使用以下提示模板：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In our example, the prompt we are submitting to the LLM would look like:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们提交给LLM的提示将如下所示：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Inference
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `generate()` function encapsulates the configuration and settings required
    for the generation process. It includes two main components: `generation_config`
    and `safety_settings`.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate()`函数封装了生成过程所需的配置和设置。它包括两个主要组件：`generation_config`和`safety_settings`。'
- en: 'The `generation_config` dictionary specifies the parameters that control the
    behavior of the language model during the generation process. In this example,
    the following settings are provided:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`generation_config`字典指定了在生成过程中控制语言模型行为的参数。在这个例子中，提供了以下设置：'
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'From Google Gemini’s documentation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Google Gemini的文档：
- en: '`max_output_tokens`: Maximum number of tokens that can be generated in the
    response. A token is approximately four characters. 100 tokens correspond to roughly
    60–80 words.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_output_tokens`: 响应中可以生成的最大标记数。一个标记大约是四个字符。100个标记相当于大约60-80个单词。'
- en: '`temperature`: The temperature is used for sampling during response generation,
    which occurs when `top_p` and `top_k` are applied. `temperature` controls the
    degree of randomness in token selection. Lower temperature values are good for
    prompts that require a less open-ended or creative response, while higher temperature
    values can lead to more diverse or creative results. A temperature of 0 means
    that the highest probability tokens are always selected. In this case, responses
    for a given prompt are mostly deterministic, but a small amount of variation is
    still possible.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature`: 温度用于在响应生成过程中进行采样，这发生在应用`top_p`和`top_k`时。`temperature`控制了标记选择的随机程度。较低的温度值适用于需要较少开放或创造性的响应的提示，而较高的温度值可能导致更多样化或创造性的结果。温度为0表示始终选择概率最高的标记。在这种情况下，对于给定的提示，响应大多是确定性的，但仍可能存在少量变化。'
- en: A temperature of 0 means the model will choose the most likely token based on
    its training data, while higher values introduce more randomness and diversity
    in the output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 温度为0表示模型将根据其训练数据选择最可能的标记，而较高的值会在输出中引入更多的随机性和多样性。
- en: '`top_p`: This parameter changes how the model selects tokens for output. Tokens
    are selected from the most to least probable until the sum of their probabilities
    equals the top-p value. For example, if tokens A, B, and C have a probability
    of 0.3, 0.2, and 0.1, respectively, and the top-p value is 0.5, then the model
    will select either A or B as the next token by using temperature and will exclude
    C as a candidate.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p`: 此参数改变模型选择输出标记的方式。标记从最有可能到最不可能依次选择，直到这些标记的概率总和等于top-p值。例如，如果标记A、B和C分别具有0.3、0.2和0.1的概率，且top-p值为0.5，那么模型将通过温度选择A或B作为下一个标记，并将C排除作为候选。'
- en: In this case, it is set to 0.95, meaning that only the top 95% of tokens with
    the highest probabilities will be considered during generation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，它被设置为0.95，这意味着在生成过程中只考虑概率最高的前95%的标记。
- en: 'Beyond the above, the `safety_settings` dictionary specifies the harm categories
    and corresponding thresholds for filtering potentially harmful or inappropriate
    content from the generated output. In this example, the following settings are
    provided:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述内容之外，`safety_settings`字典指定了危害类别及其对应的阈值，用于从生成的输出中过滤可能有害或不适当的内容。在此示例中，提供了以下设置：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These settings instruct the Gemini 1.5 Flash model to block only highly harmful
    content related to hate speech, dangerous content, sexually explicit content,
    and harassment. Any content that falls below the “high” harm threshold for these
    categories will be allowed in the generated output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置指示Gemini 1.5 Flash模型仅阻止与仇恨言论、危险内容、色情内容和骚扰高度相关的有害内容。任何低于这些类别“高”危害阈值的任何内容都将允许在生成的输出中出现。
- en: 'The `generate()` function creates an instance of the `GenerativeModel` class,
    passing the `MODEL` parameter; in this example, Gemini 1.5 Flash. It then calls
    the `generate_content()` method on the model instance, providing the prompt, generation
    configuration, and safety settings. The `stream=False` parameter indicates that
    the generation should happen in a non-streaming mode, meaning the entire response
    will be generated and returned at once:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`generate()`函数创建了一个`GenerativeModel`类的实例，传递了`MODEL`参数；在此示例中，为Gemini 1.5 Flash。然后它调用模型实例上的`generate_content()`方法，提供提示、生成配置和安全设置。`stream=False`参数表示生成应在非流模式下进行，这意味着整个响应将一次性生成并返回：'
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The generated response is stored in the `responses` variable, which is then
    returned by the `generate()` function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的响应存储在`responses`变量中，然后通过`generate()`函数返回。
- en: By submitting the formatted prompt to Vertex AI’s API endpoint for Gemini 1.5
    Flash, leveraging the provided generation configuration and safety settings, this
    RAG pipeline can obtain a contextualized and relevant response tailored to the
    user’s query while adhering to the specified parameters and content filtering
    rules.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将格式化的提示提交给Vertex AI的Gemini 1.5 Flash API端点，利用提供的生成配置和安全设置，此RAG管道可以获取一个针对用户查询的上下文化和相关响应，同时遵守指定的参数和内容过滤规则。
- en: Result post-processing
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果后处理
- en: 'After receiving the response from the language model, it is often desirable
    to present the output in a more structured and visually appealing format. Markdown
    is a lightweight markup language that allows you to add formatting elements such
    as headings, lists, code blocks, and more. In this example, we use Markdown formatting
    to enhance the presentation of the question, answer, and context:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在收到语言模型的响应后，通常希望以更结构化和视觉上吸引人的格式展示输出。Markdown是一种轻量级标记语言，允许你添加格式化元素，如标题、列表、代码块等。在此示例中，我们使用Markdown格式化来增强问题、答案和上下文的展示：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The breakdown of the components of this formatting is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此格式化组件的分解如下：
- en: '`"###Question:\n{question}"`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"###问题:\n{question}"`'
- en: This part adds a level 3 Markdown heading (`###`) for the `Question` section,
    followed by the user’s original query `({question}`) on a new line (`\n`).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分为`问题`部分添加了一个3级Markdown标题（`###`），随后是新的一行上用户的原始查询（`({question}`）。
- en: '`"\n\n###Answer:\n{result.text}"`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"\n\n###答案:\n{result.text}"`'
- en: After adding an empty line (`\n\n`) at the beginning of the section, this section
    creates another level 3 Markdown heading for the `Answer` section, followed by
    the generated response from the language model (`{result.text}`) on a new line.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在该部分开头添加一个空行（`\n\n`）之后，此部分为`答案`部分创建了一个新的3级Markdown标题，随后是新的一行上语言模型生成的响应（`{result.text}`）。
- en: '`"\n\n<details><summary>Context</summary>{context}</details>"`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"\n\n<details><summary>上下文</summary>{context}</details>"`'
- en: This part utilizes the Markdown `<details>` and `<summary>` tags to create a
    collapsible section for displaying the context information retrieved from the
    vector database. The `<summary>Context</summary>` text serves as the label for
    the collapsible section, and the actual context text (`{context}`) is enclosed
    within the `<details>` tags.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分利用Markdown `<details>` 和 `<summary>` 标签创建一个可折叠的显示从向量数据库检索到的上下文信息的部分。`<summary>上下文</summary>`
    文本作为可折叠部分的标签，实际上下文文本（`{context}`）被包含在 `<details>` 标签内。
- en: Result presentation
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果展示
- en: 'The `Markdown` class from the `IPython.display` module is a utility that allows
    you to display formatted Markdown content in a Colab notebook or other IPython
    environments. By passing the `formatted_result` string to the Markdown constructor,
    you create a Markdown object that can be rendered by the `display` function:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`IPython.display`模块中的`Markdown`类是一个实用工具，允许你在Colab笔记本或其他IPython环境中显示格式化的Markdown内容。通过将`formatted_result`字符串传递给Markdown构造函数，你创建了一个Markdown对象，该对象可以通过`display`函数进行渲染：'
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When you call `display()`, the notebook will render the Markdown-formatted content
    contained in the `formatted_result` string. This allows you to leverage the rich
    formatting capabilities of Markdown within the notebook environment.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用`display()`时，笔记本将渲染`formatted_result`字符串中包含的Markdown格式化内容。这允许你在笔记本环境中利用Markdown的丰富格式化功能。
- en: 'The following is an example of the Markdown formatted output of our demo:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们演示的Markdown格式化输出示例：
- en: '![](img/B22175_08_02.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22175_08_02.png)'
- en: 'Figure 8.2: Screenshot from the formatted result in the Google Colab notebook'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：Google Colab笔记本中格式化结果截图
- en: By using the `Markdown` class and the `display` function, you can take advantage
    of Markdown’s formatting capabilities within the Google Colab notebook environment.
    This includes features like headings, bold and italic text, lists, code blocks,
    links, and more.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`Markdown`类和`display`函数，你可以在Google Colab笔记本环境中利用Markdown的格式化功能。这包括标题、粗体和斜体文本、列表、代码块、链接等功能。
- en: The rendered output will be displayed in the notebook cell, providing a visually
    appealing and well-structured representation of the question, answer, and context
    information. This can greatly enhance the readability and usability of the chatbot’s
    responses, making it easier for users or developers to understand and interpret
    the results.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染的输出将在笔记本单元格中显示，提供对问题、答案和上下文信息的视觉吸引力和良好结构化的表示。这可以极大地提高聊天机器人响应的可读性和可用性，使用户或开发者更容易理解和解释结果。
- en: Additionally, the `<details>` and `<summary>` tags used in the `context` section
    create a collapsible section, allowing users to toggle the visibility of the context
    information. This can be particularly helpful when dealing with large amounts
    of context data, as it prevents cluttering the main output while still providing
    easy access to the relevant information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在`context`部分使用的 `<details>` 和 `<summary>` 标签创建了一个可折叠部分，允许用户切换上下文信息的可见性。这在处理大量上下文数据时尤其有用，因为它可以防止主输出混乱，同时仍然提供轻松访问相关信息。
- en: In the next section, we will dive deep into how all these components work together
    through a use case demo.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过用例演示深入探讨所有这些组件是如何协同工作的。
- en: Use case demo
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例演示
- en: 'The following is the code for building a demo using Gradio; in this case, we
    will use an additional function that will perform the RAG pipeline. When you run
    this code, a Gradio interface will open in your default web browser, displaying
    three main sections:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用Gradio构建演示的代码；在这种情况下，我们将使用一个额外的函数来执行RAG管道。当你运行此代码时，默认网络浏览器中会打开一个Gradio界面，显示三个主要部分：
- en: '**Fintech Assistant** heading'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融科技助手**标题'
- en: Chatbot area
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人区域
- en: Text input box
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本输入框
- en: Users can type their questions into the input box and submit them. The chat
    function will be called, which will use the `answer_question` function to retrieve
    the relevant context from the vector database, generate an answer using the RAG
    pipeline, and update the chatbot interface with the user’s question and the generated
    response.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以在输入框中输入问题并提交。聊天功能将被调用，它将使用`answer_question`函数从向量数据库中检索相关上下文，使用RAG管道生成答案，并更新聊天机器人界面，显示用户的问题和生成的响应。
- en: 'The Gradio interface provides a user-friendly way for users to interact with
    the RAG pipeline system, making it easier to test and demonstrate its capabilities.
    Additionally, Gradio offers various customization options and features, such as
    support for different input and output components, styling, and deployment options.
    We start by installing Gradio:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Gradio 接口为用户提供了一种用户友好的方式来与 RAG 管道系统交互，使其更容易测试和展示其功能。此外，Gradio 还提供了各种自定义选项和功能，例如支持不同的输入和输出组件、样式和部署选项。我们首先安装
    Gradio：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we define two helper functions that build upon the previously explained
    `generate()` function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义两个辅助函数，这些函数基于之前解释的 `generate()` 函数：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `answer_question(...)` function takes three arguments:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`answer_question(...)` 函数接受三个参数：'
- en: '`query`: The user’s question'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`query`: 用户的提问'
- en: '`db`: The vector database'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`db`: 向量数据库'
- en: '`number_of_results`: The maximum number of context results to retrieve from
    the database'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`number_of_results`: 从数据库中检索的最大上下文结果数量'
- en: It then calls the `get_context` function (not shown in the provided code) to
    retrieve the relevant context information – from the vector database, based on
    the user’s query and the specified number of results. The retrieved context is
    then formatted within the `prompt_template` string and passed to the `generate`
    function – covered in the previous sections – to obtain the answer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后调用 `get_context` 函数（在提供的代码中未显示）以检索相关的上下文信息——基于用户的查询和指定的结果数量，从向量数据库中获取。检索到的上下文随后在
    `prompt_template` 字符串中格式化，并传递给 `generate` 函数——如前几节所述——以获取答案。
- en: At the end of the function execution, the generated answer is returned as a
    string.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 函数执行结束时，生成的答案作为字符串返回。
- en: 'The `chat(...)` function takes two arguments:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`chat(...)` 函数接受两个参数：'
- en: '`message`: The user’s question'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`message`: 用户的提问'
- en: '`history`: A list representing the conversation history'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`history`: 表示对话历史的一个列表'
- en: It then calls the `answer_question()` function with the user’s question, the
    vector database (`db`), and the maximum number of results (`MAX_RESULTS`).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后调用 `answer_question()` 函数，传入用户的问题、向量数据库（`db`）和最大结果数量（`MAX_RESULTS`）。
- en: The generated response is appended to the history list, along with the user’s
    question. The function returns an empty string and the updated history list, which
    will be used to update the chatbot interface.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的响应附加到历史列表中，包括用户的问题。函数返回一个空字符串和更新后的历史列表，该列表将用于更新聊天机器人界面。
- en: The Gradio app
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gradio 应用程序
- en: 'With the helper functions defined, we can now create the Gradio interface:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了辅助函数后，我们现在可以创建 Gradio 接口：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here’s what’s happening in this code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中发生的事情如下：
- en: '`with gr.Blocks() as demo` creates a Gradio interface block called `demo`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`with gr.Blocks() as demo` 创建一个名为 `demo` 的 Gradio 接口块。'
- en: '`gr.Markdown(...)` displays a Markdown-formatted heading for the chatbot interface.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gr.Markdown(...)` 在聊天机器人界面中显示一个 Markdown 格式的标题。'
- en: '`gr.Chatbot(...)` creates a Gradio chatbot component, which will display the
    conversation history.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gr.Chatbot(...)` 创建一个 Gradio 聊天机器人组件，该组件将显示对话历史。'
- en: '`gr.Textbox(...)` creates a text input box where users can enter their questions.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gr.Textbox(...)` 创建一个文本输入框，用户可以在其中输入问题。'
- en: '`message.submit(...)` sets up an event handler for when the user submits their
    question. It calls the chat function with the user’s input (message) and the chatbot
    instance and updates the message and chatbot components with the returned values.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`message.submit(...)` 设置一个事件处理器，当用户提交问题时会触发。它调用 chat 函数，传入用户输入（消息）和聊天机器人实例，并使用返回的值更新消息和聊天机器人组件。'
- en: '`demo.launch(...)` launches the Gradio interface in debug mode, allowing you
    to interact with the chatbot.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`demo.launch(...)` 以调试模式启动 Gradio 接口，允许您与聊天机器人交互。'
- en: Refer to the GitHub directory of this chapter for the complete code that demonstrates
    how all the pieces described above fit together.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅本章的 GitHub 目录，以获取演示上述所有部分如何组合在一起的完整代码。
- en: Summary
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you’ve explored an integration pattern that combines RAG and
    generative AI models to build a chatbot capable of answering questions based on
    a document corpus. You’ve learned that RAG leverages the strengths of retrieval
    systems and generative models, allowing the system to retrieve relevant context
    from existing knowledge sources and generate contextual responses, preventing
    hallucinations and ensuring accuracy.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你探索了一种集成模式，它结合了RAG和生成AI模型，以构建一个能够根据文档语料库回答问题的聊天机器人。你已经了解到RAG利用了检索系统和生成模型的优势，使系统能够从现有的知识源中检索相关上下文并生成上下文响应，防止幻觉并确保准确性。
- en: We proposed an architecture that utilized a serverless, event-driven approach
    built on Google Cloud. It consists of an ingestion layer for accepting user queries,
    a document corpus management layer for storing embeddings, an AI processing layer
    integrating with Google Gemini on Vertex AI, and monitoring and logging components.
    The entry point handles various input modalities like text, audio, and images,
    pre-processing them as needed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于谷歌云的无服务器、事件驱动的架构。它包括一个用于接受用户查询的摄取层，一个用于存储嵌入的文档语料库管理层，一个与Vertex AI上的谷歌Gemini集成的AI处理层，以及监控和日志组件。入口点处理各种输入模式，如文本、音频和图像，并根据需要预处理它们。
- en: You’ve learned that the core of the RAG pipeline involves generating embeddings
    from the user query, performing a vector similarity search on a vector database
    (in this example, we used Chroma DB), retrieving relevant context, formatting
    it into a prompt with instructions, and submitting it to the Gemini model on Vertex
    AI. The generated response can be post-processed, formatted with Markdown, and
    presented in a user-friendly interface using tools like Gradio.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到RAG管道的核心涉及从用户查询生成嵌入，在向量数据库上执行向量相似性搜索（在这个例子中，我们使用了Chroma DB），检索相关上下文，将其格式化为带有指令的提示，并将其提交给Vertex
    AI上的Gemini模型。生成的响应可以后处理，用Markdown格式化，并使用Gradio等工具在用户友好的界面中展示。
- en: You’ve now gained valuable insights into implementing a powerful RAG-based chatbot
    system. You’ve learned how to combine retrieval and generation techniques to provide
    contextual, hallucination-free responses, leveraging vector databases for semantic
    search through content embeddings. The chapter has equipped you with strategies
    to improve retrieval results and customize user experiences through prompt tuning.
    These skills will enable you to enhance your organization’s ability to provide
    accurate and contextualized responses to natural language queries, effectively
    utilizing existing document repositories and the power of generative AI.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经获得了关于实现强大的基于RAG的聊天机器人系统的宝贵见解。你已经学会了如何结合检索和生成技术，以提供上下文丰富、无幻觉的响应，利用向量数据库通过内容嵌入进行语义搜索。本章为你提供了改进检索结果和通过提示调整定制用户体验的策略。这些技能将使你能够增强你组织提供准确和上下文化响应的能力，有效地利用现有的文档存储库和生成AI的力量。
- en: Join our community on Discord
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/genpat](Chapter_08.xhtml)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/genpat](Chapter_08.xhtml)'
- en: '![](img/QR_Code134841911667913109.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code134841911667913109.png)'
