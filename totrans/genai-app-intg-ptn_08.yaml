- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integration Pattern: Real-Time Retrieval Augmented Generation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore another integration pattern that combines the
    power of **Retrieval Augmented Generation** (**RAG**) and generative AI models
    to build a chatbot capable of answering questions based on the content of PDF
    files. This approach combines the strengths of both retrieval systems and generative
    models, allowing us to leverage existing knowledge sources while generating relevant
    and contextual responses.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of the RAG approach is its ability to prevent hallucinations
    and provide better context for the generated responses. Generative AI models,
    trained on broad data, can sometimes produce responses that are factually incorrect
    or outdated due to their training data being limited to up to a point in time
    or they might lack proper context at inference time. By grounding the model’s
    generation process in relevant information retrieved from a document corpus, the
    RAG approach mitigates the risk of hallucinations and ensures that the responses
    are accurate and contextually relevant.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the term *refund* can have different meanings and implications
    in different contexts. A refund in the context of retail banking may refer to
    a customer requesting a refund for a fee or charge, while a refund in the context
    of taxation may refer to a tax refund from the government. By retrieving the relevant
    context from the document corpus, the RAG-powered chatbot can generate responses
    that accurately reflect the intended meaning of *refund* based on the specific
    context of the user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image illustrates a simple RAG pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: A simple RAG pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Continuing our examples regarding financial services, these companies often
    deal with a vast amount of documentation, including legal contracts, regulatory
    filings, product disclosures, and internal policies and procedures. These document
    repositories can easily run into the tens of thousands or even hundreds of thousands
    of pages, making it challenging for employees and customers to quickly find relevant
    information when needed.
  prefs: []
  type: TYPE_NORMAL
- en: By implementing a RAG-based chatbot system, financial services companies can
    provide a user-friendly interface for employees, customers, and other stakeholders
    to ask natural language questions and receive concise, relevant answers derived
    from the vast collection of documents. The RAG approach allows the system to efficiently
    retrieve relevant information from the document corpus and then generate contextualized
    responses using a powerful generative AI model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a customer service representative could ask the chatbot a question
    about a specific clause in a loan agreement, and the system would retrieve the
    relevant section from the document corpus and generate a concise explanation tailored
    to the user’s query. Similarly, an investment advisor could ask about specific
    regulations or guidelines related to a financial product, and the chatbot would
    provide the necessary information from the relevant documents.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the RAG approach, financial services companies can greatly improve
    the accessibility and usability of their document repositories, enabling faster
    and more accurate information retrieval and reducing the time and effort required
    to manually search through thousands of pages of documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Use case definition (for a financial services company)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Architecture (overview of a RAG-based chatbot system):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingestion layer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document corpus management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AI processing layer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and logging
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Entry point (a design for handling various input modalities)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt pre-processing and vector database integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference process using Vertex AI’s Gemini 1.5 Flash model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result post-processing and presentation using Markdown
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A demo implementation (using Gradio)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full code example of the RAG pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s consider a scenario where we’re working with a large financial institution
    that deals with a vast number of legal contracts, regulatory filings, product
    disclosures, and internal policies and procedures. These documents individually
    can be into the tens or even hundreds of pages, making it challenging for employees,
    customers, and other stakeholders to quickly find relevant information when needed.
    These documents also do not have a consistent format in the way the information
    is reported, disqualifying non-AI-powered text extractor solutions like regex
    statements or plain business rules.
  prefs: []
  type: TYPE_NORMAL
- en: The institution wants to implement a chatbot system that can provide a user-friendly
    interface for users to ask natural language questions and receive concise, relevant
    answers derived from the organization’s document corpus. This system should leverage
    the power of RAG to ensure that the generated responses are accurate, contextual,
    and grounded in the relevant information from the document corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build our RAG-based chatbot system, we’ll leverage a serverless, event-driven
    architecture built on Google Cloud. This approach aligns with the cloud-native
    principles we have used in previous examples and allows for seamless integration
    with other cloud services. You can dive deep into a Google Cloud example in this
    sample architecture: [https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai](https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of this example, the architecture consists of the following
    key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingestion layer**: This layer is responsible for accepting incoming user
    queries from various channels, such as web forms, chat interfaces, or API endpoints.
    We’ll use Google Cloud Functions as the entry point for our system, which can
    be triggered by events from services like Cloud Storage, Pub/Sub, or Cloud Run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document corpus management**: In this layer, we’ll store embeddings representing
    the content of the documents. In this case, we can use a wide range of solutions
    from purpose-built vector databases such as Chroma DB, Pinecone, or Weaviate,
    to well-known industry standards such as Elastic, MongoDB, Redis, or even databases
    known for other capabilities such as PostgreSQL, SingleStore, Google AlloyDB,
    or Google BigQuery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI processing layer**: In this layer, we’ll integrate Google Gemini through
    Vertex AI. Once the results are retrieved from the vector database, they will
    be exposed to Google Gemini as context along with the prompt. This process can
    be handled by a Cloud function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and logging**: To ensure the reliability and performance of our
    system, you should implement robust monitoring and logging mechanisms. We’ll leverage
    services like Cloud Logging, Cloud Monitoring, and Cloud operations to gain visibility
    into our system’s behavior and quickly identify and resolve any issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entry point
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The entry point for a RAG-based chatbot system is designed to be user-friendly,
    allowing users to submit their natural language queries through various interfaces,
    such as web forms, chat applications, or API endpoints. However, the entry point
    should not be limited to accepting text-based inputs only; it should also handle
    different modalities, such as audio files or images, depending on the capabilities
    of the underlying language model.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of models like Google Gemini (which support multimodal inputs),
    the entry point can directly accept and process text, audio, images, or even videos.
    This versatility enables users to interact with the chatbot system in a more natural
    and intuitive manner, aligning with the way humans communicate in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In cases where the language model does not natively support multimodal inputs,
    the entry point can still accommodate various input modalities by pre-processing
    the data and extracting the textual content. This approach ensures that the chatbot
    system remains accessible and user-friendly, catering to a diverse range of input
    formats while leveraging the capabilities of the underlying language model.
  prefs: []
  type: TYPE_NORMAL
- en: For text inputs, the entry point can simply pass the query directly to the subsequent
    phases of the RAG pipeline. However, when dealing with audio or image inputs,
    the entry point needs to perform additional processing to extract the textual
    content from these modalities.
  prefs: []
  type: TYPE_NORMAL
- en: For audio inputs, the entry point can leverage speech recognition technologies,
    such as Google Chirp, Amazon Transcribe, OpenAI Whisper, or open-source libraries
    like CMU Sphinx, to transcribe the audio data into text format. This process involves
    converting the audio signals into a sequence of words or phrases that can be understood
    by the language model.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for image inputs, the entry point can employ **optical character
    recognition** (**OCR**) techniques to extract text from the provided images. This
    can be achieved by integrating with services like Google Cloud Vision API, Amazon
    Textract, or open-source tools like Tesseract OCR. These technologies leverage
    computer vision and machine learning algorithms to accurately identify and extract
    textual content from images, enabling the chatbot system to understand and process
    information presented in visual form.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will leverage text; the Python code will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Regardless of the input type, the entry point should be designed to handle a
    wide range of scenarios and input formats. It may need to perform additional pre-processing
    steps, such as noise removal, format conversion, or data cleaning, to ensure that
    the input data is in a suitable format for the subsequent phases of the RAG pipeline.
    It is also best practice to run the raw request through a rigorous security monitoring
    pipeline to prevent data leakage or model intoxication such as prompt ingestion.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following website presents a very interesting point of view regarding the
    challenges posed by prompt injection in multimodal scenarios: [https://protectai.com/blog/hiding-in-plain-sight-prompt](https://protectai.com/blog/hiding-in-plain-sight-prompt).'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt pre-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our example, we will need to construct our prompt in real time along with
    its necessary context and instructions. In this step of our RAG pipeline, we will
    utilize a vector database for efficient vector similarity search. Vector databases
    are specialized data stores designed to store and retrieve high-dimensional vectors,
    enabling fast and accurate similarity searches. Although there are numerous vector
    database providers available, we will use Chroma DB for this specific example.
  prefs: []
  type: TYPE_NORMAL
- en: The retrieval process in a RAG pipeline is relatively straightforward. First,
    we generate embeddings from the user’s query using a pretrained language model
    or embedding technique. These embeddings are numerical representations of the
    query that capture its semantic meaning. Next, we perform a similarity search
    on the vector database using the generated query embeddings. The vector database
    will return vectors that are most similar to the query embeddings, along with
    their corresponding textual information or context. These vectors are associated
    with previously ingested text passages.
  prefs: []
  type: TYPE_NORMAL
- en: Different filtering strategies can be applied to refine the search results further.
    The specific parameters and techniques available may vary depending on the vector
    database provider being used. For instance, some vector databases provide a similarity
    score that measures the closeness between the query embeddings and the retrieved
    vectors. This score can be leveraged to identify and filter out vectors that fall
    below a certain similarity threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Another common filtering strategy is to limit the number of results obtained
    from the vector database. This approach can be particularly useful when there
    are constraints on the maximum number of tokens that can be passed to the language
    model, either due to token limits imposed by the model or for cost optimization
    purposes. By limiting the number of results, we can control the amount of context
    information provided to the language model, ensuring efficient processing and
    cost-effective operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the results are filtered and the relevant textual information is obtained,
    it is used to construct the prompt that will be passed to the language model.
    In this example, we use the following prompt template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example, the prompt we are submitting to the LLM would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `generate()` function encapsulates the configuration and settings required
    for the generation process. It includes two main components: `generation_config`
    and `safety_settings`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `generation_config` dictionary specifies the parameters that control the
    behavior of the language model during the generation process. In this example,
    the following settings are provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'From Google Gemini’s documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_output_tokens`: Maximum number of tokens that can be generated in the
    response. A token is approximately four characters. 100 tokens correspond to roughly
    60–80 words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature`: The temperature is used for sampling during response generation,
    which occurs when `top_p` and `top_k` are applied. `temperature` controls the
    degree of randomness in token selection. Lower temperature values are good for
    prompts that require a less open-ended or creative response, while higher temperature
    values can lead to more diverse or creative results. A temperature of 0 means
    that the highest probability tokens are always selected. In this case, responses
    for a given prompt are mostly deterministic, but a small amount of variation is
    still possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A temperature of 0 means the model will choose the most likely token based on
    its training data, while higher values introduce more randomness and diversity
    in the output.
  prefs: []
  type: TYPE_NORMAL
- en: '`top_p`: This parameter changes how the model selects tokens for output. Tokens
    are selected from the most to least probable until the sum of their probabilities
    equals the top-p value. For example, if tokens A, B, and C have a probability
    of 0.3, 0.2, and 0.1, respectively, and the top-p value is 0.5, then the model
    will select either A or B as the next token by using temperature and will exclude
    C as a candidate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, it is set to 0.95, meaning that only the top 95% of tokens with
    the highest probabilities will be considered during generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the above, the `safety_settings` dictionary specifies the harm categories
    and corresponding thresholds for filtering potentially harmful or inappropriate
    content from the generated output. In this example, the following settings are
    provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These settings instruct the Gemini 1.5 Flash model to block only highly harmful
    content related to hate speech, dangerous content, sexually explicit content,
    and harassment. Any content that falls below the “high” harm threshold for these
    categories will be allowed in the generated output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `generate()` function creates an instance of the `GenerativeModel` class,
    passing the `MODEL` parameter; in this example, Gemini 1.5 Flash. It then calls
    the `generate_content()` method on the model instance, providing the prompt, generation
    configuration, and safety settings. The `stream=False` parameter indicates that
    the generation should happen in a non-streaming mode, meaning the entire response
    will be generated and returned at once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The generated response is stored in the `responses` variable, which is then
    returned by the `generate()` function.
  prefs: []
  type: TYPE_NORMAL
- en: By submitting the formatted prompt to Vertex AI’s API endpoint for Gemini 1.5
    Flash, leveraging the provided generation configuration and safety settings, this
    RAG pipeline can obtain a contextualized and relevant response tailored to the
    user’s query while adhering to the specified parameters and content filtering
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: Result post-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After receiving the response from the language model, it is often desirable
    to present the output in a more structured and visually appealing format. Markdown
    is a lightweight markup language that allows you to add formatting elements such
    as headings, lists, code blocks, and more. In this example, we use Markdown formatting
    to enhance the presentation of the question, answer, and context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The breakdown of the components of this formatting is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"###Question:\n{question}"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This part adds a level 3 Markdown heading (`###`) for the `Question` section,
    followed by the user’s original query `({question}`) on a new line (`\n`).
  prefs: []
  type: TYPE_NORMAL
- en: '`"\n\n###Answer:\n{result.text}"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After adding an empty line (`\n\n`) at the beginning of the section, this section
    creates another level 3 Markdown heading for the `Answer` section, followed by
    the generated response from the language model (`{result.text}`) on a new line.
  prefs: []
  type: TYPE_NORMAL
- en: '`"\n\n<details><summary>Context</summary>{context}</details>"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This part utilizes the Markdown `<details>` and `<summary>` tags to create a
    collapsible section for displaying the context information retrieved from the
    vector database. The `<summary>Context</summary>` text serves as the label for
    the collapsible section, and the actual context text (`{context}`) is enclosed
    within the `<details>` tags.
  prefs: []
  type: TYPE_NORMAL
- en: Result presentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Markdown` class from the `IPython.display` module is a utility that allows
    you to display formatted Markdown content in a Colab notebook or other IPython
    environments. By passing the `formatted_result` string to the Markdown constructor,
    you create a Markdown object that can be rendered by the `display` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When you call `display()`, the notebook will render the Markdown-formatted content
    contained in the `formatted_result` string. This allows you to leverage the rich
    formatting capabilities of Markdown within the notebook environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of the Markdown formatted output of our demo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Screenshot from the formatted result in the Google Colab notebook'
  prefs: []
  type: TYPE_NORMAL
- en: By using the `Markdown` class and the `display` function, you can take advantage
    of Markdown’s formatting capabilities within the Google Colab notebook environment.
    This includes features like headings, bold and italic text, lists, code blocks,
    links, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The rendered output will be displayed in the notebook cell, providing a visually
    appealing and well-structured representation of the question, answer, and context
    information. This can greatly enhance the readability and usability of the chatbot’s
    responses, making it easier for users or developers to understand and interpret
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the `<details>` and `<summary>` tags used in the `context` section
    create a collapsible section, allowing users to toggle the visibility of the context
    information. This can be particularly helpful when dealing with large amounts
    of context data, as it prevents cluttering the main output while still providing
    easy access to the relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive deep into how all these components work together
    through a use case demo.
  prefs: []
  type: TYPE_NORMAL
- en: Use case demo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is the code for building a demo using Gradio; in this case, we
    will use an additional function that will perform the RAG pipeline. When you run
    this code, a Gradio interface will open in your default web browser, displaying
    three main sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fintech Assistant** heading'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbot area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text input box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users can type their questions into the input box and submit them. The chat
    function will be called, which will use the `answer_question` function to retrieve
    the relevant context from the vector database, generate an answer using the RAG
    pipeline, and update the chatbot interface with the user’s question and the generated
    response.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gradio interface provides a user-friendly way for users to interact with
    the RAG pipeline system, making it easier to test and demonstrate its capabilities.
    Additionally, Gradio offers various customization options and features, such as
    support for different input and output components, styling, and deployment options.
    We start by installing Gradio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define two helper functions that build upon the previously explained
    `generate()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `answer_question(...)` function takes three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`query`: The user’s question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db`: The vector database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_results`: The maximum number of context results to retrieve from
    the database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It then calls the `get_context` function (not shown in the provided code) to
    retrieve the relevant context information – from the vector database, based on
    the user’s query and the specified number of results. The retrieved context is
    then formatted within the `prompt_template` string and passed to the `generate`
    function – covered in the previous sections – to obtain the answer.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the function execution, the generated answer is returned as a
    string.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `chat(...)` function takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`message`: The user’s question'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`history`: A list representing the conversation history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It then calls the `answer_question()` function with the user’s question, the
    vector database (`db`), and the maximum number of results (`MAX_RESULTS`).
  prefs: []
  type: TYPE_NORMAL
- en: The generated response is appended to the history list, along with the user’s
    question. The function returns an empty string and the updated history list, which
    will be used to update the chatbot interface.
  prefs: []
  type: TYPE_NORMAL
- en: The Gradio app
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the helper functions defined, we can now create the Gradio interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s what’s happening in this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`with gr.Blocks() as demo` creates a Gradio interface block called `demo`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gr.Markdown(...)` displays a Markdown-formatted heading for the chatbot interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gr.Chatbot(...)` creates a Gradio chatbot component, which will display the
    conversation history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gr.Textbox(...)` creates a text input box where users can enter their questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`message.submit(...)` sets up an event handler for when the user submits their
    question. It calls the chat function with the user’s input (message) and the chatbot
    instance and updates the message and chatbot components with the returned values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`demo.launch(...)` launches the Gradio interface in debug mode, allowing you
    to interact with the chatbot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to the GitHub directory of this chapter for the complete code that demonstrates
    how all the pieces described above fit together.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve explored an integration pattern that combines RAG and
    generative AI models to build a chatbot capable of answering questions based on
    a document corpus. You’ve learned that RAG leverages the strengths of retrieval
    systems and generative models, allowing the system to retrieve relevant context
    from existing knowledge sources and generate contextual responses, preventing
    hallucinations and ensuring accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We proposed an architecture that utilized a serverless, event-driven approach
    built on Google Cloud. It consists of an ingestion layer for accepting user queries,
    a document corpus management layer for storing embeddings, an AI processing layer
    integrating with Google Gemini on Vertex AI, and monitoring and logging components.
    The entry point handles various input modalities like text, audio, and images,
    pre-processing them as needed.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve learned that the core of the RAG pipeline involves generating embeddings
    from the user query, performing a vector similarity search on a vector database
    (in this example, we used Chroma DB), retrieving relevant context, formatting
    it into a prompt with instructions, and submitting it to the Gemini model on Vertex
    AI. The generated response can be post-processed, formatted with Markdown, and
    presented in a user-friendly interface using tools like Gradio.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve now gained valuable insights into implementing a powerful RAG-based chatbot
    system. You’ve learned how to combine retrieval and generation techniques to provide
    contextual, hallucination-free responses, leveraging vector databases for semantic
    search through content embeddings. The chapter has equipped you with strategies
    to improve retrieval results and customize user experiences through prompt tuning.
    These skills will enable you to enhance your organization’s ability to provide
    accurate and contextualized responses to natural language queries, effectively
    utilizing existing document repositories and the power of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genpat](Chapter_08.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code134841911667913109.png)'
  prefs: []
  type: TYPE_IMG
