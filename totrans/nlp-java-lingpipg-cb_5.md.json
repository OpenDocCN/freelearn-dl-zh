["```py\nLingPipe is an API. It is written in Java.\n012345678901234567890123456789012345678901\n          1         2         3         4           \n```", "```py\nSentence start=0, end=18\nSentence start =20, end=41\n```", "```py\nOrganization start=0, end=7\nOrganization start=37, end=40\n```", "```py\nOrganization start=17, end=20\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.SentenceDetection\n\n    ```", "```py\n    Enter text followed by new line\n    >A sentence. Another sentence.\n    SENTENCE 1:\n    A sentence.\n    SENTENCE 2:\n    Another sentence.\n\n    ```", "```py\n    >A sentence. another sentence.\n    SENTENCE 1:\n    A sentence. another sentence.\n\n    ```", "```py\n    >A sentence. Another sentence without a final period\n    SENTENCE 1:A sentence.\n    SENTENCE 2:Another sentence without a final period\n\n    ```", "```py\n    >(A sentence. Another sentence.)\n    SENTENCE 1: (A sentence. Another sentence.)\n\n    ```", "```py\npackage com.lingpipe.cookbook.chapter5;\n\nimport com.aliasi.chunk.Chunk;\nimport com.aliasi.chunk.Chunker;\nimport com.aliasi.chunk.Chunking;\nimport com.aliasi.sentences.IndoEuropeanSentenceModel;\nimport com.aliasi.sentences.SentenceChunker;\nimport com.aliasi.sentences.SentenceModel;\nimport com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;\nimport com.aliasi.tokenizer.TokenizerFactory;\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.Set;\n\npublic class SentenceDetection {\n\npublic static void main(String[] args) throws IOException {\n  boolean endSent = true;\n  boolean parenS = true;\n  SentenceModel sentenceModel = new IndoEuropeanSentenceModel(endSent,parenS);\n```", "```py\nTokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;\nChunker sentenceChunker = new SentenceChunker(tokFactory,sentenceModel);\n```", "```py\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\nwhile (true) {\n  System.out.print(\"Enter text followed by new line\\n>\");\n  String text = reader.readLine();\n```", "```py\nChunking chunking = sentenceChunker.chunk(text);\nSet<Chunk> sentences = chunking.chunkSet();\n```", "```py\nif (sentences.size() < 1) {\n  System.out.println(\"No sentence chunks found.\");\n  return;\n}\n```", "```py\nString textStored = chunking.charSequence().toString();\nfor (Chunk sentence : sentences) {\n  int start = sentence.start();\n  int end = sentence.end();\n  System.out.println(\"SENTENCE :\" \n    + textStored.substring(start,end));\n  }\n}\n```", "```py\nJohn said \"this is a nested sentence\" and then shut up.\n```", "```py\n[John said \"[this is a nested sentence]\" and then shut up.]\n```", "```py\n[[John ate the gorilla] and [Mary ate the burger]].\n```", "```py\n    [The Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.] [It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.]\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.EvaluateAnnotatedSentences\n    TruePos: 0-83/The Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.:S\n    TruePos: 84-233/It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.:S\n\n    ```", "```py\n    T[he Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.] [It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.]\n\n    ```", "```py\n    TruePos: 84-233/It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.:S\n    FalsePos: 0-83/The Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.:S\n    FalseNeg: 1-83/he Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.:S\n\n    ```", "```py\nString path = args.length > 0 ? args[0] \n             : \"data/hitchHikersGuide.sentDetected\";\nchar[] chars \n  = Files.readCharsFromFile(new File(path), Strings.UTF8);\nStringBuilder rawChars = new StringBuilder();\nint start = -1;\nint end = 0;\nSet<Chunk> sentChunks = new HashSet<Chunk>();\n```", "```py\nfor (int i=0; i < chars.length; ++i) {\n  if (chars[i] == '[') {\n    start = rawChars.length();\n  }\n  else if (chars[i] == ']') {\n    end = rawChars.length();\n\n    Chunk chunk = ChunkFactory.createChunk(start,end, SentenceChunker.SENTENCE_CHUNK_TYPE);\n    sentChunks.add(chunk);}\n  else {\n    rawChars.append(chars[i]);\n  }\n}\nString originalText = rawChars.toString();\n```", "```py\nChunkingImpl sentChunking = new ChunkingImpl(unannotatedText);\nfor (Chunk chunk : sentChunks) {\n  sentChunking.add(chunk);\n}\n```", "```py\nboolean eosIsSentBoundary = false;\nboolean balanceParens = true;\nSentenceModel sentenceModel = new IndoEuropeanSentenceModel(eosIsSentBoundary, balanceParens);\nTokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;\nSentenceChunker sentenceChunker = new SentenceChunker(tokFactory,sentenceModel);\n```", "```py\nSentenceEvaluator evaluator = new SentenceEvaluator(sentenceChunker);\n```", "```py\nevaluator.handle(sentChunking);\n```", "```py\nSentenceEvaluation eval = evaluator.evaluation();\nChunkingEvaluation chunkEval = eval.chunkingEvaluation();\nfor (ChunkAndCharSeq truePos : chunkEval.truePositiveSet()) {\n  System.out.println(\"TruePos: \" + truePos);\n}\nfor (ChunkAndCharSeq falsePos : chunkEval.falsePositiveSet()) {\n  System.out.println(\"FalsePos: \" + falsePos);\n}\nfor (ChunkAndCharSeq falseNeg : chunkEval.falseNegativeSet()){\n  System.out.println(\"FalseNeg: \" + falseNeg);\n}\n```", "```py\n    [All decent people live beyond their incomes nowadays, and those who aren't respectable live beyond other people's.]  [A few gifted individuals manage to do both.]\n\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.EvaluateAnnotatedSentences data/saki.sentDetected \n    FalsePos: 0-159/All decent people live beyond their incomes nowadays, and those who aren't respectable live beyond other people's.  A few gifted individuals manage to do both.:S\n    FalseNeg: 0-114/All decent people live beyond their incomes nowadays, and those who aren't respectable live beyond other people's.:S\n    FalseNeg: 116-159/A few gifted individuals manage to do both.:S\n\n    ```", "```py\nIMPOSSIBLE_PENULTIMATES.add(\"R\");\n//IMPOSSIBLE_PENULTIMATES.add(\"S\"); breaks on \"people's.\"\n//IMPOSSIBLE_PENULTIMATES.add(\"T\"); breaks on \"didn't.\"\nIMPOSSIBLE_PENULTIMATES.add(\"U\");\n```", "```py\n    String textStored = chunking.charSequence().toString();\n    Set<Chunk> chunkSet = chunking.chunkSet();\n    System.out.println(\"size: \" + chunkSet.size());\n    Chunk[] chunkArray = chunkSet.toArray(new Chunk[0]);\n    Arrays.sort(chunkArray,Chunk.LONGEST_MATCH_ORDER_COMPARATOR);\n    ```", "```py\n    StringBuilder output = new StringBuilder(textStored);\n    int sentBoundOffset = 0;\n    for (int i = chunkArray.length -1; i >= 0; --i) {\n      Chunk chunk = chunkArray[i];\n      String sentence = textStored.substring(chunk.start(), chunk.end());\n      if (sentence.contains(\"like\")) {\n        output.insert(chunk.end(),\"}\");\n        output.insert(chunk.start(),\"{\");\n      }\n    }\n    System.out.println(output.toString());\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.WriteSentDetectedChunks\n    Enter text followed by new line\n    >People like to ski. But sometimes it is terrifying. \n    size: 2\n    {People like to ski.} But sometimes it is terrifying. \n\n    ```", "```py\nSentence 1\\. Sentence 2\nSentence 3\\. Sentence 4.\n```", "```py\n{[Sentence 1.] [Sentence 2]}\n\n{[Sentence 3.] [Sentence 4.]\n}\n```", "```py\n[]designates sentences, and {} designates paragraphs. We will jump right into the code on this recipe from src/com/lingpipe/cookbook/chapter5/ParagraphSentenceDetection.java:\n```", "```py\n    public static void main(String[] args) throws IOException {\n      String document = Files.readFromFile(new File(args[0]), Strings.UTF8);\n      String[] paragraphs = document.split(\"\\n\\n\");\n      int paraSeparatorLength = 2;\n    ```", "```py\n    ChunkingImpl paraChunking = new ChunkingImpl(document.toCharArray(),0,document.length());\n    ChunkingImpl sentChunking = new ChunkingImpl(paraChunking.charSequence());\n    ```", "```py\n    boolean eosIsSentBoundary = true;\n    boolean balanceParens = false;\n    SentenceModel sentenceModel = new IndoEuropeanSentenceModel(eosIsSentBoundary, balanceParens);\n    SentenceChunker sentenceChunker = new SentenceChunker(IndoEuropeanTokenizerFactory.INSTANCE, sentenceModel);\n    ```", "```py\n    int paraStart = 0;\n    for (String paragraph : paragraphs) {\n      for (Chunk sentChunk : sentenceChunker.chunk(paragraph).chunkSet()) {\n        Chunk adjustedSentChunk = ChunkFactory.createChunk(sentChunk.start() + paraStart,sentChunk.end() + paraStart, \"S\");\n        sentChunking.add(adjustedSentChunk);\n      }\n    ```", "```py\n    paraChunking.add(ChunkFactory.createChunk(paraStart, paraStart + paragraph.length(),\"P\"));\n    paraStart += paragraph.length() + paraSeparatorLength;\n    }\n    ```", "```py\n    String underlyingString = paraChunking.charSequence().toString();\n    ChunkingImpl displayChunking = new ChunkingImpl(paraChunking.charSequence());\n    displayChunking.addAll(sentChunking.chunkSet());\n    displayChunking.addAll(paraChunking.chunkSet());\n    ```", "```py\n    Set<Chunk> chunkSet = displayChunking.chunkSet();\n    Chunk[] chunkArray = chunkSet.toArray(new Chunk[0]);\n    Arrays.sort(chunkArray, Chunk.LONGEST_MATCH_ORDER_COMPARATOR);\n    ```", "```py\n    StringBuilder output = new StringBuilder(underlyingString);\n    int sentBoundOffset = 0;\n    for (int i = chunkArray.length -1; i >= 0; --i) {\n      Chunk chunk = chunkArray[i];\n      if (chunk.type().equals(\"P\")) {\n        output.insert(chunk.end() + sentBoundOffset,\"}\");\n        output.insert(chunk.start(),\"{\");\n        sentBoundOffset = 0;\n      }\n      if (chunk.type().equals(\"S\")) {\n        output.insert(chunk.end(),\"]\");\n        output.insert(chunk.start(),\"[\");\n        sentBoundOffset += 2;\n      }\n    }\n    System.out.println(output.toString());\n    ```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.PhraseChunker\n    INPUT> The rain in Spain falls mainly on the plain.\n    The/at rain/nn in/in Spain/np falls/vbz mainly/rb on/in the/at plain/jj ./. \n     noun(0,8) The rain\n     noun(12,17) Spain\n     verb(18,30) falls mainly\n     noun(34,43) the plain\n\n    ```", "```py\npublic static void main(String[] args) throws IOException, ClassNotFoundException {\n  File hmmFile = new File(\"models/pos-en-general-brown.HiddenMarkovModel\");\n  HiddenMarkovModel posHmm = (HiddenMarkovModel) AbstractExternalizable.readObject(hmmFile);\n  HmmDecoder posTagger  = new HmmDecoder(posHmm);\n  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n```", "```py\nPhraseChunker chunker = new PhraseChunker(posTagger,tokenizerFactory);\n```", "```py\nBufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));\nwhile (true) {\n  System.out.print(\"\\n\\nINPUT> \");\n  String input = bufReader.readLine();\n```", "```py\nTokenizer tokenizer = tokenizerFactory.tokenizer(input.toCharArray(),0,input.length());\nString[] tokens = tokenizer.tokenize();\nList<String> tokenList = Arrays.asList(tokens);\nTagging<String> tagging = posTagger.tag(tokenList);\nfor (int j = 0; j < tokenList.size(); ++j) {\n  System.out.print(tokens[j] + \"/\" + tagging.tag(j) + \" \");\n}\nSystem.out.println();\n```", "```py\nChunking chunking = chunker.chunk(input);\nCharSequence cs = chunking.charSequence();\nfor (Chunk chunk : chunking.chunkSet()) {\n  String type = chunk.type();\n  int start = chunk.start();\n  int end = chunk.end();\n  CharSequence text = cs.subSequence(start,end);\n  System.out.println(\"  \" + type + \"(\" + start + \",\"+ end + \") \" + text);\n  }\n```", "```py\nFord Prefect used to live in Guildford before he needed to move.\n```", "```py\n    java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.RegexNer\n\n    ```", "```py\n    Enter text, . to quit:\n    >Hello,my name is Foo and my email is foo@bar.com or you can also contact me at foo.bar@gmail.com.\n    input=Hello,my name is Foo and my email is foo@bar.com or you can also contact me at foo.bar@gmail.com.\n    chunking=Hello,my name is Foo and my email is foo@bar.com or you can also contact me at foo.bar@gmail.com. : [37-48:email@0.0, 79-96:email@0.0]\n     chunk=37-48:email@0.0  text=foo@bar.com\n     chunk=79-96:email@0.0  text=foo.bar@gmail.com\n\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n  String emailRegex = \"[A-Za-z0-9](([_\\\\.\\\\-]?[a-zA-Z0-9]+)*)\" + + \"@([A-Za-z0-9]+)\" + \"(([\\\\.\\\\-]?[a-zA-Z0-9]+)*)\\\\.([A-Za-z]{2,})\";\n  String chunkType = \"email\";\n  double score = 1.0;\n  Chunker chunker = new RegExChunker(emailRegex,chunkType,score);\n```", "```py\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\n  String input = \"\";\n  while (true) {\n    System.out.println(\"Enter text, . to quit:\");\n    input = reader.readLine();\n    if(input.equals(\".\")){\n      break;\n    }\n    Chunking chunking = chunker.chunk(input);\n    System.out.println(\"input=\" + input);\n    System.out.println(\"chunking=\" + chunking);\n    Set<Chunk> chunkSet = chunking.chunkSet();\n    Iterator<Chunk> it = chunkSet.iterator();\n    while (it.hasNext()) {\n      Chunk chunk = it.next();\n      int start = chunk.start();\n      int end = chunk.end();\n      String text = input.substring(start,end);\n      System.out.println(\"     chunk=\" + chunk + \" text=\" + text);\n    }\n  }\n}\n```", "```py\n    java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.DictionaryChunker\n\n    ```", "```py\n    Enter text, . to quit:\n    Ford and Arthur went up the bridge of the Heart of Gold with Marvin\n    CHUNKER overlapping, case sensitive\n     phrase=|Ford| start=0 end=4 type=PERSON score=1.0\n     phrase=|Arthur| start=9 end=15 type=PERSON score=1.0\n     phrase=|Heart| start=42 end=47 type=ORGAN score=1.0\n     phrase=|Heart of Gold| start=42 end=55 type=SPACECRAFT score=1.0\n     phrase=|Marvin| start=61 end=67 type=ROBOT score=1.0\n\n    ```", "```py\nstatic final double CHUNK_SCORE = 1.0;\n\npublic static void main(String[] args) throws IOException {\n  MapDictionary<String> dictionary = new MapDictionary<String>();\n  MapDictionary<String> dictionary = new MapDictionary<String>();\n```", "```py\ndictionary.addEntry(new DictionaryEntry<String>(\"Arthur\",\"PERSON\",CHUNK_SCORE));\ndictionary.addEntry(new DictionaryEntry<String>(\"Ford\",\"PERSON\",CHUNK_SCORE));\ndictionary.addEntry(new DictionaryEntry<String>(\"Trillian\",\"PERSON\",CHUNK_SCORE));\ndictionary.addEntry(new DictionaryEntry<String>(\"Zaphod\",\"PERSON\",CHUNK_SCORE));\ndictionary.addEntry(new DictionaryEntry<String>(\"Marvin\",\"ROBOT\",CHUNK_SCORE));\ndictionary.addEntry(new DictionaryEntry<String>(\"Heart of Gold\", \"SPACECRAFT\",CHUNK_SCORE));\ndictionary.addEntry(new DictionaryEntry<String>(\"HitchhikersGuide\", \"PRODUCT\",CHUNK_SCORE));\n```", "```py\nboolean returnAllMatches = true;\nboolean caseSensitive = true;\nExactDictionaryChunker dictionaryChunker = new ExactDictionaryChunker(dictionary, IndoEuropeanTokenizerFactory.INSTANCE, returnAllMatches,caseSensitive);\n```", "```py\nBufferedReader reader = new BufferedReader(new InputStreamReader(System.in));\nString text = \"\";\nwhile (true) {\n  System.out.println(\"Enter text, . to quit:\");\n  text = reader.readLine();\n  if(text.equals(\".\")){\n    break;\n  }\n```", "```py\nSystem.out.println(\"\\nCHUNKER overlapping, case sensitive\");\nChunking chunking = dictionaryChunker.chunk(text);\n  for (Chunk chunk : chunking.chunkSet()) {\n    int start = chunk.start();\n    int end = chunk.end();\n    String type = chunk.type();\n    double score = chunk.score();\n    String phrase = text.substring(start,end);\n    System.out.println(\"     phrase=|\" + phrase + \"|\" + \" start=\" + start + \" end=\" + end + \" type=\" + type + \" score=\" + score);\n```", "```py\nJohn Jones Mary and Mr. Jones\n01234567890123456789012345678\n0         1         2         \n```", "```py\nJohn  B_PERSON\nJones  I_PERSON\nMary  B_PERSON\nand  O\nMr    B_PERSON\n.    I_PERSON\nJones  I_PERSON\n```", "```py\n0-10 \"John Jones\" PERSON\n11-15 \"Mary\" PERSON\n20-29 \"Mr. Jones\" PERSON\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.BioCodec\n\n    ```", "```py\n    Tagging for :The rain in Spain.\n    The/B_Weather\n    rain/I_Weather\n    in/O\n    Spain/B_Place\n    ./O\n\n    ```", "```py\n    Chunking from StringTagging\n    0-8:Weather@-Infinity\n    12-17:Place@-Infinity\n\n    ```", "```py\n    StringTagging from Chunking\n    The/B_Weather\n    rain/I_Weather\n    in/O\n    Spain/B_Place\n    ./O\n\n    ```", "```py\npublic static void main(String[] args) {\n  List<String> tokens = new ArrayList<String>();\n  tokens.add(\"The\");\n  tokens.add(\"rain\");\n  tokens.add(\"in\");\n  tokens.add(\"Spain\");\n  tokens.add(\".\");\n  List<String> tags = new ArrayList<String>();\n  tags.add(\"B_Weather\");\n  tags.add(\"I_Weather\");\n  tags.add(\"O\");\n  tags.add(\"B_Place\");\n  tags.add(\"O\");\n  CharSequence cs = \"The rain in Spain.\";\n  //012345678901234567\n  int[] tokenStarts = {0,4,9,12,17};\n  int[] tokenEnds = {3,8,11,17,17};\n  StringTagging tagging = new StringTagging(tokens, tags, cs, tokenStarts, tokenEnds);\n  System.out.println(\"Tagging for :\" + cs);\n  for (int i = 0; i < tagging.size(); ++i) {\n    System.out.println(tagging.token(i) + \"/\" + tagging.tag(i));\n  }\n```", "```py\nBioTagChunkCodec codec = new BioTagChunkCodec();\nChunking chunking = codec.toChunking(tagging);\nSystem.out.println(\"Chunking from StringTagging\");\nfor (Chunk chunk : chunking.chunkSet()) {\n  System.out.println(chunk);\n}\n```", "```py\nboolean enforceConsistency = true;\nBioTagChunkCodec codec2 = new BioTagChunkCodec(IndoEuropeanTokenizerFactory.INSTANCE, enforceConsistency);\nStringTagging tagging2 = codec2.toStringTagging(chunking);\nSystem.out.println(\"StringTagging from Chunking\");\nfor (int i = 0; i < tagging2.size(); ++i) {\n  System.out.println(tagging2.token(i) + \"/\" + tagging2.tag(i));\n}\n```", "```py\nEl       O \nAbogado     B-PER \nGeneral     I-PER \ndel     I-PER \nEstado     I-PER \n,       O \nDaryl     B-PER \nWilliams     I-PER \n,       O\n```", "```py\nesp.train, line 221619, change I-LOC to B-LOC\nesp.testa, line 30882, change I-LOC to B-LOC\nesp.testb, line 9291, change I-LOC to B-LOC\n\n```", "```py\n    java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.HmmNeChunker\n\n    ```", "```py\n    Training HMM Chunker on data from: data/ner/data/esp.train\n    Output written to : models/Conll2002_ESP.RescoringChunker\n    Enter text, . to quit:\n\n    ```", "```py\n    La empresa también tiene participación en Tele Leste Celular , operadora móvil de los estados de Bahía y Sergipe y que es controlada por la española Iberdrola , y además es socia de Portugal Telecom en Telesp Celular , la operadora móvil de Sao Paulo .\n    Rank   Conf      Span         Type     Phrase\n    0      1.0000   (105, 112)    LOC      Sergipe\n    1      1.0000   (149, 158)    ORG      Iberdrola\n    2      1.0000   (202, 216)    ORG      Telesp Celular\n    3      1.0000   (182, 198)    ORG      Portugal Telecom\n    4      1.0000   (97, 102)     LOC      Bahía\n    5      1.0000   (241, 250)    LOC      Sao Paulo\n    6      0.9907   (163, 169)    PER      además\n    7      0.9736   (11, 18)      ORG      también\n    8      0.9736   (39, 60)      ORG      en Tele Leste Celular\n    9      0.0264   (42, 60)      ORG      Tele Leste Celular\n\n    ```", "```py\n    Tele B-ORG\n    Leste I-ORG\n    Celular I-ORG\n    Bahía B-LOC\n    Sergipe B-LOC\n    Iberdrola B-ORG\n    Portugal B-ORG\n    Telecom I-ORG\n    Telesp B-ORG\n    Celular I-ORG\n    Sao B-LOC\n    Paulo I-LOC\n\n    ```", "```py\n    Tele Leste Celular      ORG\n    Bahía                   LOC\n    Sergipe                 LOC\n    Iberdrola               ORG\n    Portugal Telecom        ORG\n    Telesp Celular          ORG\n    Sao Paulo               LOC\n\n    ```", "```py\nString modelFilename = \"models/Conll2002_ESP.RescoringChunker\";\nString trainFilename = \"data/ner/data/esp.train\";\n```", "```py\nFile modelFile = new File(modelFilename);\nif(!modelFile.exists()){\n  System.out.println(\"Training HMM Chunker on data from: \" + trainFilename);\n  trainHMMChunker(modelFilename, trainFilename);\n  System.out.println(\"Output written to : \" + modelFilename);\n}\n\n@SuppressWarnings(\"unchecked\")\nRescoringChunker<CharLmRescoringChunker> chunker = (RescoringChunker<CharLmRescoringChunker>) AbstractExternalizable.readObject(modelFile);\n```", "```py\nstatic void trainHMMChunker(String modelFilename, String trainFilename) throws IOException{\n  File modelFile = new File(modelFilename);\n  File trainFile = new File(trainFilename);\n\n  int numChunkingsRescored = 64;\n  int maxNgram = 12;\n  int numChars = 256;\n  double lmInterpolation = maxNgram; \n  TokenizerFactory factory\n    = IndoEuropeanTokenizerFactory.INSTANCE;\n\nCharLmRescoringChunker chunkerEstimator\n  = new CharLmRescoringChunker(factory,numChunkingsRescored,\n          maxNgram,numChars,\n          lmInterpolation);\n```", "```py\nchunkerEstimator with the setHandler() method, and then, the parser.parse() method does the actual training. The last bit of code serializes the model to disk—see the *How to serialize a LingPipe object – classifier example* recipe in Chapter 1, *Simple Classifiers*, to read about what is going on:\n```", "```py\nConll2002ChunkTagParser parser = new Conll2002ChunkTagParser();\nparser.setHandler(chunkerEstimator);\nparser.parse(trainFile);\nAbstractExternalizable.compileTo(chunkerEstimator,modelFile);\n```", "```py\npublic class Conll2002ChunkTagParser extends StringParser<ObjectHandler<Chunking>>\n{\n\n  static final String TOKEN_TAG_LINE_REGEX = \"(\\\\S+)\\\\s(\\\\S+\\\\s)?(O|[B|I]-\\\\S+)\";\n  static final int TOKEN_GROUP = 1;\n  static final int TAG_GROUP = 3;\n  static final String IGNORE_LINE_REGEX = \"-DOCSTART(.*)\";\n  static final String EOS_REGEX = \"\\\\A\\\\Z\";\n  static final String BEGIN_TAG_PREFIX = \"B-\";\n  static final String IN_TAG_PREFIX = \"I-\";\n  static final String OUT_TAG = \"O\";\n```", "```py\nprivate final LineTaggingParser mParser = new LineTaggingParser(TOKEN_TAG_LINE_REGEX, TOKEN_GROUP, TAG_GROUP, IGNORE_LINE_REGEX, EOS_REGEX);\n```", "```py\nprivate final TagChunkCodec mCodec = new BioTagChunkCodec(null, false, BEGIN_TAG_PREFIX, IN_TAG_PREFIX, OUT_TAG);\n```", "```py\npublic void parseString(char[] cs, int start, int end) {\n  mParser.parseString(cs,start,end);\n}\n```", "```py\npublic void setHandler(ObjectHandler<Chunking> handler) {\n\n  ObjectHandler<Tagging<String>> taggingHandler = TagChunkCodecAdapters.chunkingToTagging(mCodec, handler);\n  mParser.setHandler(taggingHandler);\n}\n\npublic TagChunkCodec getTagChunkCodec(){\n  return mCodec;\n}\n```", "```py\nchar[] cs = text.toCharArray();\nIterator<Chunk> it = chunker.nBestChunks(cs,0,cs.length, MAX_N_BEST_CHUNKS);\nSystem.out.println(text);\nSystem.out.println(\"Rank          Conf      Span\"    + \"    Type     Phrase\");\nDecimalFormat df = new DecimalFormat(\"0.0000\");\n\nfor (int n = 0; it.hasNext(); ++n) {\n\nChunk chunk = it.next();\ndouble conf = chunk.score();\nint start = chunk.start();\nint end = chunk.end();\nString phrase = text.substring(start,end);\nSystem.out.println(n + \" \"       + \"            \"   + df.format(conf)     + \"       (\" + start  + \", \" + end  + \")    \" + chunk.type()      + \"         \" + phrase);\n}\n```", "```py\n    java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.MultipleNer\n\n    ```", "```py\n    Enter text, . to quit:\n    President Obama is scheduled to arrive in London this evening. He will address the G-8 summit.\n    neChunking: [10-15:PERSON@-Infinity, 42-48:LOCATION@-Infinity, 83-86:ORGANIZATION@-Infinity]\n    pChunking: [62-66:MALE_PRONOUN@1.0]\n    dChunking: [10-15:PRESIDENT@1.0]\n    ----Overlaps Allowed\n\n     Combined Chunks:\n    [83-86:ORGANIZATION@-Infinity, 10-15:PERSON@-Infinity, 10-15:PRESIDENT@1.0, 42-48:LOCATION@-Infinity, 62-66:MALE_PRONOUN@1.0]\n\n    ----Overlaps Not Allowed\n\n     Unique Chunks:\n    [83-86:ORGANIZATION@-Infinity, 42-48:LOCATION@-Infinity, 62-66:MALE_PRONOUN@1.0]\n\n     OverLapped Chunks:\n    [10-15:PERSON@-Infinity, 10-15:PRESIDENT@1.0]\n\n    ```", "```py\nChunker pronounChunker = new RegExChunker(\" He | he | Him | him\", \"MALE_PRONOUN\",1.0);\nFile MODEL_FILE = new File(\"models/ne-en-news.muc6.\" + \"AbstractCharLmRescoringChunker\");\nChunker neChunker = (Chunker) AbstractExternalizable.readObject(MODEL_FILE);\n\nMapDictionary<String> dictionary = new MapDictionary<String>();\ndictionary.addEntry(\n  new DictionaryEntry<String>(\"Obama\",\"PRESIDENT\",CHUNK_SCORE));\ndictionary.addEntry(\n  new DictionaryEntry<String>(\"Bush\",\"PRESIDENT\",CHUNK_SCORE));\nExactDictionaryChunker dictionaryChunker = new ExactDictionaryChunker(dictionary, IndoEuropeanTokenizerFactory.INSTANCE);\n```", "```py\nSet<Chunk> neChunking = neChunker.chunk(text).chunkSet();\nSet<Chunk> pChunking = pronounChunker.chunk(text).chunkSet();\nSet<Chunk> dChunking = dictionaryChunker.chunk(text).chunkSet();\nSet<Chunk> allChunks = new HashSet<Chunk>();\nallChunks.addAll(neChunking);\nallChunks.addAll(pChunking);\nallChunks.addAll(dChunking);\ngetCombinedChunks(allChunks,true);//allow overlaps\ngetCombinedChunks(allChunks,false);//no overlaps\n```", "```py\nstatic void getCombinedChunks(Set<Chunk> chunkSet, boolean allowOverlap){\n  Set<Chunk> combinedChunks = new HashSet<Chunk>();\n  Set<Chunk>overLappedChunks = new HashSet<Chunk>();\n  for(Chunk c : chunkSet){\n    combinedChunks.add(c);\n    for(Chunk x : chunkSet){\n      if (c.equals(x)){\n        continue;\n      }\n      if (ChunkingImpl.overlap(c,x)) {\n        if (allowOverlap){\n          combinedChunks.add(x);\n        } else {\n          overLappedChunks.add(x);\n          combinedChunks.remove(c);\n        }\n      }\n    }\n  }\n}\n```", "```py\npublic class TinyEntityCorpus extends Corpus<ObjectHandler<Chunking>> {\n\n  public void visitTrain(ObjectHandler<Chunking> handler) {\n    for (Chunking chunking : CHUNKINGS) handler.handle(chunking);\n  }\n\n  public void visitTest(ObjectHandler<Chunking> handler) {\n    /* no op */\n  }\n```", "```py\nstatic final Chunking[] CHUNKINGS = new Chunking[] {\n  chunking(\"\"), chunking(\"The\"), chunking(\"John ran.\", chunk(0,4,\"PER\")), chunking(\"Mary ran.\", chunk(0,4,\"PER\")), chunking(\"The kid ran.\"), chunking(\"John likes Mary.\", chunk(0,4,\"PER\"), chunk(11,15,\"PER\")), chunking(\"Tim lives in Washington\", chunk(0,3,\"PER\"), chunk(13,23,\"LOC\")), chunking(\"Mary Smith is in New York City\", chunk(0,10,\"PER\"), chunk(17,30,\"LOC\")), chunking(\"New York City is fun\", chunk(0,13,\"LOC\")), chunking(\"Chicago is not like Washington\", chunk(0,7,\"LOC\"), chunk(20,30,\"LOC\"))\n};\n```", "```py\nstatic Chunking chunking(String s, Chunk... chunks) {\n  ChunkingImpl chunking = new ChunkingImpl(s);\n  for (Chunk chunk : chunks) chunking.add(chunk);\n  return chunking;\n}\n\nstatic Chunk chunk(int start, int end, String type) {\n  return ChunkFactory.createChunk(start,end,type);\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.TrainAndRunSimpleCrf\n\n    ```", "```py\n    Enter text followed by new line\n    >John Smith went to New York.\n\n    ```", "```py\n    FIRST BEST\n    John Smith went to New York. : [0-10:PER@-Infinity, 19-27:LOC@-Infinity]\n\n    ```", "```py\n    10 BEST CONDITIONAL\n    Rank log p(tags|tokens)  Tagging\n    0    -1.66335590 [0-10:PER@-Infinity, 19-27:LOC@-Infinity]\n    1    -2.38671498 [0-10:PER@-Infinity, 19-28:LOC@-Infinity]\n    2    -2.77341747 [0-10:PER@-Infinity]\n    3    -2.85908677 [0-4:PER@-Infinity, 19-27:LOC@-Infinity]\n    4    -3.00398856 [0-10:PER@-Infinity, 19-22:LOC@-Infinity]\n    5    -3.23050827 [0-10:PER@-Infinity, 16-27:LOC@-Infinity]\n    6    -3.49773765 [0-10:PER@-Infinity, 23-27:PER@-Infinity]\n    7    -3.58244582 [0-4:PER@-Infinity, 19-28:LOC@-Infinity]\n    8    -3.72315571 [0-10:PER@-Infinity, 19-22:PER@-Infinity]\n    9    -3.95386735 [0-10:PER@-Infinity, 16-28:LOC@-Infinity]\n\n    ```", "```py\n    MARGINAL CHUNK PROBABILITIES\n    Rank Chunk Phrase\n    0 0-10:PER@-0.49306887565189683 John Smith\n    1 19-27:LOC@-1.1957935770408703 New York\n    2 0-4:PER@-1.3270942262839682 John\n    3 19-22:LOC@-2.484463373596263 New\n    4 23-27:PER@-2.6919267821139776 York\n    5 16-27:LOC@-2.881057607295971 to New York\n    6 11-15:PER@-3.0868632773744222 went\n    7 16-18:PER@-3.1583044940140192 to\n    8 19-22:PER@-3.2036305275847825 New\n    9 23-27:LOC@-3.536294896211011 York\n\n    ```", "```py\npublic static void main(String[] args) throws IOException {\n  Corpus<ObjectHandler<Chunking>> corpus = new TinyEntityCorpus();\n\n  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;\n  boolean enforceConsistency = true;\n  TagChunkCodec tagChunkCodec = new BioTagChunkCodec(tokenizerFactory, enforceConsistency);\n```", "```py\nJohn Smith went to New York City. : [0-10:PER@-Infinity, 19-32:LOC@-Infinity]\n```", "```py\nTok    Tag\nJohn   B_PER\nSmith  I_PER\nwent  O\nto     O\nNew    B_LOC\nYork  I_LOC\nCity  I_LOC\n.    O\n```", "```py\nChainCrfFeatureExtractor<String> featureExtractor = new SimpleCrfFeatureExtractor();\n```", "```py\nint minFeatureCount = 1;\nboolean cacheFeatures = true;\nboolean addIntercept = true;\ndouble priorVariance = 4.0;\nboolean uninformativeIntercept = true;\nRegressionPrior prior = RegressionPrior.gaussian(priorVariance, uninformativeIntercept);\nint priorBlockSize = 3;\ndouble initialLearningRate = 0.05;\ndouble learningRateDecay = 0.995;\nAnnealingSchedule annealingSchedule = AnnealingSchedule.exponential(initialLearningRate, learningRateDecay);\ndouble minImprovement = 0.00001;\nint minEpochs = 10;\nint maxEpochs = 5000;\nReporter reporter = Reporters.stdOut().setLevel(LogLevel.DEBUG);\nSystem.out.println(\"\\nEstimating\");\nChainCrfChunker crfChunker = ChainCrfChunker.estimate(corpus, tagChunkCodec, tokenizerFactory, featureExtractor, addIntercept, minFeatureCount, cacheFeatures, prior, priorBlockSize, annealingSchedule, minImprovement, minEpochs, maxEpochs, reporter);\n```", "```py\nSystem.out.println(\"\\nFIRST BEST\");\nChunking chunking = crfChunker.chunk(evalText);\nSystem.out.println(chunking);\n```", "```py\nint maxNBest = 10;\nSystem.out.println(\"\\n\" + maxNBest + \" BEST CONDITIONAL\");\nSystem.out.println(\"Rank log p(tags|tokens)  Tagging\");\nIterator<ScoredObject<Chunking>> it = crfChunker.nBestConditional(evalTextChars,0, evalTextChars.length,maxNBest);\n\n  for (int rank = 0; rank < maxNBest && it.hasNext(); ++rank) {\n    ScoredObject<Chunking> scoredChunking = it.next();\n    System.out.println(rank + \"    \" + scoredChunking.score() + \" \" + scoredChunking.getObject().chunkSet());\n  }\n```", "```py\nSystem.out.println(\"\\nMARGINAL CHUNK PROBABILITIES\");\nSystem.out.println(\"Rank Chunk Phrase\");\nint maxNBestChunks = 10;\nIterator<Chunk> nBestIt  = crfChunker.nBestChunks(evalTextChars,0, evalTextChars.length,maxNBestChunks);\nfor (int n = 0; n < maxNBestChunks && nBestIt.hasNext(); ++n) {\n  Chunk chunk = nBestChunkIt.next();\n  System.out.println(n + \" \" + chunk + \" \" + evalText.substring(chunk.start(),chunk.end()));\n}\n```", "```py\n    java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.FancyCrfFeatureExtractor\n\n    ```", "```py\n    Tagging:  John/PN\n    Node Feats:{PREF_NEXT_ra=1.0, PREF_Jo=1.0, POS_np=1.0, TOK_CAT_LET-CAP=1.0, SUFF_NEXT_an=1.0, PREF_Joh=1.0, PREF_NEXT_r=1.0, SUFF_John=1.0, TOK_John=1.0, PREF_NEXT_ran=1.0, BOS=1.0, TOK_NEXT_ran=1.0, SUFF_NEXT_n=1.0, SUFF_NEXT_ran=1.0, SUFF_ohn=1.0, PREF_J=1.0, POS_NEXT_vbd=1.0, SUFF_hn=1.0, SUFF_n=1.0, TOK_CAT_NEXT_ran=1.0, PREF_John=1.0}\n\n    ```", "```py\n    Edge Feats:{PREV_TAG_TOKEN_CAT_PN_LET-CAP=1.0, PREV_TAG_PN=1.0}\n\n    ```", "```py\npublic FancyCrfFeatureExtractor()\n  throws ClassNotFoundException, IOException {\n  File posHmmFile = new File(\"models/pos-en-general\" + \"brown.HiddenMarkovModel\");\n  @SuppressWarnings(\"unchecked\") HiddenMarkovModel posHmm = (HiddenMarkovModel)\n  AbstractExternalizable.readObject(posHmmFile);\n\n  FastCache<String,double[]> emissionCache = new FastCache<String,double[]>(100000);\n  mPosTagger = new HmmDecoder(posHmm,null,emissionCache);\n}\n```", "```py\npublic ChainCrfFeatures<String> extract(List<String> tokens, List<String> tags) {\n  return new ChunkerFeatures(tokens,tags);\n}\n```", "```py\nclass ChunkerFeatures extends ChainCrfFeatures<String> {\n  private final Tagging<String> mPosTagging;\n\n  public ChunkerFeatures(List<String> tokens, List<String> tags) {\n    super(tokens,tags);\n    mPosTagging = mPosTagger.tag(tokens);\n  }\n```", "```py\npublic Map<String,? extends Number> edgeFeatures(int n, int k) {\n  ObjectToDoubleMap<String> feats = new ObjectToDoubleMap<String>();\n  feats.set(\"PREV_TAG_\" + tag(k),1.0);\n  feats.set(\"PREV_TAG_TOKEN_CAT_\"  + tag(k) + \"_\" + tokenCat(n-1), 1.0);\n  return feats;\n}\n```", "```py\npublic Map<String,? extends Number> nodeFeatures(int n) {\n  ObjectToDoubleMap<String> feats = new ObjectToDoubleMap<String>();\n```", "```py\nboolean bos = n == 0;\nboolean eos = (n + 1) >= numTokens();\n```", "```py\nString tokenCat = tokenCat(n);\nString prevTokenCat = bos ? null : tokenCat(n-1);\nString nextTokenCat = eos ? null : tokenCat(n+1);\n\nString token = normedToken(n);\nString prevToken = bos ? null : normedToken(n-1);\nString nextToken = eos ? null : normedToken(n+1);\n\nString posTag = mPosTagging.tag(n);\nString prevPosTag = bos ? null : mPosTagging.tag(n-1);\nString nextPosTag = eos ? null : mPosTagging.tag(n+1);\n```", "```py\npublic String normedToken(int n) {\n  return token(n).replaceAll(\"\\\\d+\",\"*$0*\").replaceAll(\"\\\\d\",\"D\");\n}\n```", "```py\nif (bos) {\n  feats.set(\"BOS\",1.0);\n}\nif (eos) {\n  feats.set(\"EOS\",1.0);\n}\nif (!bos && !eos) {\n  feats.set(\"!BOS!EOS\",1.0);\n}\n```", "```py\nfeats.set(\"TOK_\" + token, 1.0);\nif (!bos) {\n  feats.set(\"TOK_PREV_\" + prevToken,1.0);\n}\nif (!eos) {\n  feats.set(\"TOK_NEXT_\" + nextToken,1.0);\n}\nfeats.set(\"TOK_CAT_\" + tokenCat, 1.0);\nif (!bos) {\n  feats.set(\"TOK_CAT_PREV_\" + prevTokenCat, 1.0);\n}\nif (!eos) {\n  feats.set(\"TOK_CAT_NEXT_\" + nextToken, 1.0);\n}\nfeats.set(\"POS_\" + posTag,1.0);\nif (!bos) {\n  feats.set(\"POS_PREV_\" + prevPosTag,1.0);\n}\nif (!eos) {\n  feats.set(\"POS_NEXT_\" + nextPosTag,1.0);\n}\n```", "```py\nfor (String suffix : suffixes(token)) {\n  feats.set(\"SUFF_\" + suffix,1.0);\n}\nif (!bos) {\n  for (String suffix : suffixes(prevToken)) {\n    feats.set(\"SUFF_PREV_\" + suffix,1.0);\n    if (!eos) {\n      for (String suffix : suffixes(nextToken)) {\n        feats.set(\"SUFF_NEXT_\" + suffix,1.0);\n      }\n      for (String prefix : prefixes(token)) {\n        feats.set(\"PREF_\" + prefix,1.0);\n      }\n      if (!bos) {\n        for (String prefix : prefixes(prevToken)) {\n          feats.set(\"PREF_PREV_\" + prefix,1.0);\n      }\n      if (!eos) {\n        for (String prefix : prefixes(nextToken)) {\n          feats.set(\"PREF_NEXT_\" + prefix,1.0);\n        }\n      }\n      return feats;\n    }\n```", "```py\nstatic int MAX_PREFIX_LENGTH = 4;\n  static List<String> prefixes(String s) {\n    int numPrefixes = Math.min(MAX_PREFIX_LENGTH,s.length());\n    if (numPrefixes == 0) {\n      return Collections.emptyList();\n    }\n    if (numPrefixes == 1) {\n      return Collections.singletonList(s);\n    }\n    List<String> result = new ArrayList<String>(numPrefixes);\n    for (int i = 1; i <= Math.min(MAX_PREFIX_LENGTH,s.length()); ++i) {\n      result.add(s.substring(0,i));\n    }\n    return result;\n  }\n\n  static int MAX_SUFFIX_LENGTH = 4;\n  static List<String> suffixes(String s) {\n    int numSuffixes = Math.min(s.length(), MAX_SUFFIX_LENGTH);\n    if (numSuffixes <= 0) {\n      return Collections.emptyList();\n    }\n    if (numSuffixes == 1) {\n      return Collections.singletonList(s);\n    }\n    List<String> result = new ArrayList<String>(numSuffixes);\n    for (int i = s.length() - numSuffixes; i < s.length(); ++i) {\n      result.add(s.substring(i));\n    }\n    return result;\n  }\n```"]