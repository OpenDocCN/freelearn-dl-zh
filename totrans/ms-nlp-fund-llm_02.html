<html><head></head><body>
<div id="_idContainer139" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-27"><a id="_idTextAnchor026" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-28" class="calibre4"><a id="_idTextAnchor027" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Mastering Linear Algebra, Probability, and Statistics for Machine Learning and NLP</span></h1>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.3.1">Natural language processing</span></strong><span class="kobospan" id="kobo.4.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.5.1">NLP</span></strong><span class="kobospan" id="kobo.6.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.7.1">machine learning</span></strong><span class="kobospan" id="kobo.8.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.9.1">ML</span></strong><span class="kobospan" id="kobo.10.1">) are two fields that have significantly benefited from mathematical concepts, particularly linear algebra and probability theory. </span><span class="kobospan" id="kobo.10.2">These fundamental tools enable the analysis of the relationships between variables, forming the basis of many NLP and ML models. </span><span class="kobospan" id="kobo.10.3">This chapter provides a comprehensive introduction to linear algebra and probability theory, including their practical applications in NLP and ML. </span><span class="kobospan" id="kobo.10.4">The c</span><a id="_idTextAnchor028" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.11.1">hapter commences with an overview of vectors and matrices and covers essential operations. </span><span class="kobospan" id="kobo.11.2">Additionally, the basics of statistics, required for understanding the concepts and models in subsequent chapters, will be explained. </span><span class="kobospan" id="kobo.11.3">Finally, the chapter introduces the fundamentals of optimization, which are critical for solving NLP problems and understanding the relationships between variables. </span><span class="kobospan" id="kobo.11.4">By the end of this chapter, you will have a solid foundation in linear algebra and probability theory and understand their essential applications in NLP </span><span><span class="kobospan" id="kobo.12.1">and ML.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.13.1">In this chapter, we’ll be covering the </span><span><span class="kobospan" id="kobo.14.1">following topics:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.15.1">Introduction to </span><span><span class="kobospan" id="kobo.16.1">linear algebra</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.17.1">Eigenvalues </span><span><span class="kobospan" id="kobo.18.1">and eigenvectors</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.19.1">Basic probability for </span><span><span class="kobospan" id="kobo.20.1">machine learning</span></span></li>
</ul>
<h1 id="_idParaDest-29" class="calibre4"><a id="_idTextAnchor029" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.21.1">Introduction to linear algebra</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.22.1">Let’s start by first understanding scalars, vectors, </span><span><span class="kobospan" id="kobo.23.1">and matrices:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.24.1">Scalars</span></strong><span class="kobospan" id="kobo.25.1">: A scalar is a single </span><a id="_idIndexMarker027" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.26.1">numerical value that usually comes from the real</span><a id="_idIndexMarker028" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.27.1"> domain in most ML applications. </span><span class="kobospan" id="kobo.27.2">Examples of scalars in NLP include the frequency of a word in a </span><span><span class="kobospan" id="kobo.28.1">text corpus.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.29.1">Vectors</span></strong><span class="kobospan" id="kobo.30.1">: A vector is a collection of numerical elements. </span><span class="kobospan" id="kobo.30.2">Each of these elements can be termed as an entry, component, or dimension, and the count of these components defines the</span><a id="_idIndexMarker029" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.31.1"> vector’s dimensionality. </span><span class="kobospan" id="kobo.31.2">Within NLP, a vector could hold components related to elements such as word frequency, sentiment ranking, and more. </span><span class="kobospan" id="kobo.31.3">NLP and ML are two domains that have reaped substantial benefits from mathematical disciplines, particularly linear algebra and probability theory. </span><span class="kobospan" id="kobo.31.4">These foundational tools aid in evaluating the correlation between variables and are at the heart of numerous NLP and ML models. </span><span class="kobospan" id="kobo.31.5">This segment presents a detailed primer on linear algebra and probability theory, along with their practical usage in NLP and ML. </span><span class="kobospan" id="kobo.31.6">For instance, a text document’s three-dimensional vector representation might be expressed as a real-number array, such as [word frequency, sentiment </span><span><span class="kobospan" id="kobo.32.1">ranking, complexity].</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.33.1">Matrices</span></strong><span class="kobospan" id="kobo.34.1">: A matrix can</span><a id="_idIndexMarker030" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.35.1"> be perceived as a rectangular collection of numerical elements composed of rows and columns. </span><span class="kobospan" id="kobo.35.2">To retrieve an element from the matrix, one needs to denote its row and column indices. </span><span class="kobospan" id="kobo.35.3">In the field of NLP, a data matrix might include rows that align with distinct text documents and columns that align with different text attributes, such as word frequency, sentiment, and so on. </span><span class="kobospan" id="kobo.35.4">The dimensions of such a matrix are represented by the notation </span><em class="italic"><span class="kobospan" id="kobo.36.1">n × d</span></em><span class="kobospan" id="kobo.37.1">, where </span><em class="italic"><span class="kobospan" id="kobo.38.1">n</span></em><span class="kobospan" id="kobo.39.1"> is the number of rows (i.e., text documents), and </span><em class="italic"><span class="kobospan" id="kobo.40.1">d</span></em><span class="kobospan" id="kobo.41.1"> is the number of columns (</span><span><span class="kobospan" id="kobo.42.1">i.e., attributes).</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.43.1">Let’s move on to the basic operations for scalars, vectors, and </span><span><span class="kobospan" id="kobo.44.1">matrices next.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.45.1">The basic operations for scalars, vectors, and matrices—addition and subtraction—can be carried out on vectors with the same dimensions. </span><span class="kobospan" id="kobo.45.2">Let’s have </span><span><span class="kobospan" id="kobo.46.1">two vectors:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.47.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/1.png" class="calibre18"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.48.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/2.png" class="calibre19"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.49.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/3.png" class="calibre20"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.50.1">For example, if we have two vectors, a = [4,1] and b = [2,4], then </span><em class="italic"><span class="kobospan" id="kobo.51.1">a  + b = [</span></em><span><em class="italic"><span class="kobospan" id="kobo.52.1">6,5]</span></em></span><span><span class="kobospan" id="kobo.53.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.54.1">Let’s visualize this </span><span><span class="kobospan" id="kobo.55.1">as </span></span><span><a id="_idIndexMarker031" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.56.1">follows:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer013">
<span class="kobospan" id="kobo.57.1"><img alt="Figure 2.1 – Adding two vectors (a = [4,1] and  b = [2,4]) means that a  + b = [6,5]" src="image/B18949_02_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.58.1">Figure 2.1 – Adding two vectors (a = [4,1] and  b = [2,4]) means that a  + b = [6,5]</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.59.1">It is possible to scale a vector by multiplying it by a scalar. </span><span class="kobospan" id="kobo.59.2">This operation is performed by multiplying each component of the vector by the scalar value. </span><span class="kobospan" id="kobo.59.3">For example, let’s consider a n-dimensional vector, </span><span class="kobospan" id="kobo.60.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/4.png" class="calibre21"/></span><span class="kobospan" id="kobo.61.1">. </span><span class="kobospan" id="kobo.61.2">The process of scaling this vector by a factor of </span><span class="kobospan" id="kobo.62.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/5.png" class="calibre22"/></span><span class="kobospan" id="kobo.63.1"> can be represented mathematically </span><span><span class="kobospan" id="kobo.64.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.65.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/1.png" class="calibre18"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.66.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/7.png" class="calibre23"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.67.1">This operation results in a new vector that has the same dimensionality as the original vector but with each component multiplied by the scalar </span><span><span class="kobospan" id="kobo.68.1">value </span></span><span><span class="kobospan" id="kobo.69.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/8.png" class="calibre24"/></span></span><span><span class="kobospan" id="kobo.70.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.71.1">There are two types of</span><a id="_idIndexMarker032" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.72.1"> multiplications between vectors: dot product (</span><span class="kobospan" id="kobo.73.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/9.png" class="calibre25"/></span><span class="kobospan" id="kobo.74.1">) and cross product (</span><span class="kobospan" id="kobo.75.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/10.png" class="calibre26"/></span><span class="kobospan" id="kobo.76.1">). </span><span class="kobospan" id="kobo.76.2">The dot product is the one we use often in </span><span><span class="kobospan" id="kobo.77.1">ML algorithms.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.78.1">The dot product is a mathematical operation that can be applied to two vectors, </span><span><span class="kobospan" id="kobo.79.1">x</span></span><span> </span><span><span class="kobospan" id="kobo.80.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.81.1">[</span></span><span><span class="kobospan" id="kobo.82.1">x</span></span><span><span class="kobospan" id="kobo.83.1"> </span></span><span><span class="kobospan" id="kobo.84.1">1</span></span><span><span class="kobospan" id="kobo.85.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.86.1">x</span></span><span><span class="kobospan" id="kobo.87.1"> </span></span><span><span class="kobospan" id="kobo.88.1">2</span></span><span><span class="kobospan" id="kobo.89.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.90.1">…</span></span><span> </span><span><span class="kobospan" id="kobo.91.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.92.1">x</span></span><span><span class="kobospan" id="kobo.93.1"> </span></span><span><span class="kobospan" id="kobo.94.1">n</span></span><span><span class="kobospan" id="kobo.95.1">]</span></span><span class="kobospan" id="kobo.96.1"> and </span><span class="kobospan" id="kobo.97.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/11.png" class="calibre27"/></span><span class="kobospan" id="kobo.98.1">. </span><span class="kobospan" id="kobo.98.2">It has many practical applications, one of which is to help determine their similarity. </span><span class="kobospan" id="kobo.98.3">It is defined as the sum of the product of the corresponding elements of the two vectors. </span><span class="kobospan" id="kobo.98.4">The dot product of </span><em class="italic"><span class="kobospan" id="kobo.99.1">x</span></em><span class="kobospan" id="kobo.100.1"> and </span><em class="italic"><span class="kobospan" id="kobo.101.1">y</span></em><span class="kobospan" id="kobo.102.1"> is represented by the symbol </span><em class="italic"><span class="kobospan" id="kobo.103.1">x</span></em><em class="italic"><span class="kobospan" id="kobo.104.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/12.png" class="calibre28"/></span></em> <em class="italic"><span class="kobospan" id="kobo.105.1">y</span></em><span class="kobospan" id="kobo.106.1"> (having a dot in the middle) and is defined </span><span><span class="kobospan" id="kobo.107.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.108.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/13.png" class="calibre29"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.109.1">where </span><em class="italic"><span class="kobospan" id="kobo.110.1">n</span></em><span class="kobospan" id="kobo.111.1"> represents the dimensionality of the vectors. </span><span class="kobospan" id="kobo.111.2">The dot product is a scalar quantity and can be used to measure the angle between two vectors, as well as the projection of one vector onto another. </span><span class="kobospan" id="kobo.111.3">It also serves a vital function in numerous ML algorithms, including linear regression and </span><span><span class="kobospan" id="kobo.112.1">neural networks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.113.1">The dot product is commutative, meaning that the order of the vectors does not affect the result. </span><span class="kobospan" id="kobo.113.2">This means that </span><em class="italic"><span class="kobospan" id="kobo.114.1">x  </span></em><em class="italic"><span class="kobospan" id="kobo.115.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/12.png" class="calibre30"/></span></em><em class="italic"><span class="kobospan" id="kobo.116.1"> y = y</span></em><em class="italic"><span class="kobospan" id="kobo.117.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/12.png" class="calibre30"/></span></em><em class="italic"><span class="kobospan" id="kobo.118.1"> x</span></em><span class="kobospan" id="kobo.119.1">. </span><span class="kobospan" id="kobo.119.2">Furthermore, the dot product maintains the distributive property of scalar multiplication, implying </span><span><span class="kobospan" id="kobo.120.1">the following:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.121.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/16.png" class="calibre31"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.122.1">The dot product of a vector with itself is also known as its squared norm or Euclidean norm. </span><span class="kobospan" id="kobo.122.2">The norm, symbolized by </span><em class="italic"><span class="kobospan" id="kobo.123.1">𝑛𝑜𝑟𝑚</span></em><em class="italic"><span class="kobospan" id="kobo.124.1">(x)</span></em><span class="kobospan" id="kobo.125.1">, signifies the </span><a id="_idIndexMarker033" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.126.1">length of the vector and is</span><a id="_idIndexMarker034" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.127.1">computed as</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.128.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/17.png" class="calibre32"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.129.1">The normalization of vectors</span><a id="_idIndexMarker035" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.130.1"> can be achieved by dividing them by their norm, also known as the Euclidean norm or the length of the vector. </span><span class="kobospan" id="kobo.130.2">This results in a vector with a unit length, denoted by </span><em class="italic"><span class="kobospan" id="kobo.131.1">x’</span></em><span class="kobospan" id="kobo.132.1">. </span><span class="kobospan" id="kobo.132.2">The normalization process can be </span><span><span class="kobospan" id="kobo.133.1">shown as</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.134.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/18.png" class="calibre33"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.135.1">where </span><span class="kobospan" id="kobo.136.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/19.png" class="calibre34"/></span><span class="kobospan" id="kobo.137.1"> is the original vector and </span><span class="kobospan" id="kobo.138.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/20.png" class="calibre35"/></span><span class="kobospan" id="kobo.139.1"> represents its norm. </span><span class="kobospan" id="kobo.139.2">It should be noted that normalizing a vector has the effect of retaining its direction while setting its length to 1, allowing the meaningful comparison of vectors in </span><span><span class="kobospan" id="kobo.140.1">different spaces.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.141.1">The cosine similarity between two vectors  </span><span class="kobospan" id="kobo.142.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/21.png" class="calibre36"/></span><span class="kobospan" id="kobo.143.1">and  </span><span class="kobospan" id="kobo.144.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/22.png" class="calibre37"/></span><span class="kobospan" id="kobo.145.1"> is mathematically represented as the dot product of the two vectors after they have been normalized to unit length. </span><span class="kobospan" id="kobo.145.2">This can be written </span><span><span class="kobospan" id="kobo.146.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.147.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/23.png" class="calibre38"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.148.1">where </span><span class="kobospan" id="kobo.149.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/24.png" class="calibre39"/></span><span class="kobospan" id="kobo.150.1"> and </span><span class="kobospan" id="kobo.151.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/25.png" class="calibre40"/></span><span class="kobospan" id="kobo.152.1"> are the norms of the vectors </span><em class="italic"><span class="kobospan" id="kobo.153.1">x</span></em><span class="kobospan" id="kobo.154.1"> and </span><em class="italic"><span class="kobospan" id="kobo.155.1">y</span></em><span class="kobospan" id="kobo.156.1">, respectively. </span><span class="kobospan" id="kobo.156.2">This computed cosine similarity between </span><em class="italic"><span class="kobospan" id="kobo.157.1">x</span></em><span class="kobospan" id="kobo.158.1"> and </span><em class="italic"><span class="kobospan" id="kobo.159.1">y</span></em><span class="kobospan" id="kobo.160.1"> is equivalent to the cosine of the angle between the two vectors, denoted </span><span><span class="kobospan" id="kobo.161.1">as </span></span><span><em class="italic"><span class="kobospan" id="kobo.162.1">θ</span></em></span><span><span class="kobospan" id="kobo.163.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.164.1">Vectors with a dot product of </span><em class="italic"><span class="kobospan" id="kobo.165.1">0</span></em><span class="kobospan" id="kobo.166.1"> are deemed orthogonal, implying that in the case of having both non-</span><em class="italic"><span class="kobospan" id="kobo.167.1">0</span></em><span class="kobospan" id="kobo.168.1"> vectors, the angle between them is 90 degrees. </span><span class="kobospan" id="kobo.168.2">We can conclude that a </span><em class="italic"><span class="kobospan" id="kobo.169.1">0</span></em><span class="kobospan" id="kobo.170.1"> vector is orthogonal to any vector. </span><span class="kobospan" id="kobo.170.2">A group of vectors is considered orthogonal if each pair of them is orthogonal and each vector possesses a norm of </span><em class="italic"><span class="kobospan" id="kobo.171.1">1</span></em><span class="kobospan" id="kobo.172.1">. </span><span class="kobospan" id="kobo.172.2">Such orthonormal sets prove to be valuable in numerous mathematical contexts. </span><span class="kobospan" id="kobo.172.3">For instance, they come into play when transforming between different orthogonal co-ordinate systems, where the new co-ordinates of a point are computed in relation to the modified direction set. </span><span class="kobospan" id="kobo.172.4">This approach, known as co-ordinate transformation in the field of analytical geometry, finds</span><a id="_idIndexMarker036" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.173.1"> widespread application in the realm of </span><span><span class="kobospan" id="kobo.174.1">linear algebra.</span></span></p>
<h2 id="_idParaDest-30" class="calibre7"><a id="_idTextAnchor030" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.175.1">Basic operations on matrices and vectors</span></h2>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.176.1">Matrix transpose</span></strong><span class="kobospan" id="kobo.177.1"> is the</span><a id="_idIndexMarker037" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.178.1"> process of obtaining the transpose of a matrix and involves interchanging its </span><a id="_idIndexMarker038" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.179.1">rows and columns. </span><span class="kobospan" id="kobo.179.2">This means that the element originally at the </span><em class="italic"><span class="kobospan" id="kobo.180.1">(i, j)</span></em><span class="kobospan" id="kobo.181.1">th position in the matrix now occupies the </span><em class="italic"><span class="kobospan" id="kobo.182.1">(j, i)</span></em><span class="kobospan" id="kobo.183.1">th position in its</span><a id="_idIndexMarker039" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.184.1"> transpose. </span><span class="kobospan" id="kobo.184.2">As a result, a matrix that was originally of size </span><em class="italic"><span class="kobospan" id="kobo.185.1">n × m</span></em><span class="kobospan" id="kobo.186.1"> becomes an </span><em class="italic"><span class="kobospan" id="kobo.187.1">m × n</span></em><span class="kobospan" id="kobo.188.1"> matrix when transposed. </span><span class="kobospan" id="kobo.188.2">The notation used to represent the transpose of matrix </span><em class="italic"><span class="kobospan" id="kobo.189.1">X</span></em><span class="kobospan" id="kobo.190.1"> is </span><span class="kobospan" id="kobo.191.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/26.png" class="calibre41"/></span><span class="kobospan" id="kobo.192.1">. </span><span class="kobospan" id="kobo.192.2">Here’s an illustrative example of a matrix </span><span><span class="kobospan" id="kobo.193.1">transposition operation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.194.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/27.png" class="calibre42"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.195.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/28.png" class="calibre43"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.196.1">Crucially, the transpose </span><span class="kobospan" id="kobo.197.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/29.png" class="calibre44"/></span><span class="kobospan" id="kobo.198.1"> of matrix </span><span class="kobospan" id="kobo.199.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/26.png" class="calibre45"/></span><span class="kobospan" id="kobo.200.1"> reverts to the original matrix </span><em class="italic"><span class="kobospan" id="kobo.201.1">X</span></em><span class="kobospan" id="kobo.202.1">. </span><span class="kobospan" id="kobo.202.2">Moreover, it is clear that row vectors can be transposed into column vectors and vice versa. </span><span class="kobospan" id="kobo.202.3">Additionally, the following holds true for both matrices </span><span><span class="kobospan" id="kobo.203.1">and vectors:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.204.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/31.png" class="calibre46"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.205.1">It’s also noteworthy that dot products are commutative for matrices </span><span><span class="kobospan" id="kobo.206.1">and vectors:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.207.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/32.png" class="calibre47"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.208.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/33.png" class="calibre48"/></span></p>
<h2 id="_idParaDest-31" class="calibre7"><a id="_idTextAnchor031" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.209.1">Matrix definitions</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.210.1">In this section, we’ll cover the different type of </span><span><span class="kobospan" id="kobo.211.1">matrix definitions:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.212.1">Symmetric matrix</span></strong><span class="kobospan" id="kobo.213.1">: A symmetric</span><a id="_idIndexMarker040" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.214.1"> matrix is a type of square matrix where the transpose of the matrix is equal to the original matrix. </span><span class="kobospan" id="kobo.214.2">In</span><a id="_idIndexMarker041" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.215.1"> mathematical terms, if a matrix </span><em class="italic"><span class="kobospan" id="kobo.216.1">X</span></em><span class="kobospan" id="kobo.217.1"> is symmetric, then </span><span class="kobospan" id="kobo.218.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/34.png" class="calibre49"/></span><span class="kobospan" id="kobo.219.1">.  </span><span><span class="kobospan" id="kobo.220.1">For example,</span></span><p class="calibre6"><span class="kobospan" id="kobo.221.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;7&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/35.png" class="calibre50"/></span></p><p class="calibre6"><span><span class="kobospan" id="kobo.222.1">is symmetric.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.223.1">Rectangular diagonal matrix</span></strong><span class="kobospan" id="kobo.224.1">: This is a matrix that is </span><em class="italic"><span class="kobospan" id="kobo.225.1">m × n</span></em><span class="kobospan" id="kobo.226.1"> in dimensions, with non-</span><em class="italic"><span class="kobospan" id="kobo.227.1">0</span></em><span class="kobospan" id="kobo.228.1"> values </span><a id="_idIndexMarker042" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.229.1">only on the </span><span><span class="kobospan" id="kobo.230.1">main </span></span><span><a id="_idIndexMarker043" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.231.1">diagonal.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.232.1">Upper (or Lower) triangular matrix</span></strong><span class="kobospan" id="kobo.233.1">: A matrix is called an upper (triangular) matrix if all the entries (i,j) below (above) its main diagonal are 0. </span><span class="kobospan" id="kobo.233.2">Next, we are going to describe </span><span><span class="kobospan" id="kobo.234.1">matrix</span></span><span><a id="_idIndexMarker044" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.235.1"> operations.</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.236.1">Determinants</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.237.1">The determinant of a square </span><a id="_idIndexMarker045" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.238.1">matrix provides a notion of its impact on the volume of a </span><em class="italic"><span class="kobospan" id="kobo.239.1">d</span></em><span class="kobospan" id="kobo.240.1">-dimensional object when multiplied by its co-ordinate vectors. </span><span class="kobospan" id="kobo.240.2">The determinant, symbolized as </span><em class="italic"><span class="kobospan" id="kobo.241.1">det(A)</span></em><span class="kobospan" id="kobo.242.1">, represents the (signed) volume of the parallelepiped formed by the row or column vectors of the matrix. </span><span class="kobospan" id="kobo.242.2">This interpretation holds consistently, as the</span><a id="_idIndexMarker046" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.243.1"> volume determined by the row and column vectors is mathematically identical. </span><span class="kobospan" id="kobo.243.2">When a diagonalizable matrix </span><em class="italic"><span class="kobospan" id="kobo.244.1">A</span></em><span class="kobospan" id="kobo.245.1"> interacts with a group of co-ordinate vectors, the ensuing distortion is termed anisotropic scaling. </span><span class="kobospan" id="kobo.245.2">The determinant can aid in establishing the scale factors of this conversion. </span><span class="kobospan" id="kobo.245.3">The determinant of a square matrix carries crucial insights about the linear alteration accomplished by the multiplication with the matrix. </span><span class="kobospan" id="kobo.245.4">Particularly, the sign of the determinant mirrors the impact of the transformation on the basis of the </span><span><span class="kobospan" id="kobo.246.1">system’s orientation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.247.1">Calculating determinant is given </span><span><span class="kobospan" id="kobo.248.1">as </span></span><span><a id="_idIndexMarker047" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.249.1">follows:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.250.1">For a </span><em class="italic"><span class="kobospan" id="kobo.251.1">1×1</span></em><span class="kobospan" id="kobo.252.1"> matrix </span><em class="italic"><span class="kobospan" id="kobo.253.1">A</span></em><span class="kobospan" id="kobo.254.1">, its determinant is equivalent to the single scalar present </span><span><span class="kobospan" id="kobo.255.1">within it.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.256.1">For larger matrices, the determinant can be calculated by securing a column, </span><em class="italic"><span class="kobospan" id="kobo.257.1">j</span></em><span class="kobospan" id="kobo.258.1">, and then broadening using the elements within that column. </span><span class="kobospan" id="kobo.258.2">As another option, it’s possible to fix a row, </span><em class="italic"><span class="kobospan" id="kobo.259.1">i</span></em><span class="kobospan" id="kobo.260.1">, and expand along that particular row. </span><span class="kobospan" id="kobo.260.2">Regardless of whether you opt to fix a row</span><a id="_idIndexMarker048" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.261.1"> or column, the end result, which is the determinant of the matrix, will </span><span><span class="kobospan" id="kobo.262.1">remain consistent.</span></span><p class="calibre6"><span class="kobospan" id="kobo.263.1">with </span><em class="italic"><span class="kobospan" id="kobo.264.1">j</span></em><span class="kobospan" id="kobo.265.1"> as a fixed value ranging from </span><em class="italic"><span class="kobospan" id="kobo.266.1">1</span></em> <span><span class="kobospan" id="kobo.267.1">to </span></span><span><em class="italic"><span class="kobospan" id="kobo.268.1">d</span></em></span><span><span class="kobospan" id="kobo.269.1">,</span></span></p></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.270.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/36.png" class="calibre51"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.271.1">Or, with the </span><span><span class="kobospan" id="kobo.272.1">fixed </span></span><span><em class="italic"><span class="kobospan" id="kobo.273.1">i</span></em></span><span><span class="kobospan" id="kobo.274.1">,</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.275.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;det&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi&gt;det&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/37.png" class="calibre52"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.276.1">Based on the following equations, we can see that some of the cases can be </span><span><span class="kobospan" id="kobo.277.1">easily calculated:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.278.1">Diagonal matrix</span></strong><span class="kobospan" id="kobo.279.1">: For a </span><a id="_idIndexMarker049" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.280.1">diagonal matrix, the determinant is the product of its </span><span><span class="kobospan" id="kobo.281.1">diagonal elements.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.282.1">Triangular matrix</span></strong><span class="kobospan" id="kobo.283.1">: In the context of </span><a id="_idIndexMarker050" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.284.1">a triangular matrix, the determinant is found by multiplying all its diagonal elements. </span><span class="kobospan" id="kobo.284.2">If all components of a matrix’s row or column are 0, the determinant is </span><span><span class="kobospan" id="kobo.285.1">also 0.</span></span><p class="calibre6"><span class="kobospan" id="kobo.286.1">For a </span><em class="italic"><span class="kobospan" id="kobo.287.1">2 × 2</span></em> <span><span class="kobospan" id="kobo.288.1">matrix of</span></span></p><p class="calibre6"><span class="kobospan" id="kobo.289.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/38.png" class="calibre53"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.290.1">Its determinant can be computed as </span><em class="italic"><span class="kobospan" id="kobo.291.1">ad - bc</span></em><span class="kobospan" id="kobo.292.1">. </span><span class="kobospan" id="kobo.292.2">If we consider a </span><em class="italic"><span class="kobospan" id="kobo.293.1">3 × </span></em><span><em class="italic"><span class="kobospan" id="kobo.294.1">3</span></em></span><span><span class="kobospan" id="kobo.295.1"> matrix,</span></span></p><p class="calibre6"><span class="kobospan" id="kobo.296.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/39.png" class="calibre54"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.297.1">The determinant is calculated </span><span><span class="kobospan" id="kobo.298.1">as follows:</span></span></p></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.299.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/40.png" class="calibre55"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.300.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/41.png" class="calibre56"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.301.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/42.png" class="calibre57"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.302.1">Let’s now move on to </span><a id="_idIndexMarker051" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.303.1">eigenvalues </span><span><span class="kobospan" id="kobo.304.1">and vectors.</span></span></p>
<h1 id="_idParaDest-32" class="calibre4"><a id="_idTextAnchor032" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.305.1">Eigenvalues and eigenvectors</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.306.1">A vector </span><em class="italic"><span class="kobospan" id="kobo.307.1">x</span></em><span class="kobospan" id="kobo.308.1">, belonging to a </span><em class="italic"><span class="kobospan" id="kobo.309.1">d × d</span></em><span class="kobospan" id="kobo.310.1"> matrix </span><em class="italic"><span class="kobospan" id="kobo.311.1">A</span></em><span class="kobospan" id="kobo.312.1">, is an </span><strong class="bold"><span class="kobospan" id="kobo.313.1">eigenvector</span></strong><span class="kobospan" id="kobo.314.1"> if it satisfies the equation </span><em class="italic"><span class="kobospan" id="kobo.315.1">Ax = λx</span></em><span class="kobospan" id="kobo.316.1">, where </span><em class="italic"><span class="kobospan" id="kobo.317.1">λ</span></em><span class="kobospan" id="kobo.318.1"> represents the</span><a id="_idIndexMarker052" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.319.1"> eigenvalue associated with the matrix. </span><span class="kobospan" id="kobo.319.2">This relationship delineates the link between matrix </span><em class="italic"><span class="kobospan" id="kobo.320.1">A</span></em><span class="kobospan" id="kobo.321.1"> and its corresponding eigenvector </span><em class="italic"><span class="kobospan" id="kobo.322.1">x</span></em><span class="kobospan" id="kobo.323.1">, which can be perceived </span><a id="_idIndexMarker053" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.324.1">as the “stretching direction” of the matrix. </span><span class="kobospan" id="kobo.324.2">In the case where </span><em class="italic"><span class="kobospan" id="kobo.325.1">A</span></em><span class="kobospan" id="kobo.326.1"> is a matrix that can be diagonalized, it can be deconstructed into a </span><em class="italic"><span class="kobospan" id="kobo.327.1">d × d</span></em><span class="kobospan" id="kobo.328.1"> invertible matrix, </span><em class="italic"><span class="kobospan" id="kobo.329.1">V</span></em><span class="kobospan" id="kobo.330.1">, and a diagonal </span><em class="italic"><span class="kobospan" id="kobo.331.1">d × d</span></em><span class="kobospan" id="kobo.332.1"> matrix, </span><em class="italic"><span class="kobospan" id="kobo.333.1">Δ</span></em><span class="kobospan" id="kobo.334.1">, </span><span><span class="kobospan" id="kobo.335.1">such that</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.336.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Δ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/43.png" class="calibre58"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.337.1">The columns of </span><em class="italic"><span class="kobospan" id="kobo.338.1">V</span></em><span class="kobospan" id="kobo.339.1"> encompass </span><em class="italic"><span class="kobospan" id="kobo.340.1">d</span></em><span class="kobospan" id="kobo.341.1"> eigenvectors, while the diagonal entries of </span><em class="italic"><span class="kobospan" id="kobo.342.1">Δ</span></em><span class="kobospan" id="kobo.343.1"> house the corresponding eigenvalues. </span><span class="kobospan" id="kobo.343.2">The linear transformation </span><em class="italic"><span class="kobospan" id="kobo.344.1">Ax</span></em><span class="kobospan" id="kobo.345.1"> can be visually understood through a sequence of three operations. </span><span class="kobospan" id="kobo.345.2">Initially, the multiplication of </span><em class="italic"><span class="kobospan" id="kobo.346.1">x</span></em><span class="kobospan" id="kobo.347.1"> by </span><span class="kobospan" id="kobo.348.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/44.png" class="calibre59"/></span><span class="kobospan" id="kobo.349.1">  calculates </span><em class="italic"><span class="kobospan" id="kobo.350.1">x</span></em><span class="kobospan" id="kobo.351.1">’s co-ordinates in a non-orthogonal basis associated with </span><em class="italic"><span class="kobospan" id="kobo.352.1">V</span></em><span class="kobospan" id="kobo.353.1">’s columns. </span><span class="kobospan" id="kobo.353.2">Subsequently, the multiplication of </span><span class="kobospan" id="kobo.354.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/45.png" class="calibre60"/></span><span class="kobospan" id="kobo.355.1"> x by </span><em class="italic"><span class="kobospan" id="kobo.356.1">Δ</span></em><span class="kobospan" id="kobo.357.1"> scales these co-ordinates using the factors in </span><em class="italic"><span class="kobospan" id="kobo.358.1">Δ</span></em><span class="kobospan" id="kobo.359.1">, aligned with the eigenvectors’ directions. </span><span class="kobospan" id="kobo.359.2">Finally, the multiplication with </span><em class="italic"><span class="kobospan" id="kobo.360.1">V</span></em><span class="kobospan" id="kobo.361.1"> restores the co-ordinates to the original basis, resulting in an anisotropic scaling along the </span><em class="italic"><span class="kobospan" id="kobo.362.1">d</span></em> <span><span class="kobospan" id="kobo.363.1">eigenvector directions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.364.1">Diagonalizable matrices signify transformations involving anisotropic scaling along </span><em class="italic"><span class="kobospan" id="kobo.365.1">d</span></em><span class="kobospan" id="kobo.366.1">-linearly independent directions. </span><span class="kobospan" id="kobo.366.2">When </span><em class="italic"><span class="kobospan" id="kobo.367.1">V</span></em><span class="kobospan" id="kobo.368.1"> ‘s columns are orthonormal vectors, </span><span class="kobospan" id="kobo.369.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;V&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/46.png" class="calibre61"/></span><span class="kobospan" id="kobo.370.1">equals its transpose, </span><span class="kobospan" id="kobo.371.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/47.png" class="calibre62"/></span><span class="kobospan" id="kobo.372.1">, indicating</span><a id="_idIndexMarker054" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.373.1"> scaling along mutually orthogonal directions. </span><span class="kobospan" id="kobo.373.2">In such cases, matrix </span><em class="italic"><span class="kobospan" id="kobo.374.1">A</span></em><span class="kobospan" id="kobo.375.1"> is always diagonalizable and exhibits symmetry when </span><em class="italic"><span class="kobospan" id="kobo.376.1">V</span></em><span class="kobospan" id="kobo.377.1">’s columns are </span><a id="_idIndexMarker055" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.378.1">orthonormal vectors, as affirmed by the </span><span><span class="kobospan" id="kobo.379.1">following relationship.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.380.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Δ&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/48.png" class="calibre63"/></span></p>
<h2 id="_idParaDest-33" class="calibre7"><a id="_idTextAnchor033" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.381.1">Numerical methods for finding eigenvectors</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.382.1">The conventional method to ascertain the eigenvectors of a </span><em class="italic"><span class="kobospan" id="kobo.383.1">d × d</span></em><span class="kobospan" id="kobo.384.1"> matrix </span><em class="italic"><span class="kobospan" id="kobo.385.1">A</span></em><span class="kobospan" id="kobo.386.1"> involves locating the </span><em class="italic"><span class="kobospan" id="kobo.387.1">d</span></em><span class="kobospan" id="kobo.388.1"> roots, </span><span class="kobospan" id="kobo.389.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/49.png" class="calibre64"/></span><span class="kobospan" id="kobo.390.1">of </span><span><span class="kobospan" id="kobo.391.1">the equation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.392.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;I&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/50.png" class="calibre65"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.393.1">Some of these roots might be</span><a id="_idIndexMarker056" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.394.1"> repeated. </span><span class="kobospan" id="kobo.394.2">The subsequent step involves solving linear systems in the form </span><span class="kobospan" id="kobo.395.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/51.png" class="calibre66"/></span><span class="kobospan" id="kobo.396.1">, typically achieved using the Gaussian elimination method. </span><span class="kobospan" id="kobo.396.2">However, this method might not always be the most stable or precise, as solvers of polynomial equations can exhibit ill-conditioning and numerical instability in practical applications. </span><span class="kobospan" id="kobo.396.3">Indeed, a prevalent technique for resolving high-degree polynomial equations in engineering involves constructing a companion matrix possessing the same characteristic polynomial as the original polynomial and then determining </span><span><span class="kobospan" id="kobo.397.1">its eigenvalues.</span></span></p>
<h2 id="_idParaDest-34" class="calibre7"><a id="_idTextAnchor034" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.398.1">Eigenvalue decomposition</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.399.1">Eigenvalue decomposition, also </span><a id="_idIndexMarker057" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.400.1">known as the eigen-decomposition or the diagonalization of a matrix, is a powerful mathematical tool used in linear algebra and computational mathematics. </span><span class="kobospan" id="kobo.400.2">The goal of eigenvalue decomposition is to decompose a given matrix into a product of matrices that represent the eigenvectors and eigenvalues of </span><span><span class="kobospan" id="kobo.401.1">the matrix.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.402.1">The eigenvalue decomposition of matrix </span><em class="italic"><span class="kobospan" id="kobo.403.1">A</span></em><span class="kobospan" id="kobo.404.1"> is a factorization of the matrix into the product of two matrices: the matrix </span><em class="italic"><span class="kobospan" id="kobo.405.1">V</span></em><span class="kobospan" id="kobo.406.1"> and the </span><span><span class="kobospan" id="kobo.407.1">matrix </span></span><span><em class="italic"><span class="kobospan" id="kobo.408.1">D</span></em></span><span><span class="kobospan" id="kobo.409.1">.</span></span></p>
<p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.410.1">V</span></em><span class="kobospan" id="kobo.411.1"> has column which are the eigenvectors of matrix </span><em class="italic"><span class="kobospan" id="kobo.412.1">A</span></em><span class="kobospan" id="kobo.413.1">, and </span><em class="italic"><span class="kobospan" id="kobo.414.1">D</span></em><span class="kobospan" id="kobo.415.1"> is a diagonal matrix that contains the corresponding eigenvalues on </span><span><span class="kobospan" id="kobo.416.1">its diagnol.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.417.1">The eigenvalue problem is to find the non-0 vectors, </span><em class="italic"><span class="kobospan" id="kobo.418.1">v</span></em><span class="kobospan" id="kobo.419.1">, and the scalars, </span><em class="italic"><span class="kobospan" id="kobo.420.1">λ</span></em><span class="kobospan" id="kobo.421.1">, such that </span><em class="italic"><span class="kobospan" id="kobo.422.1">Av</span></em> <em class="italic"><span class="kobospan" id="kobo.423.1">= λv</span></em><span class="kobospan" id="kobo.424.1">, where </span><em class="italic"><span class="kobospan" id="kobo.425.1">A</span></em><span class="kobospan" id="kobo.426.1"> is a square matrix, and thus </span><em class="italic"><span class="kobospan" id="kobo.427.1">v</span></em><span class="kobospan" id="kobo.428.1"> is an eigenvector of </span><em class="italic"><span class="kobospan" id="kobo.429.1">A</span></em><span class="kobospan" id="kobo.430.1">. </span><span class="kobospan" id="kobo.430.2">The scalar </span><em class="italic"><span class="kobospan" id="kobo.431.1">λ</span></em><span class="kobospan" id="kobo.432.1"> is called the eigenvalue of matrix </span><em class="italic"><span class="kobospan" id="kobo.433.1">A</span></em><span class="kobospan" id="kobo.434.1">. </span><span class="kobospan" id="kobo.434.2">The eigenvalue problem can be written in matrix form as </span><em class="italic"><span class="kobospan" id="kobo.435.1">Av = λIv</span></em><span class="kobospan" id="kobo.436.1">, where </span><em class="italic"><span class="kobospan" id="kobo.437.1">I</span></em><span class="kobospan" id="kobo.438.1"> is the </span><span><span class="kobospan" id="kobo.439.1">identity matrix.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.440.1">The process of determining </span><a id="_idIndexMarker058" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.441.1">eigenvalues is intimately linked to the characteristic equation of matrix </span><em class="italic"><span class="kobospan" id="kobo.442.1">A</span></em><span class="kobospan" id="kobo.443.1">, which is the polynomial equation derived from </span><span class="kobospan" id="kobo.444.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;I&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/52.png" class="calibre67"/></span><span class="kobospan" id="kobo.445.1">. </span><span class="kobospan" id="kobo.445.2">The characteristic equation can be solved for the eigenvalues, </span><em class="italic"><span class="kobospan" id="kobo.446.1">λ</span></em><span class="kobospan" id="kobo.447.1">, which are the roots of the equation. </span><span class="kobospan" id="kobo.447.2">Once the eigenvalues are found, the eigenvectors can be found by solving the system of linear </span><span><span class="kobospan" id="kobo.448.1">equations </span></span><span><span class="kobospan" id="kobo.449.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/53.png" class="calibre68"/></span></span><span><span class="kobospan" id="kobo.450.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.451.1">One important property of eigenvalue decomposition is that it allows us to diagonalize a matrix, which means that we can transform the matrix into a diagonal form by using an appropriate eigenvectors matrix. </span><span class="kobospan" id="kobo.451.2">The diagonal form of a matrix is useful because it allows us to calculate the trace and determinant of the </span><span><span class="kobospan" id="kobo.452.1">matrix easily.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.453.1">Another important property of eigenvalue decomposition is that it provides insight into the structure of the matrix. </span><span class="kobospan" id="kobo.453.2">For example, the eigenvalues of a symmetric matrix are always real, and the eigenvectors are orthogonal, which means that they are perpendicular to each other. </span><span class="kobospan" id="kobo.453.3">In the case of non-symmetric matrices, the eigenvalues can be complex, and the eigenvectors are not </span><span><span class="kobospan" id="kobo.454.1">necessarily orthogonal.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.455.1">The eigenvalue decomposition of a matrix has many applications in mathematics, physics, engineering, and computer science. </span><span class="kobospan" id="kobo.455.2">In numerical analysis, eigenvalue decomposition is used to find the solution of linear systems, compute the eigenvalues of a matrix, and find the eigenvectors of a matrix. </span><span class="kobospan" id="kobo.455.3">In physics, eigenvalue decomposition is used to analyze the stability of systems, such as the stability of an equilibrium point in a differential equation. </span><span class="kobospan" id="kobo.455.4">In engineering, eigenvalue decomposition is used to study the dynamics of systems, such as the vibrations of a </span><span><span class="kobospan" id="kobo.456.1">mechanical system.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.457.1">Within the field of computer science, eigenvalue decomposition finds versatile applications across various domains, including machine learning and data analysis. </span><span class="kobospan" id="kobo.457.2">In machine learning, eigenvalue decomposition plays a pivotal role in enabling </span><strong class="bold"><span class="kobospan" id="kobo.458.1">principal</span></strong> <strong class="bold"><span class="kobospan" id="kobo.459.1">component analysis</span></strong><span class="kobospan" id="kobo.460.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.461.1">PCA</span></strong><span class="kobospan" id="kobo.462.1">), a technique</span><a id="_idIndexMarker059" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.463.1"> employed for dimensionality reduction in extensive datasets. </span><span class="kobospan" id="kobo.463.2">In the realm of data analysis, eigenvalue decomposition is harnessed to calculate the </span><strong class="bold"><span class="kobospan" id="kobo.464.1">singular value decomposition</span></strong><span class="kobospan" id="kobo.465.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.466.1">SVD</span></strong><span class="kobospan" id="kobo.467.1">), a potent tool for dissecting and understanding </span><span><span class="kobospan" id="kobo.468.1">complex</span></span><span><a id="_idIndexMarker060" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.469.1"> datasets.</span></span></p>
</div>


<div id="_idContainer139" class="calibre2">
<h2 id="_idParaDest-35" class="calibre7"><a id="_idTextAnchor035" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.470.1">Singular value decomposition</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.471.1">The problem of minimizing </span><span class="kobospan" id="kobo.472.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/54.png" class="calibre69"/></span><span class="kobospan" id="kobo.473.1">, where </span><em class="italic"><span class="kobospan" id="kobo.474.1">x</span></em><span class="kobospan" id="kobo.475.1"> is a column vector that has a unit norm, and </span><em class="italic"><span class="kobospan" id="kobo.476.1">A</span></em><span class="kobospan" id="kobo.477.1"> is a symmetric </span><em class="italic"><span class="kobospan" id="kobo.478.1">d × d</span></em><span class="kobospan" id="kobo.479.1"> data matrix, is a typical </span><a id="_idIndexMarker061" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.480.1">problem encountered in numerous machine learning contexts. </span><span class="kobospan" id="kobo.480.2">This problem type is often found in applications such as principal component analysis, singular value decomposition, and spectral clustering, all of which involve feature engineering and dimensionality reduction. </span><span class="kobospan" id="kobo.480.3">The optimization problem can be articulated </span><span><span class="kobospan" id="kobo.481.1">as follows:</span></span></p>
<p class="calibre6"><span><span class="kobospan" id="kobo.482.1">Minimize</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.483.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/55.png" class="calibre70"/></span></p>
<p class="calibre6"><span><span class="kobospan" id="kobo.484.1">Subject to</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.485.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/56.png" class="calibre71"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.486.1">We can solve the optimization problem as a maximization or minimization form. </span><span class="kobospan" id="kobo.486.2">Imposing the constraint that vector </span><em class="italic"><span class="kobospan" id="kobo.487.1">x</span></em><span class="kobospan" id="kobo.488.1"> must be a unit vector significantly changes the nature of the optimization problem. </span><span class="kobospan" id="kobo.488.2">In contrast to the prior section, the positive semi-definiteness of matrix </span><em class="italic"><span class="kobospan" id="kobo.489.1">A</span></em><span class="kobospan" id="kobo.490.1"> is no longer crucial for determining the solution. </span><span class="kobospan" id="kobo.490.2">Even when </span><em class="italic"><span class="kobospan" id="kobo.491.1">A</span></em><span class="kobospan" id="kobo.492.1"> is indefinite, the constraint on the norm of </span><a id="_idIndexMarker062" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.493.1">vector </span><em class="italic"><span class="kobospan" id="kobo.494.1">x</span></em><span class="kobospan" id="kobo.495.1"> ensures a well-defined solution, preventing the involvement of vectors with unbounded magnitudes or trivial solutions, such as the </span><em class="italic"><span class="kobospan" id="kobo.496.1">0</span></em><span class="kobospan" id="kobo.497.1"> vector. </span><strong class="bold"><span class="kobospan" id="kobo.498.1">Value singular decomposition</span></strong><span class="kobospan" id="kobo.499.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.500.1">SVD</span></strong><span class="kobospan" id="kobo.501.1">) is a mathematical technique that takes a rectangular matrix, </span><em class="italic"><span class="kobospan" id="kobo.502.1">A</span></em><span class="kobospan" id="kobo.503.1">, and decomposes it into three matrices: </span><em class="italic"><span class="kobospan" id="kobo.504.1">U</span></em><span class="kobospan" id="kobo.505.1">, </span><em class="italic"><span class="kobospan" id="kobo.506.1">S</span></em><span class="kobospan" id="kobo.507.1">, and </span><span class="kobospan" id="kobo.508.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/57.png" class="calibre72"/></span><span class="kobospan" id="kobo.509.1">. </span><span class="kobospan" id="kobo.509.2">Matrix </span><em class="italic"><span class="kobospan" id="kobo.510.1">A</span></em><span class="kobospan" id="kobo.511.1"> is defined as an </span><em class="italic"><span class="kobospan" id="kobo.512.1">n × p</span></em><span class="kobospan" id="kobo.513.1"> matrix. </span><span class="kobospan" id="kobo.513.2">The theorem of SVD states that </span><em class="italic"><span class="kobospan" id="kobo.514.1">A</span></em><span class="kobospan" id="kobo.515.1"> can be represented as the product of three matrices: </span><span class="kobospan" id="kobo.516.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/58.png" class="calibre73"/></span><span class="kobospan" id="kobo.517.1">, where </span><span class="kobospan" id="kobo.518.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/59.png" class="calibre74"/></span><span class="kobospan" id="kobo.519.1">, and </span><span class="kobospan" id="kobo.520.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/60.png" class="calibre75"/></span><span class="kobospan" id="kobo.521.1">, and </span><em class="italic"><span class="kobospan" id="kobo.522.1">U</span></em><span class="kobospan" id="kobo.523.1"> and </span><em class="italic"><span class="kobospan" id="kobo.524.1">V</span></em><span class="kobospan" id="kobo.525.1"> are </span><span><span class="kobospan" id="kobo.526.1">orthogonal matrices.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.527.1">The </span><em class="italic"><span class="kobospan" id="kobo.528.1">U</span></em><span class="kobospan" id="kobo.529.1"> matrix’s columns are known as the left singular vectors, while the rows of the transpose of the </span><em class="italic"><span class="kobospan" id="kobo.530.1">V</span></em><span class="kobospan" id="kobo.531.1"> matrix </span><span class="kobospan" id="kobo.532.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/47.png" class="calibre76"/></span><span class="kobospan" id="kobo.533.1"> are the right singular vectors. </span><span class="kobospan" id="kobo.533.2">The </span><em class="italic"><span class="kobospan" id="kobo.534.1">S</span></em><span class="kobospan" id="kobo.535.1"> matrix, with singular values, is a diagonal matrix of the same size as </span><em class="italic"><span class="kobospan" id="kobo.536.1">A</span></em><span class="kobospan" id="kobo.537.1">. </span><span class="kobospan" id="kobo.537.2">SVD decomposes the original data into a co-ordinate system where the defining vectors are orthonormal (both orthogonal and normal). </span><span class="kobospan" id="kobo.537.3">SVD computation involves identifying the eigenvalues and eigenvectors of matrices </span><span class="kobospan" id="kobo.538.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/62.png" class="calibre77"/></span><span class="kobospan" id="kobo.539.1"> and </span><span class="kobospan" id="kobo.540.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/63.png" class="calibre78"/></span><span class="kobospan" id="kobo.541.1">. </span><span class="kobospan" id="kobo.541.2">Matrix </span><em class="italic"><span class="kobospan" id="kobo.542.1">V</span></em><span class="kobospan" id="kobo.543.1">’s columns consist of eigenvectors from </span><span class="kobospan" id="kobo.544.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/64.png" class="calibre79"/></span><span class="kobospan" id="kobo.545.1">, and matrix </span><em class="italic"><span class="kobospan" id="kobo.546.1">U</span></em><span class="kobospan" id="kobo.547.1">’s columns consist of eigenvectors from </span><span class="kobospan" id="kobo.548.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/65.png" class="calibre80"/></span><span class="kobospan" id="kobo.549.1">. </span><span class="kobospan" id="kobo.549.2">Singular values in the </span><em class="italic"><span class="kobospan" id="kobo.550.1">S</span></em><span class="kobospan" id="kobo.551.1"> matrix are derived from the square roots of eigenvalues from either </span><span class="kobospan" id="kobo.552.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/66.png" class="calibre81"/></span><span class="kobospan" id="kobo.553.1"> or </span><span class="kobospan" id="kobo.554.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/67.png" class="calibre82"/></span><span class="kobospan" id="kobo.555.1">, organized in decreasing order. </span><span class="kobospan" id="kobo.555.2">These singular </span><a id="_idIndexMarker063" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.556.1">values are real numbers. </span><span class="kobospan" id="kobo.556.2">If </span><em class="italic"><span class="kobospan" id="kobo.557.1">A</span></em><span class="kobospan" id="kobo.558.1"> is a real matrix, </span><em class="italic"><span class="kobospan" id="kobo.559.1">U</span></em><span class="kobospan" id="kobo.560.1"> and </span><em class="italic"><span class="kobospan" id="kobo.561.1">V</span></em><span class="kobospan" id="kobo.562.1"> will also </span><span><span class="kobospan" id="kobo.563.1">be real.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.564.1">To illustrate the calculation of SVD, an example is provided. </span><span class="kobospan" id="kobo.564.2">Consider a </span><em class="italic"><span class="kobospan" id="kobo.565.1">4 × 2</span></em><span class="kobospan" id="kobo.566.1"> matrix. </span><span class="kobospan" id="kobo.566.2">The eigenvalues of the matrix can be found by computing </span><span class="kobospan" id="kobo.567.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/68.png" class="calibre83"/></span><span class="kobospan" id="kobo.568.1"> and </span><span class="kobospan" id="kobo.569.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/69.png" class="calibre83"/></span><span class="kobospan" id="kobo.570.1"> and then determining the eigenvectors of these matrices. </span><em class="italic"><span class="kobospan" id="kobo.571.1">U</span></em><span class="kobospan" id="kobo.572.1">’s columns are formed by the eigenvectors of </span><span class="kobospan" id="kobo.573.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/68.png" class="calibre84"/></span><span class="kobospan" id="kobo.574.1">, and </span><em class="italic"><span class="kobospan" id="kobo.575.1">V</span></em><span class="kobospan" id="kobo.576.1">’s columns are formed by the eigenvectors of </span><span class="kobospan" id="kobo.577.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/71.png" class="calibre85"/></span><span class="kobospan" id="kobo.578.1">. </span><span class="kobospan" id="kobo.578.2">The </span><em class="italic"><span class="kobospan" id="kobo.579.1">S</span></em><span class="kobospan" id="kobo.580.1"> matrix comprises the square root of eigenvalues from either </span><span class="kobospan" id="kobo.581.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/68.png" class="calibre86"/></span><span class="kobospan" id="kobo.582.1"> or </span><span class="kobospan" id="kobo.583.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/73.png" class="calibre87"/></span><span class="kobospan" id="kobo.584.1">. </span><span class="kobospan" id="kobo.584.2">Eigenvalues are found by solving the characteristic equation in the given example </span><span class="kobospan" id="kobo.585.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/74.png" class="calibre88"/></span><span class="kobospan" id="kobo.586.1">, where </span><em class="italic"><span class="kobospan" id="kobo.587.1">W</span></em><span class="kobospan" id="kobo.588.1"> is the matrix, </span><em class="italic"><span class="kobospan" id="kobo.589.1">I</span></em><span class="kobospan" id="kobo.590.1"> is the unit matrix, and </span><em class="italic"><span class="kobospan" id="kobo.591.1">λ</span></em><span class="kobospan" id="kobo.592.1"> is the eigenvalue. </span><span class="kobospan" id="kobo.592.2">The eigenvectors are then found by solving the set of equations derived from the eigenvalue equations. </span><span class="kobospan" id="kobo.592.3">The final matrices </span><em class="italic"><span class="kobospan" id="kobo.593.1">U, </span></em><em class="italic"><span class="kobospan" id="kobo.594.1">S</span></em><span class="kobospan" id="kobo.595.1">, and </span><span class="kobospan" id="kobo.596.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/57.png" class="calibre72"/></span><span class="kobospan" id="kobo.597.1"> are then obtained by combining the eigenvectors and </span><span><span class="kobospan" id="kobo.598.1">singular values.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.599.1">It should be noted that the singular values are in descending order, </span><span><span class="kobospan" id="kobo.600.1">with </span></span><span><span class="kobospan" id="kobo.601.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/76.png" class="calibre89"/></span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.602.1">Let’s now move on to basic probability for </span><span><span class="kobospan" id="kobo.603.1">machine learning.</span></span></p>
<h1 id="_idParaDest-36" class="calibre4"><a id="_idTextAnchor036" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.604.1">Basic probability for machine learning</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.605.1">Probability provides information </span><a id="_idIndexMarker064" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.606.1">about the likelihood of an event occurring. </span><span class="kobospan" id="kobo.606.2">In this field, there are several key terms that are important </span><span><span class="kobospan" id="kobo.607.1">to understand:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.608.1">Trial or experiment</span></strong><span class="kobospan" id="kobo.609.1">: An action that results in a certain outcome with a </span><span><span class="kobospan" id="kobo.610.1">certain likelihood</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.611.1">Sample space</span></strong><span class="kobospan" id="kobo.612.1">: This encompasses all potential outcomes of a </span><span><span class="kobospan" id="kobo.613.1">given experiment</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.614.1">Event</span></strong><span class="kobospan" id="kobo.615.1">: This denotes a non-empty portion of the </span><span><span class="kobospan" id="kobo.616.1">sample space</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.617.1">Therefore, in technical terms, probability is a measure of the likelihood of an event occurring when an experiment </span><span><span class="kobospan" id="kobo.618.1">is conducted.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.619.1">In this very simple case, the</span><a id="_idIndexMarker065" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.620.1"> probability of event </span><em class="italic"><span class="kobospan" id="kobo.621.1">A</span></em><span class="kobospan" id="kobo.622.1"> with one outcome is equal to the chance of event </span><em class="italic"><span class="kobospan" id="kobo.623.1">A</span></em><span class="kobospan" id="kobo.624.1"> divided by the chance of all possible events. </span><span class="kobospan" id="kobo.624.2">For example, in flipping a fair coin, there are two outcomes with the same chance: heads and tails. </span><span class="kobospan" id="kobo.624.3">The chance of having heads will be </span><em class="italic"><span class="kobospan" id="kobo.625.1">1/(1+1) = ½</span></em><span class="kobospan" id="kobo.626.1">.</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.627.1">In order to calculate the probability, given an event, </span><em class="italic"><span class="kobospan" id="kobo.628.1">A</span></em><span class="kobospan" id="kobo.629.1">, with </span><em class="italic"><span class="kobospan" id="kobo.630.1">n</span></em><span class="kobospan" id="kobo.631.1"> outcomes and a sample space, </span><em class="italic"><span class="kobospan" id="kobo.632.1">S</span></em><span class="kobospan" id="kobo.633.1">, the probability of event A is </span><span><span class="kobospan" id="kobo.634.1">calculated as</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.635.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/77.png" class="calibre90"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.636.1">where </span><span class="kobospan" id="kobo.637.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/78.png" class="calibre91"/></span><span class="kobospan" id="kobo.638.1"> represents the outcomes in </span><em class="italic"><span class="kobospan" id="kobo.639.1">A</span></em><span class="kobospan" id="kobo.640.1">. </span><span class="kobospan" id="kobo.640.2">Assuming all results of the experiment have equal probability, and the selection of one does not influence the selection of others in subsequent rounds (meaning they are statistically </span><span><span class="kobospan" id="kobo.641.1">independent), then</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.642.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/79.png" class="calibre92"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.643.1">Hence, the value of probability ranges from </span><em class="italic"><span class="kobospan" id="kobo.644.1">0</span></em><span class="kobospan" id="kobo.645.1"> to </span><em class="italic"><span class="kobospan" id="kobo.646.1">1</span></em><span class="kobospan" id="kobo.647.1">, with the sample space embodying the complete set of potential outcomes, denoted as </span><em class="italic"><span class="kobospan" id="kobo.648.1">P(S) = </span></em><span><em class="italic"><span class="kobospan" id="kobo.649.1">1</span></em></span><span><span class="kobospan" id="kobo.650.1">.</span></span></p>
<h2 id="_idParaDest-37" class="calibre7"><a id="_idTextAnchor037" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.651.1">Statistically independent</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.652.1">In the realm of statistics, two </span><a id="_idIndexMarker066" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.653.1">events are defined as independent if the occurrence of one event doesn’t influence the likelihood of the other event’s occurrence. </span><span class="kobospan" id="kobo.653.2">To put it formally, events </span><em class="italic"><span class="kobospan" id="kobo.654.1">A</span></em><span class="kobospan" id="kobo.655.1"> and </span><em class="italic"><span class="kobospan" id="kobo.656.1">B</span></em><span class="kobospan" id="kobo.657.1"> are independent precisely when </span><em class="italic"><span class="kobospan" id="kobo.658.1">P(A and B) = P(A)P(B)</span></em><span class="kobospan" id="kobo.659.1">, where </span><em class="italic"><span class="kobospan" id="kobo.660.1">P(A)</span></em><span class="kobospan" id="kobo.661.1"> and </span><em class="italic"><span class="kobospan" id="kobo.662.1">P(B)</span></em><span class="kobospan" id="kobo.663.1"> are the respective probabilities of events </span><em class="italic"><span class="kobospan" id="kobo.664.1">A</span></em><span class="kobospan" id="kobo.665.1"> and </span><span><em class="italic"><span class="kobospan" id="kobo.666.1">B</span></em></span><span><span class="kobospan" id="kobo.667.1"> happening.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.668.1">Consider this example to clarify the concept of statistical independence: imagine we possess two coins, one fair (an equal chance of turning up heads or tails) and the other biased (showing a head is more likely than a tail). </span><span class="kobospan" id="kobo.668.2">If we flip the fair coin and the biased coin, these two events are statistically independent because the outcome of one coin flip doesn’t alter the probability</span><a id="_idIndexMarker067" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.669.1"> of the other coin turning up heads or tails. </span><span class="kobospan" id="kobo.669.2">Specifically, the likelihood of both coins showing heads is the product of the individual probabilities: </span><em class="italic"><span class="kobospan" id="kobo.670.1">(1/2) * (3/4) = </span></em><span><em class="italic"><span class="kobospan" id="kobo.671.1">3/8</span></em></span><span><span class="kobospan" id="kobo.672.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.673.1">Statistical independence is a pivotal concept in statistics and probability theory, frequently leveraged in machine learning to outline the connections between variables within a dataset. </span><span class="kobospan" id="kobo.673.2">By comprehending these relationships, machine learning algorithms can better spot patterns and deliver more precise predictions. </span><span class="kobospan" id="kobo.673.3">We will describe the relationship between different types of events in </span><span><span class="kobospan" id="kobo.674.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.675.1">Complementary event</span></strong><span class="kobospan" id="kobo.676.1">: The complementary event to </span><em class="italic"><span class="kobospan" id="kobo.677.1">A</span></em><span class="kobospan" id="kobo.678.1">, signified as </span><em class="italic"><span class="kobospan" id="kobo.679.1">A’</span></em><span class="kobospan" id="kobo.680.1">, encompasses the probability of all </span><a id="_idIndexMarker068" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.681.1">potential outcomes in the sample space not included in A. </span><span class="kobospan" id="kobo.681.2">It’s critical to understand that </span><em class="italic"><span class="kobospan" id="kobo.682.1">A</span></em><span class="kobospan" id="kobo.683.1"> and </span><em class="italic"><span class="kobospan" id="kobo.684.1">A’</span></em><span class="kobospan" id="kobo.685.1"> are </span><span><span class="kobospan" id="kobo.686.1">statistically independent:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.687.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/80.png" class="calibre93"/></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.688.1">Union and intersection</span></strong><span class="kobospan" id="kobo.689.1">: The complementary event to </span><em class="italic"><span class="kobospan" id="kobo.690.1">A</span></em><span class="kobospan" id="kobo.691.1">, signified as </span><em class="italic"><span class="kobospan" id="kobo.692.1">A’</span></em><span class="kobospan" id="kobo.693.1">, encompasses the probability of all potential outcomes in the sample space not </span><a id="_idIndexMarker069" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.694.1">included in </span><em class="italic"><span class="kobospan" id="kobo.695.1">A</span></em><span class="kobospan" id="kobo.696.1">. </span><span class="kobospan" id="kobo.696.2">It’s critical to understand that </span><em class="italic"><span class="kobospan" id="kobo.697.1">A</span></em><span class="kobospan" id="kobo.698.1"> and </span><em class="italic"><span class="kobospan" id="kobo.699.1">A’</span></em><span class="kobospan" id="kobo.700.1"> are </span><span><span class="kobospan" id="kobo.701.1">statistically independent.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.702.1">Mutually exclusive</span></strong><span class="kobospan" id="kobo.703.1">: When two</span><a id="_idIndexMarker070" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.704.1"> events have no shared outcomes, they are viewed as mutually exclusive. </span><span class="kobospan" id="kobo.704.2">In other words, if </span><em class="italic"><span class="kobospan" id="kobo.705.1">A</span></em><span class="kobospan" id="kobo.706.1"> and </span><em class="italic"><span class="kobospan" id="kobo.707.1">B</span></em><span class="kobospan" id="kobo.708.1"> are mutually exclusive events, then </span><em class="italic"><span class="kobospan" id="kobo.709.1">P(A </span></em><em class="italic"><span class="kobospan" id="kobo.710.1">∩</span></em><em class="italic"><span class="kobospan" id="kobo.711.1"> B) = 0</span></em><span class="kobospan" id="kobo.712.1">. </span><span class="kobospan" id="kobo.712.2">This conclusion can be drawn from the addition rule of probability, as </span><em class="italic"><span class="kobospan" id="kobo.713.1">A</span></em><span class="kobospan" id="kobo.714.1"> and </span><em class="italic"><span class="kobospan" id="kobo.715.1">B</span></em><span class="kobospan" id="kobo.716.1"> a</span><a id="_idTextAnchor038" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.717.1">re </span><span><span class="kobospan" id="kobo.718.1">disjointed events:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.719.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/81.png" class="calibre94"/></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.720.1">Independent</span></strong><span class="kobospan" id="kobo.721.1">: Two events are deemed independent when the occurrence of one doesn’t impact the </span><a id="_idIndexMarker071" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.722.1">occurrence of the other. </span><span class="kobospan" id="kobo.722.2">If </span><em class="italic"><span class="kobospan" id="kobo.723.1">A</span></em><span class="kobospan" id="kobo.724.1"> and </span><em class="italic"><span class="kobospan" id="kobo.725.1">B</span></em><span class="kobospan" id="kobo.726.1"> are two independent </span><span><span class="kobospan" id="kobo.727.1">events, then</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.728.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;∩&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/82.png" class="calibre95"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.729.1">Next, we are going to describe the discrete random variable, its distribution, and how to use it to calculate </span><span><span class="kobospan" id="kobo.730.1">the probabilities.</span></span></p>
<h2 id="_idParaDest-38" class="calibre7"><a id="_idTextAnchor039" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.731.1">Discrete random variables and their distribution</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.732.1">A discrete random variable </span><a id="_idIndexMarker072" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.733.1">refers to a variable that can assume a finite or countably infinite number of potential outcomes. </span><span class="kobospan" id="kobo.733.2">Examples of such variables might be the count of heads resulting from a coin toss, the tally of cars crossing a toll booth within a specific time span, or the number of blonde-haired students in </span><span><span class="kobospan" id="kobo.734.1">a classroom.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.735.1">The probability distribution of a </span><a id="_idIndexMarker073" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.736.1">discrete random variable assigns a certain likelihood to each potential outcome the variable could adopt. </span><span class="kobospan" id="kobo.736.2">For instance, in the case of a coin toss, the probability distribution assigns a 0.5 probability to both </span><em class="italic"><span class="kobospan" id="kobo.737.1">0</span></em><span class="kobospan" id="kobo.738.1"> and </span><em class="italic"><span class="kobospan" id="kobo.739.1">1</span></em><span class="kobospan" id="kobo.740.1">, representing tails and heads, respectively. </span><span class="kobospan" id="kobo.740.2">For the car toll booth scenario, the distribution could be assigning a probability of </span><em class="italic"><span class="kobospan" id="kobo.741.1">0.1</span></em><span class="kobospan" id="kobo.742.1"> to no cars passing, </span><em class="italic"><span class="kobospan" id="kobo.743.1">0.3</span></em><span class="kobospan" id="kobo.744.1"> to one car, </span><em class="italic"><span class="kobospan" id="kobo.745.1">0.4</span></em><span class="kobospan" id="kobo.746.1"> to two cars, </span><em class="italic"><span class="kobospan" id="kobo.747.1">0.15</span></em><span class="kobospan" id="kobo.748.1"> to three cars, and </span><em class="italic"><span class="kobospan" id="kobo.749.1">0.05</span></em><span class="kobospan" id="kobo.750.1"> to four or </span><span><span class="kobospan" id="kobo.751.1">more cars.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.752.1">A graphical representation of the probability distribution of a discrete random variable can be achieved through a </span><strong class="bold"><span class="kobospan" id="kobo.753.1">probability mass function</span></strong><span class="kobospan" id="kobo.754.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.755.1">PMF</span></strong><span class="kobospan" id="kobo.756.1">), which correlates each possible outcome of</span><a id="_idIndexMarker074" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.757.1"> the variable to its likelihood of occurrence. </span><span class="kobospan" id="kobo.757.2">This function is usually represented as a bar chart or histogram, with each bar signifying the probability of a </span><span><span class="kobospan" id="kobo.758.1">specific value.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.759.1">The PMF is bound by two </span><span><span class="kobospan" id="kobo.760.1">key principles:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.761.1">It must be non-negative across all potential values of the </span><span><span class="kobospan" id="kobo.762.1">random variable</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.763.1">The total sum of probabilities for all possible outcomes should equate </span><span><span class="kobospan" id="kobo.764.1">to 1</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.765.1">The expected value of a discrete random variable offers an insight into its central tendency, computed as the probability-weighted average of its possible outcomes. </span><span class="kobospan" id="kobo.765.2">This expected value is signified as </span><em class="italic"><span class="kobospan" id="kobo.766.1">E[X]</span></em><span class="kobospan" id="kobo.767.1">, with </span><em class="italic"><span class="kobospan" id="kobo.768.1">X</span></em><span class="kobospan" id="kobo.769.1"> representing the </span><span><span class="kobospan" id="kobo.770.1">random variable.</span></span></p>
<h2 id="_idParaDest-39" class="calibre7"><a id="_idTextAnchor040" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.771.1">Probability density function</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.772.1">The </span><strong class="bold"><span class="kobospan" id="kobo.773.1">probability density function</span></strong><span class="kobospan" id="kobo.774.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.775.1">PDF</span></strong><span class="kobospan" id="kobo.776.1">) is a tool used to describe the distribution of a continuous random variable. </span><span class="kobospan" id="kobo.776.2">It can be </span><a id="_idIndexMarker075" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.777.1">used to calculate the probability of a value falling within a specific range. </span><span class="kobospan" id="kobo.777.2">In simpler terms, it helps determine the chances of a continuous variable, </span><em class="italic"><span class="kobospan" id="kobo.778.1">X</span></em><span class="kobospan" id="kobo.779.1">, having a value within</span><a id="_idIndexMarker076" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.780.1"> the interval [</span><em class="italic"><span class="kobospan" id="kobo.781.1">a, b</span></em><span class="kobospan" id="kobo.782.1">], or in </span><span><span class="kobospan" id="kobo.783.1">statistical terms,</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.784.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/83.png" class="calibre96"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.785.1">For continuous variables, the probability of a single value occurring is always 0, which is in contrast to discrete variables that can assign non-</span><em class="italic"><span class="kobospan" id="kobo.786.1">0</span></em><span class="kobospan" id="kobo.787.1"> probabilities to distinct values. </span><span class="kobospan" id="kobo.787.2">PDFs provide a way to estimate the likelihood of a value falling within a given range instead of a </span><span><span class="kobospan" id="kobo.788.1">single value.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.789.1">For example, you can use a PDF to find the chances of the next IQ score measured falling between </span><em class="italic"><span class="kobospan" id="kobo.790.1">100</span></em> <span><span class="kobospan" id="kobo.791.1">and </span></span><span><em class="italic"><span class="kobospan" id="kobo.792.1">120</span></em></span><span><span class="kobospan" id="kobo.793.1">.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer094">
<span class="kobospan" id="kobo.794.1"><img alt=" Figure 2.2 – Probability density function for IQ from 100–120" src="image/B18949_02_2.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.795.1"> Figure 2.2 – Probability density function for IQ from 100–120</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.796.1">To ascertain the distribution of a discrete random variable, one can either provide its PMF or </span><strong class="bold"><span class="kobospan" id="kobo.797.1">cumulative distribution function</span></strong><span class="kobospan" id="kobo.798.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.799.1">CDF</span></strong><span class="kobospan" id="kobo.800.1">). </span><span class="kobospan" id="kobo.800.2">For continuous random variables, we primarily utilize the CDF, as it is well </span><a id="_idIndexMarker077" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.801.1">established. </span><span class="kobospan" id="kobo.801.2">However, the PMF is not suitable for these types of variables</span><a id="_idIndexMarker078" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.802.1"> because </span><em class="italic"><span class="kobospan" id="kobo.803.1">P(X=x)</span></em><span class="kobospan" id="kobo.804.1"> equals </span><em class="italic"><span class="kobospan" id="kobo.805.1">0</span></em><span class="kobospan" id="kobo.806.1"> for all </span><em class="italic"><span class="kobospan" id="kobo.807.1">x</span></em><span class="kobospan" id="kobo.808.1"> in the set of real numbers, given that </span><em class="italic"><span class="kobospan" id="kobo.809.1">X</span></em><span class="kobospan" id="kobo.810.1"> can assume any real value between </span><em class="italic"><span class="kobospan" id="kobo.811.1">a</span></em><span class="kobospan" id="kobo.812.1"> and </span><em class="italic"><span class="kobospan" id="kobo.813.1">b</span></em><span class="kobospan" id="kobo.814.1">. </span><span class="kobospan" id="kobo.814.2">Therefore, we typically define the PDF instead. </span><span class="kobospan" id="kobo.814.3">The PDF resembles the concept of mass density in physics, signifying the concentration of probability. </span><span class="kobospan" id="kobo.814.4">Its unit is the probability per unit length. </span><span class="kobospan" id="kobo.814.5">To get a grasp of the PDF, let’s analyze a continuous random variable, </span><em class="italic"><span class="kobospan" id="kobo.815.1">X</span></em><span class="kobospan" id="kobo.816.1">, and establish the function </span><em class="italic"><span class="kobospan" id="kobo.817.1">fX(x)</span></em> <span><span class="kobospan" id="kobo.818.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.819.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;lim&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/84.png" class="calibre97"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.820.1">If the </span><span><span class="kobospan" id="kobo.821.1">limit exists.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.822.1">The function </span><span class="kobospan" id="kobo.823.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/85.png" class="calibre98"/></span><span class="kobospan" id="kobo.824.1">provides the probability density at a given point, </span><em class="italic"><span class="kobospan" id="kobo.825.1">x</span></em><span class="kobospan" id="kobo.826.1">. </span><span class="kobospan" id="kobo.826.2">This is equivalent to the limit of the ratio of the probability of the interval </span><em class="italic"><span class="kobospan" id="kobo.827.1">(x, x + Δ]</span></em><span class="kobospan" id="kobo.828.1"> to the length of the interval as that length </span><span><span class="kobospan" id="kobo.829.1">approaches 0.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.830.1">Let’s contemplate a continuous random variable, </span><em class="italic"><span class="kobospan" id="kobo.831.1">X</span></em><span class="kobospan" id="kobo.832.1">, possessing an absolutely continuous CDF, denoted as </span><span class="kobospan" id="kobo.833.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/86.png" class="calibre99"/></span><span class="kobospan" id="kobo.834.1">. </span><span class="kobospan" id="kobo.834.2">If </span><span class="kobospan" id="kobo.835.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/86.png" class="calibre99"/></span><span class="kobospan" id="kobo.836.1"> is differentiable at </span><em class="italic"><span class="kobospan" id="kobo.837.1">x</span></em><span class="kobospan" id="kobo.838.1">, the function </span><span class="kobospan" id="kobo.839.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/88.png" class="calibre100"/></span><span class="kobospan" id="kobo.840.1"> is referred to as the PDF </span><span><span class="kobospan" id="kobo.841.1">of </span></span><span><em class="italic"><span class="kobospan" id="kobo.842.1">X</span></em></span><span><span class="kobospan" id="kobo.843.1">:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.844.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;lim&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/89.png" class="calibre101"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.845.1">Assuming </span><span class="kobospan" id="kobo.846.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/86.png" class="calibre99"/></span><span class="kobospan" id="kobo.847.1">  is differentiable </span><span><span class="kobospan" id="kobo.848.1">at </span></span><span><span class="kobospan" id="kobo.849.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/91.png" class="calibre102"/></span></span><span><span class="kobospan" id="kobo.850.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.851.1">For example, let’s consider </span><a id="_idIndexMarker079" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.852.1">a continuous uniform random variable, </span><em class="italic"><span class="kobospan" id="kobo.853.1">X</span></em><span class="kobospan" id="kobo.854.1">, with uniform </span><span class="kobospan" id="kobo.855.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;U&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/92.png" class="calibre103"/></span><span class="kobospan" id="kobo.856.1"> distribution. </span><span class="kobospan" id="kobo.856.2">Its CDF is </span><span><span class="kobospan" id="kobo.857.1">given by:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.858.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/93.png" class="calibre104"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.859.1">which is </span><em class="italic"><span class="kobospan" id="kobo.860.1">0</span></em><span class="kobospan" id="kobo.861.1"> for any x outside </span><span><span class="kobospan" id="kobo.862.1">the bounds.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.863.1">By using integration, the CDF can be obtained from </span><span><span class="kobospan" id="kobo.864.1">the PDF:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.865.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/94.png" class="calibre105"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.866.1">Additionally, </span><span><span class="kobospan" id="kobo.867.1">we have</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.868.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/95.png" class="calibre106"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.869.1">So, if we integrate over the entire real line, we will </span><span><span class="kobospan" id="kobo.870.1">get 1:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.871.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/96.png" class="calibre107"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.872.1">Explicitly, when integrating the PDF across the entire real number line, the result should equal </span><em class="italic"><span class="kobospan" id="kobo.873.1">1</span></em><span class="kobospan" id="kobo.874.1">. </span><span class="kobospan" id="kobo.874.2">This signifies that the area beneath the PDF curve must equate to </span><em class="italic"><span class="kobospan" id="kobo.875.1">1</span></em><span class="kobospan" id="kobo.876.1">, or </span><em class="italic"><span class="kobospan" id="kobo.877.1">P(S) = 1</span></em><span class="kobospan" id="kobo.878.1">, which remains true for the uniform distribution. </span><span class="kobospan" id="kobo.878.2">The PDF signifies the density of probability; thus, it must </span><a id="_idIndexMarker080" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.879.1">be non-negative and can </span><span><span class="kobospan" id="kobo.880.1">exceed </span></span><span><em class="italic"><span class="kobospan" id="kobo.881.1">1</span></em></span><span><span class="kobospan" id="kobo.882.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.883.1">Consider a continuous random variable, </span><em class="italic"><span class="kobospan" id="kobo.884.1">X</span></em><span class="kobospan" id="kobo.885.1">, with PDF represented as </span><span class="kobospan" id="kobo.886.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/97.png" class="calibre108"/></span><span class="kobospan" id="kobo.887.1">. </span><span class="kobospan" id="kobo.887.2">The ensuing properties </span><span><span class="kobospan" id="kobo.888.1">are applicable:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.889.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;≥&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/98.png" class="calibre109"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.890.1">for all </span><span><span class="kobospan" id="kobo.891.1">real x</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.892.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/96.png" class="calibre107"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.893.1"> Next, we’ll move on to cover </span><span><span class="kobospan" id="kobo.894.1">maximum likelihood.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.895.1">Maximum likelihood estimation</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.896.1">Maximum likelihood is a </span><a id="_idIndexMarker081" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.897.1">statistical approach, that is used to estimate the parameters of a probability distribution. </span><span class="kobospan" id="kobo.897.2">The</span><a id="_idIndexMarker082" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.898.1"> objective is to identify the parameter values that maximize the likelihood of observing the data, essentially determining the parameters most likely to have generated </span><span><span class="kobospan" id="kobo.899.1">the data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.900.1">Suppose we have a random sample, </span><span class="kobospan" id="kobo.901.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/100.png" class="calibre110"/></span><span class="kobospan" id="kobo.902.1">, from a population with a probability distribution </span><span class="kobospan" id="kobo.903.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/101.png" class="calibre111"/></span><span class="kobospan" id="kobo.904.1">, where </span><em class="italic"><span class="kobospan" id="kobo.905.1">θ</span></em><span class="kobospan" id="kobo.906.1"> is a vector of parameters. </span><span class="kobospan" id="kobo.906.2">The likelihood of observing the sample, </span><em class="italic"><span class="kobospan" id="kobo.907.1">X</span></em><span class="kobospan" id="kobo.908.1">, given the parameters, </span><em class="italic"><span class="kobospan" id="kobo.909.1">θ</span></em><span class="kobospan" id="kobo.910.1">, is defined as the product of the individual probabilities of observing each </span><span><span class="kobospan" id="kobo.911.1">data point:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.912.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/102.png" class="calibre112"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.913.1">In case of having independent and identically distributed observations, the likelihood function can be expressed as the product of the univariate density functions, each evaluated at the </span><span><span class="kobospan" id="kobo.914.1">corresponding observation:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.915.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/103.png" class="calibre113"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.916.1">The </span><strong class="bold"><span class="kobospan" id="kobo.917.1">maximum likelihood estimate</span></strong><span class="kobospan" id="kobo.918.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.919.1">MLE</span></strong><span class="kobospan" id="kobo.920.1">) is the parameter vector value that offers the maximum value for the likelihood </span><a id="_idIndexMarker083" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.921.1">function across the </span><span><span class="kobospan" id="kobo.922.1">parameter space.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.923.1">In many cases, it’s more convenient to employ the natural logarithm of the likelihood function, referred to</span><a id="_idIndexMarker084" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.924.1"> as the </span><strong class="bold"><span class="kobospan" id="kobo.925.1">log-likelihood</span></strong><span class="kobospan" id="kobo.926.1">. </span><span class="kobospan" id="kobo.926.2">The peak of the log-likelihood happens at the identical parameter vector value as the likelihood function’s maximum, and the conditions required for a maximum (or minimum) are acquired by equating the log-likelihood derivatives with respect to each parameter to 0. </span><span class="kobospan" id="kobo.926.3">If the log-likelihood is differentiable with respect to</span><a id="_idIndexMarker085" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.927.1"> the parameters, these conditions result in a set of equations that can be solved numerically to derive the MLE. </span><span class="kobospan" id="kobo.927.2">One common use case or scenario where MLE significantly impacts ML</span><a id="_idIndexMarker086" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.928.1"> model performance is in linear regression. </span><span class="kobospan" id="kobo.928.2">When building a linear regression model, MLE is often used to estimate the coefficients that define the relationship between input features and the target variable. </span><span class="kobospan" id="kobo.928.3">MLE helps find the values for the coefficients that maximize the likelihood of observing the given data under the assumed linear regression model, improving the accuracy of </span><span><span class="kobospan" id="kobo.929.1">the predictions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.930.1">The MLEs of the parameters, </span><em class="italic"><span class="kobospan" id="kobo.931.1">θ</span></em><span class="kobospan" id="kobo.932.1">, are the values that maximize the likelihood function. </span><span class="kobospan" id="kobo.932.2">In other words, the MLEs are the values of </span><em class="italic"><span class="kobospan" id="kobo.933.1">θ</span></em><span class="kobospan" id="kobo.934.1"> that make the observed data, </span><em class="italic"><span class="kobospan" id="kobo.935.1">X</span></em><span class="kobospan" id="kobo.936.1">, </span><span><span class="kobospan" id="kobo.937.1">most probable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.938.1">To find the MLEs, we typically take the natural logarithm of the likelihood function, as it is often easier to work with the logarithm of a product than with the </span><span><span class="kobospan" id="kobo.939.1">product itself:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.940.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/104.png" class="calibre114"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.941.1">The MLEs are determined by equating the partial derivatives of the log-likelihood function with respect to each parameter to </span><em class="italic"><span class="kobospan" id="kobo.942.1">0</span></em><span class="kobospan" id="kobo.943.1"> and then solving these equations for </span><span><span class="kobospan" id="kobo.944.1">the parameters:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.945.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/105.png" class="calibre115"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.946.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/106.png" class="calibre115"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.947.1">...</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.948.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/107.png" class="calibre116"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.949.1">where </span><em class="italic"><span class="kobospan" id="kobo.950.1">k</span></em><span class="kobospan" id="kobo.951.1"> is the number of parameters in </span><em class="italic"><span class="kobospan" id="kobo.952.1">θ</span></em><span class="kobospan" id="kobo.953.1">. </span><span class="kobospan" id="kobo.953.2">The goal of a maximum likelihood estimator is to find </span><em class="italic"><span class="kobospan" id="kobo.954.1">θ</span></em> <span><span class="kobospan" id="kobo.955.1">such that</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.956.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;munder&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/108.png" class="calibre117"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.957.1">Once the MLEs have been found, they can be used to make predictions about the population based on the sample data. </span><span class="kobospan" id="kobo.957.2">Maximum likelihood is widely used in many fields, including psychology, economics, engineering, and biology. </span><span class="kobospan" id="kobo.957.3">It serves as a potent tool for comprehending the connections</span><a id="_idIndexMarker087" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.958.1"> among variables and for predicting outcomes based on observed data. </span><span class="kobospan" id="kobo.958.2">For example, building a word predictor using maximum </span><span><span class="kobospan" id="kobo.959.1">likelihood estimation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.960.1">Next, we introduce the </span><a id="_idIndexMarker088" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.961.1">problem of word autocompletion, also known as </span><strong class="bold"><span class="kobospan" id="kobo.962.1">word prediction</span></strong><span class="kobospan" id="kobo.963.1">, which is a feature in where an application</span><a id="_idIndexMarker089" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.964.1"> predicts the next word a user is typing. </span><span class="kobospan" id="kobo.964.2">The aim of word prediction is to save time and make typing easier by predicting what the user is likely to type next based on their previous inputs and other contextual factors. </span><span class="kobospan" id="kobo.964.3">Word prediction can be found in various forms in many applications, including search engines, text editors, and mobile device keyboards, and is designed to save time and increase the accuracy </span><span><span class="kobospan" id="kobo.965.1">of inputs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.966.1">Given a group of words that the user typed, how would we suggest the </span><span><span class="kobospan" id="kobo.967.1">next word?</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.968.1">If the words were </span><strong class="bold"><span class="kobospan" id="kobo.969.1">The United States of</span></strong><span class="kobospan" id="kobo.970.1">, then it would be trivial to assume that the next word would be </span><strong class="bold"><span class="kobospan" id="kobo.971.1">America</span></strong><span class="kobospan" id="kobo.972.1">. </span><span class="kobospan" id="kobo.972.2">However, what about finding the next word for </span><strong class="bold"><span class="kobospan" id="kobo.973.1">How are</span></strong><span class="kobospan" id="kobo.974.1">? </span><span class="kobospan" id="kobo.974.2">One could suggest several </span><span><span class="kobospan" id="kobo.975.1">next words.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.976.1">There usually isn’t just one clear next word. </span><span class="kobospan" id="kobo.976.2">Thus, we’d want to suggest the most likely word or perhaps even the most likely words. </span><span class="kobospan" id="kobo.976.3">In that case, we would be interested in suggesting a probabilistic representation of the possible next words and picking the next word as the one that is </span><span><span class="kobospan" id="kobo.977.1">most probable.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.978.1">The maximum likelihood estimator provides us with that precise capability. </span><span class="kobospan" id="kobo.978.2">It can tell us which word is most probable given the previous words that the </span><span><span class="kobospan" id="kobo.979.1">user typed.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.980.1">In order to calculate the MLE, we need to calculate the probability function of all word combinations. </span><span class="kobospan" id="kobo.980.2">We can do that </span><a id="_idIndexMarker090" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.981.1">by processing large texts and counting how many times each combination of </span><span><span class="kobospan" id="kobo.982.1">words exists.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.983.1">Consider reviewing a large cohort of text that has the </span><span><span class="kobospan" id="kobo.984.1">following occurrences:</span></span></p>
<table class="no-table-style" id="table001-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.985.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.986.1">you”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.987.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.988.1">they”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.989.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.990.1">those”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.991.1">“</span></strong><span><strong class="bold"><span class="kobospan" id="kobo.992.1">the”</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.993.1">Any </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.994.1">other word</span></strong></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.995.1">“how </span><span><span class="kobospan" id="kobo.996.1">are …”</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.997.1">16</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.998.1">14</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.999.1">0</span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1000.1">100</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1001.1">10</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6"><span class="kobospan" id="kobo.1002.1">not “</span><span><span class="kobospan" id="kobo.1003.1">how are…”</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1004.1">200</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1005.1">100</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1006.1">300</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1007.1">1,000</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><span><span class="kobospan" id="kobo.1008.1">30,000</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1009.1">Table 2.1 – Sample of n-grams occurrences in a document</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1010.1">For instance, there are 16 occurrences in the text where the sequence “how are you” appears. </span><span class="kobospan" id="kobo.1010.2">There are 140 </span><a id="_idIndexMarker091" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1011.1">sequences that have a length of three that start with the words “how are.” </span><span class="kobospan" id="kobo.1011.2">That is </span><span><span class="kobospan" id="kobo.1012.1">calculated as:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1013.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mn&gt;16&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;14&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;140&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/109.png" class="calibre118"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1014.1">There are 216 sequences that have a length of three and that end with the word “you”. </span><span class="kobospan" id="kobo.1014.2">That is </span><span><span class="kobospan" id="kobo.1015.1">calculated as:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1016.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mn&gt;16&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;200&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;216&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/110.png" class="calibre119"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1017.1">Now, let’s suggest a formula for the most likely </span><span><span class="kobospan" id="kobo.1018.1">next word.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1019.1">Based on the common maximum likelihood estimation for the probablistic variable </span><span class="kobospan" id="kobo.1020.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/111.png" class="calibre120"/></span><span class="kobospan" id="kobo.1021.1">, the formula would be to find a value for </span><span class="kobospan" id="kobo.1022.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/112.png" class="calibre121"/></span> <span><span class="kobospan" id="kobo.1023.1">which maximizes:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1024.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/113.png" class="calibre122"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1025.1">However, this common formula has a few characteristics that wouldn’t be advantagous to </span><span><span class="kobospan" id="kobo.1026.1">our application.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1027.1">Consider the next formula which has specific advantages that are necessary for our use case. </span><span class="kobospan" id="kobo.1027.2">It is the maximum likelihood formula for parametric estimation, meaning, estimating deterministic </span><a id="_idIndexMarker092" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1028.1">parameters. </span><span class="kobospan" id="kobo.1028.2">It suggests finding a value for </span><span class="kobospan" id="kobo.1029.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/112.png" class="calibre121"/></span> <span><span class="kobospan" id="kobo.1030.1">which maximizes:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1031.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/115.png" class="calibre123"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1032.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/112.png" class="calibre121"/></span><span class="kobospan" id="kobo.1033.1"> is by no means a deterministic parameter, however, this formula suits our use case as it reduces common word bias emphasizing contextual fit, and adjusts for word specificity, thus enhancing the relevance of our predictions. </span><span class="kobospan" id="kobo.1033.2">We will elaborate more on these traits in the</span><a id="_idIndexMarker093" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1034.1"> conclusion of </span><span><span class="kobospan" id="kobo.1035.1">this exercise.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1036.1">Let’s enhance this formula so to make it easier </span><span><span class="kobospan" id="kobo.1037.1">to calculate:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1038.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/117.png" class="calibre124"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1039.1">In our case,</span><span class="kobospan" id="kobo.1040.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/118.png" class="calibre125"/></span><span class="kobospan" id="kobo.1041.1"> is “how” and </span><span class="kobospan" id="kobo.1042.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/119.png" class="calibre125"/></span> <span><span class="kobospan" id="kobo.1043.1">is “are.”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1044.1">There are five candidates for the next word; let’s calculate the probability for each </span><span><span class="kobospan" id="kobo.1045.1">of them:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1046.1">P(“how”, “are” | “you”) = 16 / (200 + 16) = 16/216 = </span><span><span class="kobospan" id="kobo.1047.1">2/27</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1048.1">P(“how”, “are” | “they”) = 14 / (100 +14) = 14/114 = </span><span><span class="kobospan" id="kobo.1049.1">7/57</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1050.1">P(“how”, “are” | “those”) = 0 / 300 = 0</span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1051.1">P(“how”, “are” | “the”) = 100 / (1000 + 100) = 100/1100 = </span><span><span class="kobospan" id="kobo.1052.1">1/11</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1053.1">P(“how”, “are” | any other word) = 10 / (30,000 + 10) = 10/30010 = </span><span><span class="kobospan" id="kobo.1054.1">1/3001</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1055.1">Out of all the options, the highest value of probability is 7/57 and it is achieved when “they” is the </span><span><span class="kobospan" id="kobo.1056.1">next word.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1057.1">Note that the intuition behind this maximum likelihood estimator is having the suggested next word make the words that the user typed most likely. </span><span class="kobospan" id="kobo.1057.2">One could wonder, why not take the word that is most probable given the first two words, meaning, the orginal maximum likelihood formula for probabilistic variables? </span><span class="kobospan" id="kobo.1057.3">From the table, we see that given the words “how are,” the most frequent third word is “the,” with a probability of 100/140. </span><span class="kobospan" id="kobo.1057.4">However, this approach wouldn’t take into account the fact that the word “the” is extremely prevalent altogether, as it is most frequently used in the text in general. </span><span class="kobospan" id="kobo.1057.5">Thus, its high frequency isn’t due to its relationship to the first two words; it is because it is </span><a id="_idIndexMarker094" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1058.1">simply a very </span><a id="_idIndexMarker095" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1059.1">common word in general. </span><span class="kobospan" id="kobo.1059.2">The maximum likelyhood formula we chose takes that </span><span><span class="kobospan" id="kobo.1060.1">into account.</span></span></p>
<h2 id="_idParaDest-40" class="calibre7"><a id="_idTextAnchor041" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1061.1">Bayesian estimation</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1062.1">Bayesian estimation is a statistical </span><a id="_idIndexMarker096" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1063.1">approach that involves updating our beliefs or probabilities about a quantity of interest based on new data. </span><span class="kobospan" id="kobo.1063.2">The term “Bayesian” refers to Thomas Bayes, an 18th-century statistician who first developed the concept of </span><span><span class="kobospan" id="kobo.1064.1">Bayesian probability.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1065.1">In Bayesian estimation, we start with prior beliefs about the quantity of interest, which are expressed as a probability distribution. </span><span class="kobospan" id="kobo.1065.2">These prior beliefs are updated as we collect new data. </span><span class="kobospan" id="kobo.1065.3">The updated beliefs are represented as a posterior distribution. </span><span class="kobospan" id="kobo.1065.4">The Bayesian framework provides a systematic way of updating prior beliefs with new data, taking into account the degree of uncertainty in both the prior beliefs and the </span><span><span class="kobospan" id="kobo.1066.1">new data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1067.1">The posterior distribution is calculated using Bayes’ theorem, which is the fundamental equation of Bayesian estimation. </span><span class="kobospan" id="kobo.1067.2">Bayes’ theorem </span><span><span class="kobospan" id="kobo.1068.1">states that</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1069.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/120.png" class="calibre126"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1070.1">where </span><em class="italic"><span class="kobospan" id="kobo.1071.1">Θ</span></em><span class="kobospan" id="kobo.1072.1"> is the quantity of interest, </span><em class="italic"><span class="kobospan" id="kobo.1073.1">X</span></em><span class="kobospan" id="kobo.1074.1"> is the new data, </span><em class="italic"><span class="kobospan" id="kobo.1075.1">P(Θ|X)</span></em><span class="kobospan" id="kobo.1076.1"> is the posterior distribution, </span><em class="italic"><span class="kobospan" id="kobo.1077.1">P(X|Θ)</span></em><span class="kobospan" id="kobo.1078.1"> is the likelihood of the data given the parameter value, </span><em class="italic"><span class="kobospan" id="kobo.1079.1">P(Θ)</span></em><span class="kobospan" id="kobo.1080.1"> is the prior distribution, and </span><em class="italic"><span class="kobospan" id="kobo.1081.1">P(X)</span></em><span class="kobospan" id="kobo.1082.1"> is the marginal likelihood </span><span><span class="kobospan" id="kobo.1083.1">or evidence.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1084.1">The marginal likelihood is calculated </span><span><span class="kobospan" id="kobo.1085.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1086.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/121.png" class="calibre127"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1087.1">where the integral is taken over the entire space of </span><em class="italic"><span class="kobospan" id="kobo.1088.1">Θ</span></em><span class="kobospan" id="kobo.1089.1">. </span><span class="kobospan" id="kobo.1089.2">The marginal likelihood is often used as a normalizing constant, ensuring that the posterior distribution integrates </span><span><span class="kobospan" id="kobo.1090.1">to </span></span><span><em class="italic"><span class="kobospan" id="kobo.1091.1">1</span></em></span><span><span class="kobospan" id="kobo.1092.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1093.1">In Bayesian estimation, the choice of prior distribution is important, as it reflects our beliefs about the quantity of interest before collecting any data. </span><span class="kobospan" id="kobo.1093.2">The prior distribution can be chosen based on prior knowledge or previous studies. </span><span class="kobospan" id="kobo.1093.3">If no prior knowledge is available, a non-informative prior can be used, such as a </span><span><span class="kobospan" id="kobo.1094.1">uniform distribution.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1095.1">Once the posterior </span><a id="_idIndexMarker097" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1096.1">distribution is calculated, it can be used to make predictions about the quantity of interest. </span><span class="kobospan" id="kobo.1096.2">As an example, the posterior distribution’s mean can serve as a point estimate, whereas the posterior distribution itself can be employed to establish credible intervals. </span><span class="kobospan" id="kobo.1096.3">These intervals represent the probable range within which the true value of the target </span><span><span class="kobospan" id="kobo.1097.1">quantity resides.</span></span></p>
<h1 id="_idParaDest-41" class="calibre4"><a id="_idTextAnchor042" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1098.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1099.1">This chapter was about linear algebra and probability for ML, and it covers the fundamental mathematical concepts that are essential to understanding many machine learning algorithms. </span><span class="kobospan" id="kobo.1099.2">The chapter began with a review of linear algebra, covering topics such as matrix multiplication, determinants, eigenvectors, and eigenvalues. </span><span class="kobospan" id="kobo.1099.3">It then moved on to discuss probability theory, introducing the basic concepts of random variables and probability distributions. </span><span class="kobospan" id="kobo.1099.4">We also covered key concepts in statistical inference, such as maximum likelihood estimation and </span><span><span class="kobospan" id="kobo.1100.1">Bayesian inference.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1101.1">In the next chapter, we will cover the fundamentals of machine learning for NLP, including topics such as data exploration, feature engineering, selection methods, and model training </span><span><span class="kobospan" id="kobo.1102.1">and validation.</span></span></p>
<h1 id="_idParaDest-42" class="calibre4"><a id="_idTextAnchor043" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1103.1">Further reading</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1104.1">Please find the additional reading content </span><span><span class="kobospan" id="kobo.1105.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1106.1">Householder reflection matrix</span></strong><span class="kobospan" id="kobo.1107.1">: A Householder reflection matrix, or Householder matrix, is a type of linear transformation utilized in numerical linear algebra due to its computational effectiveness and numerical stability. </span><span class="kobospan" id="kobo.1107.2">This matrix is used to perform reflections of a given vector about a plane or hyperplane, transforming the </span><a id="_idIndexMarker098" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1108.1">vector so that it only has non-</span><em class="italic"><span class="kobospan" id="kobo.1109.1">0</span></em><span class="kobospan" id="kobo.1110.1"> components in one specific dimension. </span><span class="kobospan" id="kobo.1110.2">The </span><strong class="bold"><span class="kobospan" id="kobo.1111.1">Householder matrix</span></strong><span class="kobospan" id="kobo.1112.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.1113.1">H</span></strong><span class="kobospan" id="kobo.1114.1">) is </span><span><span class="kobospan" id="kobo.1115.1">defined by</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.1116.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;H&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;u&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/122.png" class="calibre128"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1117.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.1118.1">I</span></em><span class="kobospan" id="kobo.1119.1"> is the identity matrix, and </span><em class="italic"><span class="kobospan" id="kobo.1120.1">u</span></em><span class="kobospan" id="kobo.1121.1"> is a unit vector defining the </span><span><span class="kobospan" id="kobo.1122.1">reflection plane.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1123.1">The main purpose of Householder transformations is to perform QR factorization and to reduce matrices to a tridiagonal or Hessenberg form. </span><span class="kobospan" id="kobo.1123.2">The properties of being symmetric and orthogonal make the Householder matrix computationally efficient and </span><span><span class="kobospan" id="kobo.1124.1">numerically stable.</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1125.1">Diagonalizable</span></strong><span class="kobospan" id="kobo.1126.1">: A matrix is said to be diagonalizable if it can be written in the form </span><span class="kobospan" id="kobo.1127.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;D&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/123.png" class="calibre129"/></span><span><span class="kobospan" id="kobo.1128.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;D&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/124.png" class="calibre130"/></span></span><span class="kobospan" id="kobo.1129.1">, where </span><em class="italic"><span class="kobospan" id="kobo.1130.1">A</span></em><span class="kobospan" id="kobo.1131.1"> is the original matrix, </span><em class="italic"><span class="kobospan" id="kobo.1132.1">D</span></em><span class="kobospan" id="kobo.1133.1"> is a diagonal matrix, and </span><em class="italic"><span class="kobospan" id="kobo.1134.1">P</span></em><span class="kobospan" id="kobo.1135.1"> is a matrix for which the columns are the eigenvectors of </span><em class="italic"><span class="kobospan" id="kobo.1136.1">A</span></em><span class="kobospan" id="kobo.1137.1">. </span><span class="kobospan" id="kobo.1137.2">Diagonalization simplifies many calculations in linear algebra, as computations with diagonal matrices are often more straightforward. </span><span class="kobospan" id="kobo.1137.3">For a matrix to be diagonalizable, it must have enough distinct eigenvectors to form a basis for its space, which is usually the case when all of its eigenvalues </span><span><span class="kobospan" id="kobo.1138.1">are distinct.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1139.1">Invertible</span></strong><span class="kobospan" id="kobo.1140.1">: An invertible matrix, also known as a non-singular matrix or a non-degenerate matrix, is a square matrix that has an inverse. </span><span class="kobospan" id="kobo.1140.2">If a matrix, </span><em class="italic"><span class="kobospan" id="kobo.1141.1">A</span></em><span class="kobospan" id="kobo.1142.1">, is invertible, there exists another matrix, often denoted as </span><span class="kobospan" id="kobo.1143.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/125.png" class="calibre131"/></span><span class="kobospan" id="kobo.1144.1">, such that when they are multiplied together, they yield the identity matrix. </span><span class="kobospan" id="kobo.1144.2">In other words, </span><span class="kobospan" id="kobo.1145.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;I&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/126.png" class="calibre132"/></span><span class="kobospan" id="kobo.1146.1">, where </span><em class="italic"><span class="kobospan" id="kobo.1147.1">I</span></em><span class="kobospan" id="kobo.1148.1"> is the identity matrix. </span><span class="kobospan" id="kobo.1148.2">The identity matrix is a special square matrix with </span><em class="italic"><span class="kobospan" id="kobo.1149.1">1</span></em><span class="kobospan" id="kobo.1150.1">s on its main diagonal and </span><em class="italic"><span class="kobospan" id="kobo.1151.1">0</span></em><span class="kobospan" id="kobo.1152.1">s everywhere else. </span><span class="kobospan" id="kobo.1152.2">The existence of an inverse heavily depends on the determinant of the matrix—a matrix is invertible if and only if its determinant is not 0. </span><span class="kobospan" id="kobo.1152.3">Invertible matrices are crucial in numerous areas of math, including solving systems of linear equations, matrix factorization, and many applications in engineering </span><span><span class="kobospan" id="kobo.1153.1">and physics.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1154.1">Gaussian elimination method</span></strong><span class="kobospan" id="kobo.1155.1">: Gaussian elimination is a fundamental algorithm in linear algebra for solving systems of linear equations. </span><span class="kobospan" id="kobo.1155.2">It accomplishes this by transforming the system to an equivalent one in which the equations are simpler to solve. </span><span class="kobospan" id="kobo.1155.3">This method uses a sequence of operations to modify the system of equations, with the objective of creating a row-echelon or reduced row-echelon form. </span><span class="kobospan" id="kobo.1155.4">Here’s a simplified step-by-step process of Gaussian elimination: First, swap the rows to move any rows with a leading coefficient (the first non-</span><em class="italic"><span class="kobospan" id="kobo.1156.1">0</span></em><span class="kobospan" id="kobo.1157.1"> number from the left, also called the pivot) so as to have </span><em class="italic"><span class="kobospan" id="kobo.1158.1">1</span></em><span class="kobospan" id="kobo.1159.1"> at the top. </span><span class="kobospan" id="kobo.1159.2">Then, multiply or divide any rows by a scalar to create a leading coefficient of </span><em class="italic"><span class="kobospan" id="kobo.1160.1">1</span></em><span class="kobospan" id="kobo.1161.1"> if not already present. </span><span class="kobospan" id="kobo.1161.2">Finally, add or subtract rows to create </span><em class="italic"><span class="kobospan" id="kobo.1162.1">0</span></em><span class="kobospan" id="kobo.1163.1">s below and above the pivot. </span><span class="kobospan" id="kobo.1163.2">Once the matrix is in row-echelon form (all </span><em class="italic"><span class="kobospan" id="kobo.1164.1">0</span></em><span class="kobospan" id="kobo.1165.1"> rows are at the bottom, and each leading coefficient is to the right of the leading coefficient of the row above it), we can use back substitution to find the variables.  </span><span class="kobospan" id="kobo.1165.2">If we further simplify the matrix to a reduced row-echelon form (each leading coefficient is the only non-</span><em class="italic"><span class="kobospan" id="kobo.1166.1">0</span></em><span class="kobospan" id="kobo.1167.1"> entry in its column), the solutions can be read directly from the matrix. </span><span class="kobospan" id="kobo.1167.2">Gaussian elimination can also be used to find the rank of a matrix, calculate the determinant, and carry out matrix inversion if the system is square and has a </span><span><span class="kobospan" id="kobo.1168.1">unique solution.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1169.1">Trace</span></strong><span class="kobospan" id="kobo.1170.1">: The trace of a square matrix is the sum of its diagonal elements. </span><span class="kobospan" id="kobo.1170.2">It’s denoted as </span><em class="italic"><span class="kobospan" id="kobo.1171.1">Tr(A)</span></em><span class="kobospan" id="kobo.1172.1"> or </span><em class="italic"><span class="kobospan" id="kobo.1173.1">trace(A)</span></em><span class="kobospan" id="kobo.1174.1">, where </span><em class="italic"><span class="kobospan" id="kobo.1175.1">A</span></em><span class="kobospan" id="kobo.1176.1"> is a square matrix. </span><span class="kobospan" id="kobo.1176.2">For </span><span><span class="kobospan" id="kobo.1177.1">example, if</span></span><p class="calibre6"><span class="kobospan" id="kobo.1178.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/38.png" class="calibre53"/></span></p><p class="calibre6"><em class="italic"><span class="kobospan" id="kobo.1179.1">Tr(A) = a + </span></em><span><em class="italic"><span class="kobospan" id="kobo.1180.1">d</span></em></span><span><span class="kobospan" id="kobo.1181.1">.</span></span></p></li>
</ul>
<h1 id="_idParaDest-43" class="calibre4"><a id="_idTextAnchor044" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1182.1">References</span></h1>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1183.1">Alter O, Brown PO, Botstein D. </span><span class="kobospan" id="kobo.1183.2">(2000) </span><em class="italic"><span class="kobospan" id="kobo.1184.1">Singular value decomposition for genome-wide expression data processing and modeling</span></em><span class="kobospan" id="kobo.1185.1">. </span><span class="kobospan" id="kobo.1185.2">Proc Natl Acad Sci U S A, </span><span><span class="kobospan" id="kobo.1186.1">97, 10101-6.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1187.1">Golub, G.H., and Van Loan, C.F. </span><span class="kobospan" id="kobo.1187.2">(1989) </span><em class="italic"><span class="kobospan" id="kobo.1188.1">Matrix Computations</span></em><span class="kobospan" id="kobo.1189.1">, 2nd ed. </span><span class="kobospan" id="kobo.1189.2">(Baltimore: Johns Hopkins </span><span><span class="kobospan" id="kobo.1190.1">University Press).</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1191.1">Greenberg, M.  </span><span class="kobospan" id="kobo.1191.2">(2001) </span><em class="italic"><span class="kobospan" id="kobo.1192.1">Differential equations &amp; Linear algebra</span></em><span class="kobospan" id="kobo.1193.1"> (Upper Saddle River, N.J. </span><span class="kobospan" id="kobo.1193.2">: </span><span><span class="kobospan" id="kobo.1194.1">Prentice Hall).</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1195.1">Strang, G.  </span><span class="kobospan" id="kobo.1195.2">(1998) </span><em class="italic"><span class="kobospan" id="kobo.1196.1">Introduction to linear algebra</span></em><span class="kobospan" id="kobo.1197.1"> (Wellesley, MA : </span><span><span class="kobospan" id="kobo.1198.1">Wellesley-Cambridge Press).</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1199.1">Lax, Peter D. </span><em class="italic"><span class="kobospan" id="kobo.1200.1">Linear algebra and its applications</span></em><span class="kobospan" id="kobo.1201.1">. </span><span class="kobospan" id="kobo.1201.2">Vol. </span><span class="kobospan" id="kobo.1201.3">78. </span><span class="kobospan" id="kobo.1201.4">John Wiley &amp; </span><span><span class="kobospan" id="kobo.1202.1">Sons, 2007.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1203.1">Dangeti, Pratap. </span><em class="italic"><span class="kobospan" id="kobo.1204.1">Statistics for machine learning</span></em><span class="kobospan" id="kobo.1205.1">. </span><span class="kobospan" id="kobo.1205.2">Packt Publishing </span><span><span class="kobospan" id="kobo.1206.1">Ltd, 2017.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1207.1">DasGupta, Anirban. </span><em class="italic"><span class="kobospan" id="kobo.1208.1">Probability for statistics and machine learning: fundamentals and advanced topics</span></em><span class="kobospan" id="kobo.1209.1">. </span><span class="kobospan" id="kobo.1209.2">New York: </span><span><span class="kobospan" id="kobo.1210.1">Springer, 2011.</span></span></li>
</ul>
</div>
</body></html>