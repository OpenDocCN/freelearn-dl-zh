<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-27"><a id="_idTextAnchor026" class="calibre5 pcalibre1 pcalibre"/>2</h1>
<h1 id="_idParaDest-28" class="calibre4"><a id="_idTextAnchor027" class="calibre5 pcalibre1 pcalibre"/>Mastering Linear Algebra, Probability, and Statistics for Machine Learning and NLP</h1>
<p class="calibre6"><strong class="bold">Natural language processing</strong> (<strong class="bold">NLP</strong>) and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) are two fields that have significantly benefited from mathematical concepts, particularly linear algebra and probability theory. These fundamental tools enable the analysis of the relationships between variables, forming the basis of many NLP and ML models. This chapter provides a comprehensive introduction to linear algebra and probability theory, including their practical applications in NLP and ML. The c<a id="_idTextAnchor028" class="calibre5 pcalibre1 pcalibre"/>hapter commences with an overview of vectors and matrices and covers essential operations. Additionally, the basics of statistics, required for understanding the concepts and models in subsequent chapters, will be explained. Finally, the chapter introduces the fundamentals of optimization, which are critical for solving NLP problems and understanding the relationships between variables. By the end of this chapter, you will have a solid foundation in linear algebra and probability theory and understand their essential applications in NLP and ML.</p>
<p class="calibre6">In this chapter, we’ll be covering the following topics:</p>
<ul class="calibre14">
<li class="calibre15">Introduction to linear algebra</li>
<li class="calibre15">Eigenvalues and eigenvectors</li>
<li class="calibre15">Basic probability for machine learning</li>
</ul>
<h1 id="_idParaDest-29" class="calibre4"><a id="_idTextAnchor029" class="calibre5 pcalibre1 pcalibre"/>Introduction to linear algebra</h1>
<p class="calibre6">Let’s start by first understanding scalars, vectors, and matrices:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Scalars</strong>: A scalar is a single <a id="_idIndexMarker027" class="calibre5 pcalibre1 pcalibre"/>numerical value that usually comes from the real<a id="_idIndexMarker028" class="calibre5 pcalibre1 pcalibre"/> domain in most ML applications. Examples of scalars in NLP include the frequency of a word in a text corpus.</li>
<li class="calibre15"><strong class="bold">Vectors</strong>: A vector is a collection of numerical elements. Each of these elements can be termed as an entry, component, or dimension, and the count of these components defines the<a id="_idIndexMarker029" class="calibre5 pcalibre1 pcalibre"/> vector’s dimensionality. Within NLP, a vector could hold components related to elements such as word frequency, sentiment ranking, and more. NLP and ML are two domains that have reaped substantial benefits from mathematical disciplines, particularly linear algebra and probability theory. These foundational tools aid in evaluating the correlation between variables and are at the heart of numerous NLP and ML models. This segment presents a detailed primer on linear algebra and probability theory, along with their practical usage in NLP and ML. For instance, a text document’s three-dimensional vector representation might be expressed as a real-number array, such as [word frequency, sentiment ranking, complexity].</li>
<li class="calibre15"><strong class="bold">Matrices</strong>: A matrix can<a id="_idIndexMarker030" class="calibre5 pcalibre1 pcalibre"/> be perceived as a rectangular collection of numerical elements composed of rows and columns. To retrieve an element from the matrix, one needs to denote its row and column indices. In the field of NLP, a data matrix might include rows that align with distinct text documents and columns that align with different text attributes, such as word frequency, sentiment, and so on. The dimensions of such a matrix are represented by the notation <em class="italic">n × d</em>, where <em class="italic">n</em> is the number of rows (i.e., text documents), and <em class="italic">d</em> is the number of columns (i.e., attributes).</li>
</ul>
<p class="calibre6">Let’s move on to the basic operations for scalars, vectors, and matrices next.</p>
<p class="calibre6">The basic operations for scalars, vectors, and matrices—addition and subtraction—can be carried out on vectors with the same dimensions. Let’s have two vectors:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/1.png" class="calibre18"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/2.png" class="calibre19"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/3.png" class="calibre20"/></p>
<p class="calibre6">For example, if we have two vectors, a = [4,1] and b = [2,4], then <em class="italic">a  + b = [</em><em class="italic">6,5]</em>.</p>
<p class="calibre6">Let’s visualize this as <a id="_idIndexMarker031" class="calibre5 pcalibre1 pcalibre"/>follows:</p>
<div><div><img alt="Figure 2.1 – Adding two vectors (a = [4,1] and  b = [2,4]) means that a  + b = [6,5]" src="img/B18949_02_1.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Adding two vectors (a = [4,1] and  b = [2,4]) means that a  + b = [6,5]</p>
<p class="calibre6">It is possible to scale a vector by multiplying it by a scalar. This operation is performed by multiplying each component of the vector by the scalar value. For example, let’s consider a n-dimensional vector, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/4.png" class="calibre21"/>. The process of scaling this vector by a factor of <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/5.png" class="calibre22"/> can be represented mathematically as follows:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/1.png" class="calibre18"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/7.png" class="calibre23"/></p>
<p class="calibre6">This operation results in a new vector that has the same dimensionality as the original vector but with each component multiplied by the scalar value <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/8.png" class="calibre24"/>.</p>
<p class="calibre6">There are two types of<a id="_idIndexMarker032" class="calibre5 pcalibre1 pcalibre"/> multiplications between vectors: dot product (<img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/9.png" class="calibre25"/>) and cross product (<img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/10.png" class="calibre26"/>). The dot product is the one we use often in ML algorithms.</p>
<p class="calibre6">The dot product is a mathematical operation that can be applied to two vectors, x = [x 1, x 2, … , x n] and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/11.png" class="calibre27"/>. It has many practical applications, one of which is to help determine their similarity. It is defined as the sum of the product of the corresponding elements of the two vectors. The dot product of <em class="italic">x</em> and <em class="italic">y</em> is represented by the symbol <em class="italic">x</em><em class="italic"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/12.png" class="calibre28"/></em> <em class="italic">y</em> (having a dot in the middle) and is defined as follows:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/13.png" class="calibre29"/></p>
<p class="calibre6">where <em class="italic">n</em> represents the dimensionality of the vectors. The dot product is a scalar quantity and can be used to measure the angle between two vectors, as well as the projection of one vector onto another. It also serves a vital function in numerous ML algorithms, including linear regression and neural networks.</p>
<p class="calibre6">The dot product is commutative, meaning that the order of the vectors does not affect the result. This means that <em class="italic">x  </em><em class="italic"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/12.png" class="calibre30"/></em><em class="italic"> y = y</em><em class="italic"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/12.png" class="calibre30"/></em><em class="italic"> x</em>. Furthermore, the dot product maintains the distributive property of scalar multiplication, implying the following:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/16.png" class="calibre31"/></p>
<p class="calibre6">The dot product of a vector with itself is also known as its squared norm or Euclidean norm. The norm, symbolized by <em class="italic">𝑛𝑜𝑟𝑚</em><em class="italic">(x)</em>, signifies the <a id="_idIndexMarker033" class="calibre5 pcalibre1 pcalibre"/>length of the vector and is<a id="_idIndexMarker034" class="calibre5 pcalibre1 pcalibre"/> computed as</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/17.png" class="calibre32"/></p>
<p class="calibre6">The normalization of vectors<a id="_idIndexMarker035" class="calibre5 pcalibre1 pcalibre"/> can be achieved by dividing them by their norm, also known as the Euclidean norm or the length of the vector. This results in a vector with a unit length, denoted by <em class="italic">x’</em>. The normalization process can be shown as</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/18.png" class="calibre33"/></p>
<p class="calibre6">where <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/19.png" class="calibre34"/> is the original vector and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/20.png" class="calibre35"/> represents its norm. It should be noted that normalizing a vector has the effect of retaining its direction while setting its length to 1, allowing the meaningful comparison of vectors in different spaces.</p>
<p class="calibre6">The cosine similarity between two vectors  <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfenced open=&quot;[&quot; close=&quot;]&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/21.png" class="calibre36"/>and  <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/22.png" class="calibre37"/> is mathematically represented as the dot product of the two vectors after they have been normalized to unit length. This can be written as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mfenced open=&quot;‖&quot; close=&quot;‖&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mfrac&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/23.png" class="calibre38"/></p>
<p class="calibre6">where <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/24.png" class="calibre39"/> and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/25.png" class="calibre40"/> are the norms of the vectors <em class="italic">x</em> and <em class="italic">y</em>, respectively. This computed cosine similarity between <em class="italic">x</em> and <em class="italic">y</em> is equivalent to the cosine of the angle between the two vectors, denoted as <em class="italic">θ</em>.</p>
<p class="calibre6">Vectors with a dot product of <em class="italic">0</em> are deemed orthogonal, implying that in the case of having both non-<em class="italic">0</em> vectors, the angle between them is 90 degrees. We can conclude that a <em class="italic">0</em> vector is orthogonal to any vector. A group of vectors is considered orthogonal if each pair of them is orthogonal and each vector possesses a norm of <em class="italic">1</em>. Such orthonormal sets prove to be valuable in numerous mathematical contexts. For instance, they come into play when transforming between different orthogonal co-ordinate systems, where the new co-ordinates of a point are computed in relation to the modified direction set. This approach, known as co-ordinate transformation in the field of analytical geometry, finds<a id="_idIndexMarker036" class="calibre5 pcalibre1 pcalibre"/> widespread application in the realm of linear algebra.</p>
<h2 id="_idParaDest-30" class="calibre7"><a id="_idTextAnchor030" class="calibre5 pcalibre1 pcalibre"/>Basic operations on matrices and vectors</h2>
<p class="calibre6"><strong class="bold">Matrix transpose</strong> is the<a id="_idIndexMarker037" class="calibre5 pcalibre1 pcalibre"/> process of obtaining the transpose of a matrix and involves interchanging its <a id="_idIndexMarker038" class="calibre5 pcalibre1 pcalibre"/>rows and columns. This means that the element originally at the <em class="italic">(i, j)</em>th position in the matrix now occupies the <em class="italic">(j, i)</em>th position in its<a id="_idIndexMarker039" class="calibre5 pcalibre1 pcalibre"/> transpose. As a result, a matrix that was originally of size <em class="italic">n × m</em> becomes an <em class="italic">m × n</em> matrix when transposed. The notation used to represent the transpose of matrix <em class="italic">X</em> is <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/26.png" class="calibre41"/>. Here’s an illustrative example of a matrix transposition operation:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/27.png" class="calibre42"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3,2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/28.png" class="calibre43"/></p>
<p class="calibre6">Crucially, the transpose <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/29.png" class="calibre44"/> of matrix <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/26.png" class="calibre45"/> reverts to the original matrix <em class="italic">X</em>. Moreover, it is clear that row vectors can be transposed into column vectors and vice versa. Additionally, the following holds true for both matrices and vectors:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/31.png" class="calibre46"/></p>
<p class="calibre6">It’s also noteworthy that dot products are commutative for matrices and vectors:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;Y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/32.png" class="calibre47"/></p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/33.png" class="calibre48"/></p>
<h2 id="_idParaDest-31" class="calibre7"><a id="_idTextAnchor031" class="calibre5 pcalibre1 pcalibre"/>Matrix definitions</h2>
<p class="calibre6">In this section, we’ll cover the different type of matrix definitions:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Symmetric matrix</strong>: A symmetric<a id="_idIndexMarker040" class="calibre5 pcalibre1 pcalibre"/> matrix is a type of square matrix where the transpose of the matrix is equal to the original matrix. In<a id="_idIndexMarker041" class="calibre5 pcalibre1 pcalibre"/> mathematical terms, if a matrix <em class="italic">X</em> is symmetric, then <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/34.png" class="calibre49"/>.  For example,<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;X&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mn&gt;7&lt;/mml:mn&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/35.png" class="calibre50"/></p><p class="calibre6">is symmetric.</p></li>
<li class="calibre15"><strong class="bold">Rectangular diagonal matrix</strong>: This is a matrix that is <em class="italic">m × n</em> in dimensions, with non-<em class="italic">0</em> values <a id="_idIndexMarker042" class="calibre5 pcalibre1 pcalibre"/>only on the main <a id="_idIndexMarker043" class="calibre5 pcalibre1 pcalibre"/>diagonal.</li>
<li class="calibre15"><strong class="bold">Upper (or Lower) triangular matrix</strong>: A matrix is called an upper (triangular) matrix if all the entries (i,j) below (above) its main diagonal are 0. Next, we are going to describe matrix<a id="_idIndexMarker044" class="calibre5 pcalibre1 pcalibre"/> operations.</li>
</ul>
<h3 class="calibre8">Determinants</h3>
<p class="calibre6">The determinant of a square <a id="_idIndexMarker045" class="calibre5 pcalibre1 pcalibre"/>matrix provides a notion of its impact on the volume of a <em class="italic">d</em>-dimensional object when multiplied by its co-ordinate vectors. The determinant, symbolized as <em class="italic">det(A)</em>, represents the (signed) volume of the parallelepiped formed by the row or column vectors of the matrix. This interpretation holds consistently, as the<a id="_idIndexMarker046" class="calibre5 pcalibre1 pcalibre"/> volume determined by the row and column vectors is mathematically identical. When a diagonalizable matrix <em class="italic">A</em> interacts with a group of co-ordinate vectors, the ensuing distortion is termed anisotropic scaling. The determinant can aid in establishing the scale factors of this conversion. The determinant of a square matrix carries crucial insights about the linear alteration accomplished by the multiplication with the matrix. Particularly, the sign of the determinant mirrors the impact of the transformation on the basis of the system’s orientation.</p>
<p class="calibre6">Calculating determinant is given as <a id="_idIndexMarker047" class="calibre5 pcalibre1 pcalibre"/>follows:</p>
<ol class="calibre16">
<li class="calibre15">For a <em class="italic">1×1</em> matrix <em class="italic">A</em>, its determinant is equivalent to the single scalar present within it.</li>
<li class="calibre15">For larger matrices, the determinant can be calculated by securing a column, <em class="italic">j</em>, and then broadening using the elements within that column. As another option, it’s possible to fix a row, <em class="italic">i</em>, and expand along that particular row. Regardless of whether you opt to fix a row<a id="_idIndexMarker048" class="calibre5 pcalibre1 pcalibre"/> or column, the end result, which is the determinant of the matrix, will remain consistent.<p class="calibre6">with <em class="italic">j</em> as a fixed value ranging from <em class="italic">1</em> to <em class="italic">d</em>,</p></li>
</ol>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/36.png" class="calibre51"/></p>
<p class="calibre6">Or, with the fixed <em class="italic">i</em>,</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;det&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/munderover&gt;&lt;msup&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi&gt;det&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/37.png" class="calibre52"/></p>
<p class="calibre6">Based on the following equations, we can see that some of the cases can be easily calculated:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Diagonal matrix</strong>: For a <a id="_idIndexMarker049" class="calibre5 pcalibre1 pcalibre"/>diagonal matrix, the determinant is the product of its diagonal elements.</li>
<li class="calibre15"><strong class="bold">Triangular matrix</strong>: In the context of <a id="_idIndexMarker050" class="calibre5 pcalibre1 pcalibre"/>a triangular matrix, the determinant is found by multiplying all its diagonal elements. If all components of a matrix’s row or column are 0, the determinant is also 0.<p class="calibre6">For a <em class="italic">2 × 2</em> matrix of</p><p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/38.png" class="calibre53"/></p><p class="calibre6">Its determinant can be computed as <em class="italic">ad - bc</em>. If we consider a <em class="italic">3 × </em><em class="italic">3</em> matrix,</p><p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/39.png" class="calibre54"/></p><p class="calibre6">The determinant is calculated as follows:</p></li>
</ul>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;det&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/40.png" class="calibre55"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/41.png" class="calibre56"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/42.png" class="calibre57"/></p>
<p class="calibre6">Let’s now move on to <a id="_idIndexMarker051" class="calibre5 pcalibre1 pcalibre"/>eigenvalues and vectors.</p>
<h1 id="_idParaDest-32" class="calibre4"><a id="_idTextAnchor032" class="calibre5 pcalibre1 pcalibre"/>Eigenvalues and eigenvectors</h1>
<p class="calibre6">A vector <em class="italic">x</em>, belonging to a <em class="italic">d × d</em> matrix <em class="italic">A</em>, is an <strong class="bold">eigenvector</strong> if it satisfies the equation <em class="italic">Ax = λx</em>, where <em class="italic">λ</em> represents the<a id="_idIndexMarker052" class="calibre5 pcalibre1 pcalibre"/> eigenvalue associated with the matrix. This relationship delineates the link between matrix <em class="italic">A</em> and its corresponding eigenvector <em class="italic">x</em>, which can be perceived <a id="_idIndexMarker053" class="calibre5 pcalibre1 pcalibre"/>as the “stretching direction” of the matrix. In the case where <em class="italic">A</em> is a matrix that can be diagonalized, it can be deconstructed into a <em class="italic">d × d</em> invertible matrix, <em class="italic">V</em>, and a diagonal <em class="italic">d × d</em> matrix, <em class="italic">Δ</em>, such that</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Δ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/43.png" class="calibre58"/></p>
<p class="calibre6">The columns of <em class="italic">V</em> encompass <em class="italic">d</em> eigenvectors, while the diagonal entries of <em class="italic">Δ</em> house the corresponding eigenvalues. The linear transformation <em class="italic">Ax</em> can be visually understood through a sequence of three operations. Initially, the multiplication of <em class="italic">x</em> by <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/44.png" class="calibre59"/>  calculates <em class="italic">x</em>’s co-ordinates in a non-orthogonal basis associated with <em class="italic">V</em>’s columns. Subsequently, the multiplication of <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/45.png" class="calibre60"/> x by <em class="italic">Δ</em> scales these co-ordinates using the factors in <em class="italic">Δ</em>, aligned with the eigenvectors’ directions. Finally, the multiplication with <em class="italic">V</em> restores the co-ordinates to the original basis, resulting in an anisotropic scaling along the <em class="italic">d</em> eigenvector directions.</p>
<p class="calibre6">Diagonalizable matrices signify transformations involving anisotropic scaling along <em class="italic">d</em>-linearly independent directions. When <em class="italic">V</em> ‘s columns are orthonormal vectors, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;V&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/46.png" class="calibre61"/>equals its transpose, <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/47.png" class="calibre62"/>, indicating<a id="_idIndexMarker054" class="calibre5 pcalibre1 pcalibre"/> scaling along mutually orthogonal directions. In such cases, matrix <em class="italic">A</em> is always diagonalizable and exhibits symmetry when <em class="italic">V</em>’s columns are <a id="_idIndexMarker055" class="calibre5 pcalibre1 pcalibre"/>orthonormal vectors, as affirmed by the following relationship.</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Δ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;Δ&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/48.png" class="calibre63"/></p>
<h2 id="_idParaDest-33" class="calibre7"><a id="_idTextAnchor033" class="calibre5 pcalibre1 pcalibre"/>Numerical methods for finding eigenvectors</h2>
<p class="calibre6">The conventional method to ascertain the eigenvectors of a <em class="italic">d × d</em> matrix <em class="italic">A</em> involves locating the <em class="italic">d</em> roots, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/49.png" class="calibre64"/>of the equation:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;I&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/50.png" class="calibre65"/></p>
<p class="calibre6">Some of these roots might be<a id="_idIndexMarker056" class="calibre5 pcalibre1 pcalibre"/> repeated. The subsequent step involves solving linear systems in the form <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/51.png" class="calibre66"/>, typically achieved using the Gaussian elimination method. However, this method might not always be the most stable or precise, as solvers of polynomial equations can exhibit ill-conditioning and numerical instability in practical applications. Indeed, a prevalent technique for resolving high-degree polynomial equations in engineering involves constructing a companion matrix possessing the same characteristic polynomial as the original polynomial and then determining its eigenvalues.</p>
<h2 id="_idParaDest-34" class="calibre7"><a id="_idTextAnchor034" class="calibre5 pcalibre1 pcalibre"/>Eigenvalue decomposition</h2>
<p class="calibre6">Eigenvalue decomposition, also <a id="_idIndexMarker057" class="calibre5 pcalibre1 pcalibre"/>known as the eigen-decomposition or the diagonalization of a matrix, is a powerful mathematical tool used in linear algebra and computational mathematics. The goal of eigenvalue decomposition is to decompose a given matrix into a product of matrices that represent the eigenvectors and eigenvalues of the matrix.</p>
<p class="calibre6">The eigenvalue decomposition of matrix <em class="italic">A</em> is a factorization of the matrix into the product of two matrices: the matrix <em class="italic">V</em> and the matrix <em class="italic">D</em>.</p>
<p class="calibre6"><em class="italic">V</em> has column which are the eigenvectors of matrix <em class="italic">A</em>, and <em class="italic">D</em> is a diagonal matrix that contains the corresponding eigenvalues on its diagnol.</p>
<p class="calibre6">The eigenvalue problem is to find the non-0 vectors, <em class="italic">v</em>, and the scalars, <em class="italic">λ</em>, such that <em class="italic">Av</em> <em class="italic">= λv</em>, where <em class="italic">A</em> is a square matrix, and thus <em class="italic">v</em> is an eigenvector of <em class="italic">A</em>. The scalar <em class="italic">λ</em> is called the eigenvalue of matrix <em class="italic">A</em>. The eigenvalue problem can be written in matrix form as <em class="italic">Av = λIv</em>, where <em class="italic">I</em> is the identity matrix.</p>
<p class="calibre6">The process of determining <a id="_idIndexMarker058" class="calibre5 pcalibre1 pcalibre"/>eigenvalues is intimately linked to the characteristic equation of matrix <em class="italic">A</em>, which is the polynomial equation derived from <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;λ&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;I&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/52.png" class="calibre67"/>. The characteristic equation can be solved for the eigenvalues, <em class="italic">λ</em>, which are the roots of the equation. Once the eigenvalues are found, the eigenvectors can be found by solving the system of linear equations <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/53.png" class="calibre68"/>.</p>
<p class="calibre6">One important property of eigenvalue decomposition is that it allows us to diagonalize a matrix, which means that we can transform the matrix into a diagonal form by using an appropriate eigenvectors matrix. The diagonal form of a matrix is useful because it allows us to calculate the trace and determinant of the matrix easily.</p>
<p class="calibre6">Another important property of eigenvalue decomposition is that it provides insight into the structure of the matrix. For example, the eigenvalues of a symmetric matrix are always real, and the eigenvectors are orthogonal, which means that they are perpendicular to each other. In the case of non-symmetric matrices, the eigenvalues can be complex, and the eigenvectors are not necessarily orthogonal.</p>
<p class="calibre6">The eigenvalue decomposition of a matrix has many applications in mathematics, physics, engineering, and computer science. In numerical analysis, eigenvalue decomposition is used to find the solution of linear systems, compute the eigenvalues of a matrix, and find the eigenvectors of a matrix. In physics, eigenvalue decomposition is used to analyze the stability of systems, such as the stability of an equilibrium point in a differential equation. In engineering, eigenvalue decomposition is used to study the dynamics of systems, such as the vibrations of a mechanical system.</p>
<p class="calibre6">Within the field of computer science, eigenvalue decomposition finds versatile applications across various domains, including machine learning and data analysis. In machine learning, eigenvalue decomposition plays a pivotal role in enabling <strong class="bold">principal</strong> <strong class="bold">component analysis</strong> (<strong class="bold">PCA</strong>), a technique<a id="_idIndexMarker059" class="calibre5 pcalibre1 pcalibre"/> employed for dimensionality reduction in extensive datasets. In the realm of data analysis, eigenvalue decomposition is harnessed to calculate the <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>), a potent tool for dissecting and understanding complex<a id="_idIndexMarker060" class="calibre5 pcalibre1 pcalibre"/> datasets.</p>
</div>


<div><h2 id="_idParaDest-35" class="calibre7"><a id="_idTextAnchor035" class="calibre5 pcalibre1 pcalibre"/>Singular value decomposition</h2>
<p class="calibre6">The problem of minimizing <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/54.png" class="calibre69"/>, where <em class="italic">x</em> is a column vector that has a unit norm, and <em class="italic">A</em> is a symmetric <em class="italic">d × d</em> data matrix, is a typical <a id="_idIndexMarker061" class="calibre5 pcalibre1 pcalibre"/>problem encountered in numerous machine learning contexts. This problem type is often found in applications such as principal component analysis, singular value decomposition, and spectral clustering, all of which involve feature engineering and dimensionality reduction. The optimization problem can be articulated as follows:</p>
<p class="calibre6">Minimize</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/55.png" class="calibre70"/></p>
<p class="calibre6">Subject to</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;‖&quot; close=&quot;‖&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/56.png" class="calibre71"/></p>
<p class="calibre6">We can solve the optimization problem as a maximization or minimization form. Imposing the constraint that vector <em class="italic">x</em> must be a unit vector significantly changes the nature of the optimization problem. In contrast to the prior section, the positive semi-definiteness of matrix <em class="italic">A</em> is no longer crucial for determining the solution. Even when <em class="italic">A</em> is indefinite, the constraint on the norm of <a id="_idIndexMarker062" class="calibre5 pcalibre1 pcalibre"/>vector <em class="italic">x</em> ensures a well-defined solution, preventing the involvement of vectors with unbounded magnitudes or trivial solutions, such as the <em class="italic">0</em> vector. <strong class="bold">Value singular decomposition</strong> (<strong class="bold">SVD</strong>) is a mathematical technique that takes a rectangular matrix, <em class="italic">A</em>, and decomposes it into three matrices: <em class="italic">U</em>, <em class="italic">S</em>, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/57.png" class="calibre72"/>. Matrix <em class="italic">A</em> is defined as an <em class="italic">n × p</em> matrix. The theorem of SVD states that <em class="italic">A</em> can be represented as the product of three matrices: <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;S&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/58.png" class="calibre73"/>, where <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/59.png" class="calibre74"/>, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/60.png" class="calibre75"/>, and <em class="italic">U</em> and <em class="italic">V</em> are orthogonal matrices.</p>
<p class="calibre6">The <em class="italic">U</em> matrix’s columns are known as the left singular vectors, while the rows of the transpose of the <em class="italic">V</em> matrix <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/47.png" class="calibre76"/> are the right singular vectors. The <em class="italic">S</em> matrix, with singular values, is a diagonal matrix of the same size as <em class="italic">A</em>. SVD decomposes the original data into a co-ordinate system where the defining vectors are orthonormal (both orthogonal and normal). SVD computation involves identifying the eigenvalues and eigenvectors of matrices <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/62.png" class="calibre77"/> and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/63.png" class="calibre78"/>. Matrix <em class="italic">V</em>’s columns consist of eigenvectors from <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/64.png" class="calibre79"/>, and matrix <em class="italic">U</em>’s columns consist of eigenvectors from <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/65.png" class="calibre80"/>. Singular values in the <em class="italic">S</em> matrix are derived from the square roots of eigenvalues from either <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/66.png" class="calibre81"/> or <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/67.png" class="calibre82"/>, organized in decreasing order. These singular <a id="_idIndexMarker063" class="calibre5 pcalibre1 pcalibre"/>values are real numbers. If <em class="italic">A</em> is a real matrix, <em class="italic">U</em> and <em class="italic">V</em> will also be real.</p>
<p class="calibre6">To illustrate the calculation of SVD, an example is provided. Consider a <em class="italic">4 × 2</em> matrix. The eigenvalues of the matrix can be found by computing <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/68.png" class="calibre83"/> and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/69.png" class="calibre83"/> and then determining the eigenvectors of these matrices. <em class="italic">U</em>’s columns are formed by the eigenvectors of <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/68.png" class="calibre84"/>, and <em class="italic">V</em>’s columns are formed by the eigenvectors of <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/71.png" class="calibre85"/>. The <em class="italic">S</em> matrix comprises the square root of eigenvalues from either <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/68.png" class="calibre86"/> or <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/73.png" class="calibre87"/>. Eigenvalues are found by solving the characteristic equation in the given example <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;|&quot; close=&quot;|&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/74.png" class="calibre88"/>, where <em class="italic">W</em> is the matrix, <em class="italic">I</em> is the unit matrix, and <em class="italic">λ</em> is the eigenvalue. The eigenvectors are then found by solving the set of equations derived from the eigenvalue equations. The final matrices <em class="italic">U, </em><em class="italic">S</em>, and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/57.png" class="calibre72"/> are then obtained by combining the eigenvectors and singular values.</p>
<p class="calibre6">It should be noted that the singular values are in descending order, with <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;&gt;&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/76.png" class="calibre89"/></p>
<p class="calibre6">Let’s now move on to basic probability for machine learning.</p>
<h1 id="_idParaDest-36" class="calibre4"><a id="_idTextAnchor036" class="calibre5 pcalibre1 pcalibre"/>Basic probability for machine learning</h1>
<p class="calibre6">Probability provides information <a id="_idIndexMarker064" class="calibre5 pcalibre1 pcalibre"/>about the likelihood of an event occurring. In this field, there are several key terms that are important to understand:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Trial or experiment</strong>: An action that results in a certain outcome with a certain likelihood</li>
<li class="calibre15"><strong class="bold">Sample space</strong>: This encompasses all potential outcomes of a given experiment</li>
<li class="calibre15"><strong class="bold">Event</strong>: This denotes a non-empty portion of the sample space</li>
</ul>
<p class="calibre6">Therefore, in technical terms, probability is a measure of the likelihood of an event occurring when an experiment is conducted.</p>
<p class="calibre6">In this very simple case, the<a id="_idIndexMarker065" class="calibre5 pcalibre1 pcalibre"/> probability of event <em class="italic">A</em> with one outcome is equal to the chance of event <em class="italic">A</em> divided by the chance of all possible events. For example, in flipping a fair coin, there are two outcomes with the same chance: heads and tails. The chance of having heads will be <em class="italic">1/(1+1) = ½</em>.</p>
<p class="calibre6">In order to calculate the probability, given an event, <em class="italic">A</em>, with <em class="italic">n</em> outcomes and a sample space, <em class="italic">S</em>, the probability of event A is calculated as</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/77.png" class="calibre90"/></p>
<p class="calibre6">where <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;E&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/78.png" class="calibre91"/> represents the outcomes in <em class="italic">A</em>. Assuming all results of the experiment have equal probability, and the selection of one does not influence the selection of others in subsequent rounds (meaning they are statistically independent), then</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mo&gt;.&lt;/mo&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/79.png" class="calibre92"/></p>
<p class="calibre6">Hence, the value of probability ranges from <em class="italic">0</em> to <em class="italic">1</em>, with the sample space embodying the complete set of potential outcomes, denoted as <em class="italic">P(S) = </em><em class="italic">1</em>.</p>
<h2 id="_idParaDest-37" class="calibre7"><a id="_idTextAnchor037" class="calibre5 pcalibre1 pcalibre"/>Statistically independent</h2>
<p class="calibre6">In the realm of statistics, two <a id="_idIndexMarker066" class="calibre5 pcalibre1 pcalibre"/>events are defined as independent if the occurrence of one event doesn’t influence the likelihood of the other event’s occurrence. To put it formally, events <em class="italic">A</em> and <em class="italic">B</em> are independent precisely when <em class="italic">P(A and B) = P(A)P(B)</em>, where <em class="italic">P(A)</em> and <em class="italic">P(B)</em> are the respective probabilities of events <em class="italic">A</em> and <em class="italic">B</em> happening.</p>
<p class="calibre6">Consider this example to clarify the concept of statistical independence: imagine we possess two coins, one fair (an equal chance of turning up heads or tails) and the other biased (showing a head is more likely than a tail). If we flip the fair coin and the biased coin, these two events are statistically independent because the outcome of one coin flip doesn’t alter the probability<a id="_idIndexMarker067" class="calibre5 pcalibre1 pcalibre"/> of the other coin turning up heads or tails. Specifically, the likelihood of both coins showing heads is the product of the individual probabilities: <em class="italic">(1/2) * (3/4) = </em><em class="italic">3/8</em>.</p>
<p class="calibre6">Statistical independence is a pivotal concept in statistics and probability theory, frequently leveraged in machine learning to outline the connections between variables within a dataset. By comprehending these relationships, machine learning algorithms can better spot patterns and deliver more precise predictions. We will describe the relationship between different types of events in the following:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Complementary event</strong>: The complementary event to <em class="italic">A</em>, signified as <em class="italic">A’</em>, encompasses the probability of all <a id="_idIndexMarker068" class="calibre5 pcalibre1 pcalibre"/>potential outcomes in the sample space not included in A. It’s critical to understand that <em class="italic">A</em> and <em class="italic">A’</em> are statistically independent:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/80.png" class="calibre93"/></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Union and intersection</strong>: The complementary event to <em class="italic">A</em>, signified as <em class="italic">A’</em>, encompasses the probability of all potential outcomes in the sample space not <a id="_idIndexMarker069" class="calibre5 pcalibre1 pcalibre"/>included in <em class="italic">A</em>. It’s critical to understand that <em class="italic">A</em> and <em class="italic">A’</em> are statistically independent.</li>
<li class="calibre15"><strong class="bold">Mutually exclusive</strong>: When two<a id="_idIndexMarker070" class="calibre5 pcalibre1 pcalibre"/> events have no shared outcomes, they are viewed as mutually exclusive. In other words, if <em class="italic">A</em> and <em class="italic">B</em> are mutually exclusive events, then <em class="italic">P(A </em><em class="italic">∩</em><em class="italic"> B) = 0</em>. This conclusion can be drawn from the addition rule of probability, as <em class="italic">A</em> and <em class="italic">B</em> a<a id="_idTextAnchor038" class="calibre5 pcalibre1 pcalibre"/>re disjointed events:</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;U&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/81.png" class="calibre94"/></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Independent</strong>: Two events are deemed independent when the occurrence of one doesn’t impact the <a id="_idIndexMarker071" class="calibre5 pcalibre1 pcalibre"/>occurrence of the other. If <em class="italic">A</em> and <em class="italic">B</em> are two independent events, then</li>
</ul>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;∩&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;∙&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/82.png" class="calibre95"/></p>
<p class="calibre6">Next, we are going to describe the discrete random variable, its distribution, and how to use it to calculate the probabilities.</p>
<h2 id="_idParaDest-38" class="calibre7"><a id="_idTextAnchor039" class="calibre5 pcalibre1 pcalibre"/>Discrete random variables and their distribution</h2>
<p class="calibre6">A discrete random variable <a id="_idIndexMarker072" class="calibre5 pcalibre1 pcalibre"/>refers to a variable that can assume a finite or countably infinite number of potential outcomes. Examples of such variables might be the count of heads resulting from a coin toss, the tally of cars crossing a toll booth within a specific time span, or the number of blonde-haired students in a classroom.</p>
<p class="calibre6">The probability distribution of a <a id="_idIndexMarker073" class="calibre5 pcalibre1 pcalibre"/>discrete random variable assigns a certain likelihood to each potential outcome the variable could adopt. For instance, in the case of a coin toss, the probability distribution assigns a 0.5 probability to both <em class="italic">0</em> and <em class="italic">1</em>, representing tails and heads, respectively. For the car toll booth scenario, the distribution could be assigning a probability of <em class="italic">0.1</em> to no cars passing, <em class="italic">0.3</em> to one car, <em class="italic">0.4</em> to two cars, <em class="italic">0.15</em> to three cars, and <em class="italic">0.05</em> to four or more cars.</p>
<p class="calibre6">A graphical representation of the probability distribution of a discrete random variable can be achieved through a <strong class="bold">probability mass function</strong> (<strong class="bold">PMF</strong>), which correlates each possible outcome of<a id="_idIndexMarker074" class="calibre5 pcalibre1 pcalibre"/> the variable to its likelihood of occurrence. This function is usually represented as a bar chart or histogram, with each bar signifying the probability of a specific value.</p>
<p class="calibre6">The PMF is bound by two key principles:</p>
<ul class="calibre14">
<li class="calibre15">It must be non-negative across all potential values of the random variable</li>
<li class="calibre15">The total sum of probabilities for all possible outcomes should equate to 1</li>
</ul>
<p class="calibre6">The expected value of a discrete random variable offers an insight into its central tendency, computed as the probability-weighted average of its possible outcomes. This expected value is signified as <em class="italic">E[X]</em>, with <em class="italic">X</em> representing the random variable.</p>
<h2 id="_idParaDest-39" class="calibre7"><a id="_idTextAnchor040" class="calibre5 pcalibre1 pcalibre"/>Probability density function</h2>
<p class="calibre6">The <strong class="bold">probability density function</strong> (<strong class="bold">PDF</strong>) is a tool used to describe the distribution of a continuous random variable. It can be <a id="_idIndexMarker075" class="calibre5 pcalibre1 pcalibre"/>used to calculate the probability of a value falling within a specific range. In simpler terms, it helps determine the chances of a continuous variable, <em class="italic">X</em>, having a value within<a id="_idIndexMarker076" class="calibre5 pcalibre1 pcalibre"/> the interval [<em class="italic">a, b</em>], or in statistical terms,</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/83.png" class="calibre96"/></p>
<p class="calibre6">For continuous variables, the probability of a single value occurring is always 0, which is in contrast to discrete variables that can assign non-<em class="italic">0</em> probabilities to distinct values. PDFs provide a way to estimate the likelihood of a value falling within a given range instead of a single value.</p>
<p class="calibre6">For example, you can use a PDF to find the chances of the next IQ score measured falling between <em class="italic">100</em> and <em class="italic">120</em>.</p>
<div><div><img alt=" Figure 2.2 – Probability density function for IQ from 100–120" src="img/B18949_02_2.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"> Figure 2.2 – Probability density function for IQ from 100–120</p>
<p class="calibre6">To ascertain the distribution of a discrete random variable, one can either provide its PMF or <strong class="bold">cumulative distribution function</strong> (<strong class="bold">CDF</strong>). For continuous random variables, we primarily utilize the CDF, as it is well <a id="_idIndexMarker077" class="calibre5 pcalibre1 pcalibre"/>established. However, the PMF is not suitable for these types of variables<a id="_idIndexMarker078" class="calibre5 pcalibre1 pcalibre"/> because <em class="italic">P(X=x)</em> equals <em class="italic">0</em> for all <em class="italic">x</em> in the set of real numbers, given that <em class="italic">X</em> can assume any real value between <em class="italic">a</em> and <em class="italic">b</em>. Therefore, we typically define the PDF instead. The PDF resembles the concept of mass density in physics, signifying the concentration of probability. Its unit is the probability per unit length. To get a grasp of the PDF, let’s analyze a continuous random variable, <em class="italic">X</em>, and establish the function <em class="italic">fX(x)</em> as follows:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;lim&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&lt;&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/84.png" class="calibre97"/></p>
<p class="calibre6">If the limit exists.</p>
<p class="calibre6">The function <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/85.png" class="calibre98"/>provides the probability density at a given point, <em class="italic">x</em>. This is equivalent to the limit of the ratio of the probability of the interval <em class="italic">(x, x + Δ]</em> to the length of the interval as that length approaches 0.</p>
<p class="calibre6">Let’s contemplate a continuous random variable, <em class="italic">X</em>, possessing an absolutely continuous CDF, denoted as <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/86.png" class="calibre99"/>. If <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/86.png" class="calibre99"/> is differentiable at <em class="italic">x</em>, the function <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/88.png" class="calibre100"/> is referred to as the PDF of <em class="italic">X</em>:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mspace width=&quot;0.25em&quot; /&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mi&gt;lim&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;msup&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mo&gt;∆&lt;/mo&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;/mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/89.png" class="calibre101"/></p>
<p class="calibre6">Assuming <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/86.png" class="calibre99"/>  is differentiable at <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/91.png" class="calibre102"/>.</p>
<p class="calibre6">For example, let’s consider <a id="_idIndexMarker079" class="calibre5 pcalibre1 pcalibre"/>a continuous uniform random variable, <em class="italic">X</em>, with uniform <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;U&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/92.png" class="calibre103"/> distribution. Its CDF is given by:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/93.png" class="calibre104"/></p>
<p class="calibre6">which is <em class="italic">0</em> for any x outside the bounds.</p>
<p class="calibre6">By using integration, the CDF can be obtained from the PDF:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/94.png" class="calibre105"/></p>
<p class="calibre6">Additionally, we have</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;F&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="img/95.png" class="calibre106"/></p>
<p class="calibre6">So, if we integrate over the entire real line, we will get 1:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/96.png" class="calibre107"/></p>
<p class="calibre6">Explicitly, when integrating the PDF across the entire real number line, the result should equal <em class="italic">1</em>. This signifies that the area beneath the PDF curve must equate to <em class="italic">1</em>, or <em class="italic">P(S) = 1</em>, which remains true for the uniform distribution. The PDF signifies the density of probability; thus, it must <a id="_idIndexMarker080" class="calibre5 pcalibre1 pcalibre"/>be non-negative and can exceed <em class="italic">1</em>.</p>
<p class="calibre6">Consider a continuous random variable, <em class="italic">X</em>, with PDF represented as <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/97.png" class="calibre108"/>. The ensuing properties are applicable:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;≥&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/98.png" class="calibre109"/></p>
<p class="calibre6">for all real x</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;∞&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/96.png" class="calibre107"/></p>
<p class="calibre6"> Next, we’ll move on to cover maximum likelihood.</p>
<h3 class="calibre8">Maximum likelihood estimation</h3>
<p class="calibre6">Maximum likelihood is a <a id="_idIndexMarker081" class="calibre5 pcalibre1 pcalibre"/>statistical approach, that is used to estimate the parameters of a probability distribution. The<a id="_idIndexMarker082" class="calibre5 pcalibre1 pcalibre"/> objective is to identify the parameter values that maximize the likelihood of observing the data, essentially determining the parameters most likely to have generated the data.</p>
<p class="calibre6">Suppose we have a random sample, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/100.png" class="calibre110"/>, from a population with a probability distribution <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="img/101.png" class="calibre111"/>, where <em class="italic">θ</em> is a vector of parameters. The likelihood of observing the sample, <em class="italic">X</em>, given the parameters, <em class="italic">θ</em>, is defined as the product of the individual probabilities of observing each data point:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/102.png" class="calibre112"/></p>
<p class="calibre6">In case of having independent and identically distributed observations, the likelihood function can be expressed as the product of the univariate density functions, each evaluated at the corresponding observation:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/103.png" class="calibre113"/></p>
<p class="calibre6">The <strong class="bold">maximum likelihood estimate</strong> (<strong class="bold">MLE</strong>) is the parameter vector value that offers the maximum value for the likelihood <a id="_idIndexMarker083" class="calibre5 pcalibre1 pcalibre"/>function across the parameter space.</p>
<p class="calibre6">In many cases, it’s more convenient to employ the natural logarithm of the likelihood function, referred to<a id="_idIndexMarker084" class="calibre5 pcalibre1 pcalibre"/> as the <strong class="bold">log-likelihood</strong>. The peak of the log-likelihood happens at the identical parameter vector value as the likelihood function’s maximum, and the conditions required for a maximum (or minimum) are acquired by equating the log-likelihood derivatives with respect to each parameter to 0. If the log-likelihood is differentiable with respect to<a id="_idIndexMarker085" class="calibre5 pcalibre1 pcalibre"/> the parameters, these conditions result in a set of equations that can be solved numerically to derive the MLE. One common use case or scenario where MLE significantly impacts ML<a id="_idIndexMarker086" class="calibre5 pcalibre1 pcalibre"/> model performance is in linear regression. When building a linear regression model, MLE is often used to estimate the coefficients that define the relationship between input features and the target variable. MLE helps find the values for the coefficients that maximize the likelihood of observing the given data under the assumed linear regression model, improving the accuracy of the predictions.</p>
<p class="calibre6">The MLEs of the parameters, <em class="italic">θ</em>, are the values that maximize the likelihood function. In other words, the MLEs are the values of <em class="italic">θ</em> that make the observed data, <em class="italic">X</em>, most probable.</p>
<p class="calibre6">To find the MLEs, we typically take the natural logarithm of the likelihood function, as it is often easier to work with the logarithm of a product than with the product itself:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/104.png" class="calibre114"/></p>
<p class="calibre6">The MLEs are determined by equating the partial derivatives of the log-likelihood function with respect to each parameter to <em class="italic">0</em> and then solving these equations for the parameters:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/105.png" class="calibre115"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/106.png" class="calibre115"/></p>
<p class="calibre6">...</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;ln&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;/&lt;/mml:mo&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="img/107.png" class="calibre116"/></p>
<p class="calibre6">where <em class="italic">k</em> is the number of parameters in <em class="italic">θ</em>. The goal of a maximum likelihood estimator is to find <em class="italic">θ</em> such that</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;munder&gt;&lt;mi&gt;max&lt;/mi&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi mathvariant=&quot;bold-italic&quot;&gt;θ&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/108.png" class="calibre117"/></p>
<p class="calibre6">Once the MLEs have been found, they can be used to make predictions about the population based on the sample data. Maximum likelihood is widely used in many fields, including psychology, economics, engineering, and biology. It serves as a potent tool for comprehending the connections<a id="_idIndexMarker087" class="calibre5 pcalibre1 pcalibre"/> among variables and for predicting outcomes based on observed data. For example, building a word predictor using maximum likelihood estimation.</p>
<p class="calibre6">Next, we introduce the <a id="_idIndexMarker088" class="calibre5 pcalibre1 pcalibre"/>problem of word autocompletion, also known as <strong class="bold">word prediction</strong>, which is a feature in where an application<a id="_idIndexMarker089" class="calibre5 pcalibre1 pcalibre"/> predicts the next word a user is typing. The aim of word prediction is to save time and make typing easier by predicting what the user is likely to type next based on their previous inputs and other contextual factors. Word prediction can be found in various forms in many applications, including search engines, text editors, and mobile device keyboards, and is designed to save time and increase the accuracy of inputs.</p>
<p class="calibre6">Given a group of words that the user typed, how would we suggest the next word?</p>
<p class="calibre6">If the words were <strong class="bold">The United States of</strong>, then it would be trivial to assume that the next word would be <strong class="bold">America</strong>. However, what about finding the next word for <strong class="bold">How are</strong>? One could suggest several next words.</p>
<p class="calibre6">There usually isn’t just one clear next word. Thus, we’d want to suggest the most likely word or perhaps even the most likely words. In that case, we would be interested in suggesting a probabilistic representation of the possible next words and picking the next word as the one that is most probable.</p>
<p class="calibre6">The maximum likelihood estimator provides us with that precise capability. It can tell us which word is most probable given the previous words that the user typed.</p>
<p class="calibre6">In order to calculate the MLE, we need to calculate the probability function of all word combinations. We can do that <a id="_idIndexMarker090" class="calibre5 pcalibre1 pcalibre"/>by processing large texts and counting how many times each combination of words exists.</p>
<p class="calibre6">Consider reviewing a large cohort of text that has the following occurrences:</p>
<table class="no-table-style" id="table001-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2"/>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">you”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">they”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">those”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">“</strong><strong class="bold">the”</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre6"><strong class="bold">Any </strong><strong class="bold">other word</strong></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">“how are …”</p>
</td>
<td class="no-table-style2">
<p class="calibre6">16</p>
</td>
<td class="no-table-style2">
<p class="calibre6">14</p>
</td>
<td class="no-table-style2">
<p class="calibre6">0</p>
</td>
<td class="no-table-style2">
<p class="calibre6">100</p>
</td>
<td class="no-table-style2">
<p class="calibre6">10</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre6">not “how are…”</p>
</td>
<td class="no-table-style2">
<p class="calibre6">200</p>
</td>
<td class="no-table-style2">
<p class="calibre6">100</p>
</td>
<td class="no-table-style2">
<p class="calibre6">300</p>
</td>
<td class="no-table-style2">
<p class="calibre6">1,000</p>
</td>
<td class="no-table-style2">
<p class="calibre6">30,000</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 2.1 – Sample of n-grams occurrences in a document</p>
<p class="calibre6">For instance, there are 16 occurrences in the text where the sequence “how are you” appears. There are 140 <a id="_idIndexMarker091" class="calibre5 pcalibre1 pcalibre"/>sequences that have a length of three that start with the words “how are.” That is calculated as:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mn&gt;16&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;14&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;100&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;10&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;140&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/109.png" class="calibre118"/></p>
<p class="calibre6">There are 216 sequences that have a length of three and that end with the word “you”. That is calculated as:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mn&gt;16&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;200&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;216&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/110.png" class="calibre119"/></p>
<p class="calibre6">Now, let’s suggest a formula for the most likely next word.</p>
<p class="calibre6">Based on the common maximum likelihood estimation for the probablistic variable <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/111.png" class="calibre120"/>, the formula would be to find a value for <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/112.png" class="calibre121"/> which maximizes:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/113.png" class="calibre122"/></p>
<p class="calibre6">However, this common formula has a few characteristics that wouldn’t be advantagous to our application.</p>
<p class="calibre6">Consider the next formula which has specific advantages that are necessary for our use case. It is the maximum likelihood formula for parametric estimation, meaning, estimating deterministic <a id="_idIndexMarker092" class="calibre5 pcalibre1 pcalibre"/>parameters. It suggests finding a value for <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/112.png" class="calibre121"/> which maximizes:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/115.png" class="calibre123"/></p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/112.png" class="calibre121"/> is by no means a deterministic parameter, however, this formula suits our use case as it reduces common word bias emphasizing contextual fit, and adjusts for word specificity, thus enhancing the relevance of our predictions. We will elaborate more on these traits in the<a id="_idIndexMarker093" class="calibre5 pcalibre1 pcalibre"/> conclusion of this exercise.</p>
<p class="calibre6">Let’s enhance this formula so to make it easier to calculate:</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;|&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/117.png" class="calibre124"/></p>
<p class="calibre6">In our case,<img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/118.png" class="calibre125"/> is “how” and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="img/119.png" class="calibre125"/> is “are.”</p>
<p class="calibre6">There are five candidates for the next word; let’s calculate the probability for each of them:</p>
<ul class="calibre14">
<li class="calibre15">P(“how”, “are” | “you”) = 16 / (200 + 16) = 16/216 = 2/27</li>
<li class="calibre15">P(“how”, “are” | “they”) = 14 / (100 +14) = 14/114 = 7/57</li>
<li class="calibre15">P(“how”, “are” | “those”) = 0 / 300 = 0</li>
<li class="calibre15">P(“how”, “are” | “the”) = 100 / (1000 + 100) = 100/1100 = 1/11</li>
<li class="calibre15">P(“how”, “are” | any other word) = 10 / (30,000 + 10) = 10/30010 = 1/3001</li>
</ul>
<p class="calibre6">Out of all the options, the highest value of probability is 7/57 and it is achieved when “they” is the next word.</p>
<p class="calibre6">Note that the intuition behind this maximum likelihood estimator is having the suggested next word make the words that the user typed most likely. One could wonder, why not take the word that is most probable given the first two words, meaning, the orginal maximum likelihood formula for probabilistic variables? From the table, we see that given the words “how are,” the most frequent third word is “the,” with a probability of 100/140. However, this approach wouldn’t take into account the fact that the word “the” is extremely prevalent altogether, as it is most frequently used in the text in general. Thus, its high frequency isn’t due to its relationship to the first two words; it is because it is <a id="_idIndexMarker094" class="calibre5 pcalibre1 pcalibre"/>simply a very <a id="_idIndexMarker095" class="calibre5 pcalibre1 pcalibre"/>common word in general. The maximum likelyhood formula we chose takes that into account.</p>
<h2 id="_idParaDest-40" class="calibre7"><a id="_idTextAnchor041" class="calibre5 pcalibre1 pcalibre"/>Bayesian estimation</h2>
<p class="calibre6">Bayesian estimation is a statistical <a id="_idIndexMarker096" class="calibre5 pcalibre1 pcalibre"/>approach that involves updating our beliefs or probabilities about a quantity of interest based on new data. The term “Bayesian” refers to Thomas Bayes, an 18th-century statistician who first developed the concept of Bayesian probability.</p>
<p class="calibre6">In Bayesian estimation, we start with prior beliefs about the quantity of interest, which are expressed as a probability distribution. These prior beliefs are updated as we collect new data. The updated beliefs are represented as a posterior distribution. The Bayesian framework provides a systematic way of updating prior beliefs with new data, taking into account the degree of uncertainty in both the prior beliefs and the new data.</p>
<p class="calibre6">The posterior distribution is calculated using Bayes’ theorem, which is the fundamental equation of Bayesian estimation. Bayes’ theorem states that</p>
<p class="calibre6"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mi&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/120.png" class="calibre126"/></p>
<p class="calibre6">where <em class="italic">Θ</em> is the quantity of interest, <em class="italic">X</em> is the new data, <em class="italic">P(Θ|X)</em> is the posterior distribution, <em class="italic">P(X|Θ)</em> is the likelihood of the data given the parameter value, <em class="italic">P(Θ)</em> is the prior distribution, and <em class="italic">P(X)</em> is the marginal likelihood or evidence.</p>
<p class="calibre6">The marginal likelihood is calculated as follows:</p>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∫&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;∙&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;Θ&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="img/121.png" class="calibre127"/></p>
<p class="calibre6">where the integral is taken over the entire space of <em class="italic">Θ</em>. The marginal likelihood is often used as a normalizing constant, ensuring that the posterior distribution integrates to <em class="italic">1</em>.</p>
<p class="calibre6">In Bayesian estimation, the choice of prior distribution is important, as it reflects our beliefs about the quantity of interest before collecting any data. The prior distribution can be chosen based on prior knowledge or previous studies. If no prior knowledge is available, a non-informative prior can be used, such as a uniform distribution.</p>
<p class="calibre6">Once the posterior <a id="_idIndexMarker097" class="calibre5 pcalibre1 pcalibre"/>distribution is calculated, it can be used to make predictions about the quantity of interest. As an example, the posterior distribution’s mean can serve as a point estimate, whereas the posterior distribution itself can be employed to establish credible intervals. These intervals represent the probable range within which the true value of the target quantity resides.</p>
<h1 id="_idParaDest-41" class="calibre4"><a id="_idTextAnchor042" class="calibre5 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre6">This chapter was about linear algebra and probability for ML, and it covers the fundamental mathematical concepts that are essential to understanding many machine learning algorithms. The chapter began with a review of linear algebra, covering topics such as matrix multiplication, determinants, eigenvectors, and eigenvalues. It then moved on to discuss probability theory, introducing the basic concepts of random variables and probability distributions. We also covered key concepts in statistical inference, such as maximum likelihood estimation and Bayesian inference.</p>
<p class="calibre6">In the next chapter, we will cover the fundamentals of machine learning for NLP, including topics such as data exploration, feature engineering, selection methods, and model training and validation.</p>
<h1 id="_idParaDest-42" class="calibre4"><a id="_idTextAnchor043" class="calibre5 pcalibre1 pcalibre"/>Further reading</h1>
<p class="calibre6">Please find the additional reading content as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Householder reflection matrix</strong>: A Householder reflection matrix, or Householder matrix, is a type of linear transformation utilized in numerical linear algebra due to its computational effectiveness and numerical stability. This matrix is used to perform reflections of a given vector about a plane or hyperplane, transforming the <a id="_idIndexMarker098" class="calibre5 pcalibre1 pcalibre"/>vector so that it only has non-<em class="italic">0</em> components in one specific dimension. The <strong class="bold">Householder matrix</strong> (<strong class="bold">H</strong>) is defined by</li>
</ul>
<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;H&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;I&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;u&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;u&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/122.png" class="calibre128"/></p>
<p class="calibre6">Here, <em class="italic">I</em> is the identity matrix, and <em class="italic">u</em> is a unit vector defining the reflection plane.</p>
<p class="calibre6">The main purpose of Householder transformations is to perform QR factorization and to reduce matrices to a tridiagonal or Hessenberg form. The properties of being symmetric and orthogonal make the Householder matrix computationally efficient and numerically stable.</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Diagonalizable</strong>: A matrix is said to be diagonalizable if it can be written in the form <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;D&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/123.png" class="calibre129"/><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;D&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;P&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/124.png" class="calibre130"/>, where <em class="italic">A</em> is the original matrix, <em class="italic">D</em> is a diagonal matrix, and <em class="italic">P</em> is a matrix for which the columns are the eigenvectors of <em class="italic">A</em>. Diagonalization simplifies many calculations in linear algebra, as computations with diagonal matrices are often more straightforward. For a matrix to be diagonalizable, it must have enough distinct eigenvectors to form a basis for its space, which is usually the case when all of its eigenvalues are distinct.</li>
<li class="calibre15"><strong class="bold">Invertible</strong>: An invertible matrix, also known as a non-singular matrix or a non-degenerate matrix, is a square matrix that has an inverse. If a matrix, <em class="italic">A</em>, is invertible, there exists another matrix, often denoted as <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="img/125.png" class="calibre131"/>, such that when they are multiplied together, they yield the identity matrix. In other words, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;A&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;I&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="img/126.png" class="calibre132"/>, where <em class="italic">I</em> is the identity matrix. The identity matrix is a special square matrix with <em class="italic">1</em>s on its main diagonal and <em class="italic">0</em>s everywhere else. The existence of an inverse heavily depends on the determinant of the matrix—a matrix is invertible if and only if its determinant is not 0. Invertible matrices are crucial in numerous areas of math, including solving systems of linear equations, matrix factorization, and many applications in engineering and physics.</li>
<li class="calibre15"><strong class="bold">Gaussian elimination method</strong>: Gaussian elimination is a fundamental algorithm in linear algebra for solving systems of linear equations. It accomplishes this by transforming the system to an equivalent one in which the equations are simpler to solve. This method uses a sequence of operations to modify the system of equations, with the objective of creating a row-echelon or reduced row-echelon form. Here’s a simplified step-by-step process of Gaussian elimination: First, swap the rows to move any rows with a leading coefficient (the first non-<em class="italic">0</em> number from the left, also called the pivot) so as to have <em class="italic">1</em> at the top. Then, multiply or divide any rows by a scalar to create a leading coefficient of <em class="italic">1</em> if not already present. Finally, add or subtract rows to create <em class="italic">0</em>s below and above the pivot. Once the matrix is in row-echelon form (all <em class="italic">0</em> rows are at the bottom, and each leading coefficient is to the right of the leading coefficient of the row above it), we can use back substitution to find the variables.  If we further simplify the matrix to a reduced row-echelon form (each leading coefficient is the only non-<em class="italic">0</em> entry in its column), the solutions can be read directly from the matrix. Gaussian elimination can also be used to find the rank of a matrix, calculate the determinant, and carry out matrix inversion if the system is square and has a unique solution.</li>
<li class="calibre15"><strong class="bold">Trace</strong>: The trace of a square matrix is the sum of its diagonal elements. It’s denoted as <em class="italic">Tr(A)</em> or <em class="italic">trace(A)</em>, where <em class="italic">A</em> is a square matrix. For example, if<p class="calibre6"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;A&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mtable&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;mml:mtr&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;mml:mtd&gt;&lt;mml:mi&gt;d&lt;/mml:mi&gt;&lt;/mml:mtd&gt;&lt;/mml:mtr&gt;&lt;/mml:mtable&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="img/38.png" class="calibre53"/></p><p class="calibre6"><em class="italic">Tr(A) = a + </em><em class="italic">d</em>.</p></li>
</ul>
<h1 id="_idParaDest-43" class="calibre4"><a id="_idTextAnchor044" class="calibre5 pcalibre1 pcalibre"/>References</h1>
<ul class="calibre14">
<li class="calibre15">Alter O, Brown PO, Botstein D. (2000) <em class="italic">Singular value decomposition for genome-wide expression data processing and modeling</em>. Proc Natl Acad Sci U S A, 97, 10101-6.</li>
<li class="calibre15">Golub, G.H., and Van Loan, C.F. (1989) <em class="italic">Matrix Computations</em>, 2nd ed. (Baltimore: Johns Hopkins University Press).</li>
<li class="calibre15">Greenberg, M.  (2001) <em class="italic">Differential equations &amp; Linear algebra</em> (Upper Saddle River, N.J. : Prentice Hall).</li>
<li class="calibre15">Strang, G.  (1998) <em class="italic">Introduction to linear algebra</em> (Wellesley, MA : Wellesley-Cambridge Press).</li>
<li class="calibre15">Lax, Peter D. <em class="italic">Linear algebra and its applications</em>. Vol. 78. John Wiley &amp; Sons, 2007.</li>
<li class="calibre15">Dangeti, Pratap. <em class="italic">Statistics for machine learning</em>. Packt Publishing Ltd, 2017.</li>
<li class="calibre15">DasGupta, Anirban. <em class="italic">Probability for statistics and machine learning: fundamentals and advanced topics</em>. New York: Springer, 2011.</li>
</ul>
</div>
</body></html>