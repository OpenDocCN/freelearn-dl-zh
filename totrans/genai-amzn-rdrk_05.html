<html><head></head><body>
<div id="_idContainer089">
<h1 class="chapter-number" id="_idParaDest-89"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-90"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.2.1">Harnessing the Power of RAG</span></h1>
<p><span class="koboSpan" id="kobo.3.1">By now, we know the FMs are trained using large datasets. </span><span class="koboSpan" id="kobo.3.2">However, the data used to train FMs might not be recent, and this can cause the models to hallucinate. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we will harness the power of RAG by augmenting the model with external data sources to overcome the challenge </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">of hallucination.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">We will explore the importance of RAG in generative AI scenarios, how RAG works, and its components. </span><span class="koboSpan" id="kobo.5.2">We will then delve into the integration of RAG with Amazon Bedrock, including a fully managed RAG experience by Amazon Bedrock called Knowledge Bases. </span><span class="koboSpan" id="kobo.5.3">The chapter will then take a hands-on approach to the implementation of Knowledge Bases and </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">using APIs.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will explore some real-world scenarios of RAG and discuss a few solution architectures for implementing RAG. </span><span class="koboSpan" id="kobo.7.2">You will also be introduced to implementing a RAG framework using Amazon Bedrock, LangChain orchestration, and other generative AI systems. </span><span class="koboSpan" id="kobo.7.3">We will end by examining current limitations and future research directions with Amazon Bedrock in the context </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">of RAG.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">By the end of this chapter, you will be able to understand the importance of RAG and will be able to implement it with Amazon Bedrock. </span><span class="koboSpan" id="kobo.9.2">Learning these methods will empower you to apply the concept of RAG in your own enterprise use cases and build production-level applications, such as conversational interfaces, question answering systems, or module </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">summarization workflows.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Here are the key topics that will be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">this chapter:</span></span></p>
<ul>
<li><a id="_idTextAnchor092"/><a id="_idTextAnchor093"/><span class="No-Break"><span class="koboSpan" id="kobo.13.1">Decoding RAG</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Implementing RAG with </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">Amazon Bedrock</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Implementing RAG with </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">other methods</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">RAG techniques</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Limitations and </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">future directions</span></span><a id="_idTextAnchor094"/></li>
</ul>
<h1 id="_idParaDest-91"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.22.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.23.1">This chapter requires you to have access to an AWS account. </span><span class="koboSpan" id="kobo.23.2">If you don’t have one already, you can go to </span><a href="https://aws.amazon.com/getting-started/"><span class="koboSpan" id="kobo.24.1">https://aws.amazon.com/getting-started/</span></a><span class="koboSpan" id="kobo.25.1"> and create an </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">AWS account.</span></span></p>
<p><span class="koboSpan" id="kobo.27.1">Secondly, you will need to install and configure AWS CLI (</span><a href="https://aws.amazon.com/cli/"><span class="koboSpan" id="kobo.28.1">https://aws.amazon.com/cli/</span></a><span class="koboSpan" id="kobo.29.1">) after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. </span><span class="koboSpan" id="kobo.29.2">Since the majority chunk of code cells we will be executing is based in Python, setting up an AWS Python SDK (Boto3) (</span><a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html"><span class="koboSpan" id="kobo.30.1">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</span></a><span class="koboSpan" id="kobo.31.1">) would be beneficial at this point. </span><span class="koboSpan" id="kobo.31.2">You can carry out the Python setup in these ways: install it on your local machine, or use AWS Cloud9, or AWS Lambda, or leverage </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">Amazon SageMaker.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.33.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.34.1">There will be a charge associated with the invocation and customization of FMs of Amazon Bedrock. </span><span class="koboSpan" id="kobo.34.2">Please refer to </span><a href="https://aws.amazon.com/bedrock/pricing/"><span class="koboSpan" id="kobo.35.1">https://aws.amazon.com/bedrock/pricing/</span></a><span class="koboSpan" id="kobo.36.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">learn more.</span></span></p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.38.1">Decoding RAG</span></h1>
<p><span class="koboSpan" id="kobo.39.1">RAG is an </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.40.1">approach in NLP that combines large-scale retrieval with neural generative models. </span><span class="koboSpan" id="kobo.40.2">The key idea is to retrieve relevant knowledge from large corpora and incorporate that knowledge into the text-generation process. </span><span class="koboSpan" id="kobo.40.3">This allows generative models such as Amazon Titan Text, Anthropic Claude, and </span><strong class="bold"><span class="koboSpan" id="kobo.41.1">Generative Pre-trained Transformer 3</span></strong><span class="koboSpan" id="kobo.42.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.43.1">GPT-3</span></strong><span class="koboSpan" id="kobo.44.1">) to produce more factual, specific, and </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.45.1">coherent text by grounding generations in </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">external knowledge.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">RAG has emerged as a promising technique to make neural generative models more knowledgeable and controllable. </span><span class="koboSpan" id="kobo.47.2">In this section, we will provide an overview of RAG, explain how it works, and discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">key applications.</span></span></p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.49.1">What is RAG?</span></h2>
<p><span class="koboSpan" id="kobo.50.1">Traditional </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.51.1">generative models, such as BART, T5 or GPT-4 are trained on vast amounts of text data in a self-supervised fashion. </span><span class="koboSpan" id="kobo.51.2">While this allows them to generate fluent and human-like text, a major limitation is that they lack world knowledge beyond what is contained in their training data. </span><span class="koboSpan" id="kobo.51.3">This can lead to factual inconsistencies, repetitions, and hallucinations in the </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">generated text.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">RAG aims to ground generations in knowledge by retrieving relevant context from large external corpora. </span><span class="koboSpan" id="kobo.53.2">For example, if the model is generating text about Paris, it could retrieve </span><em class="italic"><span class="koboSpan" id="kobo.54.1">Wikipedia</span></em><span class="koboSpan" id="kobo.55.1"> passages about Paris to inform the generation. </span><span class="koboSpan" id="kobo.55.2">This retrieved context is encoded and integrated into the model to guide the </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">text generation.</span></span></p>
<p><span class="koboSpan" id="kobo.57.1">Augmenting generative models with retrieved knowledge has been shown to produce more factual, specific, and</span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.58.1"> coherent text across a variety </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">of domains.</span></span></p>
<p><span class="koboSpan" id="kobo.60.1">The key components of RAG systems are </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.62.1">A GenAI </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.63.1">model – specifically, an FM or LLM – that</span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.64.1"> can generate fluent text (or </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">multi-modal) outputs.</span></span></li>
<li><span class="koboSpan" id="kobo.66.1">A corpus of data to retrieve relevant information from (for example, </span><em class="italic"><span class="koboSpan" id="kobo.67.1">Wikipedia</span></em><span class="koboSpan" id="kobo.68.1">, web </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">pages, documents).</span></span></li>
<li><span class="koboSpan" id="kobo.70.1">Retriever module, which encodes the input query and retrieves relevant passages from the knowledge corpus based on relevance to </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">the query.</span></span></li>
<li><span class="koboSpan" id="kobo.72.1">Re-ranker to select the optimal contextual information by re-scoring and ranking the retrieved passages based on relevance to the query. </span><span class="koboSpan" id="kobo.72.2">(This step is optional in building basic RAG systems but becomes crucial when building enterprise-scale systems with advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">RAG techniques).</span></span></li>
<li><span class="koboSpan" id="kobo.74.1">Fusion module to integrate retrieval into the language model. </span><span class="koboSpan" id="kobo.74.2">This can involve techniques such as concatenation or allowing the language model to condition on relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">external knowledge.</span></span></li>
<li><span class="koboSpan" id="kobo.76.1">Other components may also include query reformulation, hybrid search techniques, and </span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.77.1">multi-stage retrieval, which will be covered later in </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">this chapter.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.79.1">In order to gain a better understanding of RAG approaches, let us walk through a </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">simple example:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.81.1">Input query</span></strong><span class="koboSpan" id="kobo.82.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.83.1">What are </span></strong><strong class="source-inline"><a id="_idIndexMarker368"/></strong><strong class="source-inline"><span class="koboSpan" id="kobo.84.1">the key events in the life of </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.85.1">Marie Curie?</span></strong></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.86.1">Retriever module</span></strong><span class="koboSpan" id="kobo.87.1">: The retriever module encodes the query </span><strong class="source-inline"><span class="koboSpan" id="kobo.88.1">What are the key events in the life of Marie Curie?</span></strong><span class="koboSpan" id="kobo.89.1"> into a dense vector representation. </span><span class="koboSpan" id="kobo.89.2">It then searches through the knowledge corpus (for example, </span><em class="italic"><span class="koboSpan" id="kobo.90.1">Wikipedia</span></em><span class="koboSpan" id="kobo.91.1">, web pages) to find relevant passages. </span><span class="koboSpan" id="kobo.91.2">For example, it may retrieve </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">the following:</span></span><ol><li class="upper-roman"><strong class="source-inline"><span class="koboSpan" id="kobo.93.1">Marie Curie was a Polish physicist and chemist who conducted pioneering research </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.94.1">on radioactivity...</span></strong></span></li><li class="upper-roman"><strong class="source-inline"><span class="koboSpan" id="kobo.95.1">In 1903, Curie became the first woman to win a Nobel Prize for her study of </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.96.1">spontaneous radiation...</span></strong></span></li><li class="upper-roman"><strong class="source-inline"><span class="koboSpan" id="kobo.97.1">Curie won a second Nobel Prize in 1911, this time in chemistry, for her discovery of the elements radium </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.98.1">and polonium...</span></strong></span></li></ol></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.99.1">Re-ranker</span></strong><span class="koboSpan" id="kobo.100.1">: The re-ranker </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.101.1">scores and re-ranks the retrieved passages based on their relevance to the original query using cross-attention. </span><span class="koboSpan" id="kobo.101.2">It may determine that passages </span><em class="italic"><span class="koboSpan" id="kobo.102.1">II</span></em><span class="koboSpan" id="kobo.103.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.104.1">III</span></em><span class="koboSpan" id="kobo.105.1"> are more relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">than </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.107.1">I</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.109.1">Fusion module</span></strong><span class="koboSpan" id="kobo.110.1">: The top re-ranked passages (for example, </span><em class="italic"><span class="koboSpan" id="kobo.111.1">II</span></em><span class="koboSpan" id="kobo.112.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.113.1">III</span></em><span class="koboSpan" id="kobo.114.1">) are then integrated into the generative language model, either by concatenating them, summarizing them, or allowing the model to attend over them; that is, focus on different parts of the retrieved passages as needed while generating </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">the output.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.116.1">Note that the goal of the fusion step is to provide the generative language model with the most pertinent external knowledge in a manner that allows effective conditioning of the generated output on that knowledge, leading to more accurate, informative, and </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">grounded responses.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.118.1">Output</span></strong><span class="koboSpan" id="kobo.119.1">: The language model can then generate a fluent answer by conditioning on the relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">retrieved knowledge:</span></span><p class="list-inset"><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">The key events in the life of Marie </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.122.1">Curie include:</span></strong></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.123.1">In 1903, she became the first woman to win a Nobel Prize for her study of spontaneous </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.124.1">radiation (radioactivity).</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">In 1911, she won a second Nobel Prize in chemistry for her discovery of the elements radium </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.126.1">and polonium.</span></strong></span></li></ul></li>
<li><span class="koboSpan" id="kobo.127.1">By retrieving relevant knowledge from an external corpus and integrating it into the language </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.128.1">model, the RAG system can generate a more informative and accurate response, overcoming the limitations of relying solely on the model’s </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">training data.</span></span></li>
</ul>
<p class="callout-heading"><span class="koboSpan" id="kobo.130.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.131.1">Dense vector representations, also known as dense embeddings or dense vectors, are a way of encoding meaning and semantic relationships in a numerical format that can be effectively processed by machines. </span><span class="koboSpan" id="kobo.131.2">This allows techniques such as cosine similarity to identify semantically related words/texts even without exact keyword matches. </span><span class="koboSpan" id="kobo.131.3">Dense vectors power many modern NLP applications, such as semantic search, text generation, translation, and so on, by providing effective semantic </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.132.1">representations as inputs to </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">deep </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.134.1">learning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.135.1"> models.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">We will further dive </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.137.1">deep into these components in the </span><em class="italic"><span class="koboSpan" id="kobo.138.1">Components of RAG</span></em><span class="koboSpan" id="kobo.139.1"> section. </span><span class="koboSpan" id="kobo.139.2">Since you now have a brief understanding of RAG, it’s time to realize the importance of RAG in the context of the </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">GenAI universe.</span></span></p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.141.1">Importance of RAG</span></h2>
<p><span class="koboSpan" id="kobo.142.1">Before we dive into </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.143.1">how RAG works and its components, it’s important to understand why RAG is needed. </span><span class="koboSpan" id="kobo.143.2">As LLMs become more capable of generating fluent and coherent text, it also becomes more important to ground them in factual knowledge and guard against potential hallucinations. </span><span class="koboSpan" id="kobo.143.3">If you ask an LLM questions pertaining to recent events, you might notice the model to be hallucinating. </span><span class="koboSpan" id="kobo.143.4">With RAG, you can augment the latest knowledge as context to the model to improve content quality by reducing the chance of </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">factual errors.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">Another major advantage of RAG is overcoming the limited context length (input token limit) of the model. </span><span class="koboSpan" id="kobo.145.2">When providing pieces of text as a context that fits within the token limit of the model, you may not need to use RAG and leverage in-context prompting. </span><span class="koboSpan" id="kobo.145.3">However, if you want to provide a large corpus of documents as a context to the model, using RAG would be a better approach. </span><span class="koboSpan" id="kobo.145.4">However, RAG is beneficial even when the corpus can fit in the context due to needle-in-a-haystack problems, which can affect retrieval accuracy. </span><span class="koboSpan" id="kobo.145.5">To summarize, RAG becomes specifically useful in two primary </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">use cases:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.147.1">When the corpus size exceeds that of the </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">context length</span></span></li>
<li><span class="koboSpan" id="kobo.149.1">When we want to dynamically provide context to the model instead of feeding it the entire corpus </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">in context</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.151.1">RAG has many potential applications for improving GenAI. </span><span class="koboSpan" id="kobo.151.2">It can help build contextual chatbots that rely on real enterprise data. </span><span class="koboSpan" id="kobo.151.3">It can enable personalized search and recommendations based on user history and preferences. </span><span class="koboSpan" id="kobo.151.4">A RAG approach can also aid real-time summarization of large documents by retrieving and condensing key facts.  </span><span class="koboSpan" id="kobo.151.5">For example, applying RAG to summarize extensive legal texts or academic papers allows for the extraction and condensation of important information, providing succinct summaries that </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.152.1">capture the core points. </span><span class="koboSpan" id="kobo.152.2">Overall, RAG is an important technique for overcoming some limitations of current generative models and grounding them in factual knowledge. </span><span class="koboSpan" id="kobo.152.3">This helps make the generated content more useful, reliable, </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">and personalized.</span></span></p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.154.1">Key applications</span></h2>
<p><span class="koboSpan" id="kobo.155.1">Compared to other LLM customization techniques, such as prompt engineering or fine-tuning, RAG offers </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">several advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.157.1">Flexibility of knowledge source</span></strong><span class="koboSpan" id="kobo.158.1">: The knowledge base can be customized for each use case without changing the underlying LLM. </span><span class="koboSpan" id="kobo.158.2">Knowledge can be easily added, removed, or updated without costly model retraining. </span><span class="koboSpan" id="kobo.158.3">This is especially useful for organizations whose knowledge is </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">rapidly evolving.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.160.1">Cost-effective</span></strong><span class="koboSpan" id="kobo.161.1">: RAG allows a single-hosted LLM to be shared across many use cases through swappable knowledge sources. </span><span class="koboSpan" id="kobo.161.2">There is no need to train bespoke models for each use case, which means greater </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">cost efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.163.1">Natural language queries</span></strong><span class="koboSpan" id="kobo.164.1">: RAG relies on natural language for context retrieval from the knowledge source, unlike prompt engineering, which uses rigid prompt templates. </span><span class="koboSpan" id="kobo.164.2">This enables users to be more flexible when working with </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">the models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.166.1">For most organizations with a custom knowledge pool of information, RAG strikes a balance between cost, flexibility, and usability. </span><span class="koboSpan" id="kobo.166.2">Prompt engineering is sufficient for small amounts of context, while full model fine-tuning entails high training costs and rigid knowledge. </span><span class="koboSpan" id="kobo.166.3">RAG allows easy knowledge base updates and sharing of LLMs across </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.168.1">For example, RAG is well suited for </span><strong class="bold"><span class="koboSpan" id="kobo.169.1">business-to-business Software-as-a-Service</span></strong><span class="koboSpan" id="kobo.170.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.171.1">B2B SaaS</span></strong><span class="koboSpan" id="kobo.172.1">) companies </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.173.1">that manage evolving document bases across many customers. </span><span class="koboSpan" id="kobo.173.2">A single-hosted LLM can handle queries across clients by swapping their context documents, eliminating the need for </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">per-client models.</span></span></p>
<p><span class="koboSpan" id="kobo.175.1">Now that we understand the importance and potential applications of RAG in different scenarios, let us jump into exploring the working </span><span class="No-Break"><span class="koboSpan" id="kobo.176.1">of RAG.</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.177.1">How does RAG work?</span></h2>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.178.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.179.1">.1</span></em><span class="koboSpan" id="kobo.180.1"> provides</span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.181.1"> a high-level overview of how </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">RAG works:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.183.1"><img alt="Figure 5.1 – Simplified RAG" src="image/B22045_05_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.184.1">Figure 5.1 – Simplified RAG</span></p>
<p><span class="koboSpan" id="kobo.185.1">Let us now </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.186.1">understand these steps </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">in detail:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.188.1">Given a prompt from the user, the retriever module is invoked to encode the input query in a dense </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">vector representation.</span></span></li>
<li><span class="koboSpan" id="kobo.190.1">The retriever module then finds relevant context (passages or documents) from the knowledge corpus, based on maximum inner product similarity (semantic similarity) between the query vector and pre-computed dense vector representations of the </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">corpus contents.</span></span></li>
<li><span class="koboSpan" id="kobo.192.1">An optional re-ranker module can then re-score and re-rank the initially retrieved results and select the best context passages to augment the generation. </span><span class="koboSpan" id="kobo.192.2">The re-ranker helps surface the most </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">relevant passages.</span></span></li>
<li><span class="koboSpan" id="kobo.194.1">The top-ranked retrieved contexts are fused with the input query to form an augmented prompt (query </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">and context).</span></span></li>
<li><span class="koboSpan" id="kobo.196.1">The generative model, i.e. </span><span class="koboSpan" id="kobo.196.2">the FM or LLM then produces the output text conditioned on both the original query prompt and the retrieved relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">knowledge contexts.</span></span></li>
<li><span class="koboSpan" id="kobo.198.1">In some RAG systems, the retrieval and re-ranking process can be repeated during the generation step to dynamically retrieve more relevant knowledge as the output is </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">being generated.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.200.1">The key benefits of RAG are ensuring that the generated outputs are grounded in accurate and up-to-date information from trusted external sources, providing source citations for transparency, and reducing hallucinations or inaccuracies from the language model’s training </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">data alone.</span></span></p>
<p><span class="koboSpan" id="kobo.202.1">RAG systems meet</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.203.1"> enterprise requirements for GenAI, such as being comprehensive, trustworthy, transparent, and credible by properly sourcing, vetting, and customizing the underlying data sources and models for specific </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">use cases.</span></span></p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.205.1">Components of RAG</span></h2>
<p><span class="koboSpan" id="kobo.206.1">As explained earlier, once </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.207.1">a query is received, relevant context is retrieved from the knowledge source and condensed into a context document.  </span><span class="koboSpan" id="kobo.207.2">This context is then concatenated with the original query and fed into the LLM to generate a final response. </span><span class="koboSpan" id="kobo.207.3">The knowledge source acts as a dynamic long-term memory, in a way that can be frequently updated, while the LLM contributes its strong language </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">generation capabilities.</span></span></p>
<h3><span class="koboSpan" id="kobo.209.1">The knowledge base</span></h3>
<p><span class="koboSpan" id="kobo.210.1">A key component of RAG models is the knowledge base, which contains the external knowledge used for retrieval. </span><span class="koboSpan" id="kobo.210.2">The knowledge base stores information in a format optimized for fast retrieval, such as dense vectors </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">or indexes.</span></span></p>
<p><span class="koboSpan" id="kobo.212.1">Popular knowledge sources used in RAG include </span><em class="italic"><span class="koboSpan" id="kobo.213.1">Wikipedia</span></em><span class="koboSpan" id="kobo.214.1">, news archives, books, scientific papers, and proprietary knowledge bases created specifically for RAG models. </span><span class="koboSpan" id="kobo.214.2">The knowledge can consist of both structured (for example, tables and lists) and unstructured (for example, free </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">text) data.</span></span></p>
<p><span class="koboSpan" id="kobo.216.1">In a typical RAG scenario, the textual contents of the documents (or web pages) that make up the knowledge corpus and need to be converted into dense vector representations or embeddings are encoded data into smaller chunks. </span><span class="koboSpan" id="kobo.216.2">To preserve this structure of tables or lists while encoding, more advanced encoding techniques are used that can embed entire tables/lists as a single vector while retaining their </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">row/column relationships.</span></span></p>
<p><span class="koboSpan" id="kobo.218.1">Long unstructured text passages are typically chunked or split into smaller text segments or passages of a maximum length (for example, 200 tokens). </span><span class="koboSpan" id="kobo.218.2">Each of these chunks or passages is then encoded independently into a dense </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">vector representation.</span></span></p>
<p><span class="koboSpan" id="kobo.220.1">This embedding process typically happens asynchronously or as a batch process, separate from and ahead of time before any user queries are received by the </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">RAG system.</span></span></p>
<p><span class="koboSpan" id="kobo.222.1">The embeddings for the entire document corpus are pre-computed and stored before the system is deployed or used for any query answering. </span><span class="koboSpan" id="kobo.222.2">This pre-computation step is necessary for the </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">following reasons:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.224.1">The document corpus can typically be very large (for example, </span><em class="italic"><span class="koboSpan" id="kobo.225.1">Wikipedia</span></em><span class="koboSpan" id="kobo.226.1"> has millions </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">of articles)</span></span></li>
<li><span class="koboSpan" id="kobo.228.1">Embedding the full corpus at query time would be extremely slow </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">and inefficient</span></span></li>
<li><span class="koboSpan" id="kobo.230.1">Pre-computed embeddings allow fast maximum inner product search at </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">query time</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.232.1">By embedding the </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.233.1">sources asynchronously ahead of time, the RAG system can quickly retrieve relevant documents by comparing the query embedding against the pre-computed document embeddings using efficient vector </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.234.1">similarity search methods such as cosine similarity, Euclidean distance, </span><strong class="bold"><span class="koboSpan" id="kobo.235.1">Microprocessor without Interlocked Pipelined Stages</span></strong><span class="koboSpan" id="kobo.236.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.237.1">MIPS</span></strong><span class="koboSpan" id="kobo.238.1">), or </span><strong class="bold"><span class="koboSpan" id="kobo.239.1">Facebook AI Similarity Search</span></strong><span class="koboSpan" id="kobo.240.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.241.1">FAISS</span></strong><span class="koboSpan" id="kobo.242.1">). </span><span class="koboSpan" id="kobo.242.2">Readers </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.243.1">are encouraged to review the paper </span><em class="italic"><span class="koboSpan" id="kobo.244.1">A Survey on Efficient Processing of Similarity Queries over Neural Embeddings</span></em><span class="koboSpan" id="kobo.245.1"> (</span><a href="https://arxiv.org/abs/2204.07922"><span class="koboSpan" id="kobo.246.1">https://arxiv.org/abs/2204.07922</span></a><span class="koboSpan" id="kobo.247.1">), which debriefs methods on efficient processing of </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">similarity queries.</span></span></p>
<p><span class="koboSpan" id="kobo.249.1">Note that the sizes and scope of the knowledge base has a major influence on the capabilities of the RAG system. </span><span class="koboSpan" id="kobo.249.2">Larger knowledge bases with more diverse, high-quality knowledge provide more contextual information for the model to draw from, based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">user’s questions.</span></span></p>
<h3><span class="koboSpan" id="kobo.251.1">Retriever module</span></h3>
<p><span class="koboSpan" id="kobo.252.1">The retriever module is responsible for finding and retrieving the most relevant knowledge from the knowledge base for each specific context. </span><span class="koboSpan" id="kobo.252.2">The input to the retrieval model is typically the prompt or context from </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">the user.</span></span></p>
<p><span class="koboSpan" id="kobo.254.1">The embedding model encodes the prompt into a vector representation and matches it against encoded representations of the knowledge base to find the closest </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">matching entries.</span></span></p>
<p><span class="koboSpan" id="kobo.256.1">Common retrieval methods include sparse methods such as </span><strong class="bold"><span class="koboSpan" id="kobo.257.1">Term Frequency-Inverse Document Frequency</span></strong><span class="koboSpan" id="kobo.258.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.259.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.260.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.261.1">Best Match 25</span></strong><span class="koboSpan" id="kobo.262.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.263.1">BM25</span></strong><span class="koboSpan" id="kobo.264.1">), as well as dense </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.265.1">methods such as semantic</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.266.1"> search over embedded representations from a dual-encoder model. </span><span class="koboSpan" id="kobo.266.2">The retrieval model ranks the knowledge and returns the top </span><em class="italic"><span class="koboSpan" id="kobo.267.1">k</span></em><span class="koboSpan" id="kobo.268.1"> most relevant pieces back to the </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">generative model.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">The tighter the </span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.271.1">integration between the retrieval model and the generative model, the better the </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">retrieval results.</span></span></p>
<h3><span class="koboSpan" id="kobo.273.1">Conditioning the generative model</span></h3>
<p><span class="koboSpan" id="kobo.274.1">The key aspect </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.275.1">that makes the RAG process generative is the conditional generative model. </span><span class="koboSpan" id="kobo.275.2">This model takes the retrieved knowledge along with the original prompt and generates the </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">output text.</span></span></p>
<p><span class="koboSpan" id="kobo.277.1">The knowledge can be provided in different ways to condition </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">the generation:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.279.1">Concatenating the retrieved text to </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">the prompt</span></span></li>
<li><span class="koboSpan" id="kobo.281.1">Encoding the retrieved text into </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">dense vectors</span></span></li>
<li><span class="koboSpan" id="kobo.283.1">Inserting the retrieved text into the input at </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">particular positions</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.285.1">For example, in a typical scenario, the retrieved knowledge is augmented with the input prompt and fed to the LLM to provide a succinct response to the end user. </span><span class="koboSpan" id="kobo.285.2">This allows the LLM to directly condition the text generation on the relevant facts and context. </span><span class="koboSpan" id="kobo.285.3">Users are encouraged to check out the paper </span><em class="italic"><span class="koboSpan" id="kobo.286.1">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</span></em><span class="koboSpan" id="kobo.287.1"> (</span><a href="https://arxiv.org/pdf/2007.01282.pdf"><span class="koboSpan" id="kobo.288.1">https://arxiv.org/pdf/2007.01282.pdf</span></a><span class="koboSpan" id="kobo.289.1">) in order to gain a deeper understanding of the complexity of RAG in the realm of question </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">answering frameworks.</span></span></p>
<p><span class="koboSpan" id="kobo.291.1">The generative model is usually a large pre-trained language model such as GPT-4, Anthropic Claude 3, Amazon Titan Text G1, and so on. </span><span class="koboSpan" id="kobo.291.2">The model can be further fine-tuned end to end on downstream RAG tasks, if needed, in order to optimize the integration of the retrieved knowledge for domain-specific use cases. </span><span class="koboSpan" id="kobo.291.3">Now, let us dive into exploring </span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.292.1">RAG with </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">Amazon </span></span><span class="No-Break"><a id="_idIndexMarker388"/></span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">Bedrock.</span></span></p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.295.1">Implementing RAG with Amazon Bedrock</span></h1>
<p><span class="koboSpan" id="kobo.296.1">Prior to responding </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.297.1">to user queries, the system must ingest and index the provided documents. </span><span class="koboSpan" id="kobo.297.2">This process can be considered as </span><em class="italic"><span class="koboSpan" id="kobo.298.1">step 0</span></em><span class="koboSpan" id="kobo.299.1">, and consists of </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">these sub-steps:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.301.1">Ingest the raw text documents into the </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">knowledge base.</span></span></li>
<li><span class="koboSpan" id="kobo.303.1">Preprocess the documents by splitting them into smaller chunks to enable more </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">granular retrieval.</span></span></li>
<li><span class="koboSpan" id="kobo.305.1">Generate dense vector representations for each passage using an embedding model such as Amazon Bedrock’s Titan Text Embeddings model. </span><span class="koboSpan" id="kobo.305.2">This encodes the semantic meaning of each passage into a high-dimensional </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">vector space.</span></span></li>
<li><span class="koboSpan" id="kobo.307.1">Index the passages and their corresponding vector embeddings into a specialized search index optimized for</span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.308.1"> efficient </span><strong class="bold"><span class="koboSpan" id="kobo.309.1">nearest neighbor</span></strong><span class="koboSpan" id="kobo.310.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.311.1">NN</span></strong><span class="koboSpan" id="kobo.312.1">) search. </span><span class="koboSpan" id="kobo.312.2">These are also referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">vector databases</span></strong><span class="koboSpan" id="kobo.314.1">, which</span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.315.1"> store numerical representations of text in the form of vectors. </span><span class="koboSpan" id="kobo.315.2">This index powers fast retrieval of the most relevant passages in response to </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">user queries.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.317.1">By completing this workflow, the system constructs an indexed corpus ready to serve relevant results for natural language queries over the ingested document collection. </span><span class="koboSpan" id="kobo.317.2">The passage splitting, embedding, and indexing steps enable robust ranking and </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">retrieval capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">The flow diagram depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.320.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.321.1">.2</span></em><span class="koboSpan" id="kobo.322.1"> exemplifies the overall flow of the RAG process as </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">described previously:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<span class="koboSpan" id="kobo.324.1"><img alt="Figure 5.2 – RAG with Amazon Bedrock" src="image/B22045_05_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.325.1">Figure 5.2 – RAG with Amazon Bedrock</span></p>
<p><span class="koboSpan" id="kobo.326.1">When documents have been properly indexed, the system can provide contextual answers to natural language</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.327.1"> questions through the </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">following pipeline:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.329.1">Step 1</span></strong><span class="koboSpan" id="kobo.330.1">: Encode the input question into a dense vector representation (embedding) using an embedding model, such as the Amazon Titan Text Embeddings model or Cohere’s embedding model, both of which can be accessed via Amazon Bedrock. </span><span class="koboSpan" id="kobo.330.2">This captures the semantic meaning of </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">the question.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.332.1">Step 2</span></strong><span class="koboSpan" id="kobo.333.1">: Compare the question embedding to indexed document embeddings using cosine similarity or other distance metrics. </span><span class="koboSpan" id="kobo.333.2">This retrieves the most relevant document chunks. </span><span class="koboSpan" id="kobo.333.3">Append the top-ranking document chunks to the prompt as contextual information. </span><span class="koboSpan" id="kobo.333.4">This provides relevant background knowledge for </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">the model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.335.1">Step 3</span></strong><span class="koboSpan" id="kobo.336.1">: Pass the prompt with context to an LLM available on Amazon Bedrock such as Anthropic Claude 3, Meta Llama 3, or Amazon Titan Text G1 - Express. </span><span class="koboSpan" id="kobo.336.2">This leverages the model’s capabilities to generate an answer conditioned on the </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">retrieved documentation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.338.1">Finally, return the model-generated answer, which should show an understanding of the question in relation to the </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">contextual documents.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">The system thus leverages Amazon Bedrock FMs to provide natural language question answering grounded in relevant documentation and context. </span><span class="koboSpan" id="kobo.340.2">Careful indexing and encoding of documents enable seamless integration of retrieval with generative models for more informed and </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">accurate answers.</span></span></p>
<p><span class="koboSpan" id="kobo.342.1">Here is an example of RAG implementation with Amazon Bedrock and Amazon OpenSearch Serverless as a vector </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">engine: </span></span><a href="https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/"><span class="No-Break"><span class="koboSpan" id="kobo.344.1">https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.345.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">Now that we have discussed some details around implementing RAG with Amazon Bedrock, let us </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.347.1">dive deep into tackling use cases using RAG through Knowledge Bases on </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">Amazon Bedrock.</span></span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.349.1">Amazon Bedrock Knowledge Bases</span></h2>
<p><span class="koboSpan" id="kobo.350.1">Amazon Bedrock</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.351.1"> provides a</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.352.1"> fully managed RAG experience with Knowledge Bases, handling the complexity behind the scenes while giving you control over your data. </span><span class="koboSpan" id="kobo.352.2">Bedrock’s Knowledge Base capability enables the aggregation of diverse data sources into a centralized repository of machine-readable information. </span><span class="koboSpan" id="kobo.352.3">Knowledge Bases automate the creation of vector embeddings from your data, store them in a managed vector index, and handle embedding, querying, source attribution, and short-term memory for </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">production RAG.</span></span></p>
<p><span class="koboSpan" id="kobo.354.1">The key benefits of Knowledge Bases in Amazon Bedrock include </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.356.1">Seamless RAG workflow</span></strong><span class="koboSpan" id="kobo.357.1">: There’s no need to set up and manage the components yourself. </span><span class="koboSpan" id="kobo.357.2">You can just provide your data and let Amazon Bedrock handle ingestion, embedding, storage, </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">and querying.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.359.1">Custom vector embeddings</span></strong><span class="koboSpan" id="kobo.360.1">: Your data is ingested and converted into vector representations tailored to your use case with a choice of </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">embedding models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.362.1">Attribution and memory</span></strong><span class="koboSpan" id="kobo.363.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">RetrieveAndGenerate</span></strong><span class="koboSpan" id="kobo.365.1"> API within Amazon Bedrock provides attribution back to source documents and manages conversation history for </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">contextual responses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.367.1">Flexible integration</span></strong><span class="koboSpan" id="kobo.368.1">: Incorporate RAG into your workflows with API access and </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.369.1">integration</span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.370.1"> support for other </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">GenAI tools.</span></span></li>
</ul>
<h2 id="_idParaDest-100"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.372.1">Amazon Bedrock Knowledge Base setup</span></h2>
<p><span class="koboSpan" id="kobo.373.1">Objectively</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.374.1"> speaking, the </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.375.1">following steps facilitate Knowledge Base creation </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">and integration:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.377.1">Identify and prepare data sources </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">for ingestion</span></span></li>
<li><span class="koboSpan" id="kobo.379.1">Upload data </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.380.1">to </span><strong class="bold"><span class="koboSpan" id="kobo.381.1">Amazon Simple Storage Service</span></strong><span class="koboSpan" id="kobo.382.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.383.1">Amazon S3</span></strong><span class="koboSpan" id="kobo.384.1">) for </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">centralized access</span></span></li>
<li><span class="koboSpan" id="kobo.386.1">Generate embeddings for data via FMs and persist in a </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">vector store</span></span></li>
<li><span class="koboSpan" id="kobo.388.1">Connect applications and agents to query and incorporate Knowledge Base </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">into workflows</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.390.1">To create ingestion jobs, follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">next steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.392.1">Set up your Knowledge Base</span></strong><span class="koboSpan" id="kobo.393.1">: Before you can ingest data, you need to create a knowledge base. </span><span class="koboSpan" id="kobo.393.2">This involves defining the structure and schema of the knowledge base to ensure it can store and manage the </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">data effectively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.395.1">Prepare your </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.396.1">data source</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">:</span></span><ul><li><span class="koboSpan" id="kobo.398.1">Ensure your data is stored in Amazon S3. </span><span class="koboSpan" id="kobo.398.2">The data can be in various formats, including structured (for example, CSV, JSON) and unstructured (for example, text </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">files, PDFs).</span></span></li><li><span class="koboSpan" id="kobo.400.1">Organize your data in a way that makes it easy to manage </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">and retrieve.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.402.1">Create an </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.403.1">ingestion job</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">:</span></span><ul><li><span class="koboSpan" id="kobo.405.1">Navigate to the AWS Bedrock console and go to the </span><strong class="bold"><span class="koboSpan" id="kobo.406.1">Knowledge </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.407.1">base</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.408.1"> section.</span></span></li><li><span class="koboSpan" id="kobo.409.1">Select the option to create a new </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">ingestion job.</span></span></li><li><span class="koboSpan" id="kobo.411.1">Provide the necessary details, such as the name of the job, the S3 bucket location, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">data format.</span></span></li><li><span class="koboSpan" id="kobo.413.1">Configure the job to specify how the data should be processed and ingested into the </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">knowledge base.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.415.1">Configure </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.416.1">sync settings</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">:</span></span><ul><li><span class="koboSpan" id="kobo.418.1">Set up the sync settings to ensure the knowledge base is updated with the most </span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.419.1">recent </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.420.1">data from your </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">S3 location.</span></span></li><li><span class="koboSpan" id="kobo.422.1">You can configure the sync to run at regular intervals (for example, daily or weekly) or trigger it manually </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">as needed.</span></span></li><li><span class="koboSpan" id="kobo.424.1">Ensure that the sync settings are optimized to handle large volumes of </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">data efficiently.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.426.1">Run the </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.427.1">ingestion job</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">:</span></span><ul><li><span class="koboSpan" id="kobo.429.1">Once the job is configured, you can start the </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">ingestion process.</span></span></li><li><span class="koboSpan" id="kobo.431.1">Monitor the job’s progress through the AWS Bedrock console. </span><span class="koboSpan" id="kobo.431.2">You can view logs and status updates to ensure the job is </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">running smoothly.</span></span></li></ul></li>
</ol>
<p><span class="koboSpan" id="kobo.433.1">Now that we have a basic understanding of the ingestion process, let us walk through these </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">details thoroughly.</span></span></p>
<p><span class="koboSpan" id="kobo.435.1">In order to initiate this pipeline within the AWS console, one can navigate to the </span><strong class="bold"><span class="koboSpan" id="kobo.436.1">Orchestration</span></strong><span class="koboSpan" id="kobo.437.1"> section within the Amazon Bedrock page, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.438.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.439.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.441.1"><img alt="Figure 5.3 – Knowledge base" src="image/B22045_05_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.442.1">Figure 5.3 – Knowledge base</span></p>
<p><span class="koboSpan" id="kobo.443.1">Now, let’s</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.444.1"> look at these</span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.445.1"> steps in </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">greater depth:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.447.1">Click on </span><strong class="bold"><span class="koboSpan" id="kobo.448.1">Knowledge base</span></strong><span class="koboSpan" id="kobo.449.1"> and enter the details pertaining to the knowledge base you intend to create. </span><span class="koboSpan" id="kobo.449.2">You can provide a custom knowledge base name, description, and the </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.450.1">respective </span><strong class="bold"><span class="koboSpan" id="kobo.451.1">Identity and Access Management</span></strong><span class="koboSpan" id="kobo.452.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.453.1">IAM</span></strong><span class="koboSpan" id="kobo.454.1">) permissions for creating either a new service role or leveraging an existing service role for the knowledge base. </span><span class="koboSpan" id="kobo.454.2">You can also provide tags to this resource for easy searching and filtering of your resource or tracking AWS costs associated with the service in the knowledge base </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">details section.</span></span></li>
<li><span class="koboSpan" id="kobo.456.1">In the next step, you will set up the data source by specifying the S3 location where the data to be indexed resides. </span><span class="koboSpan" id="kobo.456.2">You can specify a particular data source name (or leverage the default pre-filled name) and provide the S3 URI (Uniform Resource Identifier) of the bucket containing the source data, as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.457.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.458.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.460.1"><img alt="Figure 5.4 – Knowledge base: Set up data source" src="image/B22045_05_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.461.1">Figure 5.4 – Knowledge base: Set up data source</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.462.1">You can also </span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.463.1">provide a</span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.464.1"> customer-managed </span><strong class="bold"><span class="koboSpan" id="kobo.465.1">Key Management Service</span></strong><span class="koboSpan" id="kobo.466.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.467.1">KMS</span></strong><span class="koboSpan" id="kobo.468.1">) key </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.469.1">you used for encrypting your S3 data, in order to allow the Bedrock service to decrypt it when ingesting the given data into the vector database. </span><span class="koboSpan" id="kobo.469.2">Under </span><strong class="bold"><span class="koboSpan" id="kobo.470.1">Advanced settings</span></strong><span class="koboSpan" id="kobo.471.1"> (as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.472.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.473.1">.5</span></em><span class="koboSpan" id="kobo.474.1">), users have the option to choose the default KMS key or customize encryption settings by choosing a different key of their choice by entering the </span><strong class="bold"><span class="koboSpan" id="kobo.475.1">Amazon Resource Name</span></strong><span class="koboSpan" id="kobo.476.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.477.1">ARN</span></strong><span class="koboSpan" id="kobo.478.1">) or </span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.479.1">searching for their stored customized key (or creating a new AWS KMS key on the fly). </span><span class="koboSpan" id="kobo.479.2">Providing a customer-managed KMS key for encrypting S3 data sources ingested by Amazon Bedrock is desired for enhanced data security, compliance, and control. </span><span class="koboSpan" id="kobo.479.3">It allows data sovereignty, key rotation/revocation, </span><strong class="bold"><span class="koboSpan" id="kobo.480.1">separation of duties</span></strong><span class="koboSpan" id="kobo.481.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.482.1">SoD</span></strong><span class="koboSpan" id="kobo.483.1">), auditing/logging </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.484.1">capabilities, and integration with existing key management infrastructure. </span><span class="koboSpan" id="kobo.484.2">By managing your own encryption keys, you gain greater control over data protection, meeting regulatory requirements and aligning with organizational security policies for sensitive or </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">regulated data:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.486.1"><img alt="Figure 5.5 – Knowledge base: Advanced settings" src="image/B22045_05_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.487.1">Figure 5.5 – Knowledge base: Advanced settings</span></p>
<p><span class="koboSpan" id="kobo.488.1">Under </span><strong class="bold"><span class="koboSpan" id="kobo.489.1">Chunking strategy</span></strong><span class="koboSpan" id="kobo.490.1"> (as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.491.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.492.1">.6</span></em><span class="koboSpan" id="kobo.493.1">), users have the option to select how to break down text in the source location into smaller segments before creating the embedding. </span><span class="koboSpan" id="kobo.493.2">By default, the knowledge base will automatically split your data into tiny chunks each containing, at most, 300 tokens. </span><span class="koboSpan" id="kobo.493.3">If a document or, in other words, source data contains</span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.494.1"> fewer than 300 </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.495.1">tokens, it is not split any further in </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">that case:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<span class="koboSpan" id="kobo.497.1"><img alt="Figure 5.6 – Knowledge base: Chunking strategy" src="image/B22045_05_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.498.1">Figure 5.6 – Knowledge base: Chunking strategy</span></p>
<p><span class="koboSpan" id="kobo.499.1">Alternatively, you have the option to customize the chunk size using </span><strong class="bold"><span class="koboSpan" id="kobo.500.1">Fixed size chunking</span></strong><span class="koboSpan" id="kobo.501.1"> or simply opt for </span><strong class="bold"><span class="koboSpan" id="kobo.502.1">No chunking</span></strong><span class="koboSpan" id="kobo.503.1"> in case you have already preprocessed your source documents into separate files of smaller chunks and don’t intend to chunk your documents any further </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">using Bedrock.</span></span></p>
<p><span class="koboSpan" id="kobo.505.1">In the next stage, users will select an embedding model to convert their selected data into an embedding. </span><span class="koboSpan" id="kobo.505.2">Currently, there are four embeddings models that are supported, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.506.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.507.1">.7</span></em><span class="koboSpan" id="kobo.508.1">. </span><span class="koboSpan" id="kobo.508.2">Further, under </span><strong class="bold"><span class="koboSpan" id="kobo.509.1">Vector database</span></strong><span class="koboSpan" id="kobo.510.1">, users have the option to go with the recommended route – that is, select the quick create option, which will create an Amazon OpenSearch Serverless vector store in the background automatically in the respective account of </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">their choice:</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.512.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.513.1">Vector embeddings are numeric representations of text data that encode semantic or contextual meaning. </span><span class="koboSpan" id="kobo.513.2">In NLP pipelines, text documents are passed through an embedding model to convert the chunks, including discrete tokens such as words into dense vectors in a continuous vector space. </span><span class="koboSpan" id="kobo.513.3">Good vector representations allow </span><strong class="bold"><span class="koboSpan" id="kobo.514.1">machine learning</span></strong><span class="koboSpan" id="kobo.515.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.516.1">ML</span></strong><span class="koboSpan" id="kobo.517.1">) models to </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.518.1">understand similarities, analogies, and other patterns between words and concepts. </span><span class="koboSpan" id="kobo.518.2">In other words, if the vector representations (embeddings) are trained well on a large dataset, they will capture meaningful relationships in the data. </span><span class="koboSpan" id="kobo.518.3">This allows ML models that use those embeddings to recognize things such as </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">the following:</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.520.1">- Which words are similar in meaning (for example, </span><em class="italic"><span class="koboSpan" id="kobo.521.1">king</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.522.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.523.1">queen</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">)</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.525.1">- Which concepts follow an analogical pattern (for example, </span><em class="italic"><span class="koboSpan" id="kobo.526.1">man</span></em><span class="koboSpan" id="kobo.527.1"> is to </span><em class="italic"><span class="koboSpan" id="kobo.528.1">king</span></em><span class="koboSpan" id="kobo.529.1"> as </span><em class="italic"><span class="koboSpan" id="kobo.530.1">woman</span></em><span class="koboSpan" id="kobo.531.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">to </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.533.1">queen</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">)</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.535.1">- Other patterns in how the concepts are represented in the </span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">embedding space</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.537.1">The well-trained embeddings essentially provide the ML models with a numeric map of the relationships and patterns inherent in the data. </span><span class="koboSpan" id="kobo.537.2">This makes it easier for the models to then learn and make inferences about those patterns during training on </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">downstream tasks.</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.539.1">Hence, simply put, good embeddings help ML models understand similarities and relationships between words and concepts rather than just treating them as isolated </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">data points.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<span class="koboSpan" id="kobo.541.1"><img alt="Figure 5.7 – Knowledge base: Configure vector store" src="image/B22045_05_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.542.1">Figure 5.7 – Knowledge base: Configure vector store</span></p>
<p><span class="koboSpan" id="kobo.543.1">Alternatively, you</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.544.1"> have the </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.545.1">option to choose your own vector store (as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.546.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.547.1">.8</span></em><span class="koboSpan" id="kobo.548.1">). </span><span class="koboSpan" id="kobo.548.2">At the time of writing this book, you have the option to select </span><strong class="bold"><span class="koboSpan" id="kobo.549.1">Vector engine for Amazon OpenSearch Serverless</span></strong><span class="koboSpan" id="kobo.550.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.551.1">Amazon Aurora</span></strong><span class="koboSpan" id="kobo.552.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.553.1">MongoDB Atlas</span></strong><span class="koboSpan" id="kobo.554.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.555.1">Pinecone</span></strong><span class="koboSpan" id="kobo.556.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.557.1">Redis Enterprise Cloud</span></strong><span class="koboSpan" id="kobo.558.1">. </span><span class="koboSpan" id="kobo.558.2">Once selected, you can provide the field mapping to proceed with the knowledge base creation final setup. </span><span class="koboSpan" id="kobo.558.3">Depending on the use case, developers or teams may opt for one vector database over another. </span><span class="koboSpan" id="kobo.558.4">You can read more about the role of vector datastores in GenAI applications </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">at </span></span><a href="https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/"><span class="No-Break"><span class="koboSpan" id="kobo.560.1">https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.561.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<span class="koboSpan" id="kobo.562.1"><img alt="Figure 5.8 – Knowledge base: Vector database" src="image/B22045_05_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.563.1">Figure 5.8 – Knowledge base: Vector database</span></p>
<p><span class="koboSpan" id="kobo.564.1">You can </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.565.1">check </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.566.1">out </span><a href="https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/"><span class="koboSpan" id="kobo.567.1">https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/</span></a><span class="koboSpan" id="kobo.568.1"> to learn more about how you can set up your own vector store with Pinecone, OpenSearch Serverless, </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">or Redis.</span></span></p>
<p><span class="koboSpan" id="kobo.570.1">Assuming that you opt for the default route, involving the creation of a new Amazon OpenSearch Serverless vector store, you can proceed and click on </span><strong class="bold"><span class="koboSpan" id="kobo.571.1">Create knowledge base</span></strong><span class="koboSpan" id="kobo.572.1"> post reviewing all the provided details as depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.573.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.574.1">.9</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.576.1"><img alt="Figure 5.9 – Knowledge base: Review and create" src="image/B22045_05_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.577.1">Figure 5.9 – Knowledge base: Review and create</span></p>
<p><span class="koboSpan" id="kobo.578.1">Once created, you</span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.579.1"> can sync </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.580.1">the information to ensure the knowledge base is ingesting and operating on the most recent data stored in your Amazon S3 location. </span><span class="koboSpan" id="kobo.580.2">After syncing is completed for the knowledge base, users can test said knowledge base by selecting the appropriate model suitable for their use case by clicking on </span><strong class="bold"><span class="koboSpan" id="kobo.581.1">Select Model</span></strong><span class="koboSpan" id="kobo.582.1">, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.583.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.584.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.586.1"><img alt="Figure 5.10 – Test knowledge base: Select Model" src="image/B22045_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.587.1">Figure 5.10 – Test knowledge base: Select Model</span></p>
<p><span class="koboSpan" id="kobo.588.1">Once the appropriate model has been selected, you can test it by entering a particular query in the </span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.589.1">textbox and </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.590.1">receiving a particular response generated by the model, depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.591.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.592.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.594.1"><img alt="Figure 5.11 – Test Knowledge base" src="image/B22045_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.595.1">Figure 5.11 – Test Knowledge base</span></p>
<p><span class="koboSpan" id="kobo.596.1">At its core, Amazon Bedrock transforms the user’s query into vector representations of meaning; that is, embeddings. </span><span class="koboSpan" id="kobo.596.2">It then searches the knowledge base for relevant information using these embeddings as the search criteria. </span><span class="koboSpan" id="kobo.596.3">Any knowledge retrieved is combined with the prompt engineered for the FM, providing essential context. </span><span class="koboSpan" id="kobo.596.4">The FM integrates this contextual knowledge into its response generation to answer the user’s question. </span><span class="koboSpan" id="kobo.596.5">For conversations spanning multiple turns, Amazon Bedrock leverages its knowledge base to maintain conversation context and history, delivering increasingly </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">relevant results.</span></span></p>
<p><span class="koboSpan" id="kobo.598.1">Additional information for testing the knowledge base and inspecting source chunks can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">at </span></span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html"><span class="No-Break"><span class="koboSpan" id="kobo.600.1">https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.601.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.602.1">To ensure your knowledge base is always up to date, it is essential to automate the syncing process. </span><span class="koboSpan" id="kobo.602.2">This can be achieved by using AWS Lambda functions or AWS Step Functions to trigger ingestion jobs based on specific events </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">or schedules.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.604.1">AWS Lambda</span></strong><span class="koboSpan" id="kobo.605.1"> is a serverless</span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.606.1"> compute service that allows you to run code without provisioning or managing servers. </span><span class="koboSpan" id="kobo.606.2">You can create Lambda functions to automate tasks such as triggering data ingestion jobs, processing data, or sending notifications. </span><span class="koboSpan" id="kobo.606.3">Lambda functions can be triggered by various events, including file uploads to Amazon S3, changes to DynamoDB tables, or scheduled events using Amazon </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">CloudWatch Events.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.608.1">AWS Step Functions</span></strong><span class="koboSpan" id="kobo.609.1"> is a </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.610.1">serverless function orchestrator that allows you to coordinate multiple AWS services into business workflows. </span><span class="koboSpan" id="kobo.610.2">You can create state machines that define a series of steps, including Lambda functions, data processing tasks, and </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.611.1">error-handling </span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.612.1">logic. </span><span class="koboSpan" id="kobo.612.2">Step Functions can be particularly useful for orchestrating complex data ingestion pipelines or </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">ML workflows.</span></span></p>
<p><span class="koboSpan" id="kobo.614.1">Regularly monitoring and managing data sources is crucial to maintain their relevance and accuracy. </span><strong class="bold"><span class="koboSpan" id="kobo.615.1">Amazon CloudWatch</span></strong><span class="koboSpan" id="kobo.616.1"> is a </span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.617.1">monitoring and observability service that provides data and actionable insights across your AWS resources. </span><span class="koboSpan" id="kobo.617.2">You can utilize CloudWatch to set up alarms and notifications for any issues or anomalies in the data syncing process. </span><span class="koboSpan" id="kobo.617.3">CloudWatch can monitor metrics such as Lambda function invocations, Step Functions executions, and Amazon S3 bucket activity, allowing you to proactively identify and address </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">potential issues.</span></span></p>
<p><span class="koboSpan" id="kobo.619.1">Adhering to best practices for data management, such as organizing data logically, maintaining data quality, and ensuring data security, is vital. </span><span class="koboSpan" id="kobo.619.2">AWS provides various services and tools to support data management </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">best practices:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.621.1">You can organize your data in Amazon S3 buckets and leverage features such as versioning, lifecycle policies, and access controls to maintain data quality </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">and security.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.623.1">AWS Glue</span></strong><span class="koboSpan" id="kobo.624.1"> is a fully </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.625.1">managed </span><strong class="bold"><span class="koboSpan" id="kobo.626.1">extract, transform, and load</span></strong><span class="koboSpan" id="kobo.627.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.628.1">ETL</span></strong><span class="koboSpan" id="kobo.629.1">) service that can help you </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.630.1">prepare and move data reliably between different data stores. </span><span class="koboSpan" id="kobo.630.2">Glue can be used to clean, transform, and enrich your data before ingesting it into your </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">knowledge base.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.632.1">AWS Lake Formation</span></strong><span class="koboSpan" id="kobo.633.1"> is a service </span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.634.1">that helps you build, secure, and manage data lakes on Amazon S3. </span><span class="koboSpan" id="kobo.634.2">It provides features such as data cataloging, access control, and auditing, which can help ensure data security </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">and governance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.636.1">Regular reviews and updates of the knowledge base should be conducted to remove outdated information and incorporate new, relevant data. </span><span class="koboSpan" id="kobo.636.2">AWS provides services such as </span><strong class="bold"><span class="koboSpan" id="kobo.637.1">Amazon Kendra</span></strong><span class="koboSpan" id="kobo.638.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.639.1">Amazon Comprehend</span></strong><span class="koboSpan" id="kobo.640.1"> that</span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.641.1"> can help you analyze and understand </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.642.1">your knowledge base content, identify outdated or irrelevant information, and suggest updates </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">or improvements.</span></span></p>
<p><span class="koboSpan" id="kobo.644.1">Tracking actionable metrics, such as search success rate, user engagement, and data freshness, is also </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.645.1">important. </span><span class="koboSpan" id="kobo.645.2">These</span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.646.1"> metrics can help continuously improve the knowledge base, ensuring it meets the needs of its </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">users effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.648.1">Amazon CloudWatch can be used to collect and analyze metrics from various AWS services, including your knowledge base application. </span><span class="koboSpan" id="kobo.648.2">You can create custom metrics, dashboards, and alarms to monitor the performance and usage of your </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">knowledge base.</span></span></p>
<p><span class="koboSpan" id="kobo.650.1">By leveraging AWS services such as Lambda, Step Functions, CloudWatch, S3, Glue, Lake Formation, Kendra, and Comprehend, you can automate the syncing process, monitor and manage data sources, adhere to data management best practices, and track actionable metrics to ensure your knowledge base remains up to date, relevant, and effective in meeting the needs of </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">your users.</span></span></p>
<p><span class="koboSpan" id="kobo.652.1">Readers are encouraged to visit the Amazon Bedrock RAG GitHub repository (</span><a href="https://github.com/aws-samples/amazon-bedrock-rag"><span class="koboSpan" id="kobo.653.1">https://github.com/aws-samples/amazon-bedrock-rag</span></a><span class="koboSpan" id="kobo.654.1">) to explore and implement a fully managed RAG solution using Knowledge Bases for </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">Amazon Bedrock.</span></span></p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.656.1">API calls</span></h2>
<p><span class="koboSpan" id="kobo.657.1">For users who </span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.658.1">wish </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.659.1">to invoke Bedrock outside of the console, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.660.1">RetrieveAndGenerate</span></strong><span class="koboSpan" id="kobo.661.1"> API provides programmatic access to execute this same workflow. </span><span class="koboSpan" id="kobo.661.2">This allows Bedrock’s capabilities to be tightly integrated into custom applications via API calls rather than console interaction. </span><span class="koboSpan" id="kobo.661.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.662.1">RetrieveAndGenerate</span></strong><span class="koboSpan" id="kobo.663.1"> API gives developers the flexibility to build Amazon Bedrock-powered solutions tailored to their specific needs. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.664.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.665.1">.12</span></em><span class="koboSpan" id="kobo.666.1"> illustrates the RAG workflow using Amazon Bedrock’s </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.667.1">RetrieveAndGenerate</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.668.1"> API:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<span class="koboSpan" id="kobo.669.1"><img alt="Figure 5.12 – RetrieveAndGenerate API" src="image/B22045_05_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.670.1">Figure 5.12 – RetrieveAndGenerate API</span></p>
<p><span class="koboSpan" id="kobo.671.1">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.672.1">RetrieveAndGenerate</span></strong><span class="koboSpan" id="kobo.673.1"> API, the generated response output contains three components: the text of the model-generated response itself, source attribution indicating where the FM retrieved information from, and the specific text excerpts that were retrieved from those sources as part of generating the response. </span><span class="koboSpan" id="kobo.673.2">The API provides full transparency by returning not just the final output text but also the underlying source materials and attributions that informed the FM’s response generation process. </span><span class="koboSpan" id="kobo.673.3">This allows users to inspect both the final output as well as the intermediate retrieved texts that were used by the system during </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">response generation.</span></span></p>
<p><span class="koboSpan" id="kobo.675.1">The following is a code sample for running the same operation as showcased in the console using </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">the API.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.677.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.678.1">Ensure you have the latest version of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.679.1">boto3</span></strong><span class="koboSpan" id="kobo.680.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.681.1">botocore</span></strong><span class="koboSpan" id="kobo.682.1"> packages prior to running the code shown next. </span><span class="koboSpan" id="kobo.682.2">In case the packages are not installed, run the following command in your Jupyter notebook. </span><span class="koboSpan" id="kobo.682.3">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.683.1">!</span></strong><span class="koboSpan" id="kobo.684.1"> will not be needed if you’re running Python code from a </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">Python terminal:</span></span></p>
<p class="callout"><strong class="source-inline"><span class="koboSpan" id="kobo.686.1">!pip install </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.687.1">boto3 botocore</span></strong></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.688.1">
#import the main packages and libraries
import os
import boto3
import botocore
import json
bedrock_agent_rn = boto3.client(service_name='bedrock-agent-runtime', region_name=os.environ['AWS_REGION'])
#Defining the method to invoke the RetrieveAndGenerate API
def retrieveAndGenerate(input, kb_Id):
    return bedrock_agent_rn.retrieve_and_generate(
        input={
            'text': input
        },
        retrieveAndGenerateConfiguration={
            'type': 'KNOWLEDGE_BASE',
            'knowledgeBaseConfiguration': {
                'knowledgeBaseId': kb_Id,
                'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1'
                }
            }
        )
#Invoking the API to generate the desired response
response = retrieveAndGenerate("What is Quantum Computing?", "PG0WBGY0DD")["output"]["text"]
print(response)</span></pre>
<p class="callout-heading"><span class="koboSpan" id="kobo.689.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.690.1">This script assumes that readers have already created a knowledge base and ingested the relevant documents, following the procedures outlined in the preceding section. </span><span class="koboSpan" id="kobo.690.2">With this prerequisite fulfilled, invoking the </span><strong class="source-inline"><span class="koboSpan" id="kobo.691.1">RetrieveAndGenerate</span></strong><span class="koboSpan" id="kobo.692.1"> API will enable the system to fetch the associated documents using the provided </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">code sample.</span></span></p>
<p><span class="koboSpan" id="kobo.694.1">The code </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.695.1">provided will print the extracted text output to display</span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.696.1"> the relevant information from the data source in context to the input query, formatted as desired. </span><span class="koboSpan" id="kobo.696.2">The response is generated by contextualizing pertinent details from the data source with respect to the specifics of the input query. </span><span class="koboSpan" id="kobo.696.3">The output is then formatted and presented in the requested structure. </span><span class="koboSpan" id="kobo.696.4">This allows customized extraction and formatting of relevant data from the source to provide responses tailored to the input query in a </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">suitable structure.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.698.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.699.1">Please ensure you have the right permissions to invoke Amazon Bedrock APIs by navigating to IAM roles and permissions, searching for the respective role (if you are running the notebook in Amazon SageMaker, search for the execution role that was assigned when you created the Amazon SageMaker domain), and attaching Amazon Bedrock policies for invoking Bedrock models and Bedrock agent </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">runtime APIs.</span></span></p>
<p><span class="koboSpan" id="kobo.701.1">Yet another resourceful Amazon Bedrock API, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">Retrieve</span></strong><span class="koboSpan" id="kobo.703.1"> API, enables more advanced processing and utilization of the retrieved text segments. </span><span class="koboSpan" id="kobo.703.2">This API transforms user queries into vector representations, performs similarity searches against the knowledge base, and returns the most relevant results along with relevance scores. </span><span class="koboSpan" id="kobo.703.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.704.1">Retrieve</span></strong><span class="koboSpan" id="kobo.705.1"> API provides users with more fine-grained control to build custom pipelines leveraging semantic search capabilities. </span><span class="koboSpan" id="kobo.705.2">Through the </span><strong class="source-inline"><span class="koboSpan" id="kobo.706.1">Retrieve</span></strong><span class="koboSpan" id="kobo.707.1"> API, developers can orchestrate subsequent stages of text generation based on the search results, implement additional relevance filtering, or derive other workflow optimizations. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.708.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.709.1">.13</span></em><span class="koboSpan" id="kobo.710.1"> exemplifies </span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.711.1">the usage of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.712.1">Retrieve</span></strong><span class="koboSpan" id="kobo.713.1"> API in </span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.714.1">Amazon Bedrock in a </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">RAG pipeline:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<span class="koboSpan" id="kobo.716.1"><img alt="Figure 5.13 – Retrieve API" src="image/B22045_05_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.717.1">Figure 5.13 – Retrieve API</span></p>
<p><span class="koboSpan" id="kobo.718.1">Within the Amazon Bedrock console, you can toggle the switch to disable the </span><strong class="bold"><span class="koboSpan" id="kobo.719.1">Generate responses</span></strong><span class="koboSpan" id="kobo.720.1"> feature and rely solely on retrieval. </span><span class="koboSpan" id="kobo.720.2">This configuration setting allows us to view the raw retrieval results without any generative text being produced. </span><span class="koboSpan" id="kobo.720.3">We can pose the same question </span><strong class="source-inline"><span class="koboSpan" id="kobo.721.1">What is Quantum Computing?</span></strong><span class="koboSpan" id="kobo.722.1"> again. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.723.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.724.1">.14</span></em><span class="koboSpan" id="kobo.725.1"> showcases the generated responses retrieved from the knowledge base pertaining to the question on quantum computing. </span><span class="koboSpan" id="kobo.725.2">Note that Amazon Bedrock cites the references along with the </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">generated responses:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<span class="koboSpan" id="kobo.727.1"><img alt="Figure 5.14 – Test knowledge base" src="image/B22045_05_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.728.1">Figure 5.14 – Test knowledge base</span></p>
<p><span class="koboSpan" id="kobo.729.1">This time, instead </span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.730.1">of a fluid natural language response, notice that the </span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.731.1">output displays the retrieved text chunks alongside links to the original source documents from which they were extracted. </span><span class="koboSpan" id="kobo.731.2">This approach provides transparency by explicitly showing the relevant information retrieved from the knowledge base and </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">its provenance.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.733.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.734.1">Ensure you have the latest version of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.735.1">boto3</span></strong><span class="koboSpan" id="kobo.736.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.737.1">botocore</span></strong><span class="koboSpan" id="kobo.738.1"> packages prior to running the code shown next. </span><span class="koboSpan" id="kobo.738.2">In case the packages are not installed, run the following command in your Jupyter notebook. </span><span class="koboSpan" id="kobo.738.3">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.739.1">!</span></strong><span class="koboSpan" id="kobo.740.1"> will not be needed if you’re running Python code from a </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">Python terminal:</span></span></p>
<p class="callout"><strong class="source-inline"><span class="koboSpan" id="kobo.742.1">!pip install </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.743.1">boto3 botocore</span></strong></span></p>
<p><span class="koboSpan" id="kobo.744.1">Leveraging</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.745.1"> the </span><strong class="source-inline"><span class="koboSpan" id="kobo.746.1">Retrieve</span></strong><span class="koboSpan" id="kobo.747.1"> API using </span><strong class="source-inline"><span class="koboSpan" id="kobo.748.1">boto3</span></strong><span class="koboSpan" id="kobo.749.1"> looks </span><a id="_idIndexMarker443"/><span class="No-Break"><span class="koboSpan" id="kobo.750.1">like this:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.751.1">
#import the main packages and libraries
import os
import boto3
import botocore
bedrock_agent_rn = boto3.client(service_name='bedrock-agent-runtime', region_name = os.environ['AWS_REGION'])
#Defining the method to invoke the RetrieveAndGenerate API
def retrieve(query, kb_Id, number_Of_Results=3):
    return bedrock_agent_rn.retrieve(
        retrievalQuery= {
            'text': query
        },
        knowledgeBaseId=kb_Id,
        retrievalConfiguration= {
            'vectorSearchConfiguration': {
                'numberOfResults': number_Of_Results
            }
        }
    )
#Invoking the API
output_response = retrieve("What is Quantum Computing?", "PG0WBGY0DD")["retrievalResults"]
print(output_response)</span></pre>
<p><span class="koboSpan" id="kobo.752.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.753.1">Retrieve</span></strong><span class="koboSpan" id="kobo.754.1"> API returns a response containing the retrieved text excerpts, as well as metadata about the source of each excerpt. </span><span class="koboSpan" id="kobo.754.2">Specifically, the response includes the location type and URI of the source data from which each text chunk was retrieved. </span><span class="koboSpan" id="kobo.754.3">Additionally, each retrieved text chunk is accompanied by a relevancy score. </span><span class="koboSpan" id="kobo.754.4">This score provides an indication of how closely the semantic content of the retrieved chunk matches the </span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.755.1">user’s input query. </span><span class="koboSpan" id="kobo.755.2">Text chunks with higher scores </span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.756.1">are more relevant matches to the query compared to chunks with lower scores. </span><span class="koboSpan" id="kobo.756.2">By examining the scores of the retrieved chunks, the user can focus on the most relevant excerpts returned by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.757.1">Retrieve</span></strong><span class="koboSpan" id="kobo.758.1"> API. </span><span class="koboSpan" id="kobo.758.2">Therefore, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.759.1">Retrieve</span></strong><span class="koboSpan" id="kobo.760.1"> API provides not only the retrieved text but also insightful metadata to enable productive utilization of the </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">API response.</span></span></p>
<p><span class="koboSpan" id="kobo.762.1">By tapping into the custom chunking and vector store capabilities within the RAG framework, you gain more fine-grained control over how your NLP workflows operate under the hood. </span><span class="koboSpan" id="kobo.762.2">Expertly applying these customizations helps ensure RAG is tailored to your specific needs and use cases. </span><span class="koboSpan" id="kobo.762.3">Note that at the time of writing this book, when creating a data source for your knowledge base, you can specify the chunking strategy in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.763.1">ChunkingConfiguration</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.764.1"> object:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.765.1">
chunking_config = {
    "chunkingStrategy": "FIXED_SIZE", # or "NONE"
    "fixedSizeChunkingConfiguration": {
        "chunkSize": 200 # Chunk size in tokens
    }
}</span></pre>
<p><span class="koboSpan" id="kobo.766.1">Let’s look at this in a bit </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">more detail:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.768.1">FIXED_SIZE</span></strong><span class="koboSpan" id="kobo.769.1"> allows you to set a fixed chunk size in tokens for splitting your </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">data sources</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.771.1">NONE</span></strong><span class="koboSpan" id="kobo.772.1"> treats each file as a single chunk, giving you full control over pre-chunking </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">your data</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.774.1">Further information on using the API with the AWS Python SDK can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">at </span></span><a href="https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/"><span class="No-Break"><span class="koboSpan" id="kobo.776.1">https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.777.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.778.1">Knowledge Bases for Amazon Bedrock reduces the complexity of RAG, allowing you to enhance language generation with your own grounded knowledge. </span><span class="koboSpan" id="kobo.778.2">The capabilities open new possibilities for building contextual chatbots, question answering applications, and other AI systems that need to generate informed </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">specific responses.</span></span></p>
<p><span class="koboSpan" id="kobo.780.1">Let us further </span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.781.1">explore how the RAG approach can be </span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.782.1">implemented using the LangChain orchestrator and other </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">GenAI systems.</span></span></p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.784.1">Implementing RAG with other methods</span></h1>
<p><span class="koboSpan" id="kobo.785.1">Amazon Bedrock is </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.786.1">not the only way to implement RAG, and in this section, we will learn about the other ways. </span><span class="koboSpan" id="kobo.786.2">Starting with LangChain, we will also look at some other </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">GenAI systems.</span></span></p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.788.1">Using LangChain</span></h2>
<p><span class="koboSpan" id="kobo.789.1">LangChain </span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.790.1">provides an </span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.791.1">excellent framework for building RAG models by integrating retrieval tools and LLMs. </span><span class="koboSpan" id="kobo.791.2">In this section, we will look at how to implement RAG with LangChain using the </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">following components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.793.1">LLMs</span></strong><span class="koboSpan" id="kobo.794.1">: LangChain</span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.795.1"> integrates with Amazon Bedrock’s powerful LLMs using Bedrock’s available FM invocation APIs. </span><span class="koboSpan" id="kobo.795.2">Amazon Bedrock can be used to generate fluent NL responses after reviewing the </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">retrieved documents.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.797.1">Embedding model</span></strong><span class="koboSpan" id="kobo.798.1">: Text embedding models available via Amazon Bedrock, such as Amazon Titan Text Embeddings, generate vector representations of text passages. </span><span class="koboSpan" id="kobo.798.2">This allows comparing textual similarity in order to retrieve relevant contextual information to augment the input prompt for composing a </span><span class="No-Break"><span class="koboSpan" id="kobo.799.1">final response.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.800.1">Document loader</span></strong><span class="koboSpan" id="kobo.801.1">: LangChain provides a PDF loader to ingest documents from local storage. </span><span class="koboSpan" id="kobo.801.2">This can be replaced by a loader to retrieve </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">enterprise documents.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.803.1">Vector store</span></strong><span class="koboSpan" id="kobo.804.1">: An in-memory store such as FAISS index embeddings can be used for fast retrieval. </span><span class="koboSpan" id="kobo.804.2">A persistent store such as AWS OpenSearch could also be used from an enterprise perspective. </span><span class="koboSpan" id="kobo.804.3">Alternatively, other vector stores, such as Chroma DB, Weaviate, Pinecone, and </span><strong class="bold"><span class="koboSpan" id="kobo.805.1">Relational Database Service</span></strong><span class="koboSpan" id="kobo.806.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.807.1">RDS</span></strong><span class="koboSpan" id="kobo.808.1">) and PostgreSQL </span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.809.1">with </span><strong class="source-inline"><span class="koboSpan" id="kobo.810.1">pgvector</span></strong><span class="koboSpan" id="kobo.811.1">, can be leveraged based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">use case.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.813.1">Index</span></strong><span class="koboSpan" id="kobo.814.1">: The vector index matches input embeddings with stored document embeddings to find the most </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">relevant contexts.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.816.1">Wrapper</span></strong><span class="koboSpan" id="kobo.817.1">: LangChain </span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.818.1">provides a wrapper class that abstracts away the </span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.819.1">underlying logic, handling retrieval, embeddings, indexing, </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">and generation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.821.1">The RAG workflow via LangChain orchestration is </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.823.1">Ingest a collection of documents into the </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">document loader</span></span></li>
<li><span class="koboSpan" id="kobo.825.1">Generate embeddings for all documents using the </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">embedding model</span></span></li>
<li><span class="koboSpan" id="kobo.827.1">Index all document embeddings in the </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">vector store</span></span></li>
<li><span class="koboSpan" id="kobo.829.1">For an input question, generate its embedding using the </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">embedding model</span></span></li>
<li><span class="koboSpan" id="kobo.831.1">Use the index to retrieve the most similar </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">document embeddings</span></span></li>
<li><span class="koboSpan" id="kobo.833.1">Pass the relevant documents to the LLM to generate a natural </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">language answer</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.835.1">By orchestrating retrieval and generation in this way, LangChain provides an easy yet powerful framework for developing RAG models. </span><span class="koboSpan" id="kobo.835.2">The modular architecture allows flexibility, extensibility, and scalability. </span><span class="koboSpan" id="kobo.835.3">For more details on RAG implementation with LangChain, follow </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.836.1">the</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.837.1"> steps in the Amazon Bedrock workshop </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">at </span></span><a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.839.1">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.840.1">.</span></span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.841.1">Other GenAI systems</span></h2>
<p><span class="koboSpan" id="kobo.842.1">RAG models </span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.843.1">can be integrated with other GenAI</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.844.1"> tools and applications to create more powerful and versatile AI systems. </span><span class="koboSpan" id="kobo.844.2">For instance, RAG’s knowledge retrieval capabilities can be combined with conversational agents built on Amazon Bedrock. </span><span class="koboSpan" id="kobo.844.3">This allows the agents to perform multi-step tasks and leverage external knowledge bases to generate responses that are more </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">contextually relevant.</span></span></p>
<p><span class="koboSpan" id="kobo.846.1">Additionally, the RAG knowledge base retrieval enables seamless integration of RAG into custom GenAI pipelines. </span><span class="koboSpan" id="kobo.846.2">Developers can retrieve knowledge from RAG indexes and fuse it with LangChain’s generative capabilities. </span><span class="koboSpan" id="kobo.846.3">This unlocks new use cases such as building AI assistants that can provide expert domain knowledge alongside general </span><span class="No-Break"><span class="koboSpan" id="kobo.847.1">conversational abilities.</span></span></p>
<p><span class="koboSpan" id="kobo.848.1">Further information on LangChain retrievers can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">at </span></span><a href="https://python.langchain.com/docs/integrations/retrievers"><span class="No-Break"><span class="koboSpan" id="kobo.850.1">https://python.langchain.com/docs/integrations/retrievers</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.851.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.852.1">We will cover more details about agents for Amazon Bedrock in </span><a href="B22045_10.xhtml#_idTextAnchor192"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.853.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.854.1"> where we will uncover more RAG-based integration with Amazon </span><span class="No-Break"><span class="koboSpan" id="kobo.855.1">Bedrock agents.</span></span></p>
<p><span class="koboSpan" id="kobo.856.1">With Amazon Bedrock’s managed approach, incorporating real-world knowledge into FMs has become more accessible than ever. </span><span class="koboSpan" id="kobo.856.2">Now, let us uncover some advanced RAG techniques that are rapidly growing as a mechanism to improve upon current </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">RAG approaches.</span></span></p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.858.1">Advanced RAG techniques</span></h1>
<p><span class="koboSpan" id="kobo.859.1">While basic RAG </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.860.1">pipelines involve retrieving relevant documents and directly providing them as context to the LLM, advanced RAG techniques employ various methods to enhance the quality, relevance, and factual accuracy of generated responses. </span><span class="koboSpan" id="kobo.860.2">These advanced techniques go beyond the naive approach of simple document retrieval and context augmentation, aiming to optimize various stages of the RAG pipeline for </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">improved performance.</span></span></p>
<p><span class="koboSpan" id="kobo.862.1">Let’s now look at some key areas where advanced RAG </span><span class="No-Break"><span class="koboSpan" id="kobo.863.1">techniques focus.</span></span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.864.1">Query handler – query reformulation and expansion</span></h2>
<p><span class="koboSpan" id="kobo.865.1">One key area of </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.866.1">advancement is query reformulation and expansion. </span><span class="koboSpan" id="kobo.866.2">Instead of relying solely on the user’s initial query, advanced RAG systems employ NLP techniques to generate additional related queries. </span><span class="koboSpan" id="kobo.866.3">This increases the chances of retrieving a more comprehensive set of relevant information from the knowledge base. </span><span class="koboSpan" id="kobo.866.4">Query reformulation can involve techniques such as </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.868.1">Synonym expansion</span></strong><span class="koboSpan" id="kobo.869.1">: Adding synonyms or related terms to the original query to increase the chances of retrieving relevant information. </span><span class="koboSpan" id="kobo.869.2">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">some examples:</span></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.871.1">Original query</span></em><span class="koboSpan" id="kobo.872.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.873.1">"</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.874.1">Hurricane formation"</span></strong></span></li><li><em class="italic"><span class="koboSpan" id="kobo.875.1">Expanded query</span></em><span class="koboSpan" id="kobo.876.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.877.1">"Hurricane formation" OR "Tropical cyclone genesis" OR "Tropical </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.878.1">storm development"</span></strong></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.879.1">Query rewriting</span></strong><span class="koboSpan" id="kobo.880.1">: Using an LLM to rewrite or rephrase the original user query to improve retrieval quality. </span><span class="koboSpan" id="kobo.880.2">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">some examples:</span></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.882.1">Original query</span></em><span class="koboSpan" id="kobo.883.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.884.1">What </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.885.1">causes hurricanes?</span></strong></span></li><li><em class="italic"><span class="koboSpan" id="kobo.886.1">Rewritten query</span></em><span class="koboSpan" id="kobo.887.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.888.1">Explain the meteorological conditions and processes that lead to the formation of hurricanes or </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.889.1">tropical cyclones.</span></strong></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.890.1">Entity extraction</span></strong><span class="koboSpan" id="kobo.891.1">: Identifying key entities in the query and expanding with related concepts can assist in optimal results. </span><span class="koboSpan" id="kobo.891.2">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.892.1">some examples:</span></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.893.1">Original query</span></em><span class="koboSpan" id="kobo.894.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.895.1">When was the first </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.896.1">iPhone released?</span></strong></span></li><li><em class="italic"><span class="koboSpan" id="kobo.897.1">Extracted </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.898.1">entities</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.900.1">iPhone</span></strong></span></li><li><em class="italic"><span class="koboSpan" id="kobo.901.1">Expanded query</span></em><span class="koboSpan" id="kobo.902.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.903.1">iPhone AND ("product launch" OR "release date" </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.904.1">OR "history")</span></strong></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.905.1">Multi-query retrieval</span></strong><span class="koboSpan" id="kobo.906.1">: Using the LLM to generate multiple related queries and retrieve results for </span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.907.1">each before combining them. </span><span class="koboSpan" id="kobo.907.2">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">some examples:</span></span><ul><li><em class="italic"><span class="koboSpan" id="kobo.909.1">Original query</span></em><span class="koboSpan" id="kobo.910.1">: “</span><strong class="source-inline"><span class="koboSpan" id="kobo.911.1">Causes of the American </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.912.1">Civil War</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.913.1">”</span></span></li><li><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.914.1">Generated queries</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.915.1">:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.916.1">What were the key political and economic factors that led to the American </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.917.1">Civil War?</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.918.1">How did the issue of slavery contribute to starting the American </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.919.1">Civil War?</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.920.1">What were the major events and incidents that precipitated the outbreak of the Civil War </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.921.1">in America?</span></strong></span></li></ul></li></ul></li>
</ul>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.922.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.923.1">.15</span></em><span class="koboSpan" id="kobo.924.1"> illustrates an overview of a query handler with rewriting and </span><span class="No-Break"><span class="koboSpan" id="kobo.925.1">re-ranking mechanisms:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<span class="koboSpan" id="kobo.926.1"><img alt="Figure 5.15 – Query handler with rewriting and re-ranking mechanisms" src="image/B22045_05_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.927.1">Figure 5.15 – Query handler with rewriting and re-ranking mechanisms</span></p>
<p><span class="koboSpan" id="kobo.928.1">By retrieving information for multiple reformulated queries, the system can gather a richer context to</span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.929.1"> better understand the user’s intent and provide more complete and </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">accurate responses.</span></span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.931.1">Hybrid search and retrieval</span></h2>
<p><span class="koboSpan" id="kobo.932.1">Advanced RAG systems </span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.933.1">often employ hybrid retrieval strategies that combine different retrieval methods to leverage their respective strengths. </span><span class="koboSpan" id="kobo.933.2">For example, a system might use sparse vector search for initial filtering, followed by dense vector search for re-ranking and surfacing the most relevant documents. </span><span class="koboSpan" id="kobo.933.3">Other hybrid approaches include </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.935.1">Combining keyword matching with vector </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">similarity search</span></span></li>
<li><span class="koboSpan" id="kobo.937.1">Using different retrieval methods for different types of data (for example, structured </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">versus unstructured)</span></span></li>
<li><span class="koboSpan" id="kobo.939.1">Hierarchical retrieval, where coarse-grained retrieval is followed by </span><span class="No-Break"><span class="koboSpan" id="kobo.940.1">fine-grained re-ranking</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.941.1">Here’s a simple example to illustrate hybrid search </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">and retrieval:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.943.1"><img alt="Figure 5.16 – Hybrid search and retrieval approach" src="image/B22045_05_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.944.1">Figure 5.16 – Hybrid search and retrieval approach</span></p>
<p><span class="koboSpan" id="kobo.945.1">Let’s say you are searching for information about </span><strong class="source-inline"><span class="koboSpan" id="kobo.946.1">apple products</span></strong><span class="koboSpan" id="kobo.947.1"> on a website that sells electronics and</span><a id="_idIndexMarker464"/> <span class="No-Break"><span class="koboSpan" id="kobo.948.1">grocery items.</span></span></p>
<p><span class="koboSpan" id="kobo.949.1">The hybrid search approach combines two </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">retrieval methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.951.1">Keyword search (sparse vector)</span></strong><span class="koboSpan" id="kobo.952.1">: This method looks for exact keyword matches in the text data. </span><span class="koboSpan" id="kobo.952.2">For the query </span><strong class="source-inline"><span class="koboSpan" id="kobo.953.1">apple products</span></strong><span class="koboSpan" id="kobo.954.1">, it will retrieve documents/pages that contain the words </span><strong class="source-inline"><span class="koboSpan" id="kobo.955.1">apple</span></strong><span class="koboSpan" id="kobo.956.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.957.1">products</span></strong><span class="koboSpan" id="kobo.958.1">, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">the following:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.960.1">Buy the latest Apple iPhone </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.961.1">models here</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.962.1">Apple MacBook Pro laptops </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.963.1">on sale</span></strong></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.964.1">Apple cider and apple juice in the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.965.1">grocery section</span></strong></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.966.1">Semantic/vector search (dense vector)</span></strong><span class="koboSpan" id="kobo.967.1">: This method maps the query and documents into vector representations in a high-dimensional space. </span><span class="koboSpan" id="kobo.967.2">It then finds documents whose vectors are closest/most similar to the query vector, capturing semantic relatedness beyond just </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">keyword matching.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.969.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.970.1">apple products</span></strong><span class="koboSpan" id="kobo.971.1">, it may retrieve documents such as </span><span class="No-Break"><span class="koboSpan" id="kobo.972.1">the following:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.973.1">Top tech gadgets and accessories for students</span></strong><span class="koboSpan" id="kobo.974.1"> (semantically related </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">to electronics/products)</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.976.1">Healthy fruits and snacks for kids' lunchboxes</span></strong><span class="koboSpan" id="kobo.977.1"> (semantically related to apple as </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">a fruit)</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.979.1">The hybrid search can then combine and re-rank the results from both </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">retrieval methods:</span></span></p>
<ol>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.981.1">Buy the latest Apple iPhone </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.982.1">models here</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.983.1">Apple MacBook Pro laptops </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.984.1">on sale</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.985.1">Top tech gadgets and accessories </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.986.1">for students</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.987.1">Apple cider and apple juice in the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.988.1">grocery section</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.989.1">Healthy fruits and snacks for </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.990.1">kids' lunchboxes</span></strong></span></li>
</ol>
<p><span class="koboSpan" id="kobo.991.1">By combining keyword matching (for brand/product names) and semantic understanding (for broader context), the hybrid approach can provide more comprehensive and relevant search results compared to using just </span><span class="No-Break"><span class="koboSpan" id="kobo.992.1">one method.</span></span></p>
<p><span class="koboSpan" id="kobo.993.1">The key benefit is</span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.994.1"> retrieving documents that are relevant both lexically (containing the exact query keywords) and semantically (related conceptually to the query intent), improving overall </span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">search quality.</span></span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.996.1">Embedding and index optimization</span></h2>
<p><span class="koboSpan" id="kobo.997.1">The quality of the</span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.998.1"> vector embeddings and indexes used for retrieval can significantly impact the performance of RAG systems. </span><span class="koboSpan" id="kobo.998.2">Advanced techniques in this area include </span><span class="No-Break"><span class="koboSpan" id="kobo.999.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1000.1">Embedding fine-tuning</span></strong><span class="koboSpan" id="kobo.1001.1">: Instead of</span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.1002.1"> using a general pre-trained embedding model, the embedding model can be fine-tuned on domain-specific data to better capture the semantics and nuances of </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">that domain.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.1004.1">For example, if building a RAG system for a medical question answering task, the embedding model can be further fine-tuned on a large corpus of medical literature, such as research papers, clinical notes, and so on. </span><span class="koboSpan" id="kobo.1004.2">This allows the model to better understand domain-specific terminology, abbreviations, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1005.1">contextual relationships.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1006.1">Index structuring and partitioning</span></strong><span class="koboSpan" id="kobo.1007.1">: Instead of storing all document embeddings in a</span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.1008.1"> single flat index, the index can be structured or partitioned in ways that improve retrieval efficiency; for example, clustering, hierarchical indexing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">metadata filtering:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1010.1">Clustering</span></strong><span class="koboSpan" id="kobo.1011.1">: Documents can be clustered based on their embeddings, and separate indices created for each cluster. </span><span class="koboSpan" id="kobo.1011.2">At query time, the query embedding is compared against cluster centroids to identify the relevant cluster(s) to </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">search within.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1013.1">Hierarchical indexing</span></strong><span class="koboSpan" id="kobo.1014.1">: A coarse-level index can first retrieve relevant high-level topics/categories, and then finer-grained indices are searched within </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">those topics.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1016.1">Metadata filtering</span></strong><span class="koboSpan" id="kobo.1017.1">: If document metadata such as type, source, date, and so on is available, the index can be partitioned based on that metadata to allow filtering before </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">vector search.</span></span></li></ul><p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1019.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1020.1">.17</span></em><span class="koboSpan" id="kobo.1021.1"> depicts</span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.1022.1"> an advanced retrieval mechanism enriched </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">with metadata:</span></span></p></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer087">
<span class="koboSpan" id="kobo.1024.1"><img alt="Figure 5.17 – Retrieval mechanism enriched with metadata" src="image/B22045_05_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1025.1">Figure 5.17 – Retrieval mechanism enriched with metadata</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1026.1">Approximate NN (ANN) indexing</span></strong><span class="koboSpan" id="kobo.1027.1">: For </span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.1028.1">very large vector indices, techniques such</span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.1029.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.1030.1">Hierarchical Navigable Small World</span></strong><span class="koboSpan" id="kobo.1031.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1032.1">HNSW</span></strong><span class="koboSpan" id="kobo.1033.1">), FAISS, or </span><strong class="bold"><span class="koboSpan" id="kobo.1034.1">Approximate Nearest Neighbors Oh Yeah</span></strong><span class="koboSpan" id="kobo.1035.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1036.1">Annoy</span></strong><span class="koboSpan" id="kobo.1037.1">) can be</span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.1038.1"> used to create ANN indices. </span><span class="koboSpan" id="kobo.1038.2">This allows trading off some accuracy for massive computational speedups in retrieval time over brute-force search. </span><span class="koboSpan" id="kobo.1038.3">Interested readers can read more details about indexing for NN search in the paper </span><em class="italic"><span class="koboSpan" id="kobo.1039.1">Learning to Index for Nearest Neighbor </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1040.1">Search</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1"> (</span></span><a href="https://arxiv.org/pdf/1807.02962"><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">https://arxiv.org/pdf/1807.02962</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1044.1">Index compression and quantization</span></strong><span class="koboSpan" id="kobo.1045.1">: The size of vector indices can be reduced through </span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.1046.1">compression and quantization techniques without significantly impacting retrieval accuracy. </span><span class="koboSpan" id="kobo.1046.2">This includes methods such as product quantization, scalar quantization, residual quantization, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1047.1">so on.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1048.1">The paper </span><em class="italic"><span class="koboSpan" id="kobo.1049.1">Vector Quantization for Recommender Systems: A Review and Outlook</span></em><span class="koboSpan" id="kobo.1050.1"> (</span><a href="https://arxiv.org/html/2405.03110v1"><span class="koboSpan" id="kobo.1051.1">https://arxiv.org/html/2405.03110v1</span></a><span class="koboSpan" id="kobo.1052.1">) provides a detailed overview of vector quantization for recommender systems. </span><span class="koboSpan" id="kobo.1052.2">By optimizing the embeddings and indexes, advanced RAG systems </span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.1053.1">can improve the relevance and comprehensiveness of the retrieved information, leading to better </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">overall performance.</span></span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.1055.1">Retrieval re-ranking and filtering</span></h2>
<p><span class="koboSpan" id="kobo.1056.1">Even after initial</span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.1057.1"> retrieval, advanced RAG systems often employ additional re-ranking and filtering techniques to surface the most relevant information. </span><span class="koboSpan" id="kobo.1057.2">These techniques include </span><span class="No-Break"><span class="koboSpan" id="kobo.1058.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1059.1">Cross-attention re-ranking</span></strong><span class="koboSpan" id="kobo.1060.1">: Using more expensive cross-attention models can be leveraged in order to re-score and re-rank initially retrieved documents based on their relevance to the query. </span><span class="koboSpan" id="kobo.1060.2">The paper </span><em class="italic"><span class="koboSpan" id="kobo.1061.1">Multi-Vector Attention Models for Deep Re-ranking</span></em><span class="koboSpan" id="kobo.1062.1"> (</span><a href="https://aclanthology.org/2021.emnlp-main.443.pdf"><span class="koboSpan" id="kobo.1063.1">https://aclanthology.org/2021.emnlp-main.443.pdf</span></a><span class="koboSpan" id="kobo.1064.1">) provides a mechanism for deep re-ranking using multi-vector </span><span class="No-Break"><span class="koboSpan" id="kobo.1065.1">attention models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1066.1">Learned re-rankers</span></strong><span class="koboSpan" id="kobo.1067.1">: Training neural networks or other ML models specifically for the task of re-ranking retrieved documents can assist in improving the search results augmented with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1068.1">input query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1069.1">Filtering and pruning</span></strong><span class="koboSpan" id="kobo.1070.1">: Removing less relevant or redundant documents from the initial retrieval set based on various heuristics or models can provide </span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">contextual optimization.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1072.1">For example, the user may ask a query: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1073.1">What were the causes of the American </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1074.1">Civil War?</span></strong></span></p>
<p><span class="koboSpan" id="kobo.1075.1">Here are some </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.1076.1">examples of initial retrieval via </span><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">vector search:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1078.1">The issue of slavery was a primary cause of the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1079.1">Civil War...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1080.1">Economic differences between North and South led </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1081.1">to tensions...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1082.1">The election of Abraham Lincoln in 1860 </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1083.1">triggered secession...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1084.1">The Missouri Compromise failed to resolve </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1085.1">slavery expansion...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1086.1">The Underground Railroad helped enslaved </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1087.1">people escape...</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1088.1">Here are some examples </span><span class="No-Break"><span class="koboSpan" id="kobo.1089.1">of re-ranking:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1090.1">The election of Abraham Lincoln in 1860 </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1091.1">triggered secession...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1092.1">The issue of slavery was a primary cause of the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1093.1">Civil War...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1094.1">The Missouri Compromise failed to resolve </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1095.1">slavery expansion...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1096.1">Economic differences between North and South led </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1097.1">to tensions...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1098.1">The Underground Railroad helped enslaved </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1099.1">people escape...</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1100.1">Here are the top 3 re-ranked and </span><span class="No-Break"><span class="koboSpan" id="kobo.1101.1">filtered results:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1102.1">The election of Abraham Lincoln in 1860 </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1103.1">triggered secession...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1104.1">The issue of slavery was a primary cause of the </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1105.1">Civil War...</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1106.1">The Missouri Compromise failed to resolve </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1107.1">slavery expansion...</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1108.1">The top 3 re-ranked and filtered results are then provided as context to the language model for generating a final response about the causes of the Civil War, focused on the most </span><span class="No-Break"><span class="koboSpan" id="kobo.1109.1">relevant information.</span></span></p>
<p><span class="koboSpan" id="kobo.1110.1">By re-ranking and filtering the retrieved information, advanced RAG systems can provide the LLM with a more focused and relevant context, improving the quality and factual accuracy of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1111.1">generated responses.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1112.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1113.1">.18</span></em><span class="koboSpan" id="kobo.1114.1"> demonstrates</span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.1115.1"> a complete architectural flow using Amazon Bedrock and some of the advanced RAG techniques (re-ranking with hybrid search mechanism) in order to further enhance the </span><span class="No-Break"><span class="koboSpan" id="kobo.1116.1">output response:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<span class="koboSpan" id="kobo.1117.1"><img alt="Figure 5.18 – Advanced RAG approach with Amazon Bedrock" src="image/B22045_05_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1118.1">Figure 5.18 – Advanced RAG approach with Amazon Bedrock</span></p>
<p><span class="koboSpan" id="kobo.1119.1">As depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1120.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1121.1">.18</span></em><span class="koboSpan" id="kobo.1122.1">, employing hybrid search (natively available) within Knowledge Bases for Amazon Bedrock can greatly enhance contextual search quality. </span><span class="koboSpan" id="kobo.1122.2">Additionally, instead of parsing the data chunks directly to the LLM, feeding the retrieved data chunks to a re-ranker model in order to rank the contextual results can further improve the quality of the output. </span><span class="koboSpan" id="kobo.1122.3">Cohere Rerank, Meta’s Dense Passage Retrieval, BERT for re-ranking, or open source models in Hugging Face (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1123.1">cross-encoder</span></strong><span class="koboSpan" id="kobo.1124.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.1125.1">ms-marco-MiniLM-L6-v2</span></strong><span class="koboSpan" id="kobo.1126.1">) are a few examples of re-ranking models that can be utilized for such ranking optimization tasks. </span><span class="koboSpan" id="kobo.1126.2">Finally, once the augmented prompt is created with an enhanced query and optimized context, wherein the prompt is parsed to the Amazon Bedrock LLM for outputting a </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">desirable response.</span></span></p>
<p><span class="koboSpan" id="kobo.1128.1">In such a manner, advanced RAG techniques can aim to enhance the quality, relevance, and factual accuracy of language model outputs by improving various stages of the RAG pipeline by incorporating these techniques. </span><span class="koboSpan" id="kobo.1128.2">Readers are encouraged to visit </span><a href="https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/"><span class="koboSpan" id="kobo.1129.1">https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/</span></a><span class="koboSpan" id="kobo.1130.1"> to learn how to implement </span><strong class="bold"><span class="koboSpan" id="kobo.1131.1">multimodal RAG</span></strong><span class="koboSpan" id="kobo.1132.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1133.1">mmRAG</span></strong><span class="koboSpan" id="kobo.1134.1">) with </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.1135.1">Amazon Bedrock using advanced RAG techniques. </span><span class="koboSpan" id="kobo.1135.2">This solution also uncovers comprehensive solutioning by leveraging advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.1136.1">LangChain capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.1137.1">The paper </span><em class="italic"><span class="koboSpan" id="kobo.1138.1">RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation</span></em><span class="koboSpan" id="kobo.1139.1"> (</span><a href="https://arxiv.org/html/2404.00610v1"><span class="koboSpan" id="kobo.1140.1">https://arxiv.org/html/2404.00610v1</span></a><span class="koboSpan" id="kobo.1141.1">) walks through yet another advanced RAG approach – </span><strong class="bold"><span class="koboSpan" id="kobo.1142.1">Refine Query for RAG</span></strong><span class="koboSpan" id="kobo.1143.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1144.1">RQ-RAG</span></strong><span class="koboSpan" id="kobo.1145.1">), which </span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.1146.1">can aid in further optimization of queries by equipping it with capabilities for explicit rewriting, decomposition, </span><span class="No-Break"><span class="koboSpan" id="kobo.1147.1">and disambiguation.</span></span></p>
<p><span class="koboSpan" id="kobo.1148.1">Since we have uncovered deeper details into RAG functionality, training, and its implementation with Bedrock and other GenAI systems, one should also keep in mind some limitations and</span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.1149.1"> research problems. </span><span class="koboSpan" id="kobo.1149.2">This provides an opportunity for us to evolve with further enhancements with RAG and lead GenAI pathways with more insightful thought processes. </span><span class="koboSpan" id="kobo.1149.3">Some of the limitations and future directions are discussed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">next section.</span></span></p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.1151.1">Limitations and future directions</span></h1>
<p><span class="koboSpan" id="kobo.1152.1">While promising, RAG</span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.1153.1"> models also come with challenges and open research problems, including </span><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1155.1">Knowledge selection</span></strong><span class="koboSpan" id="kobo.1156.1">:One of the critical challenges in RAG is determining the most relevant and salient knowledge to retrieve from the knowledge base. </span><span class="koboSpan" id="kobo.1156.2">With vast amounts of information available, it becomes crucial to identify and prioritize the most pertinent knowledge for the given context. </span><span class="koboSpan" id="kobo.1156.3">Existing retrieval methods may struggle to capture the nuances and subtleties of the query, leading to the retrieval of irrelevant or tangential information. </span><span class="koboSpan" id="kobo.1156.4">Developing more sophisticated query understanding and knowledge selection mechanisms is a key area </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">of research.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1158.1">Knowledge grounding</span></strong><span class="koboSpan" id="kobo.1159.1">: Seamlessly integrating retrieved knowledge into the generation process is a non-trivial task. </span><span class="koboSpan" id="kobo.1159.2">RAG models need to understand the retrieved knowledge, reason over it, and coherently weave it into the generated text. </span><span class="koboSpan" id="kobo.1159.3">This process requires </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.1160.1">advanced </span><strong class="bold"><span class="koboSpan" id="kobo.1161.1">NL understanding</span></strong><span class="koboSpan" id="kobo.1162.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1163.1">NLU</span></strong><span class="koboSpan" id="kobo.1164.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.1165.1">NL generation</span></strong><span class="koboSpan" id="kobo.1166.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1167.1">NLG</span></strong><span class="koboSpan" id="kobo.1168.1">) capabilities, as well as a deep understanding </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.1169.1">of the context and discourse structure. </span><span class="koboSpan" id="kobo.1169.2">Failure to ground the retrieved knowledge properly can lead to inconsistencies, incoherence, or factual errors in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1170.1">generated output.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1171.1">Training objectives</span></strong><span class="koboSpan" id="kobo.1172.1">: One of the major limitations of RAG is the lack of large-scale supervised datasets for end-to-end training. </span><span class="koboSpan" id="kobo.1172.2">Creating such datasets requires extensive human annotation, which is time-consuming and costly. </span><span class="koboSpan" id="kobo.1172.3">Additionally, defining suitable training objectives that balance the retrieval and generation components is challenging. </span><span class="koboSpan" id="kobo.1172.4">Existing training objectives may not adequately capture the complexity of the task, leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.1173.1">sub-optimal performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1174.1">Knowledge base construction</span></strong><span class="koboSpan" id="kobo.1175.1">: The quality and coverage of the knowledge base play a crucial role in the effectiveness of RAG models. </span><span class="koboSpan" id="kobo.1175.2">Creating broad-coverage knowledge bases that span diverse domains and topics is a daunting task. </span><span class="koboSpan" id="kobo.1175.3">Existing knowledge bases may be incomplete, biased, or outdated, limiting the model’s ability to retrieve relevant information. </span><span class="koboSpan" id="kobo.1175.4">Furthermore, ensuring the accuracy and factual correctness of the knowledge base is essential but challenging, especially for rapidly evolving or </span><span class="No-Break"><span class="koboSpan" id="kobo.1176.1">controversial topics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1177.1">Multi-step reasoning</span></strong><span class="koboSpan" id="kobo.1178.1">: RAG systems often struggle with combining retrieved knowledge across multiple steps to perform complex reasoning or inference tasks. </span><span class="koboSpan" id="kobo.1178.2">Technical domains frequently require multi-step reasoning, such as deriving conclusions from multiple premises, following intricate logical chains, or synthesizing information from diverse sources. </span><span class="koboSpan" id="kobo.1178.3">Current RAG systems may lack the capability to effectively integrate and reason over retrieved knowledge in a coherent and logical manner, limiting their applicability in scenarios involving intricate </span><span class="No-Break"><span class="koboSpan" id="kobo.1179.1">reasoning processes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1180.1">Evaluation</span></strong><span class="koboSpan" id="kobo.1181.1">: Evaluating the performance of RAG models is challenging due to the complexity of the task. </span><span class="koboSpan" id="kobo.1181.2">Traditional metrics for text generation, such as perplexity</span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.1182.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.1183.1">BiLingual Evaluation Understudy</span></strong><span class="koboSpan" id="kobo.1184.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1185.1">BLEU</span></strong><span class="koboSpan" id="kobo.1186.1">) scores, may not adequately capture the factual correctness, coherence, and consistency of the generated output. </span><span class="koboSpan" id="kobo.1186.2">Developing robust evaluation methodologies that consider these aspects, as well as the quality of the retrieved knowledge, is an open research problem. </span><span class="koboSpan" id="kobo.1186.3">Readers are encouraged to check out Ragas (</span><a href="https://docs.ragas.io/en/v0.1.6/index.html"><span class="koboSpan" id="kobo.1187.1">https://docs.ragas.io/en/v0.1.6/index.html</span></a><span class="koboSpan" id="kobo.1188.1">), which is essentially a framework to assist you evaluate your RAG pipelines </span><span class="No-Break"><span class="koboSpan" id="kobo.1189.1">at scale.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1190.1">Despite these limitations, RAG </span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.1191.1">holds significant promise for enhancing the capabilities of GenAI models by leveraging external knowledge sources. </span><span class="koboSpan" id="kobo.1191.2">Addressing these challenges will be crucial for the widespread adoption and success of RAG in various applications, such as question answering, dialogue systems, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1192.1">content generation.</span></span></p>
<p><span class="koboSpan" id="kobo.1193.1">Key research priorities going forward include improving retrieval precision, developing more sophisticated fusion methods, exploring efficient large-scale training techniques, and creating better </span><span class="No-Break"><span class="koboSpan" id="kobo.1194.1">evaluation benchmarks.</span></span></p>
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1195.1">Future directions</span></strong></span></p>
<p><span class="koboSpan" id="kobo.1196.1">Researchers are exploring advanced techniques such as dense passage retrieval, learned sparse representations, and hybrid approaches that combine symbolic and neural methods. </span><span class="koboSpan" id="kobo.1196.2">Additionally, incorporating external knowledge sources beyond traditional corpora, such as structured databases or knowledge graphs, could significantly improve retrieval precision and </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">context understanding.</span></span></p>
<p><span class="koboSpan" id="kobo.1198.1">Developing more sophisticated fusion methods is another critical area of research. </span><span class="koboSpan" id="kobo.1198.2">While current approaches such as retrieval-augmented language models have shown promising results, they often rely on simple concatenation or attention mechanisms to fuse retrieved information with the language model’s generation. </span><span class="koboSpan" id="kobo.1198.3">Researchers are investigating more advanced fusion techniques that can better capture the complex relationships between retrieved knowledge and the generation context, potentially leveraging techniques from areas such as multi-modal learning, </span><strong class="bold"><span class="koboSpan" id="kobo.1199.1">graph neural networks</span></strong><span class="koboSpan" id="kobo.1200.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1201.1">GNNs</span></strong><span class="koboSpan" id="kobo.1202.1">), and </span><a id="_idIndexMarker486"/><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">neuro-symbolic reasoning.</span></span></p>
<p><span class="koboSpan" id="kobo.1204.1">Exploring efficient large-scale training techniques is essential for scaling RAG to massive knowledge sources and complex domains. </span><span class="koboSpan" id="kobo.1204.2">Current systems are often trained on relatively small datasets due to computational constraints, limiting their ability to effectively leverage vast knowledge repositories. </span><span class="koboSpan" id="kobo.1204.3">Researchers are investigating techniques such as distributed training, knowledge distillation, and efficient retrieval indexing to enable training on large-scale knowledge sources while maintaining </span><span class="No-Break"><span class="koboSpan" id="kobo.1205.1">computational feasibility.</span></span></p>
<p><span class="koboSpan" id="kobo.1206.1">Finally, creating better evaluation benchmarks is crucial for accurately assessing the performance of RAG systems and driving progress in the field. </span><span class="koboSpan" id="kobo.1206.2">Existing benchmarks often focus on specific tasks or domains, making it challenging to evaluate the generalization capabilities of these systems. </span><span class="koboSpan" id="kobo.1206.3">Researchers are working on developing more comprehensive and challenging benchmarks that cover a wider range of knowledge sources, domains, and generation tasks, as well as incorporating more sophisticated evaluation metrics that go beyond traditional measures such as perplexity or </span><span class="No-Break"><span class="koboSpan" id="kobo.1207.1">BLEU scores.</span></span></p>
<p><span class="koboSpan" id="kobo.1208.1">By addressing these </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.1209.1">key research priorities, the field of RAG can continue to advance, enabling the development of more powerful and versatile language generation systems that can effectively leverage vast knowledge repositories to produce high-quality, informative, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1210.1">context-relevant text.</span></span></p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.1211.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1212.1">RAG is a rapidly evolving technique that overcomes knowledge limitations in neural generative models by conditioning them on relevant external contexts. </span><span class="koboSpan" id="kobo.1212.2">We uncovered how training LLMs with a RAG approach works and how to implement RAG with Amazon Bedrock, the LangChain orchestrator, and other GenAI systems. </span><span class="koboSpan" id="kobo.1212.3">We further explored the importance and limitations of RAG approaches in the GenAI realm. </span><span class="koboSpan" id="kobo.1212.4">As indicated, early results across a variety of domains are promising and demonstrate the potential of grounding text generation in real-world knowledge. </span><span class="koboSpan" id="kobo.1212.5">As research addresses current limitations, retrieval augmentation could enable GenAI systems that are factual, informative, </span><span class="No-Break"><span class="koboSpan" id="kobo.1213.1">and safe.</span></span></p>
<p><span class="koboSpan" id="kobo.1214.1">In the next chapter, we will delve into practical applications by employing various approaches on Amazon Bedrock. </span><span class="koboSpan" id="kobo.1214.2">We will commence with a text summarization use case, and then explore insights into the methodologies and techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.1215.1">in depth.</span></span></p>
</div>


<div class="Content" id="_idContainer090">
<h1 id="_idParaDest-112" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.1.1">Part 2: Amazon Bedrock Architecture Patterns</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part, we will explore various architectural patterns and use cases for leveraging the powerful capabilities of Amazon Bedrock. </span><span class="koboSpan" id="kobo.2.2">These include text generation, building question answering systems, entity extraction, code generation, image creation, and developing intelligent agents. </span><span class="koboSpan" id="kobo.2.3">In addition, we will dive deep into the real-world applications, equipping you with the knowledge and skills to maximize Amazon Bedrock’s capabilities in your </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">own projects.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B22045_06.xhtml#_idTextAnchor117"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Generating and Summarizing Text with Amazon Bedrock</span></em></li>
<li><a href="B22045_07.xhtml#_idTextAnchor132"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Building Question Answering Systems and Conversational Interfaces</span></em></li>
<li><a href="B22045_08.xhtml#_idTextAnchor151"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 8</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Extracting Entities and Generating Code with Amazon Bedrock</span></em></li>
<li><a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 9</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Generating and Transforming </span></em><em class="italic"><span class="koboSpan" id="kobo.18.1"> Images Using Amazon Bedrock</span></em></li>
<li><a href="B22045_10.xhtml#_idTextAnchor192"><em class="italic"><span class="koboSpan" id="kobo.19.1">Chapter 10</span></em></a><span class="koboSpan" id="kobo.20.1">, </span><em class="italic"><span class="koboSpan" id="kobo.21.1">Developing Intelligent Agents with Amazon Bedrock</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer091">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer092">
</div>
</div>
</body></html>