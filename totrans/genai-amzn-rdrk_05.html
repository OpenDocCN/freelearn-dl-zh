<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-89"><a id="_idTextAnchor090"/>5</h1>
<h1 id="_idParaDest-90"><a id="_idTextAnchor091"/>Harnessing the Power of RAG</h1>
<p>By now, we know the FMs are trained using large datasets. However, the data used to train FMs might not be recent, and this can cause the models to hallucinate. In this chapter, we will harness the power of RAG by augmenting the model with external data sources to overcome the challenge of hallucination.</p>
<p>We will explore the importance of RAG in generative AI scenarios, how RAG works, and its components. We will then delve into the integration of RAG with Amazon Bedrock, including a fully managed RAG experience by Amazon Bedrock called Knowledge Bases. The chapter will then take a hands-on approach to the implementation of Knowledge Bases and using APIs.</p>
<p>We will explore some real-world scenarios of RAG and discuss a few solution architectures for implementing RAG. You will also be introduced to implementing a RAG framework using Amazon Bedrock, LangChain orchestration, and other generative AI systems. We will end by examining current limitations and future research directions with Amazon Bedrock in the context of RAG.</p>
<p>By the end of this chapter, you will be able to understand the importance of RAG and will be able to implement it with Amazon Bedrock. Learning these methods will empower you to apply the concept of RAG in your own enterprise use cases and build production-level applications, such as conversational interfaces, question answering systems, or module summarization workflows.</p>
<p>Here are the key topics that will be covered in this chapter:</p>
<ul>
<li><a id="_idTextAnchor092"/><a id="_idTextAnchor093"/>Decoding RAG</li>
<li>Implementing RAG with Amazon Bedrock</li>
<li>Implementing RAG with other methods</li>
<li>Advanced RAG techniques</li>
<li>Limitations and future directions<a id="_idTextAnchor094"/></li>
</ul>
<h1 id="_idParaDest-91"><a id="_idTextAnchor095"/>Technical requirements</h1>
<p>This chapter requires you to have access to an AWS account. If you don’t have one already, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create an AWS account.</p>
<p>Secondly, you will need to install and configure AWS CLI (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>) after you create an account, which will be needed to access Amazon Bedrock FMs from your local machine. Since the majority chunk of code cells we will be executing is based in Python, setting up an AWS Python SDK (Boto3) (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>) would be beneficial at this point. You can carry out the Python setup in these ways: install it on your local machine, or use AWS Cloud9, or AWS Lambda, or leverage Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with the invocation and customization of FMs of Amazon Bedrock. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor096"/>Decoding RAG</h1>
<p>RAG is an <a id="_idIndexMarker361"/>approach in NLP that combines large-scale retrieval with neural generative models. The key idea is to retrieve relevant knowledge from large corpora and incorporate that knowledge into the text-generation process. This allows generative models such as Amazon Titan Text, Anthropic Claude, and <strong class="bold">Generative Pre-trained Transformer 3</strong> (<strong class="bold">GPT-3</strong>) to produce more factual, specific, and <a id="_idIndexMarker362"/>coherent text by grounding generations in external knowledge.</p>
<p>RAG has emerged as a promising technique to make neural generative models more knowledgeable and controllable. In this section, we will provide an overview of RAG, explain how it works, and discuss key applications.</p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor097"/>What is RAG?</h2>
<p>Traditional <a id="_idIndexMarker363"/>generative models, such as BART, T5 or GPT-4 are trained on vast amounts of text data in a self-supervised fashion. While this allows them to generate fluent and human-like text, a major limitation is that they lack world knowledge beyond what is contained in their training data. This can lead to factual inconsistencies, repetitions, and hallucinations in the generated text.</p>
<p>RAG aims to ground generations in knowledge by retrieving relevant context from large external corpora. For example, if the model is generating text about Paris, it could retrieve <em class="italic">Wikipedia</em> passages about Paris to inform the generation. This retrieved context is encoded and integrated into the model to guide the text generation.</p>
<p>Augmenting generative models with retrieved knowledge has been shown to produce more factual, specific, and<a id="_idIndexMarker364"/> coherent text across a variety of domains.</p>
<p>The key components of RAG systems are the following:</p>
<ul>
<li>A GenAI <a id="_idIndexMarker365"/>model – specifically, an FM or LLM – that<a id="_idIndexMarker366"/> can generate fluent text (or multi-modal) outputs.</li>
<li>A corpus of data to retrieve relevant information from (for example, <em class="italic">Wikipedia</em>, web pages, documents).</li>
<li>Retriever module, which encodes the input query and retrieves relevant passages from the knowledge corpus based on relevance to the query.</li>
<li>Re-ranker to select the optimal contextual information by re-scoring and ranking the retrieved passages based on relevance to the query. (This step is optional in building basic RAG systems but becomes crucial when building enterprise-scale systems with advanced RAG techniques).</li>
<li>Fusion module to integrate retrieval into the language model. This can involve techniques such as concatenation or allowing the language model to condition on relevant external knowledge.</li>
<li>Other components may also include query reformulation, hybrid search techniques, and <a id="_idIndexMarker367"/>multi-stage retrieval, which will be covered later in this chapter.</li>
</ul>
<p>In order to gain a better understanding of RAG approaches, let us walk through a simple example:</p>
<ul>
<li><code>What are </code><code><a id="_idIndexMarker368"/></code><code>the key events in the life of </code><code>Marie Curie?</code></li>
<li><code>What are the key events in the life of Marie Curie?</code> into a dense vector representation. It then searches through the knowledge corpus (for example, <em class="italic">Wikipedia</em>, web pages) to find relevant passages. For example, it may retrieve the following:<ol><li class="upper-roman"><code>Marie Curie was a Polish physicist and chemist who conducted pioneering research </code><code>on radioactivity...</code></li><li class="upper-roman"><code>In 1903, Curie became the first woman to win a Nobel Prize for her study of </code><code>spontaneous radiation...</code></li><li class="upper-roman"><code>Curie won a second Nobel Prize in 1911, this time in chemistry, for her discovery of the elements radium </code><code>and polonium...</code></li></ol></li>
<li><strong class="bold">Re-ranker</strong>: The re-ranker <a id="_idIndexMarker369"/>scores and re-ranks the retrieved passages based on their relevance to the original query using cross-attention. It may determine that passages <em class="italic">II</em> and <em class="italic">III</em> are more relevant than <em class="italic">I</em>.</li>
<li><strong class="bold">Fusion module</strong>: The top re-ranked passages (for example, <em class="italic">II</em> and <em class="italic">III</em>) are then integrated into the generative language model, either by concatenating them, summarizing them, or allowing the model to attend over them; that is, focus on different parts of the retrieved passages as needed while generating the output.<p class="list-inset">Note that the goal of the fusion step is to provide the generative language model with the most pertinent external knowledge in a manner that allows effective conditioning of the generated output on that knowledge, leading to more accurate, informative, and grounded responses.</p></li>
<li><code>The key events in the life of Marie </code><code>Curie include:</code></p><ul><li><code>In 1903, she became the first woman to win a Nobel Prize for her study of spontaneous </code><code>radiation (radioactivity).</code></li><li><code>In 1911, she won a second Nobel Prize in chemistry for her discovery of the elements radium </code><code>and polonium.</code></li></ul></li>
<li>By retrieving relevant knowledge from an external corpus and integrating it into the language <a id="_idIndexMarker370"/>model, the RAG system can generate a more informative and accurate response, overcoming the limitations of relying solely on the model’s training data.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Dense vector representations, also known as dense embeddings or dense vectors, are a way of encoding meaning and semantic relationships in a numerical format that can be effectively processed by machines. This allows techniques such as cosine similarity to identify semantically related words/texts even without exact keyword matches. Dense vectors power many modern NLP applications, such as semantic search, text generation, translation, and so on, by providing effective semantic <a id="_idIndexMarker371"/>representations as inputs to <strong class="bold">deep </strong><strong class="bold">learning</strong> models.</p>
<p>We will further dive <a id="_idIndexMarker372"/>deep into these components in the <em class="italic">Components of RAG</em> section. Since you now have a brief understanding of RAG, it’s time to realize the importance of RAG in the context of the GenAI universe.</p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor098"/>Importance of RAG</h2>
<p>Before we dive into <a id="_idIndexMarker373"/>how RAG works and its components, it’s important to understand why RAG is needed. As LLMs become more capable of generating fluent and coherent text, it also becomes more important to ground them in factual knowledge and guard against potential hallucinations. If you ask an LLM questions pertaining to recent events, you might notice the model to be hallucinating. With RAG, you can augment the latest knowledge as context to the model to improve content quality by reducing the chance of factual errors.</p>
<p>Another major advantage of RAG is overcoming the limited context length (input token limit) of the model. When providing pieces of text as a context that fits within the token limit of the model, you may not need to use RAG and leverage in-context prompting. However, if you want to provide a large corpus of documents as a context to the model, using RAG would be a better approach. However, RAG is beneficial even when the corpus can fit in the context due to needle-in-a-haystack problems, which can affect retrieval accuracy. To summarize, RAG becomes specifically useful in two primary use cases:</p>
<ul>
<li>When the corpus size exceeds that of the context length</li>
<li>When we want to dynamically provide context to the model instead of feeding it the entire corpus in context</li>
</ul>
<p>RAG has many potential applications for improving GenAI. It can help build contextual chatbots that rely on real enterprise data. It can enable personalized search and recommendations based on user history and preferences. A RAG approach can also aid real-time summarization of large documents by retrieving and condensing key facts.  For example, applying RAG to summarize extensive legal texts or academic papers allows for the extraction and condensation of important information, providing succinct summaries that <a id="_idIndexMarker374"/>capture the core points. Overall, RAG is an important technique for overcoming some limitations of current generative models and grounding them in factual knowledge. This helps make the generated content more useful, reliable, and personalized.</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor099"/>Key applications</h2>
<p>Compared to other LLM customization techniques, such as prompt engineering or fine-tuning, RAG offers several advantages:</p>
<ul>
<li><strong class="bold">Flexibility of knowledge source</strong>: The knowledge base can be customized for each use case without changing the underlying LLM. Knowledge can be easily added, removed, or updated without costly model retraining. This is especially useful for organizations whose knowledge is rapidly evolving.</li>
<li><strong class="bold">Cost-effective</strong>: RAG allows a single-hosted LLM to be shared across many use cases through swappable knowledge sources. There is no need to train bespoke models for each use case, which means greater cost efficiency.</li>
<li><strong class="bold">Natural language queries</strong>: RAG relies on natural language for context retrieval from the knowledge source, unlike prompt engineering, which uses rigid prompt templates. This enables users to be more flexible when working with the models.</li>
</ul>
<p>For most organizations with a custom knowledge pool of information, RAG strikes a balance between cost, flexibility, and usability. Prompt engineering is sufficient for small amounts of context, while full model fine-tuning entails high training costs and rigid knowledge. RAG allows easy knowledge base updates and sharing of LLMs across use cases.</p>
<p>For example, RAG is well suited for <strong class="bold">business-to-business Software-as-a-Service</strong> (<strong class="bold">B2B SaaS</strong>) companies <a id="_idIndexMarker375"/>that manage evolving document bases across many customers. A single-hosted LLM can handle queries across clients by swapping their context documents, eliminating the need for per-client models.</p>
<p>Now that we understand the importance and potential applications of RAG in different scenarios, let us jump into exploring the working of RAG.</p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor100"/>How does RAG work?</h2>
<p><em class="italic">Figure 5</em><em class="italic">.1</em> provides<a id="_idIndexMarker376"/> a high-level overview of how RAG works:</p>
<div><div><img alt="Figure 5.1 – Simplified RAG" src="img/B22045_05_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Simplified RAG</p>
<p>Let us now <a id="_idIndexMarker377"/>understand these steps in detail:</p>
<ol>
<li>Given a prompt from the user, the retriever module is invoked to encode the input query in a dense vector representation.</li>
<li>The retriever module then finds relevant context (passages or documents) from the knowledge corpus, based on maximum inner product similarity (semantic similarity) between the query vector and pre-computed dense vector representations of the corpus contents.</li>
<li>An optional re-ranker module can then re-score and re-rank the initially retrieved results and select the best context passages to augment the generation. The re-ranker helps surface the most relevant passages.</li>
<li>The top-ranked retrieved contexts are fused with the input query to form an augmented prompt (query and context).</li>
<li>The generative model, i.e. the FM or LLM then produces the output text conditioned on both the original query prompt and the retrieved relevant knowledge contexts.</li>
<li>In some RAG systems, the retrieval and re-ranking process can be repeated during the generation step to dynamically retrieve more relevant knowledge as the output is being generated.</li>
</ol>
<p>The key benefits of RAG are ensuring that the generated outputs are grounded in accurate and up-to-date information from trusted external sources, providing source citations for transparency, and reducing hallucinations or inaccuracies from the language model’s training data alone.</p>
<p>RAG systems meet<a id="_idIndexMarker378"/> enterprise requirements for GenAI, such as being comprehensive, trustworthy, transparent, and credible by properly sourcing, vetting, and customizing the underlying data sources and models for specific use cases.</p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor101"/>Components of RAG</h2>
<p>As explained earlier, once <a id="_idIndexMarker379"/>a query is received, relevant context is retrieved from the knowledge source and condensed into a context document.  This context is then concatenated with the original query and fed into the LLM to generate a final response. The knowledge source acts as a dynamic long-term memory, in a way that can be frequently updated, while the LLM contributes its strong language generation capabilities.</p>
<h3>The knowledge base</h3>
<p>A key component of RAG models is the knowledge base, which contains the external knowledge used for retrieval. The knowledge base stores information in a format optimized for fast retrieval, such as dense vectors or indexes.</p>
<p>Popular knowledge sources used in RAG include <em class="italic">Wikipedia</em>, news archives, books, scientific papers, and proprietary knowledge bases created specifically for RAG models. The knowledge can consist of both structured (for example, tables and lists) and unstructured (for example, free text) data.</p>
<p>In a typical RAG scenario, the textual contents of the documents (or web pages) that make up the knowledge corpus and need to be converted into dense vector representations or embeddings are encoded data into smaller chunks. To preserve this structure of tables or lists while encoding, more advanced encoding techniques are used that can embed entire tables/lists as a single vector while retaining their row/column relationships.</p>
<p>Long unstructured text passages are typically chunked or split into smaller text segments or passages of a maximum length (for example, 200 tokens). Each of these chunks or passages is then encoded independently into a dense vector representation.</p>
<p>This embedding process typically happens asynchronously or as a batch process, separate from and ahead of time before any user queries are received by the RAG system.</p>
<p>The embeddings for the entire document corpus are pre-computed and stored before the system is deployed or used for any query answering. This pre-computation step is necessary for the following reasons:</p>
<ul>
<li>The document corpus can typically be very large (for example, <em class="italic">Wikipedia</em> has millions of articles)</li>
<li>Embedding the full corpus at query time would be extremely slow and inefficient</li>
<li>Pre-computed embeddings allow fast maximum inner product search at query time</li>
</ul>
<p>By embedding the <a id="_idIndexMarker380"/>sources asynchronously ahead of time, the RAG system can quickly retrieve relevant documents by comparing the query embedding against the pre-computed document embeddings using efficient vector <a id="_idIndexMarker381"/>similarity search methods such as cosine similarity, Euclidean distance, <strong class="bold">Microprocessor without Interlocked Pipelined Stages</strong> (<strong class="bold">MIPS</strong>), or <strong class="bold">Facebook AI Similarity Search</strong> (<strong class="bold">FAISS</strong>). Readers <a id="_idIndexMarker382"/>are encouraged to review the paper <em class="italic">A Survey on Efficient Processing of Similarity Queries over Neural Embeddings</em> (<a href="https://arxiv.org/abs/2204.07922">https://arxiv.org/abs/2204.07922</a>), which debriefs methods on efficient processing of similarity queries.</p>
<p>Note that the sizes and scope of the knowledge base has a major influence on the capabilities of the RAG system. Larger knowledge bases with more diverse, high-quality knowledge provide more contextual information for the model to draw from, based on the user’s questions.</p>
<h3>Retriever module</h3>
<p>The retriever module is responsible for finding and retrieving the most relevant knowledge from the knowledge base for each specific context. The input to the retrieval model is typically the prompt or context from the user.</p>
<p>The embedding model encodes the prompt into a vector representation and matches it against encoded representations of the knowledge base to find the closest matching entries.</p>
<p>Common retrieval methods include sparse methods such as <strong class="bold">Term Frequency-Inverse Document Frequency</strong> (<strong class="bold">TF-IDF</strong>) or <strong class="bold">Best Match 25</strong> (<strong class="bold">BM25</strong>), as well as dense <a id="_idIndexMarker383"/>methods such as semantic<a id="_idIndexMarker384"/> search over embedded representations from a dual-encoder model. The retrieval model ranks the knowledge and returns the top <em class="italic">k</em> most relevant pieces back to the generative model.</p>
<p>The tighter the <a id="_idIndexMarker385"/>integration between the retrieval model and the generative model, the better the retrieval results.</p>
<h3>Conditioning the generative model</h3>
<p>The key aspect <a id="_idIndexMarker386"/>that makes the RAG process generative is the conditional generative model. This model takes the retrieved knowledge along with the original prompt and generates the output text.</p>
<p>The knowledge can be provided in different ways to condition the generation:</p>
<ul>
<li>Concatenating the retrieved text to the prompt</li>
<li>Encoding the retrieved text into dense vectors</li>
<li>Inserting the retrieved text into the input at particular positions</li>
</ul>
<p>For example, in a typical scenario, the retrieved knowledge is augmented with the input prompt and fed to the LLM to provide a succinct response to the end user. This allows the LLM to directly condition the text generation on the relevant facts and context. Users are encouraged to check out the paper <em class="italic">Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering</em> (<a href="https://arxiv.org/pdf/2007.01282.pdf">https://arxiv.org/pdf/2007.01282.pdf</a>) in order to gain a deeper understanding of the complexity of RAG in the realm of question answering frameworks.</p>
<p>The generative model is usually a large pre-trained language model such as GPT-4, Anthropic Claude 3, Amazon Titan Text G1, and so on. The model can be further fine-tuned end to end on downstream RAG tasks, if needed, in order to optimize the integration of the retrieved knowledge for domain-specific use cases. Now, let us dive into exploring <a id="_idIndexMarker387"/>RAG with Amazon <a id="_idIndexMarker388"/>Bedrock.</p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor102"/>Implementing RAG with Amazon Bedrock</h1>
<p>Prior to responding <a id="_idIndexMarker389"/>to user queries, the system must ingest and index the provided documents. This process can be considered as <em class="italic">step 0</em>, and consists of these sub-steps:</p>
<ul>
<li>Ingest the raw text documents into the knowledge base.</li>
<li>Preprocess the documents by splitting them into smaller chunks to enable more granular retrieval.</li>
<li>Generate dense vector representations for each passage using an embedding model such as Amazon Bedrock’s Titan Text Embeddings model. This encodes the semantic meaning of each passage into a high-dimensional vector space.</li>
<li>Index the passages and their corresponding vector embeddings into a specialized search index optimized for<a id="_idIndexMarker390"/> efficient <strong class="bold">nearest neighbor</strong> (<strong class="bold">NN</strong>) search. These are also referred to as <strong class="bold">vector databases</strong>, which<a id="_idIndexMarker391"/> store numerical representations of text in the form of vectors. This index powers fast retrieval of the most relevant passages in response to user queries.</li>
</ul>
<p>By completing this workflow, the system constructs an indexed corpus ready to serve relevant results for natural language queries over the ingested document collection. The passage splitting, embedding, and indexing steps enable robust ranking and retrieval capabilities.</p>
<p>The flow diagram depicted in <em class="italic">Figure 5</em><em class="italic">.2</em> exemplifies the overall flow of the RAG process as described previously:</p>
<div><div><img alt="Figure 5.2 – RAG with Amazon Bedrock" src="img/B22045_05_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – RAG with Amazon Bedrock</p>
<p>When documents have been properly indexed, the system can provide contextual answers to natural language<a id="_idIndexMarker392"/> questions through the following pipeline:</p>
<ul>
<li><strong class="bold">Step 1</strong>: Encode the input question into a dense vector representation (embedding) using an embedding model, such as the Amazon Titan Text Embeddings model or Cohere’s embedding model, both of which can be accessed via Amazon Bedrock. This captures the semantic meaning of the question.</li>
<li><strong class="bold">Step 2</strong>: Compare the question embedding to indexed document embeddings using cosine similarity or other distance metrics. This retrieves the most relevant document chunks. Append the top-ranking document chunks to the prompt as contextual information. This provides relevant background knowledge for the model.</li>
<li><strong class="bold">Step 3</strong>: Pass the prompt with context to an LLM available on Amazon Bedrock such as Anthropic Claude 3, Meta Llama 3, or Amazon Titan Text G1 - Express. This leverages the model’s capabilities to generate an answer conditioned on the retrieved documentation.</li>
</ul>
<p>Finally, return the model-generated answer, which should show an understanding of the question in relation to the contextual documents.</p>
<p>The system thus leverages Amazon Bedrock FMs to provide natural language question answering grounded in relevant documentation and context. Careful indexing and encoding of documents enable seamless integration of retrieval with generative models for more informed and accurate answers.</p>
<p>Here is an example of RAG implementation with Amazon Bedrock and Amazon OpenSearch Serverless as a vector engine: <a href="https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/">https://aws.amazon.com/blogs/big-data/build-scalable-and-serverless-rag-workflows-with-a-vector-engine-for-amazon-opensearch-serverless-and-amazon-bedrock-claude-models/</a>.</p>
<p>Now that we have discussed some details around implementing RAG with Amazon Bedrock, let us <a id="_idIndexMarker393"/>dive deep into tackling use cases using RAG through Knowledge Bases on Amazon Bedrock.</p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor103"/>Amazon Bedrock Knowledge Bases</h2>
<p>Amazon Bedrock<a id="_idIndexMarker394"/> provides a<a id="_idIndexMarker395"/> fully managed RAG experience with Knowledge Bases, handling the complexity behind the scenes while giving you control over your data. Bedrock’s Knowledge Base capability enables the aggregation of diverse data sources into a centralized repository of machine-readable information. Knowledge Bases automate the creation of vector embeddings from your data, store them in a managed vector index, and handle embedding, querying, source attribution, and short-term memory for production RAG.</p>
<p>The key benefits of Knowledge Bases in Amazon Bedrock include the following:</p>
<ul>
<li><strong class="bold">Seamless RAG workflow</strong>: There’s no need to set up and manage the components yourself. You can just provide your data and let Amazon Bedrock handle ingestion, embedding, storage, and querying.</li>
<li><strong class="bold">Custom vector embeddings</strong>: Your data is ingested and converted into vector representations tailored to your use case with a choice of embedding models.</li>
<li><code>RetrieveAndGenerate</code> API within Amazon Bedrock provides attribution back to source documents and manages conversation history for contextual responses.</li>
<li><strong class="bold">Flexible integration</strong>: Incorporate RAG into your workflows with API access and <a id="_idIndexMarker396"/>integration<a id="_idIndexMarker397"/> support for other GenAI tools.</li>
</ul>
<h2 id="_idParaDest-100"><a id="_idTextAnchor104"/>Amazon Bedrock Knowledge Base setup</h2>
<p>Objectively<a id="_idIndexMarker398"/> speaking, the <a id="_idIndexMarker399"/>following steps facilitate Knowledge Base creation and integration:</p>
<ol>
<li>Identify and prepare data sources for ingestion</li>
<li>Upload data <a id="_idIndexMarker400"/>to <strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">Amazon S3</strong>) for centralized access</li>
<li>Generate embeddings for data via FMs and persist in a vector store</li>
<li>Connect applications and agents to query and incorporate Knowledge Base into workflows</li>
</ol>
<p>To create ingestion jobs, follow the next steps:</p>
<ol>
<li><strong class="bold">Set up your Knowledge Base</strong>: Before you can ingest data, you need to create a knowledge base. This involves defining the structure and schema of the knowledge base to ensure it can store and manage the data effectively.</li>
<li><strong class="bold">Prepare your </strong><strong class="bold">data source</strong>:<ul><li>Ensure your data is stored in Amazon S3. The data can be in various formats, including structured (for example, CSV, JSON) and unstructured (for example, text files, PDFs).</li><li>Organize your data in a way that makes it easy to manage and retrieve.</li></ul></li>
<li><strong class="bold">Create an </strong><strong class="bold">ingestion job</strong>:<ul><li>Navigate to the AWS Bedrock console and go to the <strong class="bold">Knowledge </strong><strong class="bold">base</strong> section.</li><li>Select the option to create a new ingestion job.</li><li>Provide the necessary details, such as the name of the job, the S3 bucket location, and the data format.</li><li>Configure the job to specify how the data should be processed and ingested into the knowledge base.</li></ul></li>
<li><strong class="bold">Configure </strong><strong class="bold">sync settings</strong>:<ul><li>Set up the sync settings to ensure the knowledge base is updated with the most <a id="_idIndexMarker401"/>recent <a id="_idIndexMarker402"/>data from your S3 location.</li><li>You can configure the sync to run at regular intervals (for example, daily or weekly) or trigger it manually as needed.</li><li>Ensure that the sync settings are optimized to handle large volumes of data efficiently.</li></ul></li>
<li><strong class="bold">Run the </strong><strong class="bold">ingestion job</strong>:<ul><li>Once the job is configured, you can start the ingestion process.</li><li>Monitor the job’s progress through the AWS Bedrock console. You can view logs and status updates to ensure the job is running smoothly.</li></ul></li>
</ol>
<p>Now that we have a basic understanding of the ingestion process, let us walk through these details thoroughly.</p>
<p>In order to initiate this pipeline within the AWS console, one can navigate to the <strong class="bold">Orchestration</strong> section within the Amazon Bedrock page, as shown in <em class="italic">Figure 5</em><em class="italic">.3</em>:</p>
<div><div><img alt="Figure 5.3 – Knowledge base" src="img/B22045_05_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Knowledge base</p>
<p>Now, let’s<a id="_idIndexMarker403"/> look at these<a id="_idIndexMarker404"/> steps in greater depth:</p>
<ol>
<li>Click on <strong class="bold">Knowledge base</strong> and enter the details pertaining to the knowledge base you intend to create. You can provide a custom knowledge base name, description, and the <a id="_idIndexMarker405"/>respective <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions for creating either a new service role or leveraging an existing service role for the knowledge base. You can also provide tags to this resource for easy searching and filtering of your resource or tracking AWS costs associated with the service in the knowledge base details section.</li>
<li>In the next step, you will set up the data source by specifying the S3 location where the data to be indexed resides. You can specify a particular data source name (or leverage the default pre-filled name) and provide the S3 URI (Uniform Resource Identifier) of the bucket containing the source data, as depicted in <em class="italic">Figure 5</em><em class="italic">.4</em>:</li>
</ol>
<div><div><img alt="Figure 5.4 – Knowledge base: Set up data source" src="img/B22045_05_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Knowledge base: Set up data source</p>
<ol>
<li value="3">You can also <a id="_idIndexMarker406"/>provide a<a id="_idIndexMarker407"/> customer-managed <strong class="bold">Key Management Service</strong> (<strong class="bold">KMS</strong>) key <a id="_idIndexMarker408"/>you used for encrypting your S3 data, in order to allow the Bedrock service to decrypt it when ingesting the given data into the vector database. Under <strong class="bold">Advanced settings</strong> (as shown in <em class="italic">Figure 5</em><em class="italic">.5</em>), users have the option to choose the default KMS key or customize encryption settings by choosing a different key of their choice by entering the <strong class="bold">Amazon Resource Name</strong> (<strong class="bold">ARN</strong>) or <a id="_idIndexMarker409"/>searching for their stored customized key (or creating a new AWS KMS key on the fly). Providing a customer-managed KMS key for encrypting S3 data sources ingested by Amazon Bedrock is desired for enhanced data security, compliance, and control. It allows data sovereignty, key rotation/revocation, <strong class="bold">separation of duties</strong> (<strong class="bold">SoD</strong>), auditing/logging <a id="_idIndexMarker410"/>capabilities, and integration with existing key management infrastructure. By managing your own encryption keys, you gain greater control over data protection, meeting regulatory requirements and aligning with organizational security policies for sensitive or regulated data:</li>
</ol>
<div><div><img alt="Figure 5.5 – Knowledge base: Advanced settings" src="img/B22045_05_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Knowledge base: Advanced settings</p>
<p>Under <strong class="bold">Chunking strategy</strong> (as shown in <em class="italic">Figure 5</em><em class="italic">.6</em>), users have the option to select how to break down text in the source location into smaller segments before creating the embedding. By default, the knowledge base will automatically split your data into tiny chunks each containing, at most, 300 tokens. If a document or, in other words, source data contains<a id="_idIndexMarker411"/> fewer than 300 <a id="_idIndexMarker412"/>tokens, it is not split any further in that case:</p>
<div><div><img alt="Figure 5.6 – Knowledge base: Chunking strategy" src="img/B22045_05_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Knowledge base: Chunking strategy</p>
<p>Alternatively, you have the option to customize the chunk size using <strong class="bold">Fixed size chunking</strong> or simply opt for <strong class="bold">No chunking</strong> in case you have already preprocessed your source documents into separate files of smaller chunks and don’t intend to chunk your documents any further using Bedrock.</p>
<p>In the next stage, users will select an embedding model to convert their selected data into an embedding. Currently, there are four embeddings models that are supported, as shown in <em class="italic">Figure 5</em><em class="italic">.7</em>. Further, under <strong class="bold">Vector database</strong>, users have the option to go with the recommended route – that is, select the quick create option, which will create an Amazon OpenSearch Serverless vector store in the background automatically in the respective account of their choice:</p>
<p class="callout-heading">Note</p>
<p class="callout">Vector embeddings are numeric representations of text data that encode semantic or contextual meaning. In NLP pipelines, text documents are passed through an embedding model to convert the chunks, including discrete tokens such as words into dense vectors in a continuous vector space. Good vector representations allow <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models to <a id="_idIndexMarker413"/>understand similarities, analogies, and other patterns between words and concepts. In other words, if the vector representations (embeddings) are trained well on a large dataset, they will capture meaningful relationships in the data. This allows ML models that use those embeddings to recognize things such as the following:</p>
<p class="callout">- Which words are similar in meaning (for example, <em class="italic">king</em> and <em class="italic">queen</em>)</p>
<p class="callout">- Which concepts follow an analogical pattern (for example, <em class="italic">man</em> is to <em class="italic">king</em> as <em class="italic">woman</em> is to <em class="italic">queen</em>)</p>
<p class="callout">- Other patterns in how the concepts are represented in the embedding space</p>
<p class="callout">The well-trained embeddings essentially provide the ML models with a numeric map of the relationships and patterns inherent in the data. This makes it easier for the models to then learn and make inferences about those patterns during training on downstream tasks.</p>
<p class="callout">Hence, simply put, good embeddings help ML models understand similarities and relationships between words and concepts rather than just treating them as isolated data points.</p>
<div><div><img alt="Figure 5.7 – Knowledge base: Configure vector store" src="img/B22045_05_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Knowledge base: Configure vector store</p>
<p>Alternatively, you<a id="_idIndexMarker414"/> have the <a id="_idIndexMarker415"/>option to choose your own vector store (as shown in <em class="italic">Figure 5</em><em class="italic">.8</em>). At the time of writing this book, you have the option to select <strong class="bold">Vector engine for Amazon OpenSearch Serverless</strong>, <strong class="bold">Amazon Aurora</strong>, <strong class="bold">MongoDB Atlas</strong>, <strong class="bold">Pinecone</strong>, or <strong class="bold">Redis Enterprise Cloud</strong>. Once selected, you can provide the field mapping to proceed with the knowledge base creation final setup. Depending on the use case, developers or teams may opt for one vector database over another. You can read more about the role of vector datastores in GenAI applications at <a href="https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/">https://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/</a>:</p>
<div><div><img alt="Figure 5.8 – Knowledge base: Vector database" src="img/B22045_05_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Knowledge base: Vector database</p>
<p>You can <a id="_idIndexMarker416"/>check <a id="_idIndexMarker417"/>out <a href="https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/">https://aws.amazon.com/blogs/aws/preview-connect-foundation-models-to-your-company-data-sources-with-agents-for-amazon-bedrock/</a> to learn more about how you can set up your own vector store with Pinecone, OpenSearch Serverless, or Redis.</p>
<p>Assuming that you opt for the default route, involving the creation of a new Amazon OpenSearch Serverless vector store, you can proceed and click on <strong class="bold">Create knowledge base</strong> post reviewing all the provided details as depicted in <em class="italic">Figure 5</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 5.9 – Knowledge base: Review and create" src="img/B22045_05_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Knowledge base: Review and create</p>
<p>Once created, you<a id="_idIndexMarker418"/> can sync <a id="_idIndexMarker419"/>the information to ensure the knowledge base is ingesting and operating on the most recent data stored in your Amazon S3 location. After syncing is completed for the knowledge base, users can test said knowledge base by selecting the appropriate model suitable for their use case by clicking on <strong class="bold">Select Model</strong>, as shown in <em class="italic">Figure 5</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 5.10 – Test knowledge base: Select Model" src="img/B22045_05_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Test knowledge base: Select Model</p>
<p>Once the appropriate model has been selected, you can test it by entering a particular query in the <a id="_idIndexMarker420"/>textbox and <a id="_idIndexMarker421"/>receiving a particular response generated by the model, depicted in <em class="italic">Figure 5</em><em class="italic">.11</em>:</p>
<div><div><img alt="Figure 5.11 – Test Knowledge base" src="img/B22045_05_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Test Knowledge base</p>
<p>At its core, Amazon Bedrock transforms the user’s query into vector representations of meaning; that is, embeddings. It then searches the knowledge base for relevant information using these embeddings as the search criteria. Any knowledge retrieved is combined with the prompt engineered for the FM, providing essential context. The FM integrates this contextual knowledge into its response generation to answer the user’s question. For conversations spanning multiple turns, Amazon Bedrock leverages its knowledge base to maintain conversation context and history, delivering increasingly relevant results.</p>
<p>Additional information for testing the knowledge base and inspecting source chunks can be found at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html">https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-test.html</a>.</p>
<p>To ensure your knowledge base is always up to date, it is essential to automate the syncing process. This can be achieved by using AWS Lambda functions or AWS Step Functions to trigger ingestion jobs based on specific events or schedules.</p>
<p><strong class="bold">AWS Lambda</strong> is a serverless<a id="_idIndexMarker422"/> compute service that allows you to run code without provisioning or managing servers. You can create Lambda functions to automate tasks such as triggering data ingestion jobs, processing data, or sending notifications. Lambda functions can be triggered by various events, including file uploads to Amazon S3, changes to DynamoDB tables, or scheduled events using Amazon CloudWatch Events.</p>
<p><strong class="bold">AWS Step Functions</strong> is a <a id="_idIndexMarker423"/>serverless function orchestrator that allows you to coordinate multiple AWS services into business workflows. You can create state machines that define a series of steps, including Lambda functions, data processing tasks, and <a id="_idIndexMarker424"/>error-handling <a id="_idIndexMarker425"/>logic. Step Functions can be particularly useful for orchestrating complex data ingestion pipelines or ML workflows.</p>
<p>Regularly monitoring and managing data sources is crucial to maintain their relevance and accuracy. <strong class="bold">Amazon CloudWatch</strong> is a <a id="_idIndexMarker426"/>monitoring and observability service that provides data and actionable insights across your AWS resources. You can utilize CloudWatch to set up alarms and notifications for any issues or anomalies in the data syncing process. CloudWatch can monitor metrics such as Lambda function invocations, Step Functions executions, and Amazon S3 bucket activity, allowing you to proactively identify and address potential issues.</p>
<p>Adhering to best practices for data management, such as organizing data logically, maintaining data quality, and ensuring data security, is vital. AWS provides various services and tools to support data management best practices:</p>
<ul>
<li>You can organize your data in Amazon S3 buckets and leverage features such as versioning, lifecycle policies, and access controls to maintain data quality and security.</li>
<li><strong class="bold">AWS Glue</strong> is a fully <a id="_idIndexMarker427"/>managed <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) service that can help you <a id="_idIndexMarker428"/>prepare and move data reliably between different data stores. Glue can be used to clean, transform, and enrich your data before ingesting it into your knowledge base.</li>
<li><strong class="bold">AWS Lake Formation</strong> is a service <a id="_idIndexMarker429"/>that helps you build, secure, and manage data lakes on Amazon S3. It provides features such as data cataloging, access control, and auditing, which can help ensure data security and governance.</li>
</ul>
<p>Regular reviews and updates of the knowledge base should be conducted to remove outdated information and incorporate new, relevant data. AWS provides services such as <strong class="bold">Amazon Kendra</strong> and <strong class="bold">Amazon Comprehend</strong> that<a id="_idIndexMarker430"/> can help you analyze and understand <a id="_idIndexMarker431"/>your knowledge base content, identify outdated or irrelevant information, and suggest updates or improvements.</p>
<p>Tracking actionable metrics, such as search success rate, user engagement, and data freshness, is also <a id="_idIndexMarker432"/>important. These<a id="_idIndexMarker433"/> metrics can help continuously improve the knowledge base, ensuring it meets the needs of its users effectively.</p>
<p>Amazon CloudWatch can be used to collect and analyze metrics from various AWS services, including your knowledge base application. You can create custom metrics, dashboards, and alarms to monitor the performance and usage of your knowledge base.</p>
<p>By leveraging AWS services such as Lambda, Step Functions, CloudWatch, S3, Glue, Lake Formation, Kendra, and Comprehend, you can automate the syncing process, monitor and manage data sources, adhere to data management best practices, and track actionable metrics to ensure your knowledge base remains up to date, relevant, and effective in meeting the needs of your users.</p>
<p>Readers are encouraged to visit the Amazon Bedrock RAG GitHub repository (<a href="https://github.com/aws-samples/amazon-bedrock-rag">https://github.com/aws-samples/amazon-bedrock-rag</a>) to explore and implement a fully managed RAG solution using Knowledge Bases for Amazon Bedrock.</p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor105"/>API calls</h2>
<p>For users who <a id="_idIndexMarker434"/>wish <a id="_idIndexMarker435"/>to invoke Bedrock outside of the console, the <code>RetrieveAndGenerate</code> API provides programmatic access to execute this same workflow. This allows Bedrock’s capabilities to be tightly integrated into custom applications via API calls rather than console interaction. The <code>RetrieveAndGenerate</code> API gives developers the flexibility to build Amazon Bedrock-powered solutions tailored to their specific needs. <em class="italic">Figure 5</em><em class="italic">.12</em> illustrates the RAG workflow using Amazon Bedrock’s <code>RetrieveAndGenerate</code> API:</p>
<div><div><img alt="Figure 5.12 – RetrieveAndGenerate API" src="img/B22045_05_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – RetrieveAndGenerate API</p>
<p>In the <code>RetrieveAndGenerate</code> API, the generated response output contains three components: the text of the model-generated response itself, source attribution indicating where the FM retrieved information from, and the specific text excerpts that were retrieved from those sources as part of generating the response. The API provides full transparency by returning not just the final output text but also the underlying source materials and attributions that informed the FM’s response generation process. This allows users to inspect both the final output as well as the intermediate retrieved texts that were used by the system during response generation.</p>
<p>The following is a code sample for running the same operation as showcased in the console using the API.</p>
<p class="callout-heading">Note</p>
<p class="callout">Ensure you have the latest version of the <code>boto3</code> and <code>botocore</code> packages prior to running the code shown next. In case the packages are not installed, run the following command in your Jupyter notebook. Note that <code>!</code> will not be needed if you’re running Python code from a Python terminal:</p>
<p class="callout"><code>!pip install </code><code>boto3 botocore</code></p>
<pre class="source-code">
#import the main packages and libraries
import os
import boto3
import botocore
import json
bedrock_agent_rn = boto3.client(service_name='bedrock-agent-runtime', region_name=os.environ['AWS_REGION'])
#Defining the method to invoke the RetrieveAndGenerate API
def retrieveAndGenerate(input, kb_Id):
    return bedrock_agent_rn.retrieve_and_generate(
        input={
            'text': input
        },
        retrieveAndGenerateConfiguration={
            'type': 'KNOWLEDGE_BASE',
            'knowledgeBaseConfiguration': {
                'knowledgeBaseId': kb_Id,
                'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1'
                }
            }
        )
#Invoking the API to generate the desired response
response = retrieveAndGenerate("What is Quantum Computing?", "PG0WBGY0DD")["output"]["text"]
print(response)</pre>
<p class="callout-heading">Note</p>
<p class="callout">This script assumes that readers have already created a knowledge base and ingested the relevant documents, following the procedures outlined in the preceding section. With this prerequisite fulfilled, invoking the <code>RetrieveAndGenerate</code> API will enable the system to fetch the associated documents using the provided code sample.</p>
<p>The code <a id="_idIndexMarker436"/>provided will print the extracted text output to display<a id="_idIndexMarker437"/> the relevant information from the data source in context to the input query, formatted as desired. The response is generated by contextualizing pertinent details from the data source with respect to the specifics of the input query. The output is then formatted and presented in the requested structure. This allows customized extraction and formatting of relevant data from the source to provide responses tailored to the input query in a suitable structure.</p>
<p class="callout-heading">Note</p>
<p class="callout">Please ensure you have the right permissions to invoke Amazon Bedrock APIs by navigating to IAM roles and permissions, searching for the respective role (if you are running the notebook in Amazon SageMaker, search for the execution role that was assigned when you created the Amazon SageMaker domain), and attaching Amazon Bedrock policies for invoking Bedrock models and Bedrock agent runtime APIs.</p>
<p>Yet another resourceful Amazon Bedrock API, the <code>Retrieve</code> API, enables more advanced processing and utilization of the retrieved text segments. This API transforms user queries into vector representations, performs similarity searches against the knowledge base, and returns the most relevant results along with relevance scores. The <code>Retrieve</code> API provides users with more fine-grained control to build custom pipelines leveraging semantic search capabilities. Through the <code>Retrieve</code> API, developers can orchestrate subsequent stages of text generation based on the search results, implement additional relevance filtering, or derive other workflow optimizations. <em class="italic">Figure 5</em><em class="italic">.13</em> exemplifies <a id="_idIndexMarker438"/>the usage of the <code>Retrieve</code> API in <a id="_idIndexMarker439"/>Amazon Bedrock in a RAG pipeline:</p>
<div><div><img alt="Figure 5.13 – Retrieve API" src="img/B22045_05_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – Retrieve API</p>
<p>Within the Amazon Bedrock console, you can toggle the switch to disable the <code>What is Quantum Computing?</code> again. <em class="italic">Figure 5</em><em class="italic">.14</em> showcases the generated responses retrieved from the knowledge base pertaining to the question on quantum computing. Note that Amazon Bedrock cites the references along with the generated responses:</p>
<div><div><img alt="Figure 5.14 – Test knowledge base" src="img/B22045_05_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Test knowledge base</p>
<p>This time, instead <a id="_idIndexMarker440"/>of a fluid natural language response, notice that the <a id="_idIndexMarker441"/>output displays the retrieved text chunks alongside links to the original source documents from which they were extracted. This approach provides transparency by explicitly showing the relevant information retrieved from the knowledge base and its provenance.</p>
<p class="callout-heading">Note</p>
<p class="callout">Ensure you have the latest version of the <code>boto3</code> and <code>botocore</code> packages prior to running the code shown next. In case the packages are not installed, run the following command in your Jupyter notebook. Note that <code>!</code> will not be needed if you’re running Python code from a Python terminal:</p>
<p class="callout"><code>!pip install </code><code>boto3 botocore</code></p>
<p>Leveraging<a id="_idIndexMarker442"/> the <code>Retrieve</code> API using <code>boto3</code> looks <a id="_idIndexMarker443"/>like this:</p>
<pre class="source-code">
#import the main packages and libraries
import os
import boto3
import botocore
bedrock_agent_rn = boto3.client(service_name='bedrock-agent-runtime', region_name = os.environ['AWS_REGION'])
#Defining the method to invoke the RetrieveAndGenerate API
def retrieve(query, kb_Id, number_Of_Results=3):
    return bedrock_agent_rn.retrieve(
        retrievalQuery= {
            'text': query
        },
        knowledgeBaseId=kb_Id,
        retrievalConfiguration= {
            'vectorSearchConfiguration': {
                'numberOfResults': number_Of_Results
            }
        }
    )
#Invoking the API
output_response = retrieve("What is Quantum Computing?", "PG0WBGY0DD")["retrievalResults"]
print(output_response)</pre>
<p>The <code>Retrieve</code> API returns a response containing the retrieved text excerpts, as well as metadata about the source of each excerpt. Specifically, the response includes the location type and URI of the source data from which each text chunk was retrieved. Additionally, each retrieved text chunk is accompanied by a relevancy score. This score provides an indication of how closely the semantic content of the retrieved chunk matches the <a id="_idIndexMarker444"/>user’s input query. Text chunks with higher scores <a id="_idIndexMarker445"/>are more relevant matches to the query compared to chunks with lower scores. By examining the scores of the retrieved chunks, the user can focus on the most relevant excerpts returned by the <code>Retrieve</code> API. Therefore, the <code>Retrieve</code> API provides not only the retrieved text but also insightful metadata to enable productive utilization of the API response.</p>
<p>By tapping into the custom chunking and vector store capabilities within the RAG framework, you gain more fine-grained control over how your NLP workflows operate under the hood. Expertly applying these customizations helps ensure RAG is tailored to your specific needs and use cases. Note that at the time of writing this book, when creating a data source for your knowledge base, you can specify the chunking strategy in the <code>ChunkingConfiguration</code> object:</p>
<pre class="source-code">
chunking_config = {
    "chunkingStrategy": "FIXED_SIZE", # or "NONE"
    "fixedSizeChunkingConfiguration": {
        "chunkSize": 200 # Chunk size in tokens
    }
}</pre>
<p>Let’s look at this in a bit more detail:</p>
<ul>
<li><code>FIXED_SIZE</code> allows you to set a fixed chunk size in tokens for splitting your data sources</li>
<li><code>NONE</code> treats each file as a single chunk, giving you full control over pre-chunking your data</li>
</ul>
<p>Further information on using the API with the AWS Python SDK can be found at <a href="https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/">https://aws.amazon.com/blogs/aws/knowledge-bases-now-delivers-fully-managed-rag-experience-in-amazon-bedrock/</a>.</p>
<p>Knowledge Bases for Amazon Bedrock reduces the complexity of RAG, allowing you to enhance language generation with your own grounded knowledge. The capabilities open new possibilities for building contextual chatbots, question answering applications, and other AI systems that need to generate informed specific responses.</p>
<p>Let us further <a id="_idIndexMarker446"/>explore how the RAG approach can be <a id="_idIndexMarker447"/>implemented using the LangChain orchestrator and other GenAI systems.</p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor106"/>Implementing RAG with other methods</h1>
<p>Amazon Bedrock is <a id="_idIndexMarker448"/>not the only way to implement RAG, and in this section, we will learn about the other ways. Starting with LangChain, we will also look at some other GenAI systems.</p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor107"/>Using LangChain</h2>
<p>LangChain <a id="_idIndexMarker449"/>provides an <a id="_idIndexMarker450"/>excellent framework for building RAG models by integrating retrieval tools and LLMs. In this section, we will look at how to implement RAG with LangChain using the following components:</p>
<ul>
<li><strong class="bold">LLMs</strong>: LangChain<a id="_idIndexMarker451"/> integrates with Amazon Bedrock’s powerful LLMs using Bedrock’s available FM invocation APIs. Amazon Bedrock can be used to generate fluent NL responses after reviewing the retrieved documents.</li>
<li><strong class="bold">Embedding model</strong>: Text embedding models available via Amazon Bedrock, such as Amazon Titan Text Embeddings, generate vector representations of text passages. This allows comparing textual similarity in order to retrieve relevant contextual information to augment the input prompt for composing a final response.</li>
<li><strong class="bold">Document loader</strong>: LangChain provides a PDF loader to ingest documents from local storage. This can be replaced by a loader to retrieve enterprise documents.</li>
<li><code>pgvector</code>, can be leveraged based on the use case.</li>
<li><strong class="bold">Index</strong>: The vector index matches input embeddings with stored document embeddings to find the most relevant contexts.</li>
<li><strong class="bold">Wrapper</strong>: LangChain <a id="_idIndexMarker453"/>provides a wrapper class that abstracts away the <a id="_idIndexMarker454"/>underlying logic, handling retrieval, embeddings, indexing, and generation.</li>
</ul>
<p>The RAG workflow via LangChain orchestration is as follows:</p>
<ul>
<li>Ingest a collection of documents into the document loader</li>
<li>Generate embeddings for all documents using the embedding model</li>
<li>Index all document embeddings in the vector store</li>
<li>For an input question, generate its embedding using the embedding model</li>
<li>Use the index to retrieve the most similar document embeddings</li>
<li>Pass the relevant documents to the LLM to generate a natural language answer</li>
</ul>
<p>By orchestrating retrieval and generation in this way, LangChain provides an easy yet powerful framework for developing RAG models. The modular architecture allows flexibility, extensibility, and scalability. For more details on RAG implementation with LangChain, follow <a id="_idIndexMarker455"/>the<a id="_idIndexMarker456"/> steps in the Amazon Bedrock workshop at <a href="https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb">https://github.com/aws-samples/amazon-bedrock-workshop/blob/main/06_OpenSource_examples/01_Langchain_KnowledgeBases_and_RAG_examples/01_qa_w_rag_claude.ipynb</a>.</p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor108"/>Other GenAI systems</h2>
<p>RAG models <a id="_idIndexMarker457"/>can be integrated with other GenAI<a id="_idIndexMarker458"/> tools and applications to create more powerful and versatile AI systems. For instance, RAG’s knowledge retrieval capabilities can be combined with conversational agents built on Amazon Bedrock. This allows the agents to perform multi-step tasks and leverage external knowledge bases to generate responses that are more contextually relevant.</p>
<p>Additionally, the RAG knowledge base retrieval enables seamless integration of RAG into custom GenAI pipelines. Developers can retrieve knowledge from RAG indexes and fuse it with LangChain’s generative capabilities. This unlocks new use cases such as building AI assistants that can provide expert domain knowledge alongside general conversational abilities.</p>
<p>Further information on LangChain retrievers can be found at <a href="https://python.langchain.com/docs/integrations/retrievers">https://python.langchain.com/docs/integrations/retrievers</a>.</p>
<p>We will cover more details about agents for Amazon Bedrock in <a href="B22045_10.xhtml#_idTextAnchor192"><em class="italic">Chapter 10</em></a> where we will uncover more RAG-based integration with Amazon Bedrock agents.</p>
<p>With Amazon Bedrock’s managed approach, incorporating real-world knowledge into FMs has become more accessible than ever. Now, let us uncover some advanced RAG techniques that are rapidly growing as a mechanism to improve upon current RAG approaches.</p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor109"/>Advanced RAG techniques</h1>
<p>While basic RAG <a id="_idIndexMarker459"/>pipelines involve retrieving relevant documents and directly providing them as context to the LLM, advanced RAG techniques employ various methods to enhance the quality, relevance, and factual accuracy of generated responses. These advanced techniques go beyond the naive approach of simple document retrieval and context augmentation, aiming to optimize various stages of the RAG pipeline for improved performance.</p>
<p>Let’s now look at some key areas where advanced RAG techniques focus.</p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor110"/>Query handler – query reformulation and expansion</h2>
<p>One key area of <a id="_idIndexMarker460"/>advancement is query reformulation and expansion. Instead of relying solely on the user’s initial query, advanced RAG systems employ NLP techniques to generate additional related queries. This increases the chances of retrieving a more comprehensive set of relevant information from the knowledge base. Query reformulation can involve techniques such as the following:</p>
<ul>
<li><code>"</code><code>Hurricane formation"</code></li><li><em class="italic">Expanded query</em>: <code>"Hurricane formation" OR "Tropical cyclone genesis" OR "Tropical </code><code>storm development"</code></li></ul></li>
<li><code>What </code><code>causes hurricanes?</code></li><li><em class="italic">Rewritten query</em>: <code>Explain the meteorological conditions and processes that lead to the formation of hurricanes or </code><code>tropical cyclones.</code></li></ul></li>
<li><code>When was the first </code><code>iPhone released?</code></li><li><em class="italic">Extracted </em><em class="italic">entities</em>: <code>iPhone</code></li><li><em class="italic">Expanded query</em>: <code>iPhone AND ("product launch" OR "release date" </code><code>OR "history")</code></li></ul></li>
<li><code>Causes of the American </code><code>Civil War</code>”</li><li><em class="italic">Generated queries</em>:<ul><li><code>What were the key political and economic factors that led to the American </code><code>Civil War?</code></li><li><code>How did the issue of slavery contribute to starting the American </code><code>Civil War?</code></li><li><code>What were the major events and incidents that precipitated the outbreak of the Civil War </code><code>in America?</code></li></ul></li></ul></li>
</ul>
<p><em class="italic">Figure 5</em><em class="italic">.15</em> illustrates an overview of a query handler with rewriting and re-ranking mechanisms:</p>
<div><div><img alt="Figure 5.15 – Query handler with rewriting and re-ranking mechanisms" src="img/B22045_05_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – Query handler with rewriting and re-ranking mechanisms</p>
<p>By retrieving information for multiple reformulated queries, the system can gather a richer context to<a id="_idIndexMarker462"/> better understand the user’s intent and provide more complete and accurate responses.</p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor111"/>Hybrid search and retrieval</h2>
<p>Advanced RAG systems <a id="_idIndexMarker463"/>often employ hybrid retrieval strategies that combine different retrieval methods to leverage their respective strengths. For example, a system might use sparse vector search for initial filtering, followed by dense vector search for re-ranking and surfacing the most relevant documents. Other hybrid approaches include the following:</p>
<ul>
<li>Combining keyword matching with vector similarity search</li>
<li>Using different retrieval methods for different types of data (for example, structured versus unstructured)</li>
<li>Hierarchical retrieval, where coarse-grained retrieval is followed by fine-grained re-ranking</li>
</ul>
<p>Here’s a simple example to illustrate hybrid search and retrieval:</p>
<div><div><img alt="Figure 5.16 – Hybrid search and retrieval approach" src="img/B22045_05_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Hybrid search and retrieval approach</p>
<p>Let’s say you are searching for information about <code>apple products</code> on a website that sells electronics and<a id="_idIndexMarker464"/> grocery items.</p>
<p>The hybrid search approach combines two retrieval methods:</p>
<ul>
<li><code>apple products</code>, it will retrieve documents/pages that contain the words <code>apple</code> and <code>products</code>, such as the following:<ul><li><code>Buy the latest Apple iPhone </code><code>models here</code></li><li><code>Apple MacBook Pro laptops </code><code>on sale</code></li><li><code>Apple cider and apple juice in the </code><code>grocery section</code></li></ul></li>
<li><code>apple products</code>, it may retrieve documents such as the following:</p><ul><li><code>Top tech gadgets and accessories for students</code> (semantically related to electronics/products)</li><li><code>Healthy fruits and snacks for kids' lunchboxes</code> (semantically related to apple as a fruit)</li></ul></li>
</ul>
<p>The hybrid search can then combine and re-rank the results from both retrieval methods:</p>
<ol>
<li><code>Buy the latest Apple iPhone </code><code>models here</code></li>
<li><code>Apple MacBook Pro laptops </code><code>on sale</code></li>
<li><code>Top tech gadgets and accessories </code><code>for students</code></li>
<li><code>Apple cider and apple juice in the </code><code>grocery section</code></li>
<li><code>Healthy fruits and snacks for </code><code>kids' lunchboxes</code></li>
</ol>
<p>By combining keyword matching (for brand/product names) and semantic understanding (for broader context), the hybrid approach can provide more comprehensive and relevant search results compared to using just one method.</p>
<p>The key benefit is<a id="_idIndexMarker465"/> retrieving documents that are relevant both lexically (containing the exact query keywords) and semantically (related conceptually to the query intent), improving overall search quality.</p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor112"/>Embedding and index optimization</h2>
<p>The quality of the<a id="_idIndexMarker466"/> vector embeddings and indexes used for retrieval can significantly impact the performance of RAG systems. Advanced techniques in this area include the following:</p>
<ul>
<li><strong class="bold">Embedding fine-tuning</strong>: Instead of<a id="_idIndexMarker467"/> using a general pre-trained embedding model, the embedding model can be fine-tuned on domain-specific data to better capture the semantics and nuances of that domain.<p class="list-inset">For example, if building a RAG system for a medical question answering task, the embedding model can be further fine-tuned on a large corpus of medical literature, such as research papers, clinical notes, and so on. This allows the model to better understand domain-specific terminology, abbreviations, and contextual relationships.</p></li>
<li><strong class="bold">Index structuring and partitioning</strong>: Instead of storing all document embeddings in a<a id="_idIndexMarker468"/> single flat index, the index can be structured or partitioned in ways that improve retrieval efficiency; for example, clustering, hierarchical indexing, and metadata filtering:<ul><li><strong class="bold">Clustering</strong>: Documents can be clustered based on their embeddings, and separate indices created for each cluster. At query time, the query embedding is compared against cluster centroids to identify the relevant cluster(s) to search within.</li><li><strong class="bold">Hierarchical indexing</strong>: A coarse-level index can first retrieve relevant high-level topics/categories, and then finer-grained indices are searched within those topics.</li><li><strong class="bold">Metadata filtering</strong>: If document metadata such as type, source, date, and so on is available, the index can be partitioned based on that metadata to allow filtering before vector search.</li></ul><p class="list-inset"><em class="italic">Figure 5</em><em class="italic">.17</em> depicts<a id="_idIndexMarker469"/> an advanced retrieval mechanism enriched with metadata:</p></li>
</ul>
<div><div><img alt="Figure 5.17 – Retrieval mechanism enriched with metadata" src="img/B22045_05_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.17 – Retrieval mechanism enriched with metadata</p>
<ul>
<li><strong class="bold">Approximate NN (ANN) indexing</strong>: For <a id="_idIndexMarker470"/>very large vector indices, techniques such<a id="_idIndexMarker471"/> as <strong class="bold">Hierarchical Navigable Small World</strong> (<strong class="bold">HNSW</strong>), FAISS, or <strong class="bold">Approximate Nearest Neighbors Oh Yeah</strong> (<strong class="bold">Annoy</strong>) can be<a id="_idIndexMarker472"/> used to create ANN indices. This allows trading off some accuracy for massive computational speedups in retrieval time over brute-force search. Interested readers can read more details about indexing for NN search in the paper <em class="italic">Learning to Index for Nearest Neighbor </em><em class="italic">Search</em> (<a href="https://arxiv.org/pdf/1807.02962">https://arxiv.org/pdf/1807.02962</a>).</li>
<li><strong class="bold">Index compression and quantization</strong>: The size of vector indices can be reduced through <a id="_idIndexMarker473"/>compression and quantization techniques without significantly impacting retrieval accuracy. This includes methods such as product quantization, scalar quantization, residual quantization, and so on.</li>
</ul>
<p>The paper <em class="italic">Vector Quantization for Recommender Systems: A Review and Outlook</em> (<a href="https://arxiv.org/html/2405.03110v1">https://arxiv.org/html/2405.03110v1</a>) provides a detailed overview of vector quantization for recommender systems. By optimizing the embeddings and indexes, advanced RAG systems <a id="_idIndexMarker474"/>can improve the relevance and comprehensiveness of the retrieved information, leading to better overall performance.</p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor113"/>Retrieval re-ranking and filtering</h2>
<p>Even after initial<a id="_idIndexMarker475"/> retrieval, advanced RAG systems often employ additional re-ranking and filtering techniques to surface the most relevant information. These techniques include the following:</p>
<ul>
<li><strong class="bold">Cross-attention re-ranking</strong>: Using more expensive cross-attention models can be leveraged in order to re-score and re-rank initially retrieved documents based on their relevance to the query. The paper <em class="italic">Multi-Vector Attention Models for Deep Re-ranking</em> (<a href="https://aclanthology.org/2021.emnlp-main.443.pdf">https://aclanthology.org/2021.emnlp-main.443.pdf</a>) provides a mechanism for deep re-ranking using multi-vector attention models.</li>
<li><strong class="bold">Learned re-rankers</strong>: Training neural networks or other ML models specifically for the task of re-ranking retrieved documents can assist in improving the search results augmented with the input query.</li>
<li><strong class="bold">Filtering and pruning</strong>: Removing less relevant or redundant documents from the initial retrieval set based on various heuristics or models can provide contextual optimization.</li>
</ul>
<p>For example, the user may ask a query: <code>What were the causes of the American </code><code>Civil War?</code></p>
<p>Here are some <a id="_idIndexMarker476"/>examples of initial retrieval via vector search:</p>
<ul>
<li><code>The issue of slavery was a primary cause of the </code><code>Civil War...</code></li>
<li><code>Economic differences between North and South led </code><code>to tensions...</code></li>
<li><code>The election of Abraham Lincoln in 1860 </code><code>triggered secession...</code></li>
<li><code>The Missouri Compromise failed to resolve </code><code>slavery expansion...</code></li>
<li><code>The Underground Railroad helped enslaved </code><code>people escape...</code></li>
</ul>
<p>Here are some examples of re-ranking:</p>
<ul>
<li><code>The election of Abraham Lincoln in 1860 </code><code>triggered secession...</code></li>
<li><code>The issue of slavery was a primary cause of the </code><code>Civil War...</code></li>
<li><code>The Missouri Compromise failed to resolve </code><code>slavery expansion...</code></li>
<li><code>Economic differences between North and South led </code><code>to tensions...</code></li>
<li><code>The Underground Railroad helped enslaved </code><code>people escape...</code></li>
</ul>
<p>Here are the top 3 re-ranked and filtered results:</p>
<ul>
<li><code>The election of Abraham Lincoln in 1860 </code><code>triggered secession...</code></li>
<li><code>The issue of slavery was a primary cause of the </code><code>Civil War...</code></li>
<li><code>The Missouri Compromise failed to resolve </code><code>slavery expansion...</code></li>
</ul>
<p>The top 3 re-ranked and filtered results are then provided as context to the language model for generating a final response about the causes of the Civil War, focused on the most relevant information.</p>
<p>By re-ranking and filtering the retrieved information, advanced RAG systems can provide the LLM with a more focused and relevant context, improving the quality and factual accuracy of the generated responses.</p>
<p><em class="italic">Figure 5</em><em class="italic">.18</em> demonstrates<a id="_idIndexMarker477"/> a complete architectural flow using Amazon Bedrock and some of the advanced RAG techniques (re-ranking with hybrid search mechanism) in order to further enhance the output response:</p>
<div><div><img alt="Figure 5.18 – Advanced RAG approach with Amazon Bedrock" src="img/B22045_05_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.18 – Advanced RAG approach with Amazon Bedrock</p>
<p>As depicted in <em class="italic">Figure 5</em><em class="italic">.18</em>, employing hybrid search (natively available) within Knowledge Bases for Amazon Bedrock can greatly enhance contextual search quality. Additionally, instead of parsing the data chunks directly to the LLM, feeding the retrieved data chunks to a re-ranker model in order to rank the contextual results can further improve the quality of the output. Cohere Rerank, Meta’s Dense Passage Retrieval, BERT for re-ranking, or open source models in Hugging Face (<code>cross-encoder</code>/<code>ms-marco-MiniLM-L6-v2</code>) are a few examples of re-ranking models that can be utilized for such ranking optimization tasks. Finally, once the augmented prompt is created with an enhanced query and optimized context, wherein the prompt is parsed to the Amazon Bedrock LLM for outputting a desirable response.</p>
<p>In such a manner, advanced RAG techniques can aim to enhance the quality, relevance, and factual accuracy of language model outputs by improving various stages of the RAG pipeline by incorporating these techniques. Readers are encouraged to visit <a href="https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/">https://aws.amazon.com/blogs/machine-learning/create-a-multimodal-assistant-with-advanced-rag-and-amazon-bedrock/</a> to learn how to implement <strong class="bold">multimodal RAG</strong> (<strong class="bold">mmRAG</strong>) with <a id="_idIndexMarker478"/>Amazon Bedrock using advanced RAG techniques. This solution also uncovers comprehensive solutioning by leveraging advanced LangChain capabilities.</p>
<p>The paper <em class="italic">RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation</em> (<a href="https://arxiv.org/html/2404.00610v1">https://arxiv.org/html/2404.00610v1</a>) walks through yet another advanced RAG approach – <strong class="bold">Refine Query for RAG</strong> (<strong class="bold">RQ-RAG</strong>), which <a id="_idIndexMarker479"/>can aid in further optimization of queries by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation.</p>
<p>Since we have uncovered deeper details into RAG functionality, training, and its implementation with Bedrock and other GenAI systems, one should also keep in mind some limitations and<a id="_idIndexMarker480"/> research problems. This provides an opportunity for us to evolve with further enhancements with RAG and lead GenAI pathways with more insightful thought processes. Some of the limitations and future directions are discussed in the next section.</p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor114"/>Limitations and future directions</h1>
<p>While promising, RAG<a id="_idIndexMarker481"/> models also come with challenges and open research problems, including the following:</p>
<ul>
<li><strong class="bold">Knowledge selection</strong>:One of the critical challenges in RAG is determining the most relevant and salient knowledge to retrieve from the knowledge base. With vast amounts of information available, it becomes crucial to identify and prioritize the most pertinent knowledge for the given context. Existing retrieval methods may struggle to capture the nuances and subtleties of the query, leading to the retrieval of irrelevant or tangential information. Developing more sophisticated query understanding and knowledge selection mechanisms is a key area of research.</li>
<li><strong class="bold">Knowledge grounding</strong>: Seamlessly integrating retrieved knowledge into the generation process is a non-trivial task. RAG models need to understand the retrieved knowledge, reason over it, and coherently weave it into the generated text. This process requires <a id="_idIndexMarker482"/>advanced <strong class="bold">NL understanding</strong> (<strong class="bold">NLU</strong>) and <strong class="bold">NL generation</strong> (<strong class="bold">NLG</strong>) capabilities, as well as a deep understanding <a id="_idIndexMarker483"/>of the context and discourse structure. Failure to ground the retrieved knowledge properly can lead to inconsistencies, incoherence, or factual errors in the generated output.</li>
<li><strong class="bold">Training objectives</strong>: One of the major limitations of RAG is the lack of large-scale supervised datasets for end-to-end training. Creating such datasets requires extensive human annotation, which is time-consuming and costly. Additionally, defining suitable training objectives that balance the retrieval and generation components is challenging. Existing training objectives may not adequately capture the complexity of the task, leading to sub-optimal performance.</li>
<li><strong class="bold">Knowledge base construction</strong>: The quality and coverage of the knowledge base play a crucial role in the effectiveness of RAG models. Creating broad-coverage knowledge bases that span diverse domains and topics is a daunting task. Existing knowledge bases may be incomplete, biased, or outdated, limiting the model’s ability to retrieve relevant information. Furthermore, ensuring the accuracy and factual correctness of the knowledge base is essential but challenging, especially for rapidly evolving or controversial topics.</li>
<li><strong class="bold">Multi-step reasoning</strong>: RAG systems often struggle with combining retrieved knowledge across multiple steps to perform complex reasoning or inference tasks. Technical domains frequently require multi-step reasoning, such as deriving conclusions from multiple premises, following intricate logical chains, or synthesizing information from diverse sources. Current RAG systems may lack the capability to effectively integrate and reason over retrieved knowledge in a coherent and logical manner, limiting their applicability in scenarios involving intricate reasoning processes.</li>
<li><strong class="bold">Evaluation</strong>: Evaluating the performance of RAG models is challenging due to the complexity of the task. Traditional metrics for text generation, such as perplexity<a id="_idIndexMarker484"/> or <strong class="bold">BiLingual Evaluation Understudy</strong> (<strong class="bold">BLEU</strong>) scores, may not adequately capture the factual correctness, coherence, and consistency of the generated output. Developing robust evaluation methodologies that consider these aspects, as well as the quality of the retrieved knowledge, is an open research problem. Readers are encouraged to check out Ragas (<a href="https://docs.ragas.io/en/v0.1.6/index.html">https://docs.ragas.io/en/v0.1.6/index.html</a>), which is essentially a framework to assist you evaluate your RAG pipelines at scale.</li>
</ul>
<p>Despite these limitations, RAG <a id="_idIndexMarker485"/>holds significant promise for enhancing the capabilities of GenAI models by leveraging external knowledge sources. Addressing these challenges will be crucial for the widespread adoption and success of RAG in various applications, such as question answering, dialogue systems, and content generation.</p>
<p>Key research priorities going forward include improving retrieval precision, developing more sophisticated fusion methods, exploring efficient large-scale training techniques, and creating better evaluation benchmarks.</p>
<p><strong class="bold">Future directions</strong></p>
<p>Researchers are exploring advanced techniques such as dense passage retrieval, learned sparse representations, and hybrid approaches that combine symbolic and neural methods. Additionally, incorporating external knowledge sources beyond traditional corpora, such as structured databases or knowledge graphs, could significantly improve retrieval precision and context understanding.</p>
<p>Developing more sophisticated fusion methods is another critical area of research. While current approaches such as retrieval-augmented language models have shown promising results, they often rely on simple concatenation or attention mechanisms to fuse retrieved information with the language model’s generation. Researchers are investigating more advanced fusion techniques that can better capture the complex relationships between retrieved knowledge and the generation context, potentially leveraging techniques from areas such as multi-modal learning, <strong class="bold">graph neural networks</strong> (<strong class="bold">GNNs</strong>), and <a id="_idIndexMarker486"/>neuro-symbolic reasoning.</p>
<p>Exploring efficient large-scale training techniques is essential for scaling RAG to massive knowledge sources and complex domains. Current systems are often trained on relatively small datasets due to computational constraints, limiting their ability to effectively leverage vast knowledge repositories. Researchers are investigating techniques such as distributed training, knowledge distillation, and efficient retrieval indexing to enable training on large-scale knowledge sources while maintaining computational feasibility.</p>
<p>Finally, creating better evaluation benchmarks is crucial for accurately assessing the performance of RAG systems and driving progress in the field. Existing benchmarks often focus on specific tasks or domains, making it challenging to evaluate the generalization capabilities of these systems. Researchers are working on developing more comprehensive and challenging benchmarks that cover a wider range of knowledge sources, domains, and generation tasks, as well as incorporating more sophisticated evaluation metrics that go beyond traditional measures such as perplexity or BLEU scores.</p>
<p>By addressing these <a id="_idIndexMarker487"/>key research priorities, the field of RAG can continue to advance, enabling the development of more powerful and versatile language generation systems that can effectively leverage vast knowledge repositories to produce high-quality, informative, and context-relevant text.</p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor115"/>Summary</h1>
<p>RAG is a rapidly evolving technique that overcomes knowledge limitations in neural generative models by conditioning them on relevant external contexts. We uncovered how training LLMs with a RAG approach works and how to implement RAG with Amazon Bedrock, the LangChain orchestrator, and other GenAI systems. We further explored the importance and limitations of RAG approaches in the GenAI realm. As indicated, early results across a variety of domains are promising and demonstrate the potential of grounding text generation in real-world knowledge. As research addresses current limitations, retrieval augmentation could enable GenAI systems that are factual, informative, and safe.</p>
<p>In the next chapter, we will delve into practical applications by employing various approaches on Amazon Bedrock. We will commence with a text summarization use case, and then explore insights into the methodologies and techniques in depth.</p>
</div>


<div><h1 id="_idParaDest-112" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor116"/>Part 2: Amazon Bedrock Architecture Patterns</h1>
<p>In this part, we will explore various architectural patterns and use cases for leveraging the powerful capabilities of Amazon Bedrock. These include text generation, building question answering systems, entity extraction, code generation, image creation, and developing intelligent agents. In addition, we will dive deep into the real-world applications, equipping you with the knowledge and skills to maximize Amazon Bedrock’s capabilities in your own projects.</p>
<p>This part contains the following chapters:</p>
<ul>
<li><a href="B22045_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Generating and Summarizing Text with Amazon Bedrock</em></li>
<li><a href="B22045_07.xhtml#_idTextAnchor132"><em class="italic">Chapter 7</em></a>, <em class="italic">Building Question Answering Systems and Conversational Interfaces</em></li>
<li><a href="B22045_08.xhtml#_idTextAnchor151"><em class="italic">Chapter 8</em></a>, <em class="italic">Extracting Entities and Generating Code with Amazon Bedrock</em></li>
<li><a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Generating and Transforming </em><em class="italic"> Images Using Amazon Bedrock</em></li>
<li><a href="B22045_10.xhtml#_idTextAnchor192"><em class="italic">Chapter 10</em></a>, <em class="italic">Developing Intelligent Agents with Amazon Bedrock</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</body></html>