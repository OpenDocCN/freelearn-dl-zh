<html><head></head><body>
		<div><h1 id="_idParaDest-135" class="chapter-number"><a id="_idTextAnchor136"/>8</h1>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor137"/>Applying the Lessons of Deepfakes</h1>
			<p>The techniques in this book can be used for a lot more than face replacements. In this chapter, we’ll examine just a few examples of how you can apply the lessons and tools of this book in other fields. We’ll look at how to tweak and modify the techniques to use the results in new and unique ways.</p>
			<p>In particular, we’ll look at just a few techniques from earlier in this book and see how they can be used in a new way. The examples in this chapter are not exhaustive, and there are always more ways that you could implement the abilities that deepfakes bring. In this chapter, we are more focused on the technique than the specifics, but in examining the technique, we’ll explore the following in new ways:</p>
			<ul>
				<li>Aligning other types of images</li>
				<li>The power of masking images</li>
				<li>Getting data under control</li>
			</ul>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor138"/>Technical requirements</h1>
			<p>For this chapter, there is one section with a small amount of code that demonstrates how to use a non-module Git repo for your own uses.</p>
			<p>While this isn’t part of the hands-on section of the book, we’ve included the code to interface with a library: <code>PeCLR</code>. This code is also included in the book’s code repo with some additional functionality, including visualizing the points, but is just an example and is not meant to be a complete API for using <code>PeCLR</code> in your own project:</p>
			<ol>
				<li>First, open Anaconda Command Prompt.</li>
				<li>On Windows, hit <em class="italic">Start</em> and then type <code>anaconda</code>. This should bring up the following option:</li>
			</ol>
			<div><div><img src="img/B17535_08_001.jpg" alt="Figure 8.1 – Anaconda Prompt"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Anaconda Prompt</p>
			<p>Click on this, and it will open an Anaconda prompt for the rest of the following commands.</p>
			<ol>
				<li value="3">Next, we need to clone a copy of the <code>PeCLR</code> library:<pre class="source-code">
git clone https://github.com/dahiyaaneesh/peclr.git</pre></li>
				<li>Download the model data.</li>
			</ol>
			<p>The library includes a copy of all the pretrained models at <a href="https://dataset.ait.ethz.ch/downloads/guSEovHBpR/">https://dataset.ait.ethz.ch/downloads/guSEovHBpR/</a>. Open the link in a browser and download the files (if this URL fails, check the <code>PeCLR</code> library or book repository for any updated links).</p>
			<p>Extract the files into the <code>data/models/</code> folder inside your local copy of the <code>PeCLR</code> repo.</p>
			<ol>
				<li value="5">Create a <code>conda</code> environment with all the <code>PeCLR</code> requirements installed:<pre class="source-code">
conda create -n PeCLR
conda activate PeCLR
pip install -r requirements.txt</pre></li>
			</ol>
			<p>This will create an Anaconda environment with all the libraries that <code>PeCLR</code> needs to run.</p>
			<p>Additionally, this will install a Jupyter notebook. Jupyter Notebook is a useful tool for real-time coding. To run a cell, click on it and then either hit <em class="italic">Shift + Enter</em> or click on the <strong class="bold">Play</strong> triangle button. Jupyter will run that one chunk of code and then stop, allowing you to change the code and rerun it at will.</p>
			<ol>
				<li value="6">Copy the <code>PeCLR.ipynb</code> file into the cloned repo folder.</li>
			</ol>
			<p>If you want to follow the Jupyter Notebook file, you can just copy the file from the book’s repo into the folder that you cloned <code>PeCLR</code> into earlier. This will save you from having to retype everything.</p>
			<ol>
				<li value="7">Open Jupyter Notebook and access it with a browser:<pre class="source-code">
python -m jupyter notebook</pre></li>
			</ol>
			<p>This will run Jupyter Notebook. If you’re running the command on the same computer that you’re using it on, it should also automatically open your browser to the running Jupyter Notebook instance, and you’ll be ready to go. If not, you can open your favorite browser and go to http://&lt;jupyter server ip&gt;:8888/tree to access it.</p>
			<p>The usage of this code will be explained when we come to the <em class="italic">Writing our own interface</em> and <em class="italic">Using the library</em> parts of the next section.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor139"/>Aligning other types of images</h1>
			<p>Aligning faces is a critical tool for getting<a id="_idIndexMarker349"/> deepfakes to work. Without the alignment of faces, we’d be doomed with extremely long training times and huge models to correct the faces. It’s not a stretch to say that without alignment, modern deepfakes would effectively be impossible today.</p>
			<p>Alignment saves time and compute power by removing the need for the neural network to figure out where the face is in the image and adapt for the many different locations the face may be. By aligning in advance, the AI doesn’t even need to learn what a face <em class="italic">is</em> in order to do its job. This allows the AI to focus on learning the task at hand, such as generating realistic facial expressions or speech, rather than trying to locate and correct misaligned faces.</p>
			<p>In addition to improving the efficiency of the training process, aligning faces also helps to improve the quality and consistency of the final deepfake. Without proper alignment, the generated faces may appear distorted or unnatural, which can detract from the overall realism of the deepfake.</p>
			<p>In fact, alignment doesn’t just apply to faces. You could use it for hands, people, animals, or even cars and furniture. In fact, anything that you can detect with defined parts can be aligned. For this to work, you need to somehow find points to align. For example, with hands, this could be the individual fingers.</p>
			<p>While this works with<a id="_idIndexMarker350"/> any object, we will focus on a single example case. Here is an example process on how you could align hands. Other objects could be aligned in the same way. You’ll just want to follow the same steps but replace the hands with whatever object you want to align.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor140"/>Finding an aligner</h2>
			<p>First, we need to find a way<a id="_idIndexMarker351"/> to identify the points<a id="_idIndexMarker352"/> of the hand that we’re interested in aligning with. For this, we need to do something called pose estimation. We could develop this ourselves<a id="_idIndexMarker353"/> using YOLO (<a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a>) or another object detection tool that would identify some point, such as the tips of the fingers. You might have to do some heuristics to order them properly so that you can align with them.</p>
			<p>However, better than developing this ourselves, we could use a library that does this for us. When I want to find a library or code<a id="_idIndexMarker354"/> that does a particular task, the first place I look is <strong class="bold">Papers with Code</strong> (<a href="https://paperswithcode.com/">https://paperswithcode.com/</a>). This site has all sorts of software projects based on various AI tasks. In our example, they have<a id="_idIndexMarker355"/> a section specifically for hand pose estimation (<a href="https://paperswithcode.com/task/hand-pose-estimation">https://paperswithcode.com/task/hand-pose-estimation</a>), which lists a variety of benchmarks. These are tests that the code has been tested against. This lets you see not only the libraries that will do what you want but even show you the “best” ones.</p>
			<p>Right now, the best result is <strong class="bold">Virtual View Selection</strong>, which is located at <a href="https://github.com/iscas3dv/handpose-virtualview">https://github.com/iscas3dv/handpose-virtualview</a>. Unfortunately, this one has<a id="_idIndexMarker356"/> a restrictive “no commercial use” license. So, we’ll actually<a id="_idIndexMarker357"/> skip it and go to the next one, <strong class="bold">AWR: Adaptive Weighting Regression for 3D Hand Pose Estimation</strong>, which can be found at <a href="https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression">https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression</a>. This one is MIT-licensed, which is an open license that lets you use the software even for commercial purposes, but only works<a id="_idIndexMarker358"/> on depth images. <em class="italic">Depth</em> refers to the distance between the camera and the object in the image. These images are useful for tasks such as hand detection, but unfortunately, they require special cameras or techniques to get right, so we will have to skip this one too.</p>
			<p>Many of the others only work on depth images, too. However, if we keep looking through<a id="_idIndexMarker359"/> the posted options, we should come across <strong class="bold">PeCLR: Self-Supervised 3D Hand Pose Estimation from monocular RGB via Equivariant Contrastive Learning</strong>, which has an MIT license<a id="_idIndexMarker360"/> and works on standard RGB (color) photos. You can<a id="_idIndexMarker361"/> download it at <a href="https://github.com/dahiyaaneesh/peclr">https://github.com/dahiyaaneesh/peclr</a>.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">While in this section we’re just using the code and treating it like a library, in reality, the <code>PeCLR</code> code (and the other projects listed) was released as a part of an academic paper. It is not the intention of the authors to diminish that work, as academic work drives a lot of innovation in the AI field. However, this section of the book is about how to <em class="italic">implement</em> ideas, and that means using the code without necessarily paying attention to the innovations. If you’re interested in a deep dive into what exactly <code>PeCLR</code> is doing, we recommend that you read the paper, which is linked in the Git repo readme.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor141"/>Using the library</h2>
			<p>The <code>PeCLR</code> library has all the models<a id="_idIndexMarker362"/> and tools <a id="_idIndexMarker363"/>needed to do detection and pose estimation<a id="_idIndexMarker364"/> for the hands but not all the code to run on the external image. Unfortunately, this is very common in academic research-style projects that are often more interested in you being able to validate the results that they have already published instead of letting you run it on new data. Because of this, we’ll need to actually write some code to run our images through their model.</p>
			<p>Finding the best place to interface with the existing code can be hard if they don’t provide an easy-to-use API. Since <code>PeCLR</code> was an academic project, there is no easy API, and we’ll need to find our own place to call their code with our own API substitute.</p>
			<h3>Writing our own interface</h3>
			<p>The code to run the model<a id="_idIndexMarker365"/> on the validation data is only partially usable for our situation since the dataset that they were using expects data to be in a certain format, which would be hard to recreate with our data. Because of this, we’ll start from scratch and call the model in our own code.</p>
			<p>Let’s get started with this:</p>
			<ol>
				<li>First, we’ll want to import all the libraries we’re using:<pre class="source-code">
import torch
import torchvision.models as models
import cv2
from PIL import Image
from matplotlib import pyplot as plt
import numpy as np
import os
import json
from easydict import EasyDict as edict
from src.models.rn_25D_wMLPref import RN_25D_wMLPref</pre></li>
			</ol>
			<p>The preceding code imports all the libraries we’re going to need. Most of these are standard libraries that we’ve used before, but the last one is the model that <code>PeCLR</code> uses to do the actual detection. We’ve imported that one, so we can call it with the image to run the model.</p>
			<ol>
				<li value="2">Next, we’ll load the model from <code>PeCLR</code>:<pre class="source-code">
model_path = 'data/models/rn50_peclr_yt3d-fh_pt_fh_ft.pth'
model_type = "rn50"
model = RN_25D_wMLPref(backend_model=model_type)
checkpoint = torch.load(model_path)
model.load_state_dict(checkpoint["state_dict"])
model.eval().cuda()</pre></li>
			</ol>
			<p>The preceding code loads the model from the model data that <code>PeCLR</code> provides. To do this, first, we define the model path and type. Then, we pass the model type to generate an appropriate model. Next, we load the checkpoints and copy the weights into the model. Finally, we prepare the model for evaluation and set it to run on the GPU.</p>
			<ol>
				<li value="3">Next, we’ll load the image<a id="_idIndexMarker366"/> and prepare it for the model:<pre class="source-code">
image=io.imread(
  "https://source.unsplash.com/QyCH5jwrD_A")
img = image.astype(np.float32) / 255
image_mean = np.array([0.485, 0.456, 0.406])
image_std = np.array([0.229, 0.224, 0.225])
img = np.divide((img - image_mean), image_std)
img = cv2.resize(img, (224,224))
img = torch.from_numpy(img.transpose(2, 0, 1))
img = img.unsqueeze(0).float().cuda()</pre></li>
			</ol>
			<p>This code prepares the image. It does this by, first, loading it with the <code>SciKit</code> image loader, which, unlike <code>OpenCV</code>, can directly handle URLs or local files. It then calculates an adjustment for restoring the model’s coordinates to the ones that match the full image size. It does this by dividing 224 by the height and width of the image. Then, we convert the image data into a floating point with a range of 0–1. We then normalize the images by dividing them by a standard deviation and subtracting a mean. This brings the images down to a range that the model expects. Then, we resize the image to 224 x 224, which is the image size that the model expects. We then convert the image into a tensor and get it in the order Pytorch uses, with the channels first. Finally, we add another dimension at the front to hold the batch and convert it into a 32-bit floating point on the GPU.</p>
			<p>This all prepares the image for the model to be run on it.</p>
			<ol>
				<li value="4">Next, we run the model<a id="_idIndexMarker367"/> on the image and get 2D coordinates out of it:<pre class="source-code">
with torch.no_grad():
    output = model(img, None)
kp2d = output["kp25d"][:, :21, :2][0]
height, width = image.shape[:2]
kp2d[:,0] *= width / 224
kp2d[:,1] *= height / 224</pre></li>
			</ol>
			<p>This code first runs the image through the model without generating training gradients. To do<a id="_idIndexMarker368"/> this, we pass the image and the <code>None</code> value, which will use a default camera intrinsics matrix.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout"><em class="italic">Camera intrinsics</em> is a fancy term, but it just means the details of your camera. In the case of <code>PeCLR</code>, it wants a matrix that details how large the pixel space is so that it can attempt to guess the depth information from the 2D image. We don’t need the depth information, so we can let <code>PeCLR</code> create a default matrix instead of giving it one.</p>
			<p>Next, the code takes just the 2D alignment points. We don’t need 3D points since we’re aligning in 2D space. If we were working with a depth image, we may have wanted the third dimension, but we aren’t and we don’t need that for our scenario.</p>
			<p>Next, since the model was given a small 224 x 224 image, we’re going to adjust those coordinates to match<a id="_idIndexMarker369"/> the width and height of the original image. To do this, we divide the coordinates by 224 and multiply the result by the original image sizes.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor142"/>Using the landmarks to align</h2>
			<p>In this case, the library<a id="_idIndexMarker370"/> will mark the joints and tips of every finger and the thumb and one point near the “middle” of the hand. Unfortunately, the point in the middle of the hand is not well defined and could be anywhere from the actual middle to the wrist, so we wouldn’t want to use it for alignment. The joint and fingertip locations are going to be more consistent, so we can use those for the alignment process:</p>
			<div><div><img src="img/B17535_08_002.jpg" alt="Figure 8.2 – A hand detected and marked with detection from PeCLR (original photo by Kira auf der Heide via Unsplash)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – A hand detected and marked with detection from PeCLR (original photo by Kira auf der Heide via Unsplash)</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Aligners are never perfectly<a id="_idIndexMarker371"/> accurate. Some can vary by a significant amount, but it’s not a matter of perfect results every time. Any alignment, even an imperfect one, has benefits for neural network training as it normalizes the data into a more reliable format. Any work done before the data is given to the AI means that is one less task the AI has to waste effort on doing.</p>
			<p>Once we can get some known points on an image, we can scale, rotate, and crop the image so that it’s in the orientation that you want and run it through the detector to get a list of points. If you can, I recommend running several images through and averaging the points together. That way, you can reduce any variations in the hand images and get better alignments.</p>
			<p>Once you have the points that you want to align to, you can use them to generate aligned images using the <strong class="bold">Umeyama</strong> algorithm. The algorithm just needs two sets<a id="_idIndexMarker372"/> of points, a known “aligned” set and a second set that you can convert into an aligned set. Umeyama returns a matrix that you can feed into an Affine Warp function to get a final aligned image. See <a href="B17535_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Extracting Faces</em>, for a hands-on code example of how to do this.</p>
			<p>Once we have the aligned hand images, you can do whatever it was you were planning on doing with them, be that displaying them or using them for your<a id="_idIndexMarker373"/> AI task. It’s even possible that, once you have your aligned data, you can use it to train a pose detection model of your own to get even better alignment results. This process of using AI-processed data to feed<a id="_idIndexMarker374"/> into a model to make that model better is called <strong class="bold">bootstrapping</strong> and, with proper supervision, is an invaluable technique.</p>
			<p>Alignments are a critical part of deepfakes, and now you can use them in other fields. Next, we’ll look at how to use masking to get clean cut-outs of an object.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor143"/>The power of masking images</h1>
			<p>When you take<a id="_idIndexMarker375"/> a photograph, you are capturing<a id="_idIndexMarker376"/> everything that the camera sees. However, the chances are that you’re not equally interested in every part of the image. If you’re on vacation, you might take a selfie of yourself in front of a waterfall, and while you value yourself and the waterfall, you care less about the cars or other people in the image. While you can’t remove the cars without adding something into the gaps for your vacation photos, sometimes, you’re only interested in the main subject and might want to cut it from the rest of the image.</p>
			<p>With deepfakes, we can use a mask to help us remove the face from the image so that we replace only the face and leave the rest of the image alone. In other AI tasks, you might have similar needs but different objects that you want to cut out:</p>
			<div><div><img src="img/B17535_08_003.jpg" alt="Figure 8.3 – An example of the mask used in the deepfake process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – An example of the mask used in the deepfake process</p>
			<p>Next, let’s look at other types of masking.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor144"/>Types of masking</h2>
			<p>Masking out an image<a id="_idIndexMarker377"/> can be useful in a lot of tasks. We only used it as a part <a id="_idIndexMarker378"/>of the conversion process in a step called <strong class="bold">composition</strong>. This is only part of the power<a id="_idIndexMarker379"/> of masking. It can be used to guide <strong class="bold">inpainting</strong>, which is the process by which you fill in gaps in an image to erase an object. You could also do it to an image before it gets fed into an AI to make sure that the AI can focus on the important parts.</p>
			<p>In order to mask an image, you need to get some<a id="_idIndexMarker380"/> sort of idea of what part of an image you want to be masked. This is called <strong class="bold">segmentation</strong>, and it has a lot of sub-domains. If you want to segment<a id="_idIndexMarker381"/> based on the type of object, it would be called <strong class="bold">semantic segmentation</strong>. If you wanted to segment<a id="_idIndexMarker382"/> the image based on the subject, it is called <strong class="bold">instance segmentation</strong>. You can even use <strong class="bold">depth segmentation</strong> if you have a solid depth map<a id="_idIndexMarker383"/> of the image. Unfortunately, deciding which type of segmentation you need requires special attention in order to find the right tool.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor145"/>Finding a usable mask for your object</h2>
			<p>Libraries such as <strong class="bold">PaddleSeg</strong> (<a href="https://github.com/PaddlePaddle/PaddleSeg">https://github.com/PaddlePaddle/PaddleSeg</a>) have special tools<a id="_idIndexMarker384"/> that let you<a id="_idIndexMarker385"/> do multiple types <a id="_idIndexMarker386"/>of segmentation. They even have an interactive segmentation system that lets you “mark” what you want to segment like Photoshop’s magic wand tool. Following that, you might need to use that data to train a segmentation model that is capable of masking that particular type of object in new contexts.</p>
			<p>To find the best method of masking the given object you’re interested in, you should probably start with a search for the item that you want to mask and segment. For some objects such as faces, cars, people, and more, there are prebuilt models to segment those objects.</p>
			<p>But if a segmentation model doesn’t exist for the particular object you’re interested in, there’s no need to despair. Newer models such as <strong class="bold">CLIP</strong> (<a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a>) have opened up whole new<a id="_idIndexMarker387"/> opportunities. CLIP is made up of a pair of AI models that connect language and images together. Because of the shared nature of CLIP, it’s possible to learn the difference between objects based on their text descriptions. This means that libraries such as <strong class="bold">CLIPseg</strong> (<a href="https://github.com/timojl/clipseg">https://github.com/timojl/clipseg</a>) can use language prompts<a id="_idIndexMarker388"/> to segment<a id="_idIndexMarker389"/> objects<a id="_idIndexMarker390"/> in an image.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor146"/>Examining an example</h2>
			<p>Let’s look at an example. Let’s say that you wanted<a id="_idIndexMarker391"/> to count the cars parked in a parking lot and see whether any spaces are still available, but all you have is a webcam image of the parking lot from above. To do this, you need to know which parts of the image are cars and which are empty parking spots. This task mixes both semantic segmentation and instance segmentation but uses them together.</p>
			<p>The first step would be to mark out each parking spot in the image to define which spots you want to look at. You could pick a single spot in each parking spot or define them by the whole area. Either way, you’ll probably want to do this manually since it is unlikely to change often enough to justify having the computer do it.</p>
			<p>Now that you know where the parking spots are in the image, you can start looking for cars. To do this, we’d want to look around for a good neural network trained to do this task. In our case, we can check<a id="_idIndexMarker392"/> this example from Kaggle user Tanishq Gautam: <a href="https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch">https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch</a>. This page gives pretrained models and a solid guide for how to segment cars.</p>
			<p>While this model does not do instance segmentation (giving each car a different “color” or tag), we can use it to count the cars anyway. We can do this because we’ve already established that we’re counting cars in a parking lot and that we have manually marked each parking spot.</p>
			<p>Then, we can simply use the segmentation model to detect any cars and see whether they overlap with the parking spots. If we only marked a single point for each parking spot, we could simply check each point and see whether it is segmented as a “car” or as “background” (a common term used for anything that we’re not segmenting).</p>
			<p>If we marked the whole area of the parking spot, then we might want to calculate a minimum coverage for the spot to be considered “taken.” For example, we want to ensure that a motorcycle in a parking spot gets counted, but a car that is slightly over the line shouldn’t be counted. You can set a minimum threshold of the parking spot’s area, and if it’s filled beyond that with a “car,” then you mark the whole spot as taken.</p>
			<p>Additional functionality is possible too. For example, you could even detect whether a car is in a prohibited area by checking whether an area segmented as a “car” is not inside a parking spot. This could be used to automatically alert a parking enforcement officer to check the lot.</p>
			<p>Now that we’ve gotten a good hold on our masking, let’s look at how we can manage our data.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor147"/>Getting data under control</h1>
			<p>There’s a common<a id="_idIndexMarker393"/> saying in the AI community that an ML scientist’s job is only 10% ML and 90% data management. This, like many such sayings, is not far from the truth. While every ML task is focused on the actual training of the model, first, you must get your data into a manageable form before you can start the training. Hours of training can be completely wasted if your data isn’t properly prepared.</p>
			<p>Before you can start training a model, you have to decide what data it is that you’re going to train it with. That data must be gathered, cleaned, converted into the right format, and generally made ready to train. Often, this involves a lot of manual processes and verification.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor148"/>Defining your rules</h2>
			<p>The most important<a id="_idIndexMarker394"/> thing in the manual process is to make sure that all your data meets your requirements and meets a consistent level of quality. To do this, you need to define exactly what “good” data means. Whether you’re annotating your data or gathering large amounts, you should have a standard way of doing whatever it is you’re doing.</p>
			<p>For example, let’s say you’re annotating a dog versus cat dataset and you want to put all the pictures into one of two buckets, one bucket consisting of all the dog images and the other bucket consisting of all the cat images. What happens when an image contains both a cat and a dog? Do you put it in the bucket that is more prominent? Do you exclude it from your dataset? Do you edit the image to remove one of the animals? Do you crop it into two images so both animals end up in the dataset?</p>
			<p>It’s important to have a consistent set of rules for these situations. That ensures that your data is appropriate for the purposes. You don’t want to have these edge cases happen in different ways each time, or it will upset your training and raise confusion when you’re trying<a id="_idIndexMarker395"/> to fix issues.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor149"/>Evolving your rules</h2>
			<p>Also, it’s important<a id="_idIndexMarker396"/> to have a plan for what happens when you change your rules. As you go forward with your data management, it’s almost inevitable that you’ll find that you want to make some tweaks or changes to the rules based on what data you find, how well your training process goes, and whether your final use case changes.</p>
			<p>Looking back at our example, let’s consider the case where you decided to exclude any images that contained both cats and dogs but other animals were fine as long as the image also contained a cat or dog. What happens when you decide that you want to add rabbits to your cat/dog detector? This means not just that you add a new bucket for rabbit images but also that you have to re-process all the existing images that you have gone through to make sure that any cat or dog images that also contain rabbits get removed. What about if you find out that guinea pigs in your cat and dog buckets are being flagged as rabbits? These processes need to be considered as you manage your data.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor150"/>Dealing with errors</h2>
			<p>Errors happen, and small amounts<a id="_idIndexMarker397"/> of bad data getting through into your dataset is inevitable. However, there is a fine line between a few harmless errors and a large enough error to completely invalidate the training. Because of this, it’s often best to get a second (or third) set of eyes on the data as a part of the standard process. Sometimes, this isn’t possible, but in any situation where it is possible, it’s invaluable. A second set of eyes could find flaws in your data or even your methodology. What if you were tagging a particularly weird-looking animal as fine in your dog data but a second person identified it as a rare breed of cat?</p>
			<p>I also recommend automating as much of your data gathering as possible. When you can cut a human out of the loop, you’ll not only save timebut you'll also prevent errors and mistakes. Time spent automating a data process will almost always pay back dividends in time down the line. Even if you think that there is no way that a process will take less time to automate than to do it manually, you should consider the errors that you will avoid. Well-written automation code can be reused later for future projects, too. Anything that can be reused should<a id="_idIndexMarker398"/> be automated so that the next time you need to get it done, there is a tool to handle it for you.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor151"/>Summary</h1>
			<p>In this chapter, we looked at how you can apply the lessons and techniques of deepfakes to other environments. First, we examined how to align other types of images, using hands as an example. Then, we looked at the different types of masks and considered using them in a parking lot monitoring solution. Following this, we examined data management and considered how a dataset to detect different animals might be built.</p>
			<p>This process of figuring out how to apply techniques in new environments used throughout this chapter is itself a valuable technique that can help you throughout your development career, especially if you’re going to work at the edge of your computer’s capabilities like AI does now. Sometimes, the only difference between a successful project and an impossible one is the technique you borrow from a previous project.</p>
			<p>In the next chapter, we’re going to look at the potential and future of deepfakes and other generative AIs.</p>
		</div>
		<div><div></div>
		</div>
	<div><p>EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></p></div>
</body></html>