- en: '25'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic Multi-Step Reasoning and Tool Use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multi-step reasoning and tool use in LLMs involve the model’s ability to break
    down complex tasks into manageable steps and leverage external resources or APIs
    to accomplish these tasks. This capability significantly extends the problem-solving
    potential of LLMs, allowing them to tackle more complex, real-world scenarios.
    Its key characteristics include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task decomposition**: This refers to the model’s ability to take a complex
    input or goal and divide it into smaller, more manageable sub-tasks that can be
    solved sequentially or hierarchically. Instead of trying to solve an entire problem
    in one step, the model creates a structured plan or sequence of reasoning steps
    that progressively leads to a solution. This process mimics the way humans often
    approach complex problems by identifying dependencies, sequencing actions, and
    breaking large goals into intermediate objectives. Techniques such as chain-of-thought
    prompting explicitly encourage this behavior by prompting the model to articulate
    each reasoning step before arriving at an answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External tools**: The capabilities of LLMs can be enhanced by integrating
    additional resources, such as databases, APIs, or specialized services, that LLMs
    cannot access directly due to limitations in their training environment. These
    tools enable the LLMs to interact with real-time data, perform specific tasks
    beyond their built-in knowledge, or offer enhanced functionalities such as web
    browsing, file handling, or executing external scripts. For example, an LLM can
    use an external tool to query up-to-date weather data, retrieve specific information
    from a live API, or run computations that require specialized algorithms. This
    integration allows LLMs to offer more dynamic, relevant, and specialized responses,
    particularly for applications requiring real-time information or complex multi-step
    processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning about tool applicability**: This involves the model’s judgment
    in recognizing when an external capability is required to solve a particular sub-task.
    The model must assess the nature of the sub-task and determine whether internal
    reasoning suffices or whether delegating part of the task to a tool would yield
    better or even necessary results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tool selection and invocation**: This refers to the model’s ability to identify
    which tool is appropriate for a given sub-task and to formulate the correct input
    to trigger its use. This requires the model to understand the functionality and
    input requirements of each available tool and to match these against the demands
    of the current step in the reasoning process. For example, if the task requires
    accessing up-to-date weather information, the model must choose a weather API
    and generate a syntactically correct and semantically relevant query to that API.
    This phase includes formatting inputs, calling the tool, and ensuring that the
    request aligns with both the current problem context and the tool’s capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration of tool outputs**: This describes the model’s capability to interpret
    the results returned by the external tool and incorporate them into the ongoing
    reasoning process. After a tool is invoked and responds with data—such as a numerical
    value, a structured object, or a text snippet—the model must parse the result,
    extract relevant elements, and update its understanding or intermediate outputs
    accordingly. This step often involves interpreting heterogeneous output formats,
    managing type mismatches, and maintaining continuity in the reasoning chain. Effective
    integration ensures that tool use is not isolated but meaningfully contributes
    to solving the broader task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative problem solving**: This refers to the model’s recursive application
    of the previous stages—decomposition, tool reasoning, selection, invocation, and
    integration—in a loop until the task is resolved or further steps become unproductive.
    The model continuously reassesses its progress, determines whether additional
    sub-tasks remain, and decides whether further tool use is necessary. This iterative
    behavior enables the model to handle tasks with dynamic structure, uncertainty,
    or errors from prior steps by adjusting the plan or refining previous actions.
    In agent-based architectures, this process may be explicitly managed by a planner
    or controller, while in prompt-based settings, it often emerges through recursive
    self-queries and prompt augmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we’ll delve into advanced techniques for enabling LLMs to perform
    complex multi-step reasoning and utilize external tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing prompts for complex task decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating external tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing automatic tool selection and use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex problem solving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating multi-step reasoning and tool use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and future directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing prompts for complex task decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To enable effective multi-step reasoning, prompts should guide the LLM to break
    down complex tasks into smaller, manageable steps. Here’s an example of a task
    decomposition prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This function generates a prompt that guides the LLM to decompose a complex
    task into steps, considering the available tools.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating external tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To enable LLMs to use external tools such as search, calculations, API calls,
    and so on, we need to create an interface between the model and the tools. Here’s
    a simple implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the necessary imports and define the `ToolKit` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code defines a `ToolKit` class that organizes and offers access
    to different functionalities through its methods. In the `__init__` method, a
    dictionary named `tools` is initialized with keys representing tool names such
    as `"Twitter API"`, `"Sentiment Analysis"`, and `"Data Visualization"`, and values
    that reference the corresponding methods for fetching tweets, performing sentiment
    analysis using the TextBlob library, and creating data visualizations using Matplotlib.
    The `requests` library is imported for making HTTP requests, while `TextBlob`
    is used for natural language processing tasks such as sentiment analysis, and
    `matplotlib.pyplot` is imported for generating visualizations. The code sets up
    the structure for these tools but is incomplete as the `fetch_tweets`, `analyze_sentiment`,
    and `create_visualization` methods are not defined, leaving room for further implementation
    of these functionalities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define three methods: `fetch_tweets` for generating mock tweets based on a
    query, `analyze_sentiment` for computing sentiment polarity scores for a list
    of texts using TextBlob, and `create_visualization` for creating and saving a
    histogram of the sentiment data with a specified title:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `use_tool` method to execute a specified tool with given arguments
    if it exists in the tools dictionary; otherwise, return an error message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following example demonstrates using a `ToolKit` class to fetch tweets
    about a product launch, analyze their sentiments, create a sentiment visualization,
    and print the path to the generated visualization file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This `ToolKit` class provides an interface for the LLM to interact with external
    tools, simulating API calls and data processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing automatic tool selection and use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To enable LLMs to automatically select and use tools, we can create a system
    that interprets the model’s output and executes the appropriate tools. Here’s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a function, `auto_tool_use`, that uses a pre-trained language
    model and tokenizer from Hugging Face’s Transformers library to decompose a task
    into executable steps using a prompt, parses the decomposition into steps, executes
    tools as needed using a toolkit, and collects the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we generate a final report. The generated report contains the task description,
    a breakdown of each step along with its result, and a concluding summary. The
    model uses the provided steps and results to generate a more cohesive and comprehensive
    narrative of the task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we implement logic to parse the decomposition into structured steps.
    This is a simplified placeholder implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following example usage demonstrates loading a language model and tokenizer
    using `AutoModelForCausalLM` and `AutoTokenizer`, defining a task to analyze tweet
    sentiments and generate a summary report with visualizations, and using an `auto_tool_use`
    function to automate the task via `ToolKit`, with the final report being printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code snippet shows at a high level how to enable the LLM to automatically
    decompose tasks, select appropriate tools, and generate a final report based on
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: The first three sections of this chapter laid the groundwork by covering prompt
    design, integrating external tools, and implementing automatic tool selection
    to enhance AI functionality. In the following section, we will explore how to
    design prompts for complex problem solving.
  prefs: []
  type: TYPE_NORMAL
- en: Complex problem solving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multi-step reasoning and tool use can be applied to various complex problem-solving
    scenarios. Here’s an example of how to use this approach for market analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `market_analysis` function automates the generation of a market research
    report for a given product by constructing a structured task prompt and passing
    it to an external utility, `auto_tool_use`, which is assumed to orchestrate tool-augmented
    responses from a language model. The prompt requests a multi-part analysis—covering
    competitors, sentiment analysis of customer feedback, and visualization of market
    trends—targeted to the specific `product_name` supplied. This design leverages
    the model and toolkit to produce a consolidated report without manual intervention,
    enabling a consistent and repeatable approach to product market research through
    prompt-driven execution.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating multi-step reasoning and tool use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess the effectiveness of multi-step reasoning and tool use, we need to
    evaluate both the process and the outcome. Here’s a simple evaluation framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This evaluation framework assesses both the quality of the generated report
    and the effectiveness of tool use in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and future directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While powerful, multi-step reasoning and tool use in LLMs face several challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tool selection accuracy**: Ensure LLMs choose the most appropriate tools
    for each task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error propagation**: Mitigate the impact of errors in the early steps of
    the reasoning process; keep in mind that error propagation across multiple steps
    can be a major risk in complex tool chains if not mitigated early'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Manage the complexity of integrating a large number of diverse
    tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptability**: Enable LLMs to work with new, unseen tools without retraining'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address some of these challenges, consider implementing a self-correction
    mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Self-correcting in this context refers to a method where a language model iteratively
    refines its output by evaluating and improving its own previous responses without
    external feedback. In the `self_correcting_tooluse` function, this is implemented
    by first generating a report using `auto_tool_use` and then prompting the model
    to assess the quality of that report. If the model’s self-evaluation does not
    include indicators of adequacy—such as “satisfactory” and “no major issues”—the
    evaluation is appended to the task description, effectively guiding the next iteration
    to address identified shortcomings. This loop continues for a set number of attempts
    (`max_attempts`) until the output meets the model’s own acceptance criteria, allowing
    self-guided refinement across multiple passes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify the following three promising research areas for overcoming
    the challenges from some research conducted by AI/ML communities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced tool learning and discovery**: Future LLMs will be able to dynamically
    learn about and integrate new tools without explicit programming. This involves
    mechanisms for understanding tool documentation and API specifications and even
    experimenting with tools to infer their functionality. This will allow LLMs to
    adapt to a constantly evolving landscape of software and services, expanding their
    capabilities beyond a fixed set of pre-defined tools. This will involve techniques
    such as meta-learning, reinforcement learning from tool interactions, and semantic
    understanding of tool descriptions ([https://arxiv.org/abs/2305.17126](https://arxiv.org/abs/2305.17126)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robust and adaptive reasoning with uncertainty**: Future LLMs will incorporate
    probabilistic models to handle uncertainty in multi-step tasks. This means assigning
    probabilities to different reasoning paths, outcomes, and tool effectiveness.
    Bayesian methods, Monte Carlo simulations, and other probabilistic techniques
    will be integrated into the reasoning process. This will enable LLMs to make more
    robust decisions in complex scenarios with incomplete or noisy information and
    to better manage the inherent uncertainty of real-world problems. LLMs will be
    better equipped to handle unexpected situations, recover from errors, and provide
    more reliable solutions when faced with ambiguity ([https://arxiv.org/abs/2310.04406](https://arxiv.org/abs/2310.04406)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human-in-the-loop multi-step reasoning with explainability**: Future systems
    will involve closer collaboration between humans and LLMs in multi-step problem
    solving. This means creating interfaces that allow humans to understand the LLM’s
    reasoning process, provide guidance, correct errors, and work together on complex
    tasks. Explainability will be key, with LLMs able to articulate their reasoning
    steps, justify tool choices, and present alternative solution paths. This will
    foster trust and allow for more effective human-AI collaboration, especially in
    critical domains such as healthcare, finance, and scientific research. This could
    involve visualizations of reasoning graphs, natural language explanations, and
    interactive debugging tools: [https://www.microsoft.com/en-us/research/blog/guidance-for-developing-with-large-language-models-llms/](https://www.microsoft.com/en-us/research/blog/guidance-for-developing-with-large-language-models-llms/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic multi-step reasoning and tool use significantly expand the problem-solving
    capabilities of LLMs, enabling them to tackle complex, real-world tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to design prompts for complex task decomposition
    and implement systems that allow LLMs to interact with external tools and APIs.
    We looked at strategies for automatic tool selection and use and explored applications
    in complex problem-solving scenarios. You also learned how to evaluate the effectiveness
    of multi-step reasoning and tool use in LLMs. By implementing the techniques and
    considerations discussed in this chapter, you can create sophisticated AI systems
    that can decompose problems, leverage external tools, and generate comprehensive
    solutions to multi-faceted challenges.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, the next part of the book will focus on retrieval and knowledge
    integration. This will build upon the tool use capabilities we’ve discussed here,
    exploring how LLMs can be enhanced with external knowledge, improving their ability
    to access and utilize information effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 5: Retrieval and Knowledge Integration in Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We conclude this book by examining techniques that enhance LLMs with external
    knowledge through retrieval-augmented generation (RAG) methods. You will learn
    how to design retrieval systems that efficiently access relevant information,
    integrate structured knowledge into model outputs, and leverage graph-based retrieval
    to enrich responses with contextual relationships. Advanced RAG patterns, such
    as iterative and adaptive retrieval, will be explored, helping you create models
    capable of dynamic knowledge integration. We also discuss evaluation methodologies
    to measure retrieval quality and effectiveness. The final chapter introduces agentic
    patterns, enabling you to build autonomous systems that combine reasoning, planning,
    and decision-making. By mastering these techniques, you will be able to create
    LLMs that are not only informed but also capable of goal-directed behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 26*](B31249_26.xhtml#_idTextAnchor366), *Retrieval-Augmented Generation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 27*](B31249_27.xhtml#_idTextAnchor378), *Graph-Based RAG*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 28*](B31249_28.xhtml#_idTextAnchor389), *Advanced RAG*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 29*](B31249_29.xhtml#_idTextAnchor400), *Evaluating RAG Systems*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 30*](B31249_30.xhtml#_idTextAnchor469), *Agentic Patterns*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
