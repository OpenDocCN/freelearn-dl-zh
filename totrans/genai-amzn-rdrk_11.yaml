- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating and Monitoring Models with Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To work with the best-performing model for your generative AI solution, you
    need to evaluate the models that are available to you. This chapter explores various
    techniques to assess the performance of different models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter introduces two primary evaluation methods provided by Amazon Bedrock:
    automatic model evaluation and human evaluation. We will do a detailed walk-through
    of these two methods. In addition, we will look at open source tools such as **Foundation
    Models Evaluation** (**FMEval**) and **RAG Assessment** (**Ragas**) for model
    evaluation and evaluating RAG pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the chapter goes into monitoring. We will explore how to
    leverage Amazon CloudWatch for real-time monitoring of model performance, latency,
    and token counts. We will further look at model invocation logging to capture
    requests, responses, and metadata for model invocations. Furthermore, we will
    highlight the integration of Amazon Bedrock with AWS CloudTrail for auditing API
    calls and with Amazon EventBridge for event-driven monitoring and automation of
    model customization jobs.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand how to evaluate FMs
    and monitor their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring Amazon Bedrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have access to an AWS account. If you don’t have
    it already, you can go to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    and create an AWS account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, you will need to set up AWS Python SDK (Boto3): [https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can carry out the Python setup in any way: install it on your local machine,
    or use AWS Cloud9, or utilize AWS Lambda, or leverage Amazon SageMaker.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There will be a charge associated with the invocation and customization of the
    FMs of Amazon Bedrock. Please refer to [https://aws.amazon.com/bedrock/pricing/](https://aws.amazon.com/bedrock/pricing/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, we have gained a comprehensive understanding of Amazon Bedrock’s capabilities,
    exploring techniques such as prompt engineering, RAG, and model customization.
    We have also examined various architectural design patterns and analyzed the responses
    generated by different models. With the vast selection of FMs available within
    Amazon Bedrock, identifying the most suitable option for your specific use case
    and business requirements can be challenging. To address this, we will now focus
    on the topic of model evaluation and how to compare the outputs of different models
    to choose the one that best meets the needs of your application and business.
    This is a critical initial phase in implementing any generative AI solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – The generative AI life cycle](img/B22045_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – The generative AI life cycle
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 11**.1*, after defining the specific business use case that
    you’re aiming to solve with generative AI, the **choice** stage involves both
    selecting potential models from the available options and rigorously evaluating
    these candidate models. Following this, the **responsible AI** stage focuses on
    ensuring data privacy and security, as well as implementing guardrails for responsible
    model behavior, which we will cover in [*Chapter 12*](B22045_12.xhtml#_idTextAnchor226).
  prefs: []
  type: TYPE_NORMAL
- en: Before talking about model evaluation, one quick way to compare the responses
    from the models on Amazon Bedrock’s **Chat playground** screen is to use the **Compare**
    **mode** toggle.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Compare mode in Bedrock’s Chat playground](img/B22045_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Compare mode in Bedrock’s Chat playground
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Model metrics in Compare mode](img/B22045_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Model metrics in Compare mode
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 11**.2*, you can enable compare mode in **Chat playground**
    and add up to three models to view a side-by-side comparison of model responses
    to the same prompt. In addition, you can view the metrics of each of the selected
    models (as shown in *Figure 11**.3*) and compare latency, input token count, output
    token count, and the associated cost. We will cover the model metrics in depth
    in the *Monitoring Amazon* *Bedrock* section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Bedrock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within Amazon Bedrock, you can create model evaluation jobs to compare the responses
    from the models for use cases such as text generation, summarization, Q&A, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model evaluation within Amazon Bedrock primarily consists of two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic model evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us dive deeper into both of these evaluation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With automatic model evaluation, an evaluation algorithm script is run behind
    the scenes on either Amazon Bedrock’s provided **built-in dataset** or on a **custom
    dataset** that you provide for the recommended metrics (accuracy, toxicity, and
    robustness). Let us go through the steps for creating an automatic model evaluation
    job:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation name**: This refers to choosing a descriptive name that accurately
    reflects the purpose of the job. This name should be unique within your AWS account
    in the specific AWS region.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model selector**: Select the model that you would like to evaluate (as shown
    in *Figure 11**.4*). At the time of writing this book, automatic model evaluation
    is carried out on a single model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Model selector](img/B22045_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Model selector
  prefs: []
  type: TYPE_NORMAL
- en: Within **Model selector**, you can optionally modify the inference parameters,
    such as **Temperature**, **Top P**, **Response length**, and so on (as shown in
    *Figure 11**.5*). You can get this screen by clicking on **Inference configuration:**
    **Default update**.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the values of this inference configuration will alter the output of
    the model. To learn more about the inference configuration parameters, you can
    go back to [*Chapter 2*](B22045_02.xhtml#_idTextAnchor034) of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Inference configuration](img/B22045_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Inference configuration
  prefs: []
  type: TYPE_NORMAL
- en: '**Task type**: Currently, the following task types are supported while using
    Amazon Bedrock Model evaluation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: General text generation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Question and answer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics and datasets**: There are a total of three metrics provided by Amazon
    Bedrock that you can choose to use to measure the performance of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accuracy**: The capability to encode factual knowledge about the real world
    is a critical aspect of generative AI models. This metric evaluates the model’s
    ability to generate outputs that align with established facts and data. It assesses
    the model’s understanding of the subject matter and its ability to synthesize
    information accurately. A high accuracy score indicates that the model’s outputs
    are reliable and can be trusted for tasks that require factual precision.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toxicity**: This refers to the propensity of the model to generate harmful,
    offensive, or inappropriate content. This metric gauges the model’s tendency to
    produce outputs that could be considered unethical, biased, or discriminatory.
    Evaluating toxicity is crucial for ensuring the responsible and ethical deployment
    of AI systems, particularly in applications that involve direct interaction with
    users or the dissemination of information to the public.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: Robustness is a measure of the model’s resilience to minor,
    semantic-preserving changes in the input data. It assesses the degree to which
    the model’s output remains consistent and reliable when faced with slight variations
    or perturbations in the input. This metric is particularly important for generative
    AI models that operate in dynamic or noisy environments, where the input data
    may be subject to minor fluctuations or disturbances. A robust model is less likely
    to produce erratic or inconsistent outputs in response to small input changes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The text classification task supports the accuracy and robustness metrics, while
    the other tasks support all three metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'For every task type and metric that you choose, Amazon Bedrock provides you
    with built-in datasets. For example, for the general text generation task type,
    you will get the following built-in datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TREX: [https://hadyelsahar.github.io/t-rex/](https://hadyelsahar.github.io/t-rex/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BOLD: [https://github.com/amazon-science/bold](https://github.com/amazon-science/bold)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'WikiText2: [https://huggingface.co/datasets/wikitext](https://huggingface.co/datasets/wikitext)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'English Wikipedia: [https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RealToxicityPrompts: [https://github.com/allenai/real-toxicity-prompts](https://github.com/allenai/real-toxicity-prompts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a complete list of built-in datasets based on different metrics and task
    types, you can go through [https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-builtin.html](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-prompt-datasets-builtin.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you would like to use your own custom dataset, it needs to be *JSON line*
    (*.jsonl*) format. Each line within the dataset must be a valid JSON object and
    you can include up to 1,000 prompts per evaluation job.
  prefs: []
  type: TYPE_NORMAL
- en: 'To construct your custom prompt dataset, you’ll need to incorporate the following
    keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt**: This key is mandatory and serves as the input for various tasks,
    such as general text generation, question answering, text summarization, and classification.
    Depending on the task, the value associated with this key will vary – it could
    be a prompt for the model to respond to, a question to answer, text to summarize,
    or content to classify.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`referenceResponse` is required to provide the ground truth response against
    which your model’s output will be evaluated. For tasks such as question answering,
    accuracy evaluation, and robustness testing, this key will hold the correct answer
    or expected response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`category` key. This optional key allows you to group prompts and their corresponding
    reference responses, enabling a more granular analysis of your model’s performance
    across different domains or categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate the usage of these keys, consider the following example for a
    question answering task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this JSON line, the `prompt` key contains the `What is the process that converts
    raw materials into finished goods?` question, while the `referenceResponse` key
    holds the correct answer, `Manufacturing`. Additionally, the category key is set
    to `Manufacturing`, allowing you to group this prompt and response with others
    related to manufacturing.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have created a custom prompt dataset, you will need to store the dataset
    file in an Amazon S3 bucket and specify the correct S3 path (such as **s3://test/data/**)
    when creating the model evaluation job (as shown in *Figure 11**.6*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Choosing a prompt dataset](img/B22045_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Choosing a prompt dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that the S3 bucket should have the following **Cross-Origin Resource
    Sharing** (**CORS**) policy attached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The CORS policy is a set of rules that specify which origins (domains or websites)
    are allowed to access the S3 bucket. To learn more about CORS, you can check [https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html).
  prefs: []
  type: TYPE_NORMAL
- en: By meticulously crafting your custom prompt dataset, you can ensure that your
    LLMs are thoroughly evaluated against a diverse range of scenarios, covering various
    tasks, domains, and complexity levels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Metrics and datasets](img/B22045_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Metrics and datasets
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.7* depicts the **Metrics and Datasets** options for the general
    text generation task type.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at the other parameters that you can specify while creating the
    model evaluation job:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation results**: Here, you can specify the S3 path where the results
    of the evaluation job should be stored. We will cover the evaluation results in
    the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IAM role and KMS key**: Certain permissions are required to perform actions
    such as accessing data from an S3 bucket or storing the evaluation results. Here
    is the policy that is needed, at minimum, for an automatic model evaluation job:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can find details on the permissions needed for a model evaluation job at
    [https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you start the model evaluation job, you can view the results for each of
    the metrics as shown in *Figure 11**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – An evaluation summary](img/B22045_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – An evaluation summary
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the metrics results are stored in the S3 bucket that you have specified,
    as shown in *Figure 11**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Metrics results in the S3 bucket](img/B22045_11_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Metrics results in the S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand how the evaluation is performed for each of the task types.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For text generation task type, here is how the evaluation is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This metric is evaluated using the **Real World Knowledge** (**RWK**)
    score, which assesses the model’s ability to understand the real world. The RWK
    score measures the extent to which a language model can produce output that is
    consistent with real-world facts and common sense. It assesses the model’s ability
    to reason about the physical world, understand social norms, and avoid generating
    nonsensical or contradictory statements. A high RWK score indicates that the model
    is performing accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: Semantic robustness is the metric used to measure robustness
    in this task type. It is calculated using the word error rate, which quantifies
    how much the model’s output changes in response to minor, semantic-preserving
    perturbations in the input. A low semantic robustness score signifies that the
    model is performing well, as it is robust to such perturbations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toxicity**: This metric is calculated using the detoxify algorithm ([https://github.com/unitaryai/detoxify](https://github.com/unitaryai/detoxify)),
    which measures the presence of toxic content in the model’s output. A low toxicity
    value indicates that the selected model is not generating a significant amount
    of harmful or offensive content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For text summarization task type, here is how the evaluation is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: The BERTScore is used to evaluate accuracy in this task type.
    It is calculated using pre-trained contextual embeddings from BERT models and
    matches words in candidate and reference sentences by cosine similarity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: This metric is expressed as a percentage and is calculated
    by taking the difference between the BERTScores of a perturbed prompt and the
    original prompt, which is then divided by the BERTScore of the original prompt
    and multiplied by 100\. The lower the score, the more robust the selected model
    is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toxicity**: As with the general text generation task type, the detoxify algorithm
    is used to calculate the toxicity of the model’s output, with a low value indicating
    minimal toxic content generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question and answer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For question and answer task type, here is how the evaluation is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: The F1 score is used to evaluate accuracy in this task type.
    It is calculated by dividing the precision score (the ratio of correct predictions
    to all predictions) by the recall score (the ratio of correct predictions to the
    total number of relevant predictions). Higher F1 scores indicate better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: This metric is expressed as a percentage and is calculated
    by taking the difference between the F1 scores of a perturbed prompt and the original
    prompt, which is then divided by the F1 score of the original prompt and multiplied
    by 100\. A lower score indicates that the selected model is more robust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toxicity**: As with the other task types, the detoxify algorithm is used
    to calculate the toxicity of the model’s output, with a low value indicating minimal
    toxic content generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For text classification task type, here is how the evaluation is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: In this task type, accuracy is calculated by comparing the predicted
    class to its ground truth label. A higher accuracy score indicates that the model
    is correctly classifying text based on the provided ground truth labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: This metric is expressed as a percentage and is calculated
    by taking the difference between the classification accuracy scores of a perturbed
    prompt and the original prompt, which is then divided by the classification accuracy
    score of the original prompt and multiplied by 100\. A lower score indicates that
    the selected model is more robust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us analyze the output of the accuracy and robustness metrics for the built-in
    dataset from T-Rex: ([https://hadyelsahar.github.io/t-rex/](https://hadyelsahar.github.io/t-rex/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that a prompt (`al-Hira is the capital of`) is provided to
    the Anthropic Claude v2 model, and that the model’s response (`Al-Hira was an
    ancient city in Mesopotamia...`) is assessed against a reference response (`Lakhmid`).
    The evaluation computes scores for metrics such as accuracy and robustness, providing
    insights into the model’s performance on this specific input.
  prefs: []
  type: TYPE_NORMAL
- en: Using human evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Human evaluation allows you to incorporate human input into the evaluation
    process, so the models are not only accurate but also align with real-world expectations
    and requirements. There are two types of human evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: Bringing your own work team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using an AWS-managed work team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bringing your own work team
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to automatic model evaluation, when you choose human evaluation with
    your own work team, Amazon Bedrock guides you through a straightforward setup
    process, allowing you to select the models you want to evaluate, the task type
    (for example, text summarization), and the evaluation metrics. It also shows you
    how to upload your custom prompt dataset. Let us consider the step-by-step process
    for setting up human evaluation by bringing your own team:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evaluation name**: Choose a descriptive name that accurately represents the
    purpose of the job. This name should be unique within your AWS account in the
    specific AWS region. Along with the name, you can optionally provide the description
    and tags.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model selector**: Select the model that you would like to evaluate (as shown
    in *Figure 11**.10*). At the time of writing this book, human model evaluation
    with bringing your own team can only performed on up to two models. Within the
    model selector, you can optionally modify the inference parameters such as temperature,
    Top P, response length, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.10 – The model selector](img/B22045_11_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – The model selector
  prefs: []
  type: TYPE_NORMAL
- en: '**Task Type**: Currently, the following task types are supported in this mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: General text generation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Question and answer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The last task type of these, Custom, allows you to specify custom evaluation
    metrics that the human workers can use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Based on the task type, you will see the list of evaluation metrics and rating
    methods that you would have to choose from, as shown in *Figures 11.11* and *11.12*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Evaluation metrics](img/B22045_11_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Evaluation metrics
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Rating method options](img/B22045_11_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Rating method options
  prefs: []
  type: TYPE_NORMAL
- en: '**Specifying paths**: Next, you will need to specify the s3 path to your custom
    prompt dataset. As we have seen in the previous subsection, a custom prompt dataset
    needs to be in the *.jsonl* format. Here is an example of the custom prompt dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note that the s3 path to your dataset requires you to have your **Cross
    Origin Resource Sharing (CORS)** settings configured as shown in *Figure 11**.13*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.13 – The CORS policy window](img/B22045_11_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – The CORS policy window
  prefs: []
  type: TYPE_NORMAL
- en: To learn more, you can visit [https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security-cors.html](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security-cors.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**IAM role and KMS key**: Certain permissions are required to perform actions
    such as accessing data from an S3 bucket or storing the evaluation results. You
    can find more details on the IAM permissions needed for the model evaluation job
    at [https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation-security.html?icmpid=docs_bedrock_help_panel_model_evaluation).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Setting up a work team**: Next, you will need to set up a work team (as shown
    in *Figure 11**.14*). This involves inviting the appropriate team members. The
    console provides you with sample email templates for inviting new workers and
    existing workers, which you can use as a reference when sending out the invitations.
    The worker receives the link to the private worker portal where they complete
    the labeling task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Setting up a custom work team](img/B22045_11_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Setting up a custom work team
  prefs: []
  type: TYPE_NORMAL
- en: Next, you would need to specify the instructions (as shown in *Figure 11**.15*)
    of the task to the workers. These instructions will be visible to the human workers
    in the private worker portal where they will perform the labeling task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Providing instructions to human workers](img/B22045_11_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Providing instructions to human workers
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have reviewed and created the job, the human worker team will receive
    an email along with the link to the portal to perform the task. Once the task
    has been completed by the workers, Amazon Bedrock will provide an evaluation report
    card. Let us understand the report card in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1` to `5`. When using this method, it is essential to provide clear instructions
    that define the meaning of each rating point. For instance, a rating of `1` could
    indicate a poor or irrelevant response, while a rating of `5` could signify an
    excellent and highly relevant output. The results are then presented as a histogram
    showcasing the distribution of ratings across the dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the scale points explicitly (for example, 1 = poor, 2 = fair, 3 = good,
    4 = very good, 5 = excellent)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide clear guidelines for evaluators on how to interpret and apply the scale
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Present the results as a histogram, allowing for easy visual interpretation
    of the rating distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choice buttons**: When you choose the choice buttons method, evaluators are
    presented with two model responses and asked to select their preferred option.
    This approach is particularly useful when comparing the performance of multiple
    models on the same task. The results are typically reported as a percentage, indicating
    the proportion of responses that evaluators preferred for each model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` (most preferred). This method provides a more nuanced understanding of
    the relative performance of different models. The results are presented as a histogram
    showing the distribution of rankings across the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thumbs up/down**: Evaluators rate each response as acceptable or unacceptable.
    The final report showcases the percentage of responses that received a **thumbs-up**
    rating for each model, enabling a straightforward assessment of acceptability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another form of human evaluation method is through an AWS-managed work team.
  prefs: []
  type: TYPE_NORMAL
- en: Using an AWS-managed work team
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you opt for an AWS-managed team, you can simply describe your model evaluation
    needs, including the task type, expertise level required, and approximate number
    of prompts. Based on these details, an AWS expert will then reach out to discuss
    your project requirements in detail, providing a custom quote and project timeline
    tailored to your specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.16* shows how you can create a managed workforce with all the
    details such as task type and expertise required.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – AWS-managed team for model evaluation](img/B22045_11_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – AWS-managed team for model evaluation
  prefs: []
  type: TYPE_NORMAL
- en: An AWS-managed workforce is useful when you do not want to manage or assign
    tasks to your own workforce and when you require an AWS team to perform evaluations
    on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from using Bedrock’s model evaluation job, there are other open source
    techniques that you can utilize for model evaluation, such as `fmeval` and Ragas.
  prefs: []
  type: TYPE_NORMAL
- en: FMEval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FMEval is the open source library made available by AWS, which you can access
    at [https://github.com/aws/fmeval](https://github.com/aws/fmeval).
  prefs: []
  type: TYPE_NORMAL
- en: This library enables comprehensive evaluation of LLMs across various aspects
    such as accuracy, toxicity, semantic robustness, and prompt stereotyping. It offers
    a range of algorithms tailored for assessing LLMs’ performance on different tasks,
    ensuring a thorough understanding of their capabilities and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you plan to use your own dataset for evaluation, you’ll need to configure
    a `DataConfig` object, like in the following code block. This object specifies
    the dataset name, URI, and MIME type, as well as the locations of the input prompts,
    target outputs, and other relevant columns. By customizing the `DataConfig` object,
    you can tailor the evaluation process to your specific dataset and task requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The library provides a flexible `ModelRunner` interface, allowing for seamless
    integration with Amazon Bedrock, and is used to perform invocations on the model.
    The following code block shows how an invocation can be carried out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: If you want to learn more about `fmeval`, you can visit [https://github.com/aws/fmeval/tree/main](https://github.com/aws/fmeval/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, you can try out `fmeval` with Amazon Bedrock. Here are some sample
    examples that you can test with Anthropic Claude v2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-factual-knowledge.ipynb](https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-factual-knowledge.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-summarization-accuracy.ipynb](https://github.com/aws/fmeval/blob/main/examples/bedrock-claude-summarization-accuracy.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ragas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ragas is a framework designed to assess the performance of your RAG pipelines,
    which combine language models with external data sources to enhance their output.
    It offers practical tools grounded in the latest research to analyze the text
    generated by your language model, giving you valuable insights into the effectiveness
    of your RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key features and benefits of using Ragas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated evaluation metrics**: Ragas offers a suite of automated metrics
    tailored specifically for assessing the quality of RAG-generated text. These metrics
    go beyond traditional measures such as perplexity and BLEU, providing a more nuanced
    understanding of the generated output’s coherence, relevance, and factual accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizable evaluation strategies**: Recognizing that every RAG pipeline
    is unique, Ragas allows for flexible and customizable evaluation strategies. You
    can tailor the evaluation process to align with your specific use case, data domain,
    and performance requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration with CI/CD pipelines**: Ragas is designed to integrate with **CI/CD**
    (**Continuous Integration and Continuous Deployment**) pipelines. This integration
    enables continuous monitoring and evaluation of your RAG pipeline’s performance,
    ensuring that any deviations or regressions are promptly detected and addressed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretable insights**: Ragas generates interpretable and actionable insights,
    highlighting areas where your RAG pipeline excels and identifying potential weaknesses
    or bottlenecks. These insights can guide your optimization efforts, helping you
    refine and enhance your pipeline’s performance iteratively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ragas provides a list of metrics that you can import. Here is how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'These metrics can then be passed to the `evaluate` function in Ragas, along
    with the Bedrock model and embeddings. Here is how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, `df` is assumed to be a pandas DataFrame containing
    the data you want to evaluate. Note that `llm=bedrock_model` and `embeddings=bedrock_embeddings`
    are the instances of the Bedrock and embeddings models, respectively, that we
    have created beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: For the complete tutorial on how to use Amazon Bedrock with Ragas, you can go
    to [https://docs.ragas.io/en/stable/howtos/customisations/aws-bedrock.html](https://docs.ragas.io/en/stable/howtos/customisations/aws-bedrock.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen various techniques to perform Amazon Bedrock model evaluation,
    let us look at monitoring and logging solutions integrated with Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring the performance and usage of your generative AI applications is crucial
    for ensuring optimal functionality, maintaining security and privacy standards,
    and gaining insights for future enhancements. Amazon Bedrock seamlessly integrates
    with Amazon CloudWatch, CloudTrail, and EventBridge, which provides a comprehensive
    monitoring and logging solution.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon CloudWatch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon CloudWatch is a monitoring and observability service that collects and
    visualizes data from various AWS resources, including Amazon Bedrock. By leveraging
    CloudWatch, you can gain valuable insights into your Bedrock models’ performance
    so you can identify and address potential issues proactively. Through CloudWatch,
    you can track usage metrics and construct tailored dashboards for auditing purposes,
    ensuring transparency and accountability throughout the AI model development process.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of using CloudWatch with Amazon Bedrock is that you
    can gain insights into model usage across multiple accounts and FMs within a single
    account. You can monitor critical aspects such as model invocations and token
    counts, so you can make informed decisions and optimize resource allocation effectively.
    If you would like to configure monitoring across multiple accounts in one or more
    regions, you can check the Amazon CloudWatch documentation at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Cross-Account-Methods.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Cross-Account-Methods.html).
  prefs: []
  type: TYPE_NORMAL
- en: This provides all the steps that you will need to take to enable this feature.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Bedrock offers a feature called model *invocation logging*. This
    functionality allows users to collect metadata, requests, and responses for all
    model invocations within the account. While this feature is disabled by default,
    you can easily enable it by going to **Settings** in the Bedrock console and toggling
    **Model invocation logging**. By enabling this, you allow Bedrock to publish invocation
    logs for enhanced visibility and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look at how CloudWatch can be leveraged to monitor Bedrock in near real-time,
    utilizing metrics and logs to trigger alarms and initiate actions when predefined
    thresholds are exceeded.
  prefs: []
  type: TYPE_NORMAL
- en: Bedrock metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon Bedrock’s CloudWatch metrics cover a wide range of performance indicators,
    including the number of invocations, invocation latency, invocation client and
    server errors, invocation throttling instances, input and output tokens, and much
    more. You can see the full list of supported metrics at [https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring-cw.html#runtime-cloudwatch-metrics](https://docs.aws.amazon.com/bedrock/latest/userguide/monitoring-cw.html#runtime-cloudwatch-metrics).
  prefs: []
  type: TYPE_NORMAL
- en: With these metrics, you can compare latency between different models and measure
    token counts to assist in purchasing provisioned throughput, as well as detect
    and alert for throttling events.
  prefs: []
  type: TYPE_NORMAL
- en: When you are using a chat playground, you can view these metrics after running
    the prompt, as shown in *Figure 11**.17*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Model metrics](img/B22045_11_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Model metrics
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you can define metric criteria, which allow you to provide specific
    conditions or thresholds for the model metrics. You can set criteria such as **latency
    less than 100ms** or **output token count greater than 500** based on your requirements.
    These criteria can be used to evaluate and compare the performance of different
    models against your desired metrics. When comparing multiple models, setting metric
    criteria helps identify which models meet or fail to meet your specified conditions,
    which helps in the selection of the most suitable model for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let us also look at these metrics in the CloudWatch metrics dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.18 – The CloudWatch metrics dashboard](img/B22045_11_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – The CloudWatch metrics dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 11**.18*, you can see the CloudWatch metrics for the Anthropic Claude
    3 Sonnet model: **Invocations** (sample count), **InvocationLatency** (in milliseconds),
    **OutputTokenCount** (sample count), and **InputTokenCount** (sample count). Let
    us understand what these terminologies (metrics and statistics) mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample count**: This statistic represents total data points or observations
    recorded within a specified timeframe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Converse`, `ConverseStream`, `InvokeModel`, and `InvokeModelWithResponseStream`
    APIs within a given time frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**InvocationLatency**: This metric refers to the delay or amount of time that
    has elapsed between when the invocation request is made and when the response
    is received.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OutputTokenCount** and **InputTokenCount** are useful metrics when you are
    analyzing and calculating the cost of model invocations. A token is essentially
    a small group of characters from an input prompt and the response. **OutputTokenCount**
    signifies the total count of tokens in the response provided by the model, whereas
    **InputTokenCount** signifies the total count of tokens in the input and prompt
    provided to the model.'
  prefs: []
  type: TYPE_NORMAL
- en: To streamline monitoring and analysis, Bedrock’s logs and metrics can be presented
    in a single view using CloudWatch dashboards. These dashboards provide a comprehensive
    overview of the same KPIs, including the number of invocations over time by model,
    invocation latency by model, and token counts for input and output. The following
    figure shows the dashboard view of the metrics in a one-week time frame for the
    two models, Anthropic Claude v2 and Anthropic Claude v3 Sonnet.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – The CloudWatch dashboard](img/B22045_11_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – The CloudWatch dashboard
  prefs: []
  type: TYPE_NORMAL
- en: For organizations with multiple AWS accounts, Bedrock supports CloudWatch cross-account
    observability, enabling the creation of rich cross-account dashboards in monitoring
    accounts. This feature ensures a centralized view of performance metrics across
    various accounts, facilitating better oversight and decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Model invocation logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model invocation logging allows you to capture and analyze the requests and
    responses generated by the models, along with the metadata of all the invocation
    calls that are made. It provides a comprehensive view of how your models are utilized,
    enabling you to monitor their performance, identify potential issues, and optimize
    their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling model invocation logging is a straightforward process. You can configure
    it through the Amazon Bedrock console or via the API.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – Enabling model invocation logging](img/B22045_11_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – Enabling model invocation logging
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.20* shows the console view of **Model invocation logging** in
    the Amazon Bedrock console. You will need to enable the feature. The first step
    is to choose the type of data you want to log, such as text, images, or embeddings.
    Next, you’ll need to select the destination for your logs, which can be Amazon
    S3, Amazon CloudWatch Logs, or both, and provide the path.'
  prefs: []
  type: TYPE_NORMAL
- en: If you opt for Amazon S3, your logs will be stored as compressed JSON files,
    each containing a batch of invocation records. These files can be queried using
    Amazon Athena or sent to various AWS services such as Amazon EventBridge. On the
    other hand, if you choose Amazon CloudWatch Logs, your invocation logs will be
    delivered to a specified log group as JSON events. This allows you to leverage
    CloudWatch log insights for querying and analyzing your logs in real time.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of model invocation logging is its ability to capture
    large input and output data. For data exceeding 100 KB , or data in binary formats
    (for example, images, audio, and so on), Amazon Bedrock automatically uploads
    it to your designated Amazon S3 bucket. This ensures that no valuable information
    is lost, even for large or non-text data.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging this feature, you can optimize your models, identify potential
    issues, and ensure that your systems are operating efficiently and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample model invocation logs in the CloudWatch logs console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The preceding log snippet provides the `ConverseStream` conversational request
    made to the `anthropic.claude-v2` model. It captures various data points such
    as the input prompt, output response, performance metrics, and usage statistics.
    With this comprehensive logging, you can perform effective analysis and evaluation
    of the model’s capabilities and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: AWS CloudTrail
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS CloudTrail is a compliance and auditing service that allows you to capture
    and analyze all API calls made within your AWS environment. Here’s how you can
    leverage CloudTrail to gain invaluable insights.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock seamlessly integrates with AWS CloudTrail, capturing every API
    call as an event. These events encompass actions initiated from the Amazon Bedrock
    console, as well as programmatic calls made through the Amazon Bedrock API operations.
    In CloudTrail, you gain a comprehensive record of who initiated the request, the
    source IP address, the timestamp, and additional details surrounding the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Bedrock logs two distinct categories of events with CloudTrail: data
    events and management events. When it comes to data events, CloudTrail doesn’t
    log Amazon Bedrock Runtime API operations (`InvokeModel` and `InvokeModelWithResponseStream`)
    as data events by default. However, it does log all actions related to agents
    for Amazon Bedrock Runtime API operations, categorized as data events:'
  prefs: []
  type: TYPE_NORMAL
- en: To log `InvokeAgent` calls, you would need to configure advanced event selectors
    on the CloudTrail trail to record data events for the `AWS::Bedrock::AgentAlias`
    resource type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To log `Retrieve` and `RetrieveAndGenerate` calls, configure advanced event
    selectors to record data events for the `AWS::Bedrock::KnowledgeBase` resource
    type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced Event Selector enables the creation of precision and granular filters
    for monitoring and managing CloudTrail activities related to both management and
    data events.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data events** provide insights into resource operations such as reading or
    writing to resources such as Amazon Bedrock Knowledge Bases or agent aliases.
    These events are not logged by default due to their high volume but logging can
    be enabled through advanced event selectors.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, **management events** capture control plane operations such
    as API calls for creating, updating, or deleting Amazon Bedrock resources. CloudTrail
    automatically logs these management events, providing a comprehensive audit trail
    of administrative activities within your Amazon Bedrock environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to learn more about CloudTrail, please check the AWS documentation:
    [https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging AWS CloudTrail in conjunction with Amazon Bedrock provides you with
    a powerful auditing and monitoring solution. By capturing and analyzing API calls,
    you can maintain visibility in your Amazon Bedrock environment, ensure adherence
    to best practices, and promptly address any potential security or operational
    concerns.
  prefs: []
  type: TYPE_NORMAL
- en: EventBridge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon EventBridge provides a solution for tracking and responding to events
    in near-real time. It acts as a centralized event bus, ingesting and processing
    state change data from various sources including Amazon Bedrock. Whenever there’s
    a shift in the status of a model customization job you’ve initiated, Bedrock publishes
    a new event to EventBridge. This event contains detailed information about the
    job, such as its current state, output model ARN, and any failure messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can harness the power of Amazon EventBridge to monitor Amazon
    Bedrock events effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Event streaming and delivery**: Amazon Bedrock emits events on a best-effort
    basis whenever there’s a state change in a model customization job you’ve initiated.
    These events are streamed to Amazon EventBridge, which acts as a centralized event
    bus, ingesting and processing event data from various AWS services and external
    sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event pattern matching**: Within Amazon EventBridge, you can create rules
    that define event patterns based on specific criteria such as the source service,
    event type, or job status. By crafting rules tailored to your needs, you can filter
    and capture only the events that are relevant to your Amazon Bedrock workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated responses and integrations**: Once an event matches a rule you’ve
    defined, Amazon EventBridge routes it to one or more targets you’ve specified.
    These targets can be various AWS services such as AWS Lambda functions, Amazon
    **Simple Queue Service** (**SQS**) queues, or Amazon **Simple Notification Service**
    (**SNS**) topics. With this flexibility, you can trigger automated actions, invoke
    downstream workflows, or receive notifications based on event data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and alerting**: One common use case for Amazon EventBridge is
    setting up alerting mechanisms for critical events. For instance, you can configure
    a rule to send an email notification to a designated address whenever a model
    customization job fails, enabling you to promptly investigate and address the
    issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event data enrichment**: The event data emitted by Amazon Bedrock contains
    valuable information about the model customization job, such as the job ARN, output
    model ARN, job status, and failure messages (if applicable). By leveraging this
    data, you can build robust monitoring and alerting systems tailored to your specific
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To receive and process Amazon Bedrock events through Amazon EventBridge, you
    will need to create *rules* and *targets*. Rules define the event patterns to
    match, while targets specify the actions to be taken when an event matches a rule.
    Let’s learn more about how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a rule, follow the ensuing steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open the Amazon EventBridge console.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Create rule**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a name for your rule.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Event pattern**, as shown in *Figure 11**.21*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the event pattern to match Amazon Bedrock events (for instance, set **source**
    to **aws.bedrock** and **detail-type** to **Model Customization Job** **State
    Change**).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.21 – The Event pattern window](img/B22045_11_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – The Event pattern window
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the ensuing steps for configuring targets:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the target type (for example, AWS Lambda, Amazon SNS, Amazon SQS).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the target resource (for example, a Lambda function ARN or an SNS topic
    ARN).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, add additional configurations or transformations for the target.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One practical use case is to receive email notifications whenever there is
    a change in the status of your model customization jobs. Here’s how you can set
    it up:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an Amazon SNS topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subscribe your email address to the SNS topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create an Amazon EventBridge rule with the following event pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '{'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '"source": ["aws.bedrock"],'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '"detail-type": ["Model Customization Job State Change"]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set the SNS topic as the target for the rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this set up, you’ll receive email notifications whenever there is a state
    change in your Amazon Bedrock model customization jobs, keeping you informed about
    job progress and potential failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the Amazon EventBridge integration with Amazon Bedrock opens
    up various advanced use cases, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Triggering Lambda functions to perform custom actions based on job events (for
    example, sending notifications to a Slack channel, updating a dashboard, or triggering
    downstream workflows)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating with Amazon Step Functions to orchestrate complex workflows based
    on job events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending job events to Amazon Kinesis data streams for real-time processing and
    analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Archiving job events in Amazon S3 or Amazon CloudWatch logs for auditing and
    compliance purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By leveraging Amazon EventBridge to monitor and respond to Amazon Bedrock events,
    you can enhance the security, automation, and visibility of your machine learning
    operations. With the ability to define custom rules and integrate with various
    AWS services, you can create a robust and secure environment tailored to your
    specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about various methods for evaluating and monitoring
    the Amazon Bedrock models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We began by exploring the two primary model evaluation methods offered by Amazon
    Bedrock: automatic model evaluation and human evaluation. The automatic model
    evaluation process involves running an evaluation algorithm script on either a
    built-in or custom dataset, assessing metrics such as accuracy, toxicity, and
    robustness. Human evaluation, on the other hand, incorporates human input into
    the evaluation process, ensuring that the models not only deliver accurate results
    but also that those results align with real-world expectations and requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we discussed open source tools such as `fmeval` and Ragas, which
    provide additional evaluation capabilities that are specifically tailored for
    LLMs and RAG pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to the section on monitoring, we discussed how Amazon CloudWatch can
    be leveraged to gain valuable insights into model performance, latency, and token
    counts. We explored the various metrics provided by Amazon Bedrock and considered
    how they can be visualized and monitored through CloudWatch dashboards. Additionally,
    we covered model invocation logging, a powerful feature that allows you to capture
    and analyze requests, responses, and metadata for all model invocations. Next,
    we looked at the integration of Amazon Bedrock with AWS CloudTrail and EventBridge.
    CloudTrail provides a comprehensive audit trail of API calls made within your
    AWS environment, enabling you to monitor and ensure adherence to best practices.
    EventBridge, on the other hand, allows you to track and respond to events in near-real
    time, enabling automated responses and integrations based on the state changes
    of your model customization jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring security and privacy is the top priority at Amazon and also in today’s
    digital landscape. In the next chapter, we are going to look at how security and
    privacy can be ensured in Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
