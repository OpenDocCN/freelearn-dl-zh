- en: '<title>LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem</title>'
  prefs: []
  type: TYPE_NORMAL
- en: 'LlamaIndex: The Hidden Jewel - An Introduction to the LlamaIndex Ecosystem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you’ve got a solid understanding of what **large language models**
    ( **LLMs** ) are and what they can (and cannot) do. It’s time to discover how
    **LlamaIndex** can take your interactive AI applications to the next level. We’ll
    explore how **retrieval-augmented generation** ( **RAG** ) using LlamaIndex can
    provide the missing link between the vast knowledge of LLMs and your proprietary
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing language models – The symbiosis of fine-tuning, RAG, and LlamaIndex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering the advantages of progressively disclosing complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing **personalized intelligent tutoring system** ( **PITS** ) – our
    hands-on LlamaIndex project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing our coding environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Familiarizing ourselves with the structure of the LlamaIndex code repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <title>Technical requirements</title>
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following elements will be required for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python* *3.11* ( https://www.python.org/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Git* ( https://git-scm.com/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LlamaIndex* ( https://github.com/run-llama/llama_index )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI account* and an *API key*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Streamlit* ( https://github.com/streamlit/streamlit )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PyPDF* ( https://pypi.org/project/pypdf/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*DOC2Txt* ( https://github.com/ankushshah89/python-docx2txt/blob/master/docx2txt/docx2txt.py
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the sample code snippets presented throughout this book as well as the
    entire project code base can be found in this GitHub repository: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
    .'
  prefs: []
  type: TYPE_NORMAL
- en: <title>Optimizing language models – the symbiosis of fine-tuning, RAG, and LlamaIndex</title>
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing language models – the symbiosis of fine-tuning, RAG, and LlamaIndex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw that vanilla LLMs have some limitations right
    outside of the box. Their knowledge is static and they occasionally spit out nonsense.
    We also learned about RAG as a potential way to mitigate these issues. Blending
    **prompt engineering** techniques with programmatic methods, RAG can elegantly
    solve many of the LLM shortcomings.
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt engineering?
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering involves crafting text inputs designed to be effectively
    processed by a **generative AI** ( **GenAI** ) model. Composed in natural language,
    these prompts describe the specific tasks to be carried out by the AI. We’ll have
    a much deeper conversation on this topic during *Chapter 10* , *Prompt Engineering
    Guidelines and* *Best Practices* .
  prefs: []
  type: TYPE_NORMAL
- en: Is RAG the only possible solution?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course not. Another approach is to fine-tune the AI model, which involves
    additional training on proprietary data to adapt the LLM and embed new data. It
    takes a model that is pre-trained on a general collection of data and continues
    its training on a more specialized dataset. This specialized dataset can be tailored
    to a particular domain, language, or set of tasks that you are interested in.
    The result is a model that maintains its broad knowledge base while gaining expertise
    in a specific area.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at *Figure 2* *.1* for a graphical explanation of the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – An illustration of the LLM fine-tuning process
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning can improve performance but has drawbacks, such as being expensive,
    requiring large datasets, and being difficult to update with fresh information.
    It also has the disadvantage of permanently altering the original AI model, which
    makes it inappropriate for personalizing purposes. Think of the original AI model
    as a classic recipe for a beloved dish. Fine-tuning this model is akin to modifying
    the traditional recipe to suit specific tastes or requirements. While these changes
    can make the dish more suitable for some, they also fundamentally alter the original
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all fine-tuning methods permanently alter the base AI model. Take **Low-Rank
    Adaptation** ( **LoRA** ) for example. LoRA is a fine-tuning method for LLMs that
    offers a more efficient approach compared to traditional **full fine-tuning**
    . In full fine-tuning, all layers of a neural network are optimized, which, while
    effective, is resource-intensive and time-consuming. LoRA, on the other hand,
    involves fine-tuning only two smaller matrices that approximate the larger weight
    matrix of the pre-trained LLM. In the LoRA method, the original weights of the
    model are *frozen* , meaning they are not directly updated during the fine-tuning
    process. The changes to the model’s behavior are achieved by the addition of these
    low-rank matrices. This approach allows for the original model to be preserved,
    while still enabling it to be adapted for new tasks or improved performance. You
    can find more information on this method here: https://ar5iv.labs.arxiv.org/html/2106.09685
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Even though LoRA is more efficient in terms of memory usage compared to full
    fine-tuning, it still requires computational resources and expertise to implement
    and optimize effectively, which might be a barrier for some users. Using fine-tuning
    to create a more personalized experience for a large number of different users
    requires re-running the tuning process for every user, which is definitely not
    cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: I’m not trying to say that RAG is a better alternative to LLM fine-tuning. In
    fact, RAG and fine-tuning are complementary techniques that are often used together.
    However, to rapidly incorporating changing data and personalization, RAG is preferable.
  prefs: []
  type: TYPE_NORMAL
- en: What LlamaIndex does
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With LlamaIndex, you can rapidly create *smart* LLMs that can adapt to your
    specific use case. Instead of relying only on their generic pre-trained knowledge,
    you can inject targeted information so that they give you accurate, relevant answers.
    It provides an easy way to connect external datasets to LLMs such as GPT-4, Claude,
    and Llama. LlamaIndex builds a bridge between your custom knowledge and the vast
    capabilities of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Created in 2022 by Princeton University graduate and entrepreneur Jerry Liu,
    the *LlamaIndex framework* has quickly become very popular in the developer community.
    LlamaIndex allows you to take advantage of the computational power and language
    understanding capabilities of LLMs while focusing their responses on specific,
    reliable data. This unique combination enables businesses and individuals to get
    the most out of their AI investments, as they can use the same underlying technology
    for a wide array of specialized applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could index a collection of your company’s documents. Then,
    when you ask questions related to your business, the LLM augmented with LlamaIndex
    provides responses based on real data rather than just making up vague answers!
  prefs: []
  type: TYPE_NORMAL
- en: The result is that you get all the expressive power of LLMs while greatly reducing
    the amount of incorrect or irrelevant information. LlamaIndex guides the LLM to
    pull from trusted sources you provide, and these sources could contain both *structured*
    and *unstructured* data. In fact, as we will see in the next chapters, the framework
    can ingest data from pretty much *any* data source available. That’s pretty cool,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not already thinking about the many possible uses for this framework,
    let me give you some quick ideas. With LlamaIndex, you could do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Build a search engine for your document collection** : One of its most powerful
    applications is the ability to index all your documents – they could be PDFs,
    Word files, Notion documents, GitHub repos, or other formats. Once indexed, you
    can query the LLM to search for specific information, making it a powerful search
    engine tailored specifically for your resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create a company chatbot with customized knowledge** : If your business has
    specific jargon, policies, or expertise, you can make the LLM *understand* these
    nuances. The chatbot could then handle a range of queries, from basic customer
    service questions to more specialized interactions that would typically require
    human expertise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate summaries of large reports or papers** : If your organization deals
    with lengthy documents or reports, LlamaIndex can be used to feed the LLM with
    their contents. Then, you can ask the LLM to generate concise summaries, capturing
    the most important points'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Develop a smart assistant for complex workflows** : By training the LLM on
    the nuances of multi-step tasks or procedures unique to your organization, you
    can transform it into a smart assistant data agent that provides valuable insights
    and guidance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And these are just the tip of the iceberg.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, *Figure 2* *.2* shows how implementing smart RAG strategies can
    offset some of the costs associated with fine-tuning the model on a specific domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – The relative costs of updating data in a pre-trained LLM
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive deeper into the applications and use cases of the LlamaIndex
    framework, let’s talk a bit about the architecture and the design principles behind
    it!
  prefs: []
  type: TYPE_NORMAL
- en: <title>Discovering the advantages of progressively disclosing complexity</title>
  prefs: []
  type: TYPE_NORMAL
- en: Discovering the advantages of progressively disclosing complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from llama_index.core import VectorStoreIndex, SimpleDirectoryReader documents
    = SimpleDirectoryReader('files').load_data() index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine() response = query_engine.query(     "summarize
    each document in a few sentences" ) print(response)
  prefs: []
  type: TYPE_NORMAL
- en: The creator of LlamaIndex wanted to make it accessible to everyone – from beginners
    just getting started with LLMs all the way to expert developers building complex
    systems. That’s why LlamaIndex uses a design principle called **progressive disclosure
    of complexity** . Don’t worry about the fancy name – it just means that the framework
    starts simple and gradually reveals more advanced features when you need them.
  prefs: []
  type: TYPE_NORMAL
- en: When you first use LlamaIndex, it feels like magic! With just a few lines of
    code, you can connect data and start querying the LLM. Under the hood, LlamaIndex
    converts the data into an efficient index that the LLM can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at this very simple example that first loads a set of text documents
    from a local directory. It then builds an index over the documents and queries
    that index to get a summarized view of the documents based on a natural language
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s that simple. Just six lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Don’t try to run the code just yet. It’s more for illustration purposes. There
    is a bit of environmental preparation we need to handle before that. Don’t worry,
    we’ll cover that a bit later in this chapter and then you’ll be ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: As you use LlamaIndex more, you will uncover its more powerful capabilities.
    There are plenty of parameters you can tweak. You can select specialized index
    structures optimized for different uses, carry out detailed cost analyses for
    different prompt strategies, customize query algorithms, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: But LlamaIndex always starts you off gently before getting into more detailed
    workings, and for quick and simple projects, you don’t need to go much deeper
    than that. This way, both beginners and experts can benefit from its versatility
    and capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s go on a quick tour of our hands-on project and then start prepping
    for the fun part: writing the code.'
  prefs: []
  type: TYPE_NORMAL
- en: An important aspect to consider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you go further through this book, and you will most likely want to experiment
    based on the examples it gives, you need to keep one very important point in mind.
    By default, the LlamaIndex framework is configured to use AI models provided by
    OpenAI. Although these models are extremely powerful and versatile, they incur
    costs. Many of the LlamaIndex functionalities presented in this book, be it metadata
    extraction, indexing, retrieval, or response synthesis, are based on either LLMs
    or embedding models. I have tried to use as simple examples as possible with small
    sample datasets in an attempt to limit these costs as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'I strongly advise you to keep a close eye on the OpenAI API consumption. In
    case you don’t already have it, the link where you can monitor the API usage is
    here: https://platform.openai.com/usage . I also advise you to be careful from
    a privacy perspective. These issues are discussed in more detail in *Chapters
    4* and *5* .'
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if you want to avoid both the costs of using an external LLM
    and the potential privacy risks, you can apply the methods described in *Chapter
    9* , *Customizing and Deploying Our LlamaIndex Project* . It is important to note,
    however, that all examples provided in the book are written and tested using the
    default models provided by OpenAI. There is a (quite likely) possibility that
    some examples may not work as well – or at all – running on locally hosted alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Introducing PITS – our LlamaIndex hands-on project</title>
  prefs: []
  type: TYPE_NORMAL
- en: Introducing PITS – our LlamaIndex hands-on project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Nothing beats learning* *by doing* .'
  prefs: []
  type: TYPE_NORMAL
- en: So, I’ve cooked up a fun and useful project for us to start using LlamaIndex!
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will introduce PITS. Wouldn’t it be cool to have an AI tutor that helps
    you learn new concepts interactively? Well, we’re going to build one together!
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how it will work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you will introduce yourself to PITS. You’ll have the chance to describe
    the topic you want to learn about and specify any personal learning preferences
    you may have.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you will be able to upload any existing study materials you may have on
    the topic. PITS will accept and ingest any PDFs, Word documents, or text files
    you may provide.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the documents provided, the tutor will first build a quiz. You’ll have
    the option to complete the quiz. That way, the tutor will be able to gauge your
    current knowledge of the topic and adjust the learning experience.
  prefs: []
  type: TYPE_NORMAL
- en: Our nifty tutor will then build learning material for you. This will consist
    of slides and narration for each slide. The training material will be divided
    into chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Then, your learning journey begins. During each learning session, PITS you will
    advance through the chapters, presenting each topic in your preferred style and
    adapting to your knowledge level.
  prefs: []
  type: TYPE_NORMAL
- en: After each concept is explained, you’ll have a chance to ask for more explanations
    or examples to learn more about the topic. It will answer your questions, create
    quizzes, explain concepts, and adapt responses based on your needs.
  prefs: []
  type: TYPE_NORMAL
- en: The best part is that your entire conversation with the agent will be recorded.
    It will remember both your questions and its own answers so it won’t repeat itself
    or lose the conversation context.
  prefs: []
  type: TYPE_NORMAL
- en: Too tired to continue in one session? Not a problem. When you’re ready to start
    another lesson, it will just resume from where you left off and give you a summary
    of the previous discussion.
  prefs: []
  type: TYPE_NORMAL
- en: But, hey! They say a picture’s worth a thousand words, right?
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find an overview in *Figure 2* *.3* .
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – An overview of the PITS workflow
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t really get more customized than this. This is the ultimate learning
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can imagine, PITS needs to be smart on several fronts. It needs to be
    able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand and index the study materials we provide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converse fluently with users and retain the context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teach effectively based on the indexed knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlamaIndex will help with the first part by ingesting the study material. The
    user will be able to upload any relevant training material such as manuals, slides
    or even student notes, and sample questions.
  prefs: []
  type: TYPE_NORMAL
- en: For the second part, we’ll mostly use the capabilities of GPT-4 to power the
    actual teaching interactions.
  prefs: []
  type: TYPE_NORMAL
- en: However, the foundation will be the knowledge augmentation capabilities of LlamaIndex.
    Pretty neat, right? We’ll have a personally customized tutor!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: I’m not sure whether you’ve read my biography, but I work as a trainer. The
    moment I first learned of the power of GenAI and discovered GPT-3, I knew exactly
    that a few years from now, systems such as PITS would emerge sooner or later.
    I was thrilled about their potential to provide free, quality education to people
    around the world, regardless of their location, background, or financial status.
    Later, when I discovered RAG and tools such as LlamaIndex, I became convinced
    that they would appear rather sooner than later.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, enough daydreaming – let’s start setting up the pieces.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Preparing our coding environment</title>
  prefs: []
  type: TYPE_NORMAL
- en: Preparing our coding environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: pip install llama-index pip install streamlit pip install pypdf pip install
    docx2txt python --version git --version pip show llama-index echo %OPENAI_API_KEY%
    pip show streamlit pip show pypdf pip show docx2txt python sample1.py
  prefs: []
  type: TYPE_NORMAL
- en: Before we embark on the LlamaIndex coding journey, it’s essential to set up
    our development environment properly. This setup is the first step toward ensuring
    that we can smoothly run through the examples and exercises I’ve prepared for
    you.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To maintain simplicity and ensure consistency across all examples, I’ve designed
    the sample code to be run in a local Python environment. I’m aware that many of
    you are fond of using web-based coding environments such as Google Colab and Jupyter
    Notebooks for your coding projects, so I kindly ask for your understanding if
    these examples do not directly translate to or run in these platforms. My goal
    was to keep our setup straightforward, allowing us to focus on the learning experience
    without compatibility concerns. Thank you for your understanding and happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s quickly get our computer set up for some cool LlamaIndex coding.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll need a Python 3.7+ environment. I recommend Python 3.11 if possible.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have Python, install it from https://www.python.org . If you already
    have an older version, you can upgrade or install a newer Python version side
    by side.
  prefs: []
  type: TYPE_NORMAL
- en: For a coding environment, my personal preference is **NotePad++** ( https://notepad-plus-plus.org/
    ), which is not quite an IDE but is very fast. However, you can also use Microsoft’s
    **VSCode** ( https://code.visualstudio.com/ ), **PyCharm** ( https://www.jetbrains.com/pycharm/
    ), or anything else you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Git
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we proceed, it’s important to have Git installed. Git is a version control
    system that lets you manage changes to your code and collaborate with others.
    It’s also essential for cloning code repositories, like the one we’ll be using
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Head over to the official Git website ( https://git-scm.com/book/en/v2/Getting-Started-Installing-Git
    ) and download the installer for your operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Follow the installation steps, and you should have Git up and running in no
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the sample code snippets presented throughout the book as well as the entire
    project code base can be found in this GitHub repository: https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you want to download the project files locally, once you have finished
    installing Git, you can simply follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Navigate to the desired directory** : Open a new command prompt or terminal
    window. Use the `cd` command to navigate to the directory where you’d like to
    store the project. Here is an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Clone the repository** : Run the following command to clone the GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will download a copy of the project to your local machine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Enter the project directory** : Navigate into the newly created project folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we move forward with our project, you have two options:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can either write the code on your own and then compare it with what’s in
    the repository
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Or you can directly explore the code files in the repository to get a better
    understanding of the code structure
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you correctly performed all of the preceding steps, listing the contents
    of the current folder should return several subfolders called `chX` – where `X`
    is the chapter number, and a separate subfolder called `PITS_APP` . The chapter
    folders contain all sample source files corresponding to each chapter. The `PITS_APP`
    folder contains the source code for our main project.
  prefs: []
  type: TYPE_NORMAL
- en: Installing LlamaIndex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, let’s get the LlamaIndex library installed. At your command prompt, run
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: This will include a LlamaIndex package that contains the core LlamaIndex components
    as well as a selection of useful integrations. For the most efficient deployment
    possible, there is also the option of installing just the minimum core components
    and only the necessary integrations, but for the purpose of this book, the presented
    option will do just fine.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you’re already running a version older than v0.10, it is recommended
    that you start with a fresh install in a virtual environment to avoid any conflicts
    with the legacy version. You can find detailed instructions here: https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77
    .'
  prefs: []
  type: TYPE_NORMAL
- en: We’re now ready to import and start using it.
  prefs: []
  type: TYPE_NORMAL
- en: Signing up for an OpenAI API key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we’ll be using OpenAI’s GPT models via LlamaIndex, you’ll need an API
    key to authenticate. Head to https://platform.openai.com and sign up. Once logged
    in, you can create a new secret API key. Make sure to keep it safe!
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex will use this key every time it interacts with OpenAI’s models. Because
    it has to be kept secret, it’s a good idea to store it in an environment variable
    on your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: A short guide for Windows users
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On Windows, you can accomplish that by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open **Environment Variables** : Open the Start menu and search for **Environment
    Variables** or right-click on **This PC** or **My Computer** and select **Properties**
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, click on **Advanced system settings** followed by the **Environment Variables**
    button in the **Advanced** tab as shown in *Figure 2* *.4:*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21861_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Editing Windows environment variables
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a new environment variable** : In the **Environment Variables** window,
    under the **User variables** section, click the **New** button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Enter the variable details** : For the **Variable name** , enter `OPENAI_API_KEY`
    . For **Variable value** , paste the secret API key you received from OpenAI.
    See *Figure 2* *.5* for an illustration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B21861_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Creating the OPENAI_API_KEY environment variable
  prefs: []
  type: TYPE_NORMAL
- en: '**Confirm and apply** : Click **OK** to close all of the dialog boxes. You
    will need to restart your computer for the changes to take effect.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Verify the environment variable** : To ensure the variable is set correctly,
    open a new command prompt, and run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This should display the API key you just stored.
  prefs: []
  type: TYPE_NORMAL
- en: A short guide for Linux/Mac users
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On Linux/Mac, you can accomplish Signing up for an OpenAI API key by following
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command in your terminal, replacing `<yourkey>` with your
    API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the shell with the new variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure that you have set your environment variable with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your OpenAI API key is now securely stored in an environment variable and can
    be easily accessed by LlamaIndex when needed, without exposing it in your code
    or system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: While OpenAI provides a free trial option for their GPT models through their
    API, you’ll only receive a limited number of free credits. Currently, the free
    credit is limited to $5 and expires after 3 months. That should be more than enough
    to experiment for the purpose of our project and for reading the book. However,
    If you wish to get serious about building LLM-based applications, you’ll have
    to sign up for a paid account on their platform. Alternatively, you can always
    choose to use another AI model for LlamaIndex. We will discuss customizing the
    AI model in more detail in *Chapter 10* *, Prompt Engineering Guidelines and*
    *Best Practices* .
  prefs: []
  type: TYPE_NORMAL
- en: OK. The backend is all set up. Let’s talk about the rest of the stack.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering Streamlit – the perfect tool for rapid building and deployment!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can build cool apps such as PITS, we need somewhere to … well, build
    and run them! That’s where Streamlit comes in. Streamlit is an awesome open-source
    Python library that makes it super easy to create and deploy web apps and dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: With just a few lines of Python code, you can build complete web interfaces
    and see the results instantly. The best part is that Streamlit apps can be deployed
    nearly anywhere – on servers, on platforms such as Heroku, or even directly from
    GitHub!
  prefs: []
  type: TYPE_NORMAL
- en: I love Streamlit because it lets me focus on the fun stuff – such as creating
    PITS with LlamaIndex – rather than fussing over complex web development. For AI
    experimentation, it’s perfect!
  prefs: []
  type: TYPE_NORMAL
- en: We’ll primarily use it to create the interface for uploading study guides and
    interacting with our PITS tutor. For the purpose of the next chapters, we’ll be
    using Streamlit for running and testing our app locally. However, in *Chapter
    9* , *Customizing and Deploying Our LlamaIndex Project* , we will also discover
    how we can easily deploy our app using **Streamlit Share** or any other hosting
    service you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Streamlit has tons of cool capabilities such as data frames, charts, and widgets
    – but don’t worry about learning it all now. As we build up features, I’ll explain
    the relevant parts so you can gain Streamlit skills along the way!
  prefs: []
  type: TYPE_NORMAL
- en: Installing Streamlit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lastly, we need to install the Streamlit library:'
  prefs: []
  type: TYPE_NORMAL
- en: Great! We have our backend tool (LlamaIndex), our frontend layer (Streamlit),
    and our goal (PITS). It’s time for a final touch.
  prefs: []
  type: TYPE_NORMAL
- en: Finishing up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because our project should be able to ingest PDF and DOCX documents, we will
    also need to install two additional libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! Our environment is LlamaIndex ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap what we have:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlamaIndex package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI account and an API key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streamlit for app building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyPDF and DOC2Txt libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One final check
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To verify that everything was installed correctly, open a new command prompt
    or terminal window, and run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple way to check whether your environment is ready is to try navigating
    into the `ch2` subfolder of your local `git` folder and run the file called `sample1.py`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: You should get a nice summary of the two sample documents provided in the `ch2/files`
    subfolder if everything has been properly installed.
  prefs: []
  type: TYPE_NORMAL
- en: If anything is missing, please go back and retake the necessary steps before
    proceeding further. Trust me, you’ll avoid a lot of pain and frustration further
    down the line.
  prefs: []
  type: TYPE_NORMAL
- en: We’re all set to start ingesting data, constructing indices with LlamaIndex,
    and building our PITS tutor app! I don’t know about you, but I’m *kid-in-a-candy-store*
    excited to start experimenting.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters, we’ll get hands-on with our first LlamaIndex program.
    This is where the real fun begins! We’ll explore ingesting data, constructing
    indexes, executing queries, and more.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll explain each concept and line of code in simple terms along the way. In
    no time, you’ll be implementing the basics like a LlamaIndex pro! Once we’ve got
    these fundamentals down, we can start expanding the capabilities of our tutor
    app.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s clarify the overall code structure of the framework’s GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Familiarizing ourselves with the structure of the LlamaIndex code repository</title>
  prefs: []
  type: TYPE_NORMAL
- en: Familiarizing ourselves with the structure of the LlamaIndex code repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: from llama_index.llms.mistralai import MistralAI pip install llama-index.llms.mistralai
  prefs: []
  type: TYPE_NORMAL
- en: 'Because you’ll probably spend a lot of time browsing the official code repository
    of the LlamaIndex framework, it’s good to have an overall image of its general
    structure. You can always consult the repository here: https://github.com/run-llama/llama_index
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Starting with version 0.10, the code has been thoroughly reorganized into a
    more modular structure. The purpose of this new structure is to improve efficiency,
    by avoiding loading any unnecessary dependencies, while also improving readability
    and overall user experience for developers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2* *.6* describes the main components of the code structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21861_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – The LlamaIndex GitHub repository code structure
  prefs: []
  type: TYPE_NORMAL
- en: The `llama-index-core` folder serves as the foundational package for LlamaIndex,
    enabling developers to install the essential framework and then selectively add
    from over 300 integration packages and different Llama-packs to tailor functionality
    for their specific application needs.
  prefs: []
  type: TYPE_NORMAL
- en: The `llama-index-integrations` folder of LlamaIndex consists of various add-on
    packages that extend the functionality of the core framework. These allow developers
    to customize their build with specific elements such as custom LLMs, data loaders,
    embedding models, and vector store providers to best fit their application’s requirements.
    We’ll cover some of these integrations later in our book, starting with *Chapter
    4* , *Ingesting Data into Our* *RAG Workflow* .
  prefs: []
  type: TYPE_NORMAL
- en: The `llama-index-packs` folder contains more than 50 Llama packs. Developed
    and constantly improved by the LlamaIndex developer community, these packs serve
    as ready-made templates designed to kickstart a user’s application. We’ll talk
    about them in more detail during *Chapter 9* , *Customizing and Deploying Our*
    *LlamaIndex Project* .
  prefs: []
  type: TYPE_NORMAL
- en: The `llama-index-cli` folder is used by the LlamaIndex command-line interface,
    which we will also cover briefly during *Chapter 9* *, Customizing and Deploying
    Our* *LlamaIndex Project* .
  prefs: []
  type: TYPE_NORMAL
- en: The last section, called **OTHERS** in *Figure 2* *.6* , consists of two folders
    that currently contain fine-tuning abstractions and some experimental features
    that we will not cover in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The subfolders in `llama-index-integrations` and `llama-index-packs` represent
    individual packages. The folder name corresponds to the package name. For example,
    the `llama-index-integrations/llms/llama-index-llms-mistralai` folder corresponds
    to the `llama-index-llms-mistralai` PyPI package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this example, there is something you need to do before you import
    and use the `mistralai` package in your code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll have to first install the corresponding PyPI package by running the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry too much about missing any necessary packages for the examples included
    in the book, as you will find them nicely listed at the beginning of each chapter
    under the *Technical* *requirements* heading.
  prefs: []
  type: TYPE_NORMAL
- en: <title>Summary</title>
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced LlamaIndex, a framework for connecting LLMs to
    external datasets. We discovered how LlamaIndex allows LLMs to incorporate real-world
    knowledge into their responses.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter discussed the benefits of LlamaIndex over fine-tuning, such as easier
    updating and personalization. It introduced the concept of progressive disclosure
    of complexity, where LlamaIndex starts simple but reveals advanced capabilities
    when needed.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter then presented an overview of the hands-on project PITS, a personalized
    intelligent tutoring system. It covered setting up the required tools such as
    Python, Git, and Streamlit, and getting an OpenAI API key. The chapter finished
    by verifying that the environment is ready for building LlamaIndex apps.
  prefs: []
  type: TYPE_NORMAL
- en: We’re now ready to continue our journey and proceed with a more technical understanding
    of the inner workings of the LlamaIndex framework. See you in the next chapter!
  prefs: []
  type: TYPE_NORMAL
- en: '<title>Part 2: Starting Your First LlamaIndex Project</title>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Starting Your First LlamaIndex Project'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we explore the detailed aspects of LlamaIndex, including data
    ingestion through LlamaHub connectors, text-chunking tools, metadata infusion,
    data privacy, and efficient ingestion pipelines, before moving on to a comprehensive
    guide to the indexing functionality within LlamaIndex, detailing types of indexes,
    customization, and strategies for building scalable RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 3* , *Kickstarting Your Journey with LlamaIndex*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 4* , *Ingesting Data into Our RAG Workflow*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 5* , *Indexing with LlamaIndex*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
