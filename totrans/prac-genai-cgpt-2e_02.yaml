- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hello! Welcome to *Practical Generative AI with ChatGPT*! In this book, we will
    explore the fascinating world of generative **artificial intelligence** (**AI**)
    and its groundbreaking applications, with a particular focus on ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI has transformed the way we interact with machines, enabling computers
    to create, predict, and learn without explicit human instruction. Since the launch
    of OpenAI’s ChatGPT in November 2022, we have witnessed unprecedented advances
    in natural language processing, image and video synthesis, and many other fields.
    Whether you are a curious beginner or an experienced practitioner, this guide
    will equip you with the knowledge and skills to effectively navigate the exciting
    landscape of generative AI. So, let’s dive in and start the book with some definitions
    of the context we are moving in.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focus on the applications of generative AI to various fields,
    such as image synthesis, text generation, and music composition, highlighting
    the potential of generative AI to revolutionize various industries with concrete
    examples and recent developments. Being aware of the research journey toward the
    current state of the art of generative AI will give you an understanding of the
    foundations of recent developments and state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: 'All this, we will cover through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the domains of generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main trends and innovation after 2 years of ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Legal and ethical landscape of generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be familiar with the exciting world of
    generative AI, its applications, the research history behind it, and the current
    developments that could have – and are currently having – a disruptive impact
    on businesses.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative AI is an exciting branch of AI that focuses on creating new content,
    such as text, images, music, or even videos, that is often indistinguishable from
    something made by humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand where it fits, let’s break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AI**: AI is the broad field that enables machines to mimic human-like tasks,
    such as decision-making or problem-solving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**): Within AI, ML refers to techniques where machines
    learn patterns from data to make predictions or decisions without being explicitly
    programmed. The process of learning is made possible by sophisticated mathematical
    models called algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning** (**DL**): A subset of ML, DL uses complex algorithms inspired
    by the human brain to process large amounts of data and recognize intricate patterns.
    Because of their architecture – inspired by our brains and neural connections
    – these algorithms are called artificial neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An artificial neural network is a type of computer program designed to learn
    patterns by processing information in a way that’s inspired by the human brain.
    Instead of following strict, step-by-step rules, it uses interconnected “nodes”
    (like virtual brain cells) that work together and adjust their connections over
    time. By repeatedly reviewing examples, it gradually improves at tasks like recognizing
    images, understanding speech, or predicting outcomes—all without needing explicit
    instructions for each step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Generative AI emerges from DL and uses specialized algorithms to generate something
    entirely new based on what it has learned from existing data. For example, a generative
    AI model trained on thousands of paintings could create brand-new art that blends
    different styles or themes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how these areas of research are related to each
    other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Relationship between AI, ML, DL, and generative AI'
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI models are trained on vast amounts of data and then they can generate
    new examples based on user’s requests. And the game-changer element here is that
    these requests are made in the easiest way possible – using our natural language.
    These models are called **large language models** (**LLMs**).
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are a type of artificial neural network featured by a particular architectural
    framework called “Transformer.” They are characterized by a huge number of parameters
    (in the order of billions) and have been trained on billions of words. Given the
    training set, LLMs are capable of inferring language patterns and intents in user
    queries and generating natural language responses.
  prefs: []
  type: TYPE_NORMAL
- en: The possibility of interacting in natural language with LLMs is disruptive,
    and a whole new science has been born around that activity. This science is called
    “prompt engineering,” named after the term “prompt,” which we are going to cover
    in *Chapter 3*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: A prompt is the specific text, question, or description you provide to a generative
    AI model to guide it toward producing the kind of output you want—whether that’s
    a helpful explanation, a creative story, or a detailed solution. How you phrase
    the prompt can greatly affect the AI’s response. This practice of carefully designing
    and refining prompts, often called “prompt engineering,” involves experimenting
    with different word choices, instructions, and formats to improve both the quality
    and accuracy of the AI’s output. By learning how to craft effective prompts, you
    help ensure the AI more consistently gives you results that are useful, engaging,
    and aligned with your goals.
  prefs: []
  type: TYPE_NORMAL
- en: Even though text understanding and generation is probably one of the most outstanding
    features of Generative AI, this field covers many domains, which we will cover
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Domains of generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, generative AI has made significant advancements and has expanded
    its applications to a wide range of domains, such as art, music, fashion, and
    architecture. In some of them, it is indeed transforming the way we create, design,
    and understand the world around us. In others, it is improving and making existing
    processes and operations more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the context of the pharmaceutical industry, generative AI is
    revolutionizing drug discovery by enabling the rapid design of novel therapeutic
    molecules, thereby significantly reducing development timelines and costs. By
    analyzing extensive datasets of chemical and biological information, generative
    AI models can identify promising drug candidates and predict their interactions
    within the human body. For instance, Insilico Medicine utilized generative AI
    to develop ISM001-055, a drug candidate for idiopathic pulmonary fibrosis, which
    progressed to Phase II clinical trials in 2023 (https://insilico.com/blog/first_phase2).
  prefs: []
  type: TYPE_NORMAL
- en: Another example is the way generative AI is revolutionizing game development
    by enabling the creation of dynamic and adaptive environments that respond to
    player actions, thereby enhancing immersion and replayability. By leveraging generative
    AI, developers can procedurally generate vast, ever-changing game worlds, ensuring
    that each playthrough offers a unique experience. This technology facilitates
    the creation of realistic **non-playable characters** (**NPCs**) with behaviors
    that adapt to player interactions, making game narratives more engaging. Additionally,
    generative AI streamlines the development process by automating asset creation,
    which reduces production time and costs.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, developers can focus more on crafting innovative gameplay mechanics
    and rich storytelling, ultimately delivering more personalized and captivating
    gaming experiences (https://www.xcubelabs.com/blog/generative-ai-in-game-development-creating-dynamic-and-adaptive-environments/).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, generative AI can have a great impact on advertising and visual asset
    generation. For example, in March 2023, Coca-Cola launched the “Create Real Magic”
    platform (https://www.coca-colacompany.com/media-center/coca-cola-invites-digital-artists-to-create-real-magic-using-new-ai-platform),
    inviting digital artists worldwide to craft original artwork using iconic brand
    assets from its archives. Developed in collaboration with OpenAI and Bain & Company,
    this innovative platform combines the capabilities of GPT-4 and DALL-E, enabling
    users to generate unique pieces that blend Coca-Cola’s heritage with modern AI
    technology. Participants had the opportunity to submit their creations for a chance
    to be featured on Coca-Cola’s digital billboards in New York’s Times Square and
    London’s Piccadilly Circus, exemplifying the brand’s commitment to fostering creativity
    through cutting-edge technology. These are just a few examples of how generative
    AI can reshape business processes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the fact that generative AI is used in many domains also implies that its
    models can deal with different kinds of data, from natural language to audio or
    images. In the next section, we’ll explore how generative AI models address different
    types of data and domains.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evolution of text generation within AI has been a journey from early theoretical
    concepts to today’s sophisticated language models. The 1950s marked the formal
    inception of AI as a field, with pioneers like Alan Turing exploring machine intelligence.
    Early efforts in **natural language processing** (**NLP**) during the 1960s and
    1970s led to programs such as ELIZA, which simulated conversation through pattern
    matching. The 1980s and 1990s saw the development of statistical models that improved
    language modeling by probabilistically predicting word sequences. The advent of
    ML algorithms during this period further advanced text generation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: A significant breakthrough occurred in 2017 with the introduction of the Transformer
    architecture which, as aforementioned, is the framework that features today’s
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The unique element of this new series of models featuring the landscape of generative
    AI is that – once they are trained – they can be consumed, queried, and instructed
    in the easiest way possible. The introduction of LLMs marked a paradigm shift
    in the context of AI since no advanced skills were needed to benefit from them.
  prefs: []
  type: TYPE_NORMAL
- en: Today, one of the greatest applications of generative AI—and the one we are
    going to cover the most throughout this book—is its ability to produce new content
    in natural language. Indeed, generative AI models can be used to generate new
    coherent and grammatically correct text in different languages, such as articles,
    poetry, and product descriptions. They can also extract relevant features from
    text such as keywords, topics, or full summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of working with GPT-4o, one of the latest models released
    by OpenAI and available through ChatGPT:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31559_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Example of ChatGPT responding to a user’s query in natural language'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model was not only able to answer my question with an explanation
    of what a proton is; it also adapted its style and jargon to a specific target
    audience – in my case, a 5-year-old child. This is remarkable since it paves the
    way for many scenarios of hyper-personalization that were not possible before.
    In the next chapters, we will cover many examples of that.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT is the main focus of this book, and in the upcoming chapters, you will
    see examples that showcase this powerful application.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on to image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Image generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the earliest and most well-known examples of generative AI in image synthesis
    is the **generative adversarial network (GAN)** architecture introduced in the
    2014 paper by I. Goodfellow et al., *Generative Adversarial Networks*. The purpose
    of GANs is to generate realistic images that are indistinguishable from real images.
    This ability has several interesting business applications, such as generating
    synthetic datasets for training computer vision models, generating realistic product
    images, and generating realistic images for virtual reality and augmented reality
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in 2021, a new generative AI model was introduced in this field by OpenAI,
    **DALL-E**. Different from GANs, the DALL-E model is designed to generate images
    from descriptions in natural language and can generate a wide range of images.
    The main difference here is that while GANs are often used to create or improve
    realistic images, models like DALL-E are ideal for visual creativity, turning
    any description in natural language into an illustration.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E has great potential in creative industries such as advertising, product
    design, and fashion to create unique and creative images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since its first release to the time of writing (December 2024), DALL-E has
    improved dramatically, as you can see in the following examples. Below is an artistic
    creation by DALL-E at the dawn of its life:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Images generated by DALL-E with a natural language prompt as input'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now see what **DALL-E3**, the most recent version of the model at the
    time of writing this book, can produce (here, we will use Microsoft Image Creator,
    powered by DALL-E3\. You can try it at https://copilot.microsoft.com/images/create):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31559_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Images generated by DALL-E3 with a natural language prompt as input'
  prefs: []
  type: TYPE_NORMAL
- en: It’s impressive to see the level of improvement of this model in less than 2
    years. We are just scraping the surface of the massive improvements occurring
    at a fast pace.
  prefs: []
  type: TYPE_NORMAL
- en: Music generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first approaches to generative AI for music generation trace back to the
    1950s, with research in the field of algorithmic composition, a technique that
    uses algorithms to generate musical compositions. In 1957, Lejaren Hiller and
    Leonard Isaacson created the *Illiac Suite* for *String Quartet* (https://www.youtube.com/watch?v=n0njBFLQSk8),
    the first piece of music entirely composed by AI. Since then, the field of generative
    AI for music has been the subject of ongoing research.
  prefs: []
  type: TYPE_NORMAL
- en: Among recent years’ developments, new architectures and frameworks have become
    widespread among the general public, such as the WaveNet architecture introduced
    by Google in 2016, which has been able to generate high-quality audio samples,
    and the Magenta project, also developed by Google, which uses **recurrent neural
    networks** (**RNNs**) and other ML techniques to generate music and other forms
    of art.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: '**RNNs** are a type of neural network designed to process sequential data by
    retaining information from previous inputs through a loop-like structure. This
    allows them to recognize patterns and dependencies over time, making them ideal
    for tasks like language modeling, time-series prediction, and speech recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, OpenAI also announced Jukebox, a neural network that generates music
    when provided with genre, artist, and lyrics as input.
  prefs: []
  type: TYPE_NORMAL
- en: These and other frameworks became the foundations of many AI composer assistants
    for music generation. An example is Flow Machines, developed by Sony CSL Research.
    This generative AI system was trained on a large database of musical pieces to
    create new music in a variety of styles. It was used by French composer Benoît
    Carré to compose an album called *Hello World* (https:// www.helloworldalbum.net/),
    which features collaborations with several human musicians.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, you can see an example of a track generated entirely by Music Transformer,
    one of the models within the Magenta project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Music Transformer allows users to listen to musical performances
    generated by AI (https://magenta.tensorflow.org/music-transformer)'
  prefs: []
  type: TYPE_NORMAL
- en: Another incredible application of generative AI within the music domain is speech
    synthesis. This refers to AI tools that can create audio based on text inputs
    in the voices of well-known singers.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have always wondered how your songs would sound if Lady
    Gaga performed them, well, you can now fulfill your dreams with tools such as
    FakeYou *Text to Speech* (https://fakeyou.com/tts) or UberDuck.ai (https://uberduck.ai/)!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Text-to-speech synthesis with fakeyou.com'
  prefs: []
  type: TYPE_NORMAL
- en: The results are really impressive! If you want to have fun, you can also try
    voices from your favorite cartoons, such as *Winnie the Pooh*. The only thing
    you need to do is input the text of the song you want your favorite voice to sing
    aloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go even further. What if we could generate a song from scratch, just
    asking the generative AI to do that for us in natural language? Well, we can do
    that seamlessly today and without any knowledge about music. Among the generative
    AI products that are rising in the music market today is Suno, whose mission is
    *“[...]building a future where anyone can make great music. Whether you’re a shower
    singer or a charting artist, we break barriers between you and the song you dream
    of making. No instrument needed, just imagination. From your mind to music.”*
    (source: https://suno.com/about).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31559_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: Example of an entire song generated by Suno.com from a description
    in natural language'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, on the left-hand side of the picture, I provided a very brief
    song description in natural language – this was my prompt. From that, the model
    was able to generate not only the title and lyrics of a song (on the right-hand
    side) but also the music!
  prefs: []
  type: TYPE_NORMAL
- en: Can you believe that it became my summer 2024 hit? If you want to create your
    summer hit too, you can try it for free at https://suno.com/create.
  prefs: []
  type: TYPE_NORMAL
- en: Video generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI for video generation shares a similar timeline of development
    with image generation. One of the key developments in the field of video generation
    has been the development of GANs. Thanks to their accuracy in producing realistic
    images, researchers have started to apply this technique to video generation as
    well. One of the most notable examples of GAN-based video generation is DeepMind’s
    Veo, which generates high-quality videos from a single image and a sequence of
    motions. Another great example is NVIDIA’s **video-to-video synthesis** (**Vid2Vid**)
    DL-based framework, which uses GANs to synthesize high-quality videos from input
    videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vid2Vid system can generate temporally consistent videos, meaning that
    they maintain smooth and realistic motion over time. The technology can be used
    to perform a variety of video synthesis tasks, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting videos from one domain into another (for example, converting a daytime
    video into a nighttime video or a sketch into a realistic image)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying existing videos (for example, changing the style or appearance of
    objects in a video)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new videos from static images (for example, animating a sequence of
    still images)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In September 2022, Meta’s researchers announced the general availability of
    **Make-A-Video** (https://makeavideo.studio/), a new AI system that allows users
    to convert their natural language prompts into video clips. Behind this technology,
    you can recognize many of the models that we mentioned in other domains – language
    understanding for the prompt, image and motion generation with image generation,
    and background music made by AI composers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, everything we’ve mentioned above pales in comparison to the latest text-to-video
    models. To name one, OpenAI announced a text-to-video model called **SORA** in
    February 2024 and released some early experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a black jacket and red dress standing on a wet street  Description
    automatically generated](img/B31559_01_08.png)![A group of mammoths in the snow  Description
    automatically generated](img/B31559_01_09.png)![A person in a space suit  Description
    automatically generated](img/B31559_01_10.png) ![A cartoon animal looking at a
    candle  Description automatically generated](img/B31559_01_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Videos generated by SORA from prompts in natural language. Source:
    https://openai.com/index/sora/'
  prefs: []
  type: TYPE_NORMAL
- en: I do encourage you to visit the SORA webpage to have a look at the amazing videos
    it created. At the time of writing, SORA is not publicly available, as it is going
    through several tests by the OpenAI Red Team.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, generative AI has impacted many domains for years, and some AI tools
    already consistently support artists, organizations, and general users. Despite
    the fact we’ve been experimenting and building applications with generative AI
    for only two years, there are already some consolidated trends and future innovations
    to keep in mind. Let’s explore them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Main trends and innovations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From November 2022 to today, we have witnessed a huge amount of innovation in
    the field of generative AI. Many of these innovations are linked to the brand-new
    models developed and released to the public, like OpenAI’s GPT-4o and DALL-E3,
    but also Google Gemini, Meta Llama 3, Microsoft Phi3, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: However, the most remarkable achievements probably lie in the way we interact
    with and build applications around those models. In this section, we are going
    to explore three main advancements that have marked the most popular reference
    architectures for generative-AI-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval augmented generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first limitations of ChatGPT and, generally speaking, of LLMs was
    the knowledge base cutoff. The knowledge of LLMs is limited to the training set
    they have been trained on and, although this can be exhaustive, it’s not up to
    date (in fact, once the model is trained, any new data or information that emerges
    afterward won’t be part of its knowledge, since it wasn’t included in the original
    training set). Plus, the data is likely missing the proprietary knowledge base
    that might be relevant for us or our organization. For example, if you ask ChatGPT,
    “What is my company’s policy for employee health insurance?”, the model won’t
    be able to answer since it has no access to this information.
  prefs: []
  type: TYPE_NORMAL
- en: To bypass this limitation, a new framework was designed to allow LLMs to navigate
    through customized documentation that we can provide. This framework is called
    **retrieval augmented generation** (**RAG**).
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind RAG is to augment the LLM’s knowledge by adding external sources
    of information, yet without modifying the structure of the model at all.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: An embedding is a way to turn complex information—like words, sentences, or
    images—into a list of numbers (a vector). This makes it easier for a computer
    to understand what those words or sentences mean. If two pieces of text have similar
    meanings, their vectors will be close together in the numerical space. In other
    words, embeddings let computers measure how alike different inputs are based on
    their content, not just their exact wording.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if two concepts are similar, then their vector representations
    should also be similar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_01_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Example of vector representation of four different words'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we can see that the mathematical distance between the two
    vectors corresponding to “Queen” and “King” is more or less the same as the difference
    between “Woman” and “Man.” Semantically speaking, this makes sense, as we are
    talking about a similar relationship. A similar example might be applied to the
    relationship between countries and capital cities: once embedded in a vector space,
    the distance between “Italy” and “Rome” should be similar to the distance between
    “France” and “Paris” as they are mapping the same relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is made of three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval**: Given a user’s query and its corresponding numerical representation,
    the most similar pieces of documents (those corresponding to the vectors that
    are closest to the user query’s vector) are retrieved and used as the base context
    for the LLM.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A close-up of a text box  Description automatically generated](img/B31559_01_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Example of retrieving three different chunks from different documents,
    since they are represented by the closest vectors to the user query'
  prefs: []
  type: TYPE_NORMAL
- en: '**Augmentation**: The retrieved context is enriched through additional instructions,
    rules, safety guardrails, and similar practices that are typical of prompt engineering
    techniques (we will cover the topic of prompt engineering in *Chapter 3*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B31559_01_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: Example of adding more context to the retrieved chunks of documents'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generation**: Based on the augmented context, the LLM generates the response
    to the user’s query.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated](img/B31559_01_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12: Example of using the augmented context as the system message for
    the model to generate the final answer'
  prefs: []
  type: TYPE_NORMAL
- en: RAG combines the strengths of generative models and information retrieval systems
    to enhance the quality and relevance of generated content. Traditional generative
    models rely solely on their training data to produce responses, which can sometimes
    result in outdated or irrelevant information. RAG addresses this limitation by
    integrating external knowledge bases during the generation process.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier in this chapter, we looked at the various domains of generative AI,
    ranging from text to images, from videos to music. Typically, large foundation
    models tend to be domain-specific, as we saw for LLMs in the case of language
    understanding and generation, or DALL-E3 in the case of image generation.
  prefs: []
  type: TYPE_NORMAL
- en: However, the recent advances in generative AI have enabled the development of
    **large multimodal models** (**LMMs**) that can process and generate different
    types of data, such as text, images, audio, and video.
  prefs: []
  type: TYPE_NORMAL
- en: LMMs share with *standard* LLMs the ability to generalize and adapt typical
    large foundation models. However, LMMs are capable of processing diverse data
    with the idea of mirroring the way humans interact with the surrounding ecosystem
    – that is, with all our senses.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great example of a multimodal model is OpenAI’s GPT-4o, which is able to
    interact with users via text, images, and audio. Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31559_01_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: Example of providing ChatGPT-4o with a picture and asking it to
    name the building'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the model was able to analyze the image and reason over it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now go ahead and ask the model to generate an illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A black and white drawing of a tall building  Description automatically generated](img/B31559_01_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14: Example of ChatGPT-4o generating an illustration based on a previously
    provided picture'
  prefs: []
  type: TYPE_NORMAL
- en: What sets LLMs apart is their ability to retain advanced reasoning capabilities,
    making them uniquely suited for tackling complex reasoning tasks across diverse
    data contexts, unlike traditional AI models. Let’s consider, for example, traditional
    computer vision models, which are task-specific, and they do not *reason* over
    an image, but rather perform tasks like detecting objects or extracting text from
    images. On the other hand, LMMs can use the same reasoning capabilities as LLMs,
    yet they can apply these capabilities to data other than text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider this last example (showing only the first lines of the response):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A crossword puzzle with text  Description automatically generated](img/B31559_01_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.15: Example of ChatGPT 4o solving a crossword game'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the model was able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Read and understand the scenario the image is posing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reason about it and solve the complex task that it is offering, which is solving
    a puzzle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you may imagine, this opens a landscape of applications in various industries,
    and we are going to see some concrete examples in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: AI agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous sections, we uncovered how LLMs are great when it comes to generating
    content. However, they lack one ability, which is taking action and interacting
    with the surrounding ecosystem that goes beyond the single user. For example,
    what if we want our LLM to be able not only to generate an amazing LinkedIn post
    but also publish it on our page?
  prefs: []
  type: TYPE_NORMAL
- en: AI agents emerge as key players in overcoming this limitation. But what exactly
    are they? Agents can be seen as AI systems powered by LLMs that, given a user’s
    query, are able to interact with the surrounding ecosystem to the extent to which
    we allow them. The perimeter of the ecosystem is delimited by the tools (or plugins)
    we provide the agents with (in our previous example, we might provide the agent
    with a LinkedIn plugin so that it is able to post the generated content).
  prefs: []
  type: TYPE_NORMAL
- en: 'Agents are made of the following ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: An LLM, which acts as the reasoning engine of the AI system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A system message, which instructs the agent to behave and think in a given
    way. For example, you can design an agent as a teaching assistant for students
    with the following system message: “You are a teaching assistant. Given a student’s
    query, NEVER provide the final answer, but rather provide some hints to get there.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A set of tools the agent can leverage to interact with the surrounding ecosystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI agents are a perfect representation of the meaning of “LLM as reasoning
    engine of an application.” In fact, the beauty of agents is that they can pick
    the best tool to use to accomplish a user’s request. For example, let’s say we
    have an AI agent to produce LinkedIn content, and we provide it with two tools:
    a LinkedIn plugin and a web search plugin (each one with a correct description
    of its functionality). Let’s explore the behavior of the agent in three different
    scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate a story about a little dog walking around the mountains**: The agent
    will generate the story without using a plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate a story about the current weather in Milan**: The agent will invoke
    the web search plugin to get the current weather in Milan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate a LinkedIn post about the current weather in Milan and publish it
    on my profile**: The agent will invoke the web search plugin to get the current
    weather in Milan and the LinkedIn plugin to post it on my profile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combination of instructions and a set of plugins makes AI agents extremely
    versatile, and you can create highly specialized entities to address specific
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s not all.
  prefs: []
  type: TYPE_NORMAL
- en: Why have only one agent if you can create your own crew of agents talking to
    and cooperating with each other? Imagine multiple agents, each one with a specific
    expertise and goal, communicating and interacting to accomplish a task. This is
    what **multi-agent applications** look like, and in the last few months, this
    pattern started showing very interesting results.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the following example. We want to generate an elevator pitch
    about climate change. We need up-to-date information to do so (latest trends and
    research, future perspectives, and so on), as well as solid research grounded
    by academic papers. Plus, we need to be concise yet sharp and effective, delivering
    all the key information in a very short pitch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we could ask a single agent to do all of that, providing it with all the
    required tools and long instructions to accomplish the task. However, if the task
    gets very complex, a single agent might not be the best approach as it might lead
    to inaccurate results. Instead, let’s use a multi-agent approach, creating a team
    with the following AI professionals:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A market analyst who can search the web for the latest news about climate change:
    This will be an agent with a web search plugin and specific instructions to search
    for news.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An expert researcher who can easily navigate through academic research papers
    about climate change: This will be an agent with an Arxiv (a curated research-sharing
    platform) plugin and specific instructions on how to retrieve relevant information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An expert in public speaking who can easily consolidate all the information
    in one elevator pitch: This will be an agent with instructions on how to deliver
    perfect pitches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A critic who will review the pitch and propose some changes to the expert in
    public speaking, if needed: This will be an agent with instructions on how to
    review and improve a pitch by identifying pitfalls and areas of improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, when the user asks the agents to generate an elevator pitch about the current
    issue of climate change, all the agents can start working on the project.
  prefs: []
  type: TYPE_NORMAL
- en: There are many frameworks that can help developers with multi-agent applications
    (including AutoGen, LangGraph, and CrewAI), especially when it comes to the *flow*
    that we want our agents to follow. For example, we might want to enforce a specific
    number of iterations; or that all agents are invoked at least once; or even to
    involve us, as users, in every iteration to provide further feedback to be incorporated
    in the upcoming iteration.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the multi-agent framework is showing promising advancements,
    and it is a glimpse of the outstanding reasoning capabilities behind LLMs and
    how they can unlock new ways of problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: Small language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are, unsurprisingly, large. This means that the architecture of an **ANN**
    featuring LLMs is made of a huge number of parameters, in the order of billions.
    Typically, a large number of parameters is associated with a better-performing
    model, since it is able to deal with more information and examples and henceforth
    is able to recognize and infer more patterns the moment users ask their questions.
    However, with large numbers of parameters typically comes a high cost of training
    and hosting, since a powerful AI infrastructure is needed. Plus, the energy consumption
    of these models raises serious questions about the environmental impact of LLM
    training and their overall sustainability in the long run.
  prefs: []
  type: TYPE_NORMAL
- en: These smaller models are called **small language models** (**SLMs**) and, besides
    being lighter and less demanding in terms of infrastructure, they are also showing
    surprisingly high performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we might think that GPT-3.5-turbo is deprecated; however, we have to remember
    that it used to be the most powerful model on the market just one year ago, and
    it is remarkable to see that a 7B model is capable of better results.
  prefs: []
  type: TYPE_NORMAL
- en: SLMs are definitely a research stream to keep an eye on, especially when it
    comes to scenarios where we might want to deploy a model locally or even customize
    it with fine-tuning (we will cover fine-tuning in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Legal and ethical landscape of generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing and deploying generative AI systems, a broad range of legal
    and ethical considerations must be carefully addressed to ensure responsible and
    sustainable use. These considerations extend beyond mere compliance and enter
    a domain where moral responsibility, public trust, and technological accountability
    intersect.
  prefs: []
  type: TYPE_NORMAL
- en: Copyright and intellectual property issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are often trained on vast corpora scraped from the internet, including
    content that may be copyrighted. As a result, there is a real risk of embedding
    copyrighted text, music, images, or video segments directly into AI output, inadvertently
    producing infringements when these outputs are shared or commercialized.
  prefs: []
  type: TYPE_NORMAL
- en: This concrete risk also escalated in November 2024, when major Canadian news
    organizations (https://www.reuters.com/sustainability/boards-policy-regulation/major-canadian-news-media-companies-launch-legal-action-against-openai-2024-11-29/),
    including The Globe and Mail and CBC/Radio-Canada, filed a lawsuit against OpenAI.
    They alleged that OpenAI used their copyrighted content without authorization
    to train its AI models, seeking damages and an injunction to prevent further unauthorized
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Misinformation, hallucinations, and the risk of fake news
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the known limitations of current generative AI models is their tendency
    to **hallucinate** – to produce entirely plausible-sounding but factually incorrect
    statements. This can result in the inadvertent spread of misinformation, especially
    when AI-generated content is taken at face value by consumers, journalists, or
    public officials.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, in December 2024, misinformation researcher Jeff Hancock (https://www.theverge.com/2024/12/4/24313132/jeff-hancock-minnesota-deepfake-law-ai-hallucinations-citation)
    admitted that ChatGPT fabricated details in a court filing he prepared, leading
    to the submission of non-existent citations. This incident emphasizes the risk
    of AI-generated content introducing inaccuracies in critical documents.
  prefs: []
  type: TYPE_NORMAL
- en: Continual exposure to unreliable AI output may lead to widespread skepticism
    regarding all digital content, undermining the credibility of legitimate sources
    and diminishing trust in expert commentary and reputable journalism. Organizations
    must therefore invest in factual verification processes, human-in-the-loop validation,
    and transparent model evaluation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Deepfakes and deceptive manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deepfake technology, an advanced subset of generative AI that synthesizes highly
    realistic images, videos, and voice recordings, can be weaponized to impersonate
    public figures, fabricate scandalous events, or produce manipulative political
    propaganda.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: A deepfake is a type of artificial media created using DL algorithms, where
    a person’s likeness, voice, or movements are digitally manipulated to create realistic
    but fake content. Typically, deepfakes involve altering videos or images to make
    it appear as if someone said or did something they never actually did.
  prefs: []
  type: TYPE_NORMAL
- en: A recent example occurred back in 2023, when a finance clerk at a Hong Kong
    branch of a multinational corporation was deceived into transferring over $25
    million after scammers used deepfake audio to impersonate senior executives, directing
    unauthorized fund transfers (https://www.secureworld.io/industry-news/hong-kong-deepfake-cybercrime).
  prefs: []
  type: TYPE_NORMAL
- en: Companies, governments, and individuals targeted by deepfakes may suffer severe
    reputational harm, leading to public embarrassment, financial losses, or diminished
    trust. Building detection tools, implementing digital watermarking techniques,
    and establishing legal frameworks that penalize malicious deepfake creators are
    crucial steps in mitigating these risks.
  prefs: []
  type: TYPE_NORMAL
- en: Bias, discrimination, and social harm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI models can unintentionally reproduce and magnify existing societal
    prejudices present in their training data. For example, models might consistently
    portray certain professions as male-dominated or depict particular cultural groups
    in stereotypical roles.
  prefs: []
  type: TYPE_NORMAL
- en: These biased outputs can influence hiring decisions, product recommendations,
    and policy-making processes, ultimately disadvantaging underrepresented groups.
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, a 2023 study, *Demographic Stereotypes in Text-to-Image Generation*
    (https://hai.stanford.edu/sites/default/files/2023-11/Demographic-Stereotypes.pdf),
    highlighted that text-to-image generative AI models tend to encode substantial
    bias and stereotypes. For example, prompts requesting images of professionals
    often resulted in depictions aligning with traditional gender roles, such as male
    doctors and female nurses, thereby reinforcing outdated and discriminatory views.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another study, *Social Dangers of Generative Artificial Intelligence: Review
    and Guidelines* (https://dl.acm.org/doi/fullHtml/10.1145/3657054.3664243), investigates
    the extent to which these technologies can exacerbate existing inequality. For
    instance, AI-generated content may marginalize certain communities by underrepresenting
    them or portraying them negatively, leading to social harm and reinforcing systemic
    discrimination.'
  prefs: []
  type: TYPE_NORMAL
- en: Organizations must commit to comprehensive bias audits, regularly updating training
    datasets, implementing fairness constraints, and involving diverse stakeholders
    in model development and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: These are just some examples of the potential risks and issues associated with
    generative AI. Furthermore, it is important to acknowledge that similar legal
    and ethical implications are not limited to generative AI, but rather they apply
    to the broader landscape of AI, whose applications have always been raising some
    concerns (for example, privacy considerations when it comes to face recognition).
  prefs: []
  type: TYPE_NORMAL
- en: However, the extremely rapid evolvement and – most importantly – adoption of
    generative AI tools has highlighted the pressing need for organizations, policymakers,
    and developers to collaborate to craft robust governance frameworks that address
    the unique challenges posed by generative AI. This involves adopting standards
    for transparent data sourcing, obtaining explicit permissions for copyrighted
    content, implementing strict verification procedures to counter misinformation,
    and working closely with regulators to establish legal guardrails. It also demands
    that AI practitioners remain continuously vigilant in updating models, refining
    algorithms, and engaging with interdisciplinary experts to ensure that generative
    AI serves as a force for innovation and positive societal impact, rather than
    a source of harm or ethical compromise.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored the exciting world of generative AI and its
    various domains of application, including image generation, text generation, music
    generation, and video generation. We learned how generative AI models such as
    ChatGPT and DALL-E, trained by OpenAI, use DL techniques to learn patterns in
    large datasets and generate new content that is both novel and coherent. We also
    discussed the history of generative AI, its origins, and the current status of
    research on it.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter was to provide a solid foundation in the basics of
    generative AI and to inspire you to explore this fascinating field further.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on one of the most promising technologies
    available on the market today, ChatGPT. We will go through the research behind
    it and its development by OpenAI, the architecture of its model, and the main
    use cases it can address as of today.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative adversarial networks: https://arxiv.org/abs/1406.2661'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyzing and improving the image quality of StyleGAN: https://arxiv.org/abs/1912.04958'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Video-to-video synthesis: https://arxiv.org/abs/1808.06601'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A deep generative model trifecta: Three advances that work towards harnessing
    large-scale power: https://www.microsoft.com/en-us/research/blog/a-deep-generative-
    model-trifecta-three-advances-that-work-towards-harnessing- large-scale-power/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vid2Vid: https://tcwang0509.github.io/vid2vid/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLaMA: Open and Efficient Foundation Language Models: https://arxiv.org/pdf/2302.13971'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Introducing Phi-3: https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our communities on Discord and Reddit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have questions about the book or want to contribute to discussions on Generative
    AI and LLMs? Join our Discord server at [https://packt.link/I1tSU](Chapter_1.xhtml)
    and our Reddit channel at [https://packt.link/jwAmA](Chapter_1.xhtml) to connect,
    share, and collaborate with like-minded enthusiasts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Discord.png) ![](img/QR_Code757615820155951000.png)'
  prefs: []
  type: TYPE_IMG
