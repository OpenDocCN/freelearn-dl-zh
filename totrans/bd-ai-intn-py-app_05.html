<html><head></head><body>
		<div id="_idContainer054">
			<h1 class="chapter-number" id="_idParaDest-78"><a id="_idTextAnchor115"/><a id="_idTextAnchor116"/>5</h1>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor117"/>Vector Databases</h1>
			<p>Sometimes, data is rich with information and has a well-defined structure. If you know what you want, then this data is straightforward to work with in a modern database system. However, you often don’t know exactly what you need. Without specific search terms or phrases, you may not receive optimal search results. For example, you might not know the brand or name of your picky pet’s favorite food. In such complex cases, traditional information search and retrieval methods can <span class="No-Break">fall short.</span></p>
			<p>Modern AI research has given rise to a new class of methods that can encode the underlying semantic meaning of something instead of just its raw data. For example, AI models can understand that when you ask for <strong class="source-inline">the new action movie with that one actor who was also in the movie with green falling numbers</strong>, you’re asking for the latest <em class="italic">John Wick</em> film, which stars Keanu Reeves, who was also the star of <em class="italic">The </em><span class="No-Break"><em class="italic">Matrix</em></span><span class="No-Break"> films.</span></p>
			<p>To achieve this result, these methods convert their inputs into a numerical format called a <strong class="bold">vector embedding</strong>. <strong class="bold">Vector databases</strong> provide a means to efficiently store, organize, and search these vector representations. This makes vector databases valuable tools for retrieval tasks, which are common in AI applications. In this chapter, you will learn about vector search, the key concepts and algorithms associated with it, and the significance of vector databases. By the end of this chapter, you will understand the workings of graph connectivity and its application in architecture patterns such as RAG. You will also understand the best practices for building vector <span class="No-Break">search systems.</span></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Vector embeddings <span class="No-Break">and similarity</span></li>
				<li>Nearest neighbor <span class="No-Break">vector search</span></li>
				<li>The need for <span class="No-Break">vector databases</span></li>
				<li>Case studies and <span class="No-Break">real-world applications</span></li>
				<li>Vector search <span class="No-Break">best practices</span></li>
			</ul>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor118"/>Technical requirements</h1>
			<p>While not required, it may help to have some familiarity with graph data structures and operations. You may also want to know about the embedding models that are used to create vectors, which are discussed in more detail in <a href="B22495_04.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <span class="No-Break"><em class="italic">Embedding Models</em></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor119"/>What is a vector embedding?</h1>
			<p>At the most basic level, a <strong class="bold">vector</strong> is a list of numbers plus an implicit structure that determines how those numbers are defined and how you can compare them. The number of elements in a vector is the <span class="No-Break">vector’s dimension.</span></p>
			<p><strong class="bold">Dimensions</strong> represent different aspects of the thing that they describe. You might think of a list of properties that describe a car and list them out in a structured way such that the order is always <strong class="source-inline">[year, make, model, color, mileage]</strong>. These properties form a <strong class="bold">vector space</strong> that can describe any car for which these properties hold. For example, you could describe a specific car with these values as <strong class="source-inline">[2000, Honda, Accord, </strong><span class="No-Break"><strong class="source-inline">Gold, 122000]</strong></span><span class="No-Break">.</span></p>
			<p>This is a useful model for building intuition on how vectors can encode information. However, each element may not always correspond to a concrete idea with a numerable set of possible values. The vectors used in AI applications are more abstract and have significantly more dimensions. In a way, they smear concrete ideas across many dimensions and standardize to a single set of possible values for every dimension. For example, vectors from OpenAI’s <strong class="source-inline">text-embedding-ada-002</strong> model always have 1,536 elements, and each element is a floating-point number between -1 <span class="No-Break">and 1.</span></p>
			<p>The vectors used in AI applications are the output of <strong class="bold">embedding models</strong>. These are <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models that are pre-trained to convert inputs, typically a string of text tokens, into vectors that encode the semantic meaning of the input. For humans, the many dimensions of these vectors are basically impossible to decipher. However, the embedding model learns an implicit meaning for every dimension during training and can reliably encode that meaning for <span class="No-Break">its inputs.</span></p>
			<p>The exact structure of the vectors varies between embedding models, but a specific model always outputs vectors of the same size. To use a vector, it’s imperative to know which model <span class="No-Break">created it.</span></p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor120"/>Vector similarity</h2>
			<p>Beyond storing high-dimensional vector data, vector databases also support various operations that let you query and search for <span class="No-Break">the vectors.</span></p>
			<p>The most common operation is <strong class="bold">nearest neighbor search</strong>, which returns a list of stored vectors that are most similar to an input query vector. Common search interfaces are familiar territory. For instance, e-commerce searches often prioritize products relevant to your query, even if they aren’t exact matches. Nearest neighbor search uses the semantic nature of embedding model vectors to make finding <em class="italic">similar</em> vectors the same as finding <span class="No-Break"><em class="italic">relevant</em></span><span class="No-Break"> results.</span></p>
			<p>But what does it mean for two vectors to be similar? In short, similar vectors are close together, which you can measure as a distance. There are many ways to define <strong class="bold">distance</strong>, including some that become more relevant in higher dimensions. It’s not possible to visualize how distance works for high-dimensional vectors but it’s straightforward to see how the ideas work for small vectors and then scale <span class="No-Break">them up.</span></p>
			<p>If you think back to geometry class, you’ll remember that you can find the distance between two coordinate vectors using the distance formula. For example, 2D coordinates such as <strong class="source-inline">(x, y)</strong> use the distance formula <strong class="source-inline">distance(a, b) = sqrt((a_x - b_x)**2 + (a_y - b_y)**2)</strong>. It also works for 3D coordinates, where the formula has another component for the extra dimension: <strong class="source-inline">sqrt((a_x - b_x)**2 + (a_y - b_y)**2 + (a_z - b_z)**2)</strong>. This pattern generalizes to any number of dimensions and is referred to as the <strong class="bold">Euclidean distance</strong> between two <span class="No-Break"><em class="italic">n</em></span><span class="No-Break">-dimensional points.</span></p>
			<p>In theory, you can also use Euclidean distance to measure distances between high-dimensional vectors such as those used in AI applications. Practically, however, the usefulness of Euclidean distance breaks down as you continue to increase the number of dimensions. This pattern of intuitions and tools that work in small dimensions breaking down at higher dimensions is common and often referred to as the <strong class="bold">curse </strong><span class="No-Break"><strong class="bold">of dimensionality</strong></span><span class="No-Break">.</span></p>
			<p>Instead of Euclidean distance, most applications use a different distance metric called <strong class="bold">cosine similarity</strong>. Unlike Euclidean distance, which measures the space between the <em class="italic">tips</em> of two vectors, cosine similarity uses a different formula that measures the size of the angle between two vectors that share a common base. It effectively determines whether two vectors are identical, completely unrelated, or (most likely) somewhere in between in a mathematically precise way, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.1</em>. Similar vectors point in almost the same direction, unrelated vectors are orthogonal, and opposite vectors point in <span class="No-Break">opposite directions.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer045">
					<img alt="" role="presentation" src="image/B22495_05_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1: A comparison of vector measurements</p>
			<p>Cosine similarity equips you with a tool to measure the distance between two vectors. Due to the nature of how vector embeddings carry semantic information, it’s also a tool to measure how related or relevant two vectors are to one another. If you extend this idea to more vectors, you can figure out how related a given vector is to any of the others and even rank them all by relevance. This is the core idea behind <strong class="bold">vector </strong><span class="No-Break"><strong class="bold">search algorithms</strong></span><span class="No-Break">.</span></p>
			<p>The process of comparing many vectors in this way brings its own complexity and challenges. To deal with them, search providers have developed various approaches to nearest neighbor search that balance trade-offs and optimize for different use cases. The next section will discuss two approaches to handle real search <span class="No-Break">use cases.</span></p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor121"/>Exact versus approximate search</h2>
			<p>Sometimes, your use case requires that searches return only the true nearest neighbors. For example, think about an authentication app that stores biometric information about its users as embedded vectors so that they can identify themselves later. When they scan their fingerprint or face, the app creates a vector embedding of the scanned data and uses it as the query vector in a nearest neighbor search. An app like this should never misidentify the user as someone else with a similar fingerprint <span class="No-Break">or face.</span></p>
			<p>This use case is perfect for an <strong class="bold">exact nearest neighbor</strong> (<strong class="bold">ENN</strong>) search, which guarantees that the search results are the best possible matches. This type of search must always return the closest matching stored vector and ensure that it appears before other similar but more <span class="No-Break">distant matches.</span></p>
			<p>One straightforward approach is to brute-force the problem: calculate the distance between the query vector and every stored vector, then return a list of the results sorted from closest to farthest. By checking every vector, you can guarantee that the search results include precisely the most relevant vectors in order. While effective for small datasets, this method quickly becomes computationally expensive and time-consuming as the number of stored vectors increases. Some clever approaches can help exact search scale to larger datasets, such as using tree-based indexes to avoid calculating similarity for every vector. This makes exact search useful for some additional kinds of applications, but ultimately, the problem does not scale well and can take a long time on large datasets. In cases where exactness is required, you have to accept its constraints and find ways <span class="No-Break">around them.</span></p>
			<p>For other common cases, however, it is enough to know that your search results are <em class="italic">close enough</em> to be the best match. This use case is called an <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search and is powerful enough for many <span class="No-Break">everyday applications.</span></p>
			<p>For example, if you search for <strong class="source-inline">movies like Inception</strong> in a recommendations app, you don’t need the results to include a specific movie. Rather, you probably just want a list of a few similar sci-fi thrillers with mind-bending plots. A list of results such as <strong class="source-inline">["Minority Report", "Memento", "Shutter Island"]</strong> is useful, even if it turns out that the movie <em class="italic">Interstellar</em> is technically a closer semantic match than any of the <span class="No-Break">returned results.</span></p>
			<p>The choice between exact and approximate search comes down to your application’s requirements. You may have strict requirements that necessitate an exact search. However, you may also have a use case where an exact search, while useful, is not necessary to provide value. Or it might not make sense to do an exact search at all. In the next section, you’ll learn how to evaluate search algorithms to help you determine <span class="No-Break">your requirements.</span></p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor122"/>Measuring search</h2>
			<p>You can describe a search algorithm in terms of its precision, recall, <span class="No-Break">and latency:</span></p>
			<ul>
				<li><strong class="bold">Precision</strong> measures how accurate the search results are. Precise searches try to return only matches that are relevant to the query and few, if any, <span class="No-Break">irrelevant results.</span></li>
				<li><strong class="bold">Recall</strong> measures how complete the search results are. If a search returns a large fraction of all relevant results, then it has a <span class="No-Break">high recall.</span></li>
				<li><strong class="bold">Latency</strong> measures how long a search query takes from start to finish. Every search takes some amount of time to return results. The exact latency varies between searches but, on average, it’s a function of how many vectors are in the search space and your precision and <span class="No-Break">recall requirements.</span></li>
			</ul>
			<p>These factors are tightly coupled and require trade-offs that define the nature of nearest neighbor searches. For example, an ENN search has perfect precision and will include the most relevant results. However, to keep the latency reasonable, it might omit some relevant results if there are too many. Because it misses valid results, this search would have a relatively low recall. If the ENN search also required a high recall, then the search would have to run for longer to ensure that enough relevant results <span class="No-Break">are included.</span></p>
			<p>In an ANN search, you can relax your precision requirements, which allows you to optimize the other factors instead. You can get more complete results by either allowing the search to take more time or by returning more results that potentially include false positives. If you can tolerate false positives, for example, by filtering them out after the search in your app, then you can use ANN to run fast searches that return highly relevant <span class="No-Break">result sets.</span></p>
			<p>You should evaluate your application and determine its top priority regarding these factors. Then, you can choose the appropriate search operation and tune the algorithm until the other factors are <span class="No-Break">appropriately balanced.</span></p>
			<p><strong class="bold">Tuning</strong> a search algorithm involves modifying the configuration parameters that determine how it constructs and traverses its index data structure. To get a better feel for what that means, you’ll spend the next few sections going over the concepts and data structures used to enable vector search operations, starting with the idea <span class="No-Break">of connectivity.</span></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor123"/>Graph connectivity</h1>
			<p>If you’ve ever used a city’s public transit network to get around, you may have wondered about how the city chose to put the train or bus stops where they did. There are many factors at play, but if you look at an ideal case, then you can boil the choice down to two related factors: <strong class="bold">connectivity</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">latency</strong></span><span class="No-Break">.</span></p>
			<p>Think about the experience of a train rider, let’s call her Alice, visiting her friend, Bob, across the city. It would be great if there was a stop right next to Bob’s house because, then, Alice could see him right after stepping off the train. Of course, you can’t put a train station in front of every house, and after a certain point, adding more stops would increase the average <span class="No-Break">trip time.</span></p>
			<p>Every time you change the number of stops or connections, you may affect how long it takes to get between any two destinations in the system. Typically, the job of planning where to place public transit stops is done with thought and consideration by knowledgeable civil engineers, city planners, and other stakeholders. The primary goal of a transit network is to take a rider to a stop that is relatively close to their true final destination in a reasonable amount of time. By understanding their goal and applying specific strategies, city planners try to connect distant parts of the city in a way that’s useful and efficient for <span class="No-Break">transit riders.</span></p>
			<p>Similarly, the goal of an ANN search is to find a vector that is close to a given query vector, also in a reasonable amount of time. If you were to take inspiration from transit planners, you could use this similarity to your advantage and design an effective <span class="No-Break">ANN index.</span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor124"/>Navigable small worlds</h2>
			<p>In essence, both transit planning and nearest neighbor search boil down to a problem of building and traversing a graph that trades off connectivity and latency. You can use an algorithm called <strong class="bold">navigable small worlds</strong> (<strong class="bold">NSW</strong>) to build such a graph. It takes in vectors one at a time and adds a node to the graph for each one. Each node can also have connections to other nodes, called <strong class="bold">neighbors</strong>, that are assigned during <span class="No-Break">graph construction.</span></p>
			<p>The NSW algorithm is designed to balance how relevant a node’s immediate neighbors are with how connected the node is to the rest of the graph. It will mostly assign neighbors that are closely related to a node. However, it may also sometimes connect two less similar nodes that are relatively far apart on the graph. If you think about the transit example, this is like having a bus route that has several stops in the same neighborhood but that also runs downtown. Residents can easily get to their local destinations. If they need to go outside of the neighborhood, then they still have access to the rest of <span class="No-Break">the city.</span></p>
			<p>For an example of an NSW graph, refer to <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em>. Notice that each node is connected to a maximum of three neighbors and that, in general, nearby nodes are closely connected. Each node represents a vector and nodes connected with lines are neighbors. The highlighted connections show the path of a greedy nearest <span class="No-Break">neighbor search.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer046">
					<img alt="" role="presentation" src="image/B22495_05_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2: An NSW graph</p>
			<p>Once you’ve constructed an NSW graph of your vectors, you can use it as an index for ANN searches. You can start at a random node and use a search algorithm to follow the neighbor connections until you reach the nearest neighbor. This lets you limit your similarity comparison to only a subset of the total search space. For example, notice how the search path in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em> arrives at the nearest neighbor without visiting every node in <span class="No-Break">the graph.</span></p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor125"/>How to search a navigable small world</h2>
			<p>The exact search algorithm that you use to traverse an NSW graph may vary and affect the behavior of the search as a whole. The most common algorithm is a simple <strong class="bold">greedy search</strong>, where at every step, you find and take the best immediate option with no regard to previous or future steps. For example, a greedy search of an NSW graph first randomly selects a node to start at and then measures to see how close the node is to the query vector. Then, it measures the distance to each of the node’s neighbors. If one of the neighbors is closer than the current node, then the search moves on to that node and continues with the same measure-and-compare process. Otherwise, the search is complete and the current node is an approximate <span class="No-Break">nearest neighbor.</span></p>
			<p>In this basic example of NSW with greedy search, the definition of <em class="italic">approximate</em> is very broad and the search may return suboptimal results. This comes down to the nature of graph search, which, in this case, is designed to find a local minimum of the graph. This local minimum is not guaranteed to be the <em class="italic">global</em> minimum, which is what makes the search approximate rather than exact. A greedy search algorithm alone can return false positives if it settles on a local minimum that is too far from the <span class="No-Break">global minimum.</span></p>
			<p>You can partially guard against this by tuning the graph’s construction parameters. However, due to the dynamic nature of search queries and the underlying data being searched, you can’t entirely prevent false positive local minima from existing. Instead, you need to find a way to minimize <span class="No-Break">their impact.</span></p>
			<p>One way is to run the search multiple times, starting from different randomized entry nodes. This method, called <strong class="bold">randomized retries</strong>, collects multiple samples from the graph and returns the best result out of all the samples. You can also add additional machinery to the algorithm to make it more robust. A common architecture pairs the greedy search algorithm with a configurable <strong class="bold">priority queue</strong> that keeps a sorted list of the nearest neighbors the search has seen. If the search encounters a false positive local minimum, the queue lets it backtrack and explore other branches of the graph that might lead to a <span class="No-Break">nearer neighbor.</span></p>
			<p>The exact search method you use depends on the dataset and your goals. For example, randomized retries are easy to implement and can run in parallel. They are useful for subtle, exploratory searches that might match many local minima. However, their random nature makes them non-deterministic, and each retry does a full search, which can quickly scale your costs. Conversely, priority queues are deterministic and precise but are harder to implement <span class="No-Break">and tune.</span></p>
			<p>With this information, you have the basis for a useful vector search index. You could stop building the index here and start searching. However, you will quickly find that there are issues with this approach, particularly as you scale the search space to sizes commonly seen in AI apps. Randomized retries have significant computational overhead, and you must do more of them as you scale your data set. A priority queue keeps a search from getting stuck in local minima but does not prevent it from meandering through many nodes on the way to <span class="No-Break">its target.</span></p>
			<p>To address these issues, you need to go beyond a single NSW graph. In the next section, you will see how combining multiple NSW graphs together can circumvent meandering searches and make randomized retries <span class="No-Break">less necessary.</span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor126"/>Hierarchical navigable small worlds</h2>
			<p>Think back to Alice’s public transit experience. What if, instead of the same city, she and Bob lived in different cities on opposite sides of the country? Alice could, in theory, limit herself to public transit services by crisscrossing the nation via a series of trains, buses, taxis, and bike shares. This would obviously take a lot of time and require many stops along the way. That’s because transit networks are only effective at the scale of an individual city. Once you zoom out farther, you need a <span class="No-Break">different system.</span></p>
			<p>Instead of just using transit, Alice could instead start at her city’s airport and fly to Bob’s city. Even if her trip included a layover and multiple flights, it would still probably be faster than using transit alone. Once she gets to Bob’s city, she can use the subway system to get from the airport to his neighborhood quickly <span class="No-Break">and efficiently.</span></p>
			<p>Alice’s trip took place at two distinct levels. First, she started at the level of airports, where she was free to travel to any destination airport connected to her home airport. At this layer, she had direct access to many different cities, but that access was limited to only one location in each city: the airport. She used the airports to get closer to Bob without spending too much time planning her route and traveling. Once she got to the closest airport to Bob, she dropped down into the second layer and gained access to a transit network that could get her even closer <span class="No-Break">to Bob.</span></p>
			<p>This is basically the idea of <strong class="bold">hierarchical navigable small worlds</strong> (<strong class="bold">HNSW</strong>). You can create a hierarchy of layers where each layer is an NSW graph. For example, look at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.3</em> to see a typical HNSW graph structure. The top layer has relatively few nodes that are all fairly distant from one another and sparsely connected. Each lower layer has all the nodes of the layer above it plus additional nodes and connections that make the graph denser and more connected. In this chapter’s example, the distinction between transit nodes and airport nodes is a natural way to split the layers. The airports are the top layer and the next layer down includes both the airports and the <span class="No-Break">transit stops.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer047">
					<img alt="" role="presentation" src="image/B22495_05_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3: An HNSW graph structure</p>
			<p>An actual HNSW algorithm would decide the <em class="italic">top</em> layer for each vector probabilistically with a node that exists only in lower layers being more likely than one that also exists in higher layers. A search starts in the top layer by finding the node that’s nearest to the query vector. Then, it moves to the same node but in the next layer down and continues the search from there. This continues until it reaches the nearest neighbor on the final layer, at which point, the search is complete. In <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.3</em>, the highlighted connections show the path of a greedy nearest neighbor search across <span class="No-Break">multiple layers.</span></p>
			<p>HNSW is the foundation of many modern vector search applications. It’s battle-tested and proven to give useful results in a reasonable amount of time. The algorithm is highly suited for ANN use cases with configurable parameters that put you in control of how your <span class="No-Break">searches perform.</span></p>
			<p>Now that you have an idea of the inner workings of vector search, you can see how it requires purpose-built logic and data structures. In the next section, you’ll learn how vector databases encapsulate all of the technical details in order to make vector search available <span class="No-Break">to developers.</span></p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor127"/>The need for vector databases</h1>
			<p>Vectors carry deep semantic information and have many potential use cases that will make them increasingly common over the next few years. Working with them requires specific and complex operations that only process vector data. Additionally, the demand for search can often vary substantially from the demand for more structured <span class="No-Break">database queries.</span></p>
			<p>Together, these factors mean vector operations and traditional database workloads are largely independent. This gives rise to the concept of a vector database that’s designed specifically to handle vector data, indexes, and workloads. From a developer’s perspective, vector databases can take <span class="No-Break">several forms.</span></p>
			<p>The most basic is a <strong class="bold">standalone product</strong> that’s independent from other operational databases. This type of vector database has the freedom to focus solely on implementing and optimizing vector operations without considering other database operations. However, often, vector search applications require additional filtering or metadata and may perform more traditional database operations based on search results. These use cases require either multiple queries to different databases at runtime or an additional syncing layer that copies data from your operational database to the <span class="No-Break">vector store.</span></p>
			<p>Alternatively, a vector database can be baked into an existing database or data service. For example, a <strong class="bold">general-purpose database management system</strong> might support vector search operations in its query language if you’ve defined the appropriate vector search index. This allows applications to piggyback off of the existing system’s features and access search within the same system. The vector database can be scaled and run independently within the system but exposed to the user along with traditional operations as part of a unified API. This couples your vector store to your existing database but leads to simpler and <span class="No-Break">easier-to-maintain architectures.</span></p>
			<p>Regardless of form, vector databases are a key tool in AI applications. They are purpose-built to store and query vector data. You can configure them to deliver optimal search results and power <span class="No-Break">AI applications.</span></p>
			<p>The next section will cover some ways that vector search can be used to enhance ML and AI models, including during training, fine-tuning, and runtime. You’ll also learn how vector search itself enables AI applications without additional functions <span class="No-Break">or models.</span></p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor128"/>How vector search enhances AI models</h2>
			<p>AI models encompass a broad class of data structures and techniques. ML forms the core of most modern vector-based AI models, aiming to “teach” computers to do specific tasks via a training process. In general, ML processes work by feeding a curated dataset to a base model that can detect and infer patterns from the data. Once a model has learned these patterns, it’s able to recreate or interpolate them to process new inputs. These techniques and models are ubiquitous in the world of AI and are the secret sauce that powers novel <span class="No-Break">use cases.</span></p>
			<p>In general, ML training and AI applications can be split into two concerns, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Information retrieval</strong> involves finding relevant information that’s useful as input to an AI process. Vector search is very well suited for this task. Embedding models can encode the semantics of a huge variety of inputs into a standard vector form. Then, you can use search to find matches for an equally huge range of inputs, both structured <span class="No-Break">and unstructured.</span></li>
				<li><strong class="bold">Information synthesis</strong> combines multiple pieces of information, possibly from different sources, into a coherent and useful result. This is the domain of GenAI models. These models can’t reliably find or generate true facts, but they can effectively process and reformat <span class="No-Break">input information.</span></li>
			</ul>
			<p>Vector search enhances ML and AI models by providing them with access to the most relevant data at every stage, from training to fine-tuning to <span class="No-Break">runtime execution.</span></p>
			<p>During training, you can use a vector database to store and search your training data. You can design a process that finds the most relevant data from the corpus to use for each training task. For example, when training a language model for a specific domain such as medicine, you could use vector search to retrieve the most relevant chapters from medical textbooks for each training batch. This ensures that the model learns the most pertinent information without being distracted <span class="No-Break">by noise.</span></p>
			<p>You can apply the same idea during fine-tuning, which is essentially a secondary training stage on top of a more generic base model. For example, you could fine-tune the medicine language model to generate reports using a hospital system’s preferred style and structure. Vector search could help find human-written reports that are relevant to each <span class="No-Break">training topic.</span></p>
			<p>Whether your model is specialized or general purpose, you can customize its runtime behavior by modifying the inputs you give to it. Vector search can analyze raw input and find related information. Then, you can augment or refine the raw input to include the retrieved context. For example, you might maintain a vector database of rare diseases and search for anything that matches a user’s description in order to get a more <span class="No-Break">tailored diagnosis.</span></p>
			<p>AI applications come in many forms, but modern apps increasingly use a runtime customization approach to provide relevant context to generative transformer models. This architecture is the basis of a technique called <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>), which you’ll learn about in greater depth in <a href="B22495_08.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Implementing Vector Search in </em><span class="No-Break"><em class="italic">AI Applications</em></span><span class="No-Break">.</span></p>
			<p>Up to this point, you’ve learned the theory and mechanics of vector databases and search operations. Next, you’ll look at some examples of real vector database use cases that highlight how vectors are the core of modern <span class="No-Break">AI apps.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor129"/>Case studies and real-world applications</h1>
			<p>Vector search is a powerful tool that enables you to build sophisticated systems for finding information based on its meaning, rather than just its exact words. By understanding the context and relationships between data points, vector search helps you retrieve highly relevant results. So far, you have learned about the different concepts involved with vector search and some of the different offerings that exist in the market, but how do businesses integrate vector search into <span class="No-Break">their applications?</span></p>
			<p>In this section, you will explore three popular methods for leveraging vector search: semantic search, RAG, and <strong class="bold">robotic process automation</strong> (<strong class="bold">RPA</strong>). You will look at existing case studies of <strong class="bold">MongoDB Atlas Vector Search</strong> that fit into each of these buckets, and how these applications deliver value to the end user through more accurate search that wasn’t previously possible. Each of the following case studies was originally published as a part of the <em class="italic">Building AI with MongoDB</em> series of customer stories (<a href="https://www.mongodb.com/resources/use-cases/artificial-intelligence?tck=blog-genai&amp;section=resources&amp;contentType=case-study"><span class="P---URL">https://www.mongodb.com/resources/use-cases/artificial-intelligence?tck=blog-genai&amp;section=resources&amp;contentType=case-study</span></a>). These stories are presented here to showcase the variety of vector search use cases that can be built on the flexible, scalable, and multifaceted MongoDB <span class="No-Break">Atlas platform.</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor130"/>Okta – natural language access request (semantic search)</h2>
			<p><strong class="bold">Okta</strong>, one of the world's leading identity security providers, uses a natural language RAG interface to allow users to easily request roles for new technologies in their organizations. They built a system called <strong class="bold">Okta Inbox</strong> using Atlas Vector Search and their own custom embedding model that makes it possible for users to map natural language queries to the <span class="No-Break">right roles.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer048">
					<img alt="" role="presentation" src="image/B22495_05_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4: Okta Inbox user request form</p>
			<p>This is an example of leveraging semantic search to solve a problem, where the embedding models trained by Okta’s data science team were capable of mapping natural language requests to the right user roles to <span class="No-Break">be assigned.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer049">
					<img alt="" role="presentation" src="image/B22495_05_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5: Okta Inbox administrator view</p>
			<p>These requests would get routed to a manager via Slack through an existing workflow. The end result is a simple user experience that makes identity management between both the requesters and the access managers much simpler, thus making the value proposition of Okta as an identity and access management solution <span class="No-Break">even greater.</span></p>
			<p>Okta chose to use Atlas Vector Search to query these vectors since they were already using Atlas as their operational data store, and this provided a simplified developer experience. You can read more about this case study <span class="No-Break">at </span><a href="https://www.mongodb.com/solutions/customer-case-studies/okta"><span class="No-Break"><span class="P---URL">https://www.mongodb.com/solutions/customer-case-studies/okta</span></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor131"/>One AI – language-based AI (RAG over business data)</h2>
			<p><strong class="bold">One AI</strong> provides verticalized AI agents and chatbots for different industries. These services allow detailed AI-assisted analysis to be performed over documents with applications in industries ranging from financial services and real estate to manufacturing <span class="No-Break">and retail.</span></p>
			<p>The chatbots offered by One AI are all built using the MongoDB Atlas platform, with over 150 million indexed documents from over 20 different internal services. One AI’s goal of bringing AI to everyday life is made feasible by simply adding a vector search index to the data that they store in Atlas and making it queryable via embedded natural <span class="No-Break">language input.</span></p>
			<p class="author-quote">“A very common use case in language AI is creating vectors that represent language. The ability to have that vectorized language representation in the same database as other representations, which you can then access via a single query interface, solves a core problem for us as an API company.”</p>
			<p>—Amit Ben, CEO and founder of <span class="No-Break">One AI</span></p>
			<p>This is a prime example of a multitenant RAG application, where data that is indexed and provided for one type of AI service provided by One AI might not be relevant to another service. As discussed later in this chapter, this is a common data modeling pattern that is easy to build within the Atlas platform. You can further read about this case study <span class="No-Break">at </span><a href="https://www.mongodb.com/solutions/customer-case-studies/one-ai-success-story"><span class="No-Break"><span class="P---URL">https://www.mongodb.com/solutions/customer-case-studies/one-ai-success-story</span></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor132"/>Novo Nordisk – automatic clinical study generation (advanced RAG/RPA)</h2>
			<p><strong class="bold">Novo Nordisk</strong> is one of the world’s leading healthcare companies with a mission to defeat some of the world’s most serious chronic diseases such as diabetes. As a part of the process of getting new medicines approved and delivered to patients, they must generate a <strong class="bold">clinical study report</strong> (<strong class="bold">CSR</strong>). This is a detailed record of the methodology, execution, results, and analyses of a clinical trial and is meant as a critical source of truth for regulatory authorities and other stakeholders in the drug <span class="No-Break">approval process.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer050">
					<img alt="" role="presentation" src="image/B22495_05_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6: Example of a CSR</p>
			<p>Typically, a CSR takes around 12 weeks to complete, but the content digitalization team at Novo Nordisk was able to build a tool using Atlas Vector Search to shorten this process to ten minutes. They built a RAG workflow called <strong class="bold">NovoScribe</strong> leveraging <strong class="bold">Claude 3</strong> and <strong class="bold">ChatGPT</strong> as their chat completion models, and <strong class="bold">Titan</strong> for text embedding hosted on the <strong class="bold">Amazon Bedrock</strong> service. They used MongoDB Atlas Vector Search as a knowledge base to serve relevant data to <span class="No-Break">these models.</span></p>
			<p>Functionally, NovoScribe generates validated text using defined content rules and statistical outputs. Atlas Vector Search computes the similarity of each text snippet to the relevant statistics, which is then fed into a structured prompt to the LLM to produce a CSR that is ready for review by a subject-matter expert, including the lineage of all of the <span class="No-Break">data presented.</span></p>
			<p class="author-quote">“What’s great about MongoDB Atlas is that we can store native vector embeddings of the report right alongside all of their associated text snippets and metadata. This means we can run really powerful and complex queries quickly. For each vector embedding we can filter on which source document it’s coming from, who wrote it, and when.”</p>
			<p>—Tobias Kröpelin, PhD, <span class="No-Break">Novo Nordisk</span></p>
			<p>This project allowed Novo Nordisk to build an advanced clinical report generation system by intelligently arranging their data in the right format within MongoDB and defining a vector search index against it. They were allowed to go further with their data in more ways using novel embedding models and LLMs to dramatically improve the process of authoring CSRs as a result. You can read more about this case study <span class="No-Break">at </span><a href="https://www.mongodb.com/solutions/customer-case-studies/novo-nordisk"><span class="No-Break"><span class="P---URL">https://www.mongodb.com/solutions/customer-case-studies/novo-nordisk</span></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor133"/>Vector search best practices</h1>
			<p>This section covers the best practices for improving the accuracy of your vector search through intelligent data modeling, deployment model options, and considerations for prototype and production use cases. By following the guidance in this section, you will be more likely to improve the quality of your vector search results and operate your search system in a scalable, <span class="No-Break">production-ready manner.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor134"/>Data modeling</h2>
			<p>In the context of MongoDB, <strong class="bold">data modeling</strong> refers to the process of designing the structure of the data stored in the database. Unlike traditional relational databases, MongoDB is a NoSQL database that uses a flexible, schema-less model, allowing for more dynamic and hierarchical data storage. The big idea about data modeling for vector search centers around the notion that embedding models are not infinitely capable, and users can take control of the relevance search problems in embedding models by using vectors along with the other data they have. Taking control can be done in simple ways, such as incorporating user-based fields for metadata filtering. You can take control in more complicated ways, too, such as by using LLMs to define graph relationships between chunks and looking these up at query time subsequent to a <strong class="source-inline">$</strong><span class="No-Break"><strong class="source-inline">vectorSearch</strong></span><span class="No-Break"> query.</span></p>
			<p>One can broadly think about leveraging metadata as using documents to deliver the data back to the user, rather than vectors. Working with documents as the results of an aggregation stage means that different aggregation stages can be composed together to yield greater functionality than any one alone and can benefit from query optimization. This has been the bread and butter of the document model since MongoDB was invented, and it continues to be the case today in the age of <span class="No-Break">GenAI applications.</span></p>
			<p>This section will dive deeper into the ways other data can be used prior to, alongside, and following vector search to improve the accuracy of your vector-based information <span class="No-Break">retrieval system.</span></p>
			<h3>Filtering</h3>
			<p>The most basic yet most effective form of metadata usage is to limit the scope of the vector search by considering only vector data that meets a prefilter. This restricts the scope of valid documents to be considered, which, for selective filters (the most common kind of filter), increases accuracy and reduces <span class="No-Break">query latency.</span></p>
			<p>At query time, these prefilters can be considered as a part of a <strong class="source-inline">$vectorSearch</strong> query using a <strong class="source-inline">$match</strong> MQL semantic. This means that in addition to point filters such as <strong class="source-inline">$eq</strong>, the user can define range filters such as <strong class="source-inline">$gt</strong> or <strong class="source-inline">$lt</strong> to only search against documents that fit a range of values rather than matching a specific one. This can dramatically reduce the number of valid documents that need to be searched, reducing the amount of work that needs to be done and generally improving the accuracy of your search. <strong class="source-inline">$match</strong> filters can also leverage logical operators such as <strong class="source-inline">$and</strong> and <strong class="source-inline">$or</strong> to allow users to compose filters together and build more complex logic into their <span class="No-Break">search applications.</span></p>
			<p>Let’s look at two common types of filters, and when and how you might <span class="No-Break">use them.</span></p>
			<h4>Dynamic filters</h4>
			<p><strong class="bold">Dynamic filters</strong> are pieces of metadata that vary based on the content of the search query. These can be attributes of the data, such as when a book was published or its price. They are typically selected by a user when executing their search along with their plain English query. Here is <span class="No-Break">an example:</span></p>
			<pre class="source-code">
[
    {
        "_id": ObjectID("662043cfb084403cdcf5210a"),
        "paragraph_embedding": [0.43, 0.57, ...],
        "page_number": 12,
        "book_title": "A Philosophy of Software Design",
        "publication_year": 2018
    },
    {
        "_id": ObjectID("662043cfb084403cdcf5210b"),
        "paragraph_embedding": [0.72, 0.63, ...],
        "page_number": 6,
        "book_title": "Design Patterns: Elements of Reusable Object-Oriented  Software",
        "publication_year": 1994
    },
    {
        "_id": ObjectID("662043cfb084403cdcf5210c"),
        "paragraph_embedding": [0.12, 0.48, ...],
        "page_number": 3,
        "book_title": "Guide to Fortran",
        "publication_year": 2008
    }, ...
]</pre>			<p>Dynamic filters are most common when building a semantic search application since they are typically input by the user prior to executing a query within a search bar. This contrasts with a RAG interface, which is entirely <span class="No-Break">natural language.</span></p>
			<h4>Static filters and multitenancy</h4>
			<p>There are cases where the filter is associated not with the body of the query, but by the user’s profile. The user may be querying data that is accessible only to their company but is stored in a multi tenanted fashion with many other tenants’ data. In this case, the user ID or company ID that the user belongs to may be used to filter what results are searched against. For cases where there are a high number of tenants and few vectors, filters are the recommended approach for modeling data rather than storing many bits of data across multiple collections <span class="No-Break">and indexes.</span></p>
			<p>It is recommended to set the <strong class="source-inline">exact</strong> flag to <strong class="source-inline">true</strong> in <strong class="source-inline">$vectorSearch</strong> when you have a high degree of variation between the number of vectors per tenant and a high number of tenants modeled within the same collection or index. This will lead to an exhaustive search performed in parallel on all segments corresponding to a vector index. In many cases, this will accelerate the search, given the high selectivity of the filter and the large number of potential vectors that would need to be searched and discarded while running a filtered <span class="No-Break">HNSW search.</span></p>
			<h3>Chunking</h3>
			<p>In the context of RAG, an interesting analogy emerges. Just as chat models require intelligent prompt engineering, embedding models require intelligent chunking. <strong class="bold">Intelligent chunking</strong> requires finding the right level of context that can effectively be mapped to a search or natural language query. This may also be the right level of context to provide to the LLM, but as you’ll see later in the <em class="italic">Parent document retrieval </em>section, this is not a strict requirement if you intelligently model <span class="No-Break">your data.</span></p>
			<p>You will learn more about basic and advanced chunking strategies in <a href="B22495_08.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Implementing Vector Search in AI Applications</em>. For the sake of this section, let’s consider one basic chunking strategy, <strong class="bold">fixed token count with overlap</strong>, and how you can experiment to assess what works best on your <span class="No-Break">own dataset.</span></p>
			<h4>Fixed token count with overlap</h4>
			<p>A fixed token count with overlap, which is a common default in many RAG integration frameworks such as LangChain, splits unstructured data into chunks based on the specified maximum number of tokens per chunk and the desired overlap between chunks. This method is more granular than the whole-page ingestion method, and it allows for greater experimentation on your specific dataset. It doesn’t involve exploiting any structure within the unstructured data. This is a positive in terms of simplicity of development but can be a negative when sentences, paragraphs, or other boundaries demarcate semantic significance in a way you would want <span class="No-Break">to model.</span></p>
			<p>This technique may be a good fit if you have little control over the source data or are working with unstructured data that doesn’t lend itself well to boundary chunking methods that leverage document structure, such as HTML tags, because this technique is compatible with any text format. <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.7</em> shows an example with different colors indicating separate chunks <span class="No-Break">and overlaps:</span></p>
			<p class="IMG---Figure"><img alt="" role="presentation" src="image/B22495_05_07.png"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7: An example of chunking based on fixed token count with overlap</p>
			<h4>Experimentation</h4>
			<p>Evaluating which chunking strategy or embedding model works best for your use case requires curating judgment lists of documents along with the queries that you would expect to map to those documents. You would also want to play around with the different embedding models and chunking strategies that can be applied before embedding data to see which works best for your <span class="No-Break">use case.</span></p>
			<p>A given embedding model might perform better or worse with a fixed chunking strategy. You can more easily evaluate which combination of chunking and embedding models is best suited for your use case. You could have multiple versions of the same data, each split and processed differently. By comparing these versions, you can determine the optimal splitting method and embedding model for your specific <span class="No-Break">search needs.</span></p>
			<p>The best way to determine whether an embedding model is effectively mapping your documents to a sample query is to inspect the similarity score that is returned for a set of queried documents and see how well that aligns with what good responses might be for the actual question, as shown in <span class="No-Break"><em class="italic">Table 5.1</em></span><span class="No-Break">.</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Rank</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Raw document</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Embedding</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Cosine similarity</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>“One of the main challenges of building software is <span class="No-Break">managing complexity.”</span></p>
						</td>
						<td class="No-Table-Style">
							<p>[0.23, <span class="No-Break">0.45, …]</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.901</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p>“Deep modules provide deep functionality behind a <span class="No-Break">simple interface”</span></p>
						</td>
						<td class="No-Table-Style">
							<p>[0.86, <span class="No-Break">0.34, …]</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.874</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p>“Software systems often grow in complexity due to <span class="No-Break">evolving requirements.”</span></p>
						</td>
						<td class="No-Table-Style">
							<p>[0.46, <span class="No-Break">0.51, …]</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.563</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 5.1: Vector search results ranked by cosine similarity</p>
			<p>In the case of a fixed token count with overlap strategy, you will have to figure out the token count that you would like to start with. The 300–500 token range seems sufficient for experimentation in the information <span class="No-Break">retrieval community.</span></p>
			<h3>Hybridization</h3>
			<p><strong class="bold">Hybridization</strong> involves modeling multiple sources of relevance within a single document and jointly considering them alongside a single vector search at query time. This technique embodies the flexibility of the aggregation pipelines supported by MongoDB and allows for a great amount of experimentation and tuning of your search system leveraging vector search, lexical search, traditional database operators, geospatial queries, <span class="No-Break">and more.</span></p>
			<p>In the following sections, you will explore some of the more popular methods for hybridization, as well as some promising avenues of exploration that you might find relevant to your <span class="No-Break">use case.</span></p>
			<h4>Vector plus lexical</h4>
			<p>Vector search is a sound methodology for exploiting semantic similarity between a query and indexed document as defined by the capabilities of an embedding model. Lexical search systems such as <strong class="bold">BM25</strong>, which <strong class="bold">Lucene</strong> and, correspondingly, Atlas Search use, are helpful in a completely different way in that they index tokens directly and use a bag-of-words style approach that ranks a set of documents based on the query terms appearing in each document, regardless of their proximity within <span class="No-Break">the document.</span></p>
			<p>Despite being based on an original probabilistic retrieval framework developed in the 1980s, this approach is still fairly good at mapping keywords in a query to keywords in a document, especially when that word is used outside the context of what an embedding model was trained on. Small datasets can contain tokens either not seen in the training dataset or with alternative meanings, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer052">
					<img alt="" role="presentation" src="image/B22495_05_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8: Out-of-sample terms</p>
			<p>Some vector search providers provide sparse vector search as an alternative to lexical search, which can be made to operate similarly but has been considered insufficient for customers’ purposes. It also lacks out-of-the-box support for many lexical search features, such as synonym lists, pagination, <span class="No-Break">and faceting.</span></p>
			<p>Smaller levels of context are good fits for embedding models, whereas broader levels can be well represented by keyword search. MongoDB allows users to experiment in this direction as much as possible, while also allowing the joint query pattern to be joined on a foreign key, rather than simply a document <strong class="source-inline">_id</strong>. This makes it possible to have windowing levels of representation for a given document that can be considered by different methodologies. The following code shows how some documents containing <strong class="source-inline">paragraph_embeddings</strong> can be indexed and queried using a vector search index, while other documents containing <strong class="source-inline">full_page_content</strong> can be indexed and queried using a text <span class="No-Break">search index:</span></p>
			<pre class="source-code">
[
    {
        "_id": ObjectID("662043cfb084403cdcf5210d"),
         "page_number": 81,
        "paragraph_embedding": [0.43, 0.91, ...],
    },
    {
        "_id": ObjectID("662043cfb084403cdcf5210e"),
        "full_page_content": "Pulling complexity down makes the most sense if (a) the complexity being pulled down is closely related to the class's existing functionality, (b) pulling the complexity down will result in many simplifications elsewhere in the application, and (c) pulling the complexity down simplifies the class's interface. ...",
        "page_number": 36,
    }, ...
]</pre>			<p>Jointly considering the result sets from the two queries shown in the preceding code is what you call <strong class="bold">hybrid search</strong> and can be done using the <strong class="bold">reciprocal rank fusion method</strong>, as shown at <a href="https://www.mongodb.com/docs/atlas/atlas-search/tutorial/hybrid-search/"><span class="P---URL">https://www.mongodb.com/docs/atlas/atlas-search/tutorial/hybrid-search/</span></a>. In the future, Atlas Vector Search will offer support for dedicated stages that make combining result sets based on rank or score much simpler. However, the fundamental concepts will remain <span class="No-Break">the same.</span></p>
			<h4>Vector plus vector</h4>
			<p>There might be multiple sources of vector relevance in your dataset that you would want to consider jointly, similar to how you might jointly consider paragraph embeddings and keyword relevance for a whole page. The secondary embedding field you are considering might be a derivative field, such as an LLM-generated chapter summary that is then embedded, or it could be an entirely different source of data. The following code shows a single document with a set of source fields that could be embedded and indexed using a vector <span class="No-Break">search index:</span></p>
			<pre class="source-code">
[
    {
        "_id": ObjectID("662043cfb084403cdcf5210d"),
        "book_title": "A Philosophy of Software Design",
         "book_title_embedding": [0.67, 0.45, ...],
         "chapter_title": "The Nature of Complexity",
         "chapter_title_embedding": [0.51, 0.89, ...],
         "chapter_summary": "This book is about how to design software systems to minimize their complexity. The first step is to understand the enemy. Exactly what is 'complexity'?...",
         "chapter_summary_embedding": [0.36, 0.90, ...],
         "raw_text "System designers sometimes assume that complexity can be measured by lines of code. They assume that if one implementation is shorter than another, then it must be simpler; if it only takes a few lines of code to make a change, then the change must be easy...",
         "raw_text_embedding": [0.43, 0.11, ...],
    }, ...</pre>			<p>The results of independent <strong class="source-inline">$vectorSearch</strong> queries could be hybridized and fused using a similar pattern to the vector plus lexical search query pattern seen in the previous section and would allow for multiple sources of relevance to be used to find the most relevant document to <span class="No-Break">a query.</span></p>
			<p>In e-commerce search use cases, it is common for a single item to have many sources of relevance that can be embedded and stored within the same document representing that item. These include <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break">Product description</span></li>
				<li>User reviews (and summaries of <span class="No-Break">user reviews)</span></li>
				<li><span class="No-Break">Product images</span></li>
			</ul>
			<p>Each of these sources of relevance can be embedded and jointly considered using the same query pattern as one would use to jointly consider vector and <span class="No-Break">lexical relevance.</span></p>
			<h4>Incorporating user feedback</h4>
			<p><strong class="bold">Incorporating user feedback</strong> for RAG applications is conventionally thought of as providing signals to the chat model to modify their weights through a process known as <strong class="bold">reinforcement learning with human feedback</strong>. However, search systems have incorporated user signals to inform how results are ranked for decades, and similar principles can be applied to RAG. An interface that provides a ranking mechanism for the sources that are provided to the LLM would allow for feedback to be directly modeled within the document, as seen in the following code. These signals can then be jointly considered using the hybrid search query pattern combining <strong class="source-inline">$vectorSearch</strong> and the <strong class="source-inline">$sort</strong> stage using the upvotes or downvotes as a proxy for <span class="No-Break">user relevance.</span></p>
			<pre class="source-code">
[
    {
        "_id": ObjectID("662043cfb084403cdcf5210a"),
        "paragraph_embedding": [0.43, 0.57, ...],
        "page_number": 12,
        "score": 0.95,
        "upvotes": 2,
        "downvotes": 58
    },
    {
        "_id": ObjectID("662043cfb084403cdcf5210b"),
        "paragraph_embedding": [0.72, 0.63, ...],
        "page_number": 6,
        "score": 0.90
        "upvotes": 81,
        "downvotes": 3
    },
    {
        "_id": ObjectID("662043cfb084403cdcf5210c"),
        "paragraph_embedding": [0.12, 0.48, ...],
        "page_number": 3,
        "score": 0.67
        "upvotes": 2,
        "downvotes": 5
    }, ...
]</pre>			<p>This is a very naive approach, but the principle behind it can be extended to allow for greater personalization of content where similar users are defined by similar interactions with different content, which is the basis for the popular recommendation system algorithm known as <span class="No-Break"><strong class="bold">collaborative filtering</strong></span><span class="No-Break">.</span></p>
			<p>While it is still early days in terms of intelligently incorporating user feedback into your RAG application, the flexibility of the document model should allow for a rich amount of experimentation in this area as your search system, and how your users engage with it, evolves <span class="No-Break">over time.</span></p>
			<h3>Document lookups</h3>
			<p>Once you have a sorted result set of documents, possibly produced from multiple methodologies in an optimized manner, there are still additional operations that can be performed that might leverage relationships inherent within your data. With <strong class="bold">document lookups</strong>, some data may be easier to model outside of the document itself using a foreign lookup key to model tree structures within your data, such as hierarchies within documents, organizations, or some <span class="No-Break">other taxonomy.</span></p>
			<h4>Parent document retrieval</h4>
			<p><strong class="bold">Parent document retrieval</strong> involves performing a vector search at one level of context, and then retrieving a document connected to the most relevant retrieved documents via a foreign key. This foreign key is usually a child-parent relationship, such as an embedded paragraph belonging to a specific page of a larger body of text, where that larger bit of context may be stored in another <span class="No-Break">document completely.</span></p>
			<p>With this pattern, you can store only the embeddings at the lower level, and then look up a higher level of context containing a much larger amount of text. This may be useful if you find that the queries are more easily mapped semantically to a smaller amount of text, but the amount of data you want to serve to the user or an LLM is much larger, which is often the case. The following code example for hybridizing lexical and vector search is also an example of parent document retrieval, as vector embeddings are searched against to yield a full page of content to provide to the LLM. The foreign key is <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">page_number</strong></span><span class="No-Break">.</span></p>
			<pre class="source-code">
[
    {
        "_id": ObjectID("662043cfb084403cdcf5210d"),
         "page_number": 81,
        "paragraph_embedding": [0.43, 0.91, ...],
    },
    {
        "_id": ObjectID("662043cfb084403cdcf5210e"),
        "full_page_content": "Pulling complexity down makes the most sense if (a) the complexity being pulled down is closely related to the class's existing functionality, (b) pulling the complexity down will result in many simplifications elsewhere in the application, and (c) pulling the complexity down simplifies the class's interface. ...",
        "page_number": 36,
    }, ...
]</pre>			<p>It’s important to note that like all other metadata, capturing relationships between MongoDB documents in this manner must be extracted at <span class="No-Break">ingestion time.</span></p>
			<h4>Graph relationships</h4>
			<p>You can exploit even more relationships between documents using the <strong class="source-inline">$graphLookup</strong> stage. This allows an arbitrary number of hops to be jumped from the results of <strong class="source-inline">$vectorSearch</strong>. If the customer’s data already contains relationships that can be traversed in a hierarchical manner, this is an immediate benefit <span class="No-Break">to them.</span></p>
			<p>Just as you might define a relationship between a document and a page, you might recursively chunk a document into ever smaller chunks, relate each chunk to a parent document using a <strong class="source-inline">parent_id</strong> field, and embed those chunks. At query time, you could search against all of the chunks and recursively jump up all of the <strong class="source-inline">parent_id</strong> values to the desired level of resolution to provide to <span class="No-Break">the LLM.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor135"/>Deployment</h2>
			<p>Successfully deploying your AI application is the final hurdle. This section outlines various deployment options and provides guidance on estimating necessary resources to ensure optimal performance <span class="No-Break">and scalability.</span></p>
			<h3>Deployment options</h3>
			<p>The simplest deployment model for getting started with Atlas Vector Search is to define a search index definition within your existing cluster or a new cluster. This can be configured using <strong class="bold">search index management commands</strong> for paid tier clusters or the <strong class="bold">UI/Atlas Administration API</strong> for shared <span class="No-Break">tier clusters.</span></p>
			<p>When you feel confident in your vector search use case and are ready for increased usage or increased scale of ingested data, it is recommended to move to dedicated search nodes. <strong class="bold">Dedicated search resources</strong> provide a robust and scalable platform for serving demanding <span class="No-Break">search workloads.</span></p>
			<p>This will allow for high-availability vector search, more cost-effective resource utilization, and resource isolation from your core database in a way that is more practical for production workloads, as visualized in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer053">
					<img alt="" role="presentation" src="image/B22495_05_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9: The benefits of dedicated search nodes</p>
			<p>Migrating to dedicated search nodes is a zero-downtime process that allows for your existing base cluster to continue to serve vector search queries as new resources are spun up and your indexes are built on them. Once that build process completes, <strong class="source-inline">$vectorSearch</strong> queries will be routed to your dedicated search nodes and the indexes on the original cluster will <span class="No-Break">be deleted.</span></p>
			<p>Dedicated search nodes can be configured from the <strong class="bold">Cluster Configuration UI</strong> by following <span class="No-Break">these steps:</span></p>
			<ol>
				<li>On the <strong class="bold">Create New Cluster/Edit Configuration</strong> page, change the radio button for <strong class="bold">AWS or Google Cloud</strong> for <strong class="bold">Multi-cloud, multi-region &amp; workload isolation</strong> <span class="No-Break">to enabled.</span></li>
				<li>Toggle the radio button for <strong class="bold">Search Nodes for workload isolation</strong> to enabled. Select the number of nodes in <span class="No-Break">the textbox.</span></li>
				<li>Check the <span class="No-Break">agreement box.</span></li>
				<li>Select the right node for <span class="No-Break">your workload.</span></li>
				<li>Click <span class="No-Break"><strong class="bold">Create cluster</strong></span><span class="No-Break">.</span></li>
			</ol>
			<h3>Resource requirements</h3>
			<p>The current index type supported within Atlas Vector Search is HNSW, which is memory-resident. This means that you need approximately 3 KB of memory for every 768d vector you plan on indexing, scaling linearly with the number and dimensionality <span class="No-Break">of vectors.</span></p>
			<p>If you expect your workload will have low query volume, it is recommended to select the cheapest option on <strong class="source-inline">M</strong> tier clusters that can allocate 50% of the available resources to storing the index in memory. When using dedicated search nodes, 90% of the available RAM can be used to host the index. Note that when using <strong class="source-inline">M</strong> tier clusters, the index will need to be warmed into the cache using representative queries. For dedicated search nodes, the index will be automatically loaded into the cache upon an <span class="No-Break">index build.</span></p>
			<p>If you expect your workload to have a high indexing or query concurrency, it is recommended to use dedicated search nodes with the high CPU option or to scale up the number of dedicated search nodes in your replica set. This will scale up the number of available vCPUs to serve the <strong class="source-inline">$vectorSearch</strong> queries in a <span class="No-Break">round-robin fashion.</span></p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor136"/>Summary</h1>
			<p>In this chapter, you explored a variety of concepts related to vector search. The chapter delved into how high-dimensional vectors produced from embedding models can be useful measures of semantic similarity among the unstructured data passed into those models. It examined the HNSW index and how it can be used to accelerate vector similarity comparisons between a query vector and many <span class="No-Break">indexed vectors.</span></p>
			<p>The chapter then illustrated how this type of index can be applied in various real-world contexts by large organizations, including such architecture patterns as RAG, semantic search, and RPA. Finally, the chapter reviewed some of the best practices for building vector search systems within MongoDB Atlas, ranging from ingestion time considerations, such as metadata extraction, to deployment model considerations, such as dedicated <span class="No-Break">search nodes.</span></p>
			<p>In the next chapter, you will discover the crucial aspects of designing AI/ML applications. You will learn how to effectively manage data storage, flow, freshness, and retention along with techniques to ensure <span class="No-Break">robust security.</span></p>
		</div>
	</body></html>