<html><head></head><body>
<div><div><h1 class="chapterNumber"><a id="_idTextAnchor152"/>4</h1>
<h1 class="chapterTitle" id="_idParaDest-88"><a id="_idTextAnchor153"/>Building Intelligent RAG Systems</h1>
<p class="normal">So far in this book, we’ve talked about LLMs and tokens and working with them in LangChain. <strong class="keyWord">Retrieval-Augmented Generation</strong> (<strong class="keyWord">RAG</strong>) extends LLMs by dynamically incorporating external knowledge during generation, addressing limitations of fixed training data, hallucinations, and context windows. A RAG system, in simple terms, takes a query, converts it directly into a semantic vector embedding, runs a search extracting relevant documents, and passes these to a model that generates a context-appropriate user-facing response.</p>
<p class="normal">This chapter explores RAG systems and the core components of RAG, including vector stores, document processing, retrieval strategies, implementation, and evaluation techniques. After that, we’ll put into practice a lot of what we’ve learned so far in this book by building a chatbot. We’ll build a production-ready RAG pipeline that streamlines the creation and validation of corporate project documentation. This corporate use case demonstrates how to generate initial documentation, assess it for compliance and consistency, and incorporate human feedback—all in a modular and scalable workflow.</p>
<p class="normal">The chapter has the following sections:</p>
<ul>
<li class="b lletList">From indexes to intelligent retrieval</li>
<li class="b lletList">Components of a RAG system</li>
<li class="b lletList">From embeddings to search</li>
<li class="b lletList">Breaking down the RAG pipeline</li>
<li class="b lletList">Developing a corporate documentation chatbot</li>
<li class="b lletList">Troubleshooting RAG systems</li>
</ul>
<div><p class="normal">Let’s begin by introducing RAG, its importance, and the main considerations when using the RAG framework.</p>
<h1 class="heading-1" id="_idParaDest-89"><a id="_idTextAnchor154"/>From indexes to intelligent retrieval</h1>
<p class="normal">Information retrieval has been a <a id="_idIndexMarker283"/>fundamental human need since the dawn of recorded knowledge. For the past 70 years, retrieval systems have operated under the same core paradigm:</p>
<ol>
<li class="numberedList" value="1">First, a user frames an information need as a query.</li>
<li class="numberedList">They then submit this query to the retrieval system.</li>
<li class="numberedList">Finally, the system returns references to documents that may satisfy the information need:<ul><li class="bulletList level-2">References may be rank-ordered by decreasing relevance</li>
<li class="bulletList level-2">Results may contain <a id="_idIndexMarker284"/>relevant excerpts from each document (known as snippets)</li>
</ul></li>
</ol>
<p class="normal">While this paradigm has remained constant, the implementation and user experience have undergone remarkable transformations. Early information retrieval systems relied on manual indexing and basic keyword matching. The advent of computerized indexing in the 1960s introduced the inverted index—a data structure that maps each word to a list of documents containing it. This lexical approach powered the first generation of search engines like AltaVista (1996), where results were primarily based on exact keyword matches.</p>
<p class="normal">The limitations of this approach quickly became apparent, however. Words can have multiple meanings (polysemy), different words can express the same concept (synonymy), and users often struggle to articulate their information needs precisely.</p>
<p class="normal">Information-seeking activities come with non-monetary costs: time investment, cognitive load, and interactivity costs—what researchers call “Delphic costs.” User satisfaction with search engines correlates not just with the relevance of results, but with how easily users can extract the information they need.</p>
<p class="normal">Traditional retrieval systems aimed to reduce these costs through various optimizations:</p>
<ul>
<li class="b lletList">Synonym expansion to lower cognitive load when framing queries</li>
<li class="b lletList">Result ranking to reduce the time cost of scanning through results</li>
<li class="b lletList">Result snippeting (showing brief, relevant excerpts from search results) to lower the cost of evaluating document relevance</li>
</ul>
<div><p class="normal">These improvements reflected an understanding that the ultimate goal of search is not just finding documents but satisfying information needs.</p>
<p class="normal">Google’s PageRank algorithm (late 1990s) improved results by considering link structures, but even modern search engines faced fundamental limitations in understanding meaning. The search experience evolved from simple lists of matching documents to richer presentations with contextual snippets (beginning with Yahoo’s highlighted terms in the late 1990s and evolving to Google’s dynamic document previews that extract the most relevant sentences containing search terms), but the underlying challenge remained: bridging the semantic gap between query terms and relevant information.</p>
<p class="normal">A fundamental limitation of traditional retrieval systems lies in their lexical approach to document retrieval. In the Uniterm model, query terms were mapped to documents through inverted indices, where each word in the vocabulary points to a “postings list” of document positions. This approach efficiently supported complex boolean queries but fundamentally missed semantic relationships between terms. For example, “turtle” and “tortoise” are treated as completely separate words in an inverted index, despite being semantically related. Early retrieval systems attempted to<a id="_idIndexMarker285"/> bridge this gap through pre-retrieval stages that augmented queries with synonyms, but the underlying limitation remained.</p>
<p class="normal">The breakthrough came with advances in neural network models that could capture the meaning of words and documents as dense vector<a id="_idIndexMarker286"/> representations—known as embeddings. Unlike traditional keyword systems, embeddings create a <em class="italic">semantic map</em> where related concepts cluster together—”turtle,” “tortoise,” and “reptile” would appear as neighbors in this space, while “bank” (financial) would cluster with “money” but far from “river.” This geometric organization of meaning enabled retrieval based on conceptual similarity rather than exact word matching.</p>
<p class="normal">This transformation gained momentum with models like Word2Vec (2013) and later transformer-based models such as BERT (2018), which introduced contextual understanding. BERT’s innovation was to recognize that the same word could have different meanings depending on its context—”bank” as a financial institution versus “bank” of a river. These distributed representations fundamentally changed what was possible in information retrieval, enabling the development of systems that could understand the intent behind queries rather than just matching keywords.</p>
<div><p class="normal">As transformer-based language models grew in scale, researchers discovered they not only learned linguistic patterns but also memorized factual knowledge from their training data. Studies by Google researchers showed that models like T5 could answer factual questions without external retrieval, functioning as implicit knowledge bases. This suggested a paradigm shift—from retrieving documents containing answers to directly generating answers from internalized knowledge. However, these “closed-book” generative systems faced limitations: hallucination risks, knowledge cutoffs limited to training data, inability to cite sources, and challenges with complex reasoning. The solution emerged in <strong class="keyWord">RAG</strong>, which bridges traditional retrieval systems with generative language models, combining their respective strengths while addressing their individual weakness<a id="_idTextAnchor155"/>es.</p>
<h1 class="heading-1" id="_idParaDest-90"><a id="_idTextAnchor156"/>Components of a RAG system</h1>
<p class="normal">RAG enables language models to <a id="_idIndexMarker287"/>ground their outputs in external knowledge, providing an elegant solution to the limitations that plague pure LLMs: hallucinations, outdated information, and restricted context windows. By retrieving only relevant information on demand, RAG systems effectively bypass the context window constraints of language models, allowing them to leverage vast knowledge bases without squeezing everything into the model’s fixed attention span.</p>
<p class="normal">Rather than simply retrieving documents for human review (as traditional search engines do) or generating answers solely from internalized knowledge (as pure LLMs do), RAG systems retrieve information to inform and ground AI-generated responses. This approach combines the verifiability of retrieval with the<a id="_idIndexMarker288"/> fluency and comprehension of generative AI.</p>
<p class="normal">At its core, RAG consists of these main components working in concert:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Knowledge base</strong>: The storage layer for<a id="_idIndexMarker289"/> external information</li>
<li class="b lletList"><strong class="keyWord">Retriever</strong>: The knowledge access layer<a id="_idIndexMarker290"/> that finds relevant information</li>
<li class="b lletList"><strong class="keyWord">Augmenter</strong>: The integration layer that <a id="_idIndexMarker291"/>prepares retrieved content</li>
<li class="b lletList"><strong class="keyWord">Generator</strong>: The response<a id="_idIndexMarker292"/> layer that produces the final output</li>
</ul>
<p class="normal">From a process perspective, RAG operates through two interconnected pipelines:</p>
<ul>
<li class="b lletList">An indexing pipeline that processes, chunks, and stores documents in the knowledge base</li>
<li class="b lletList">A query pipeline that retrieves relevant information and generates responses using that information</li>
</ul>
<div><p class="normal">The workflow in a RAG system follows a clear sequence: when a query arrives, it’s processed for retrieval; the retriever then searches the knowledge base for relevant information; this retrieved context is combined with the original query through augmentation; finally, the language model generates a response grounded in both the query and the retrieved information. We can see this in the following diagram:</p>
<figure class="mediaobject"><img alt="Figure 4.1: RAG architecture and workflow" src="img/B32363_04_01.png"/></figure>
<p class="packt_figref">Figure 4.1: RAG architecture and workflow</p>
<div><p class="normal">This architecture offers several advantages for production systems: modularity allows components to be developed independently; scalability enables resources to be allocated based on specific needs; maintainability is<a id="_idIndexMarker293"/> improved through the clear separation of concerns; and flexibility permits different implementation strategies to be swapped in as requirements evolve.</p>
<p class="normal">In the following sections, we’ll explore each component in <em class="italic">Figure 4.1</em> in detail, beginning with the fundamental building blocks of modern RAG systems: <strong class="screenText">embeddings</strong> and <strong class="screenText">vector stores</strong> that power the knowledge base and retriever components. But before we dive in, it’s important to first consider the decision between implementing RAG or using pure LLMs. This choice will fundamentally impact your application’s overall architecture and operational characteristics. Let’s discuss the tr<a id="_idTextAnchor157"/>ade-offs!</p>
<h2 class="heading-2" id="_idParaDest-91"><a id="_idTextAnchor158"/>When to implement RAG</h2>
<p class="normal">Introducing RAG brings architectural complexity that must be carefully weighed against your application requirements. RAG proves particularly <a id="_idIndexMarker294"/>valuable in specialized domains where current or verifiable information is crucial. Healthcare applications must process both medical images and time-series data, while financial systems need to handle high-dimensional market data alongside historical analysis. Legal applications benefit from RAG’s ability to process complex document structures and maintain source attribution. These domain-specific requirements often justify the additional complexity of implementing RAG.</p>
<p class="normal">The benefits of RAG, however, come with significant implementation considerations. The system requires efficient indexing and retrieval mechanisms to maintain reasonable response times. Knowledge bases need regular updates and maintenance to remain valuable. Infrastructure must be designed to handle errors and edge cases gracefully, especially where different components interact. Development teams must be prepared to manage these ongoing operational requirements.</p>
<p class="normal">Pure LLM implementations, on the other hand, might be more appropriate when these complexities outweigh the benefits. Applications focusing on creative tasks, general conversation, or scenarios requiring rapid response times often perform well without the overhead of retrieval systems. When working with static, limited knowledge bases, techniques like fine-tuning or prompt engineering might provide simpler solutions.</p>
<div><p class="normal">This analysis, drawn from both research and practical implementations, suggests that specific requirements for knowledge currency, accuracy, and domain expertise should guide the choice between RAG and pure LLMs, balanced against the organizational capacity to manage the additional architectural complexity.</p>
<div><div><p class="normal">At Chelsea AI Ventures, our team has observed that clients in regulated industries particularly benefit from RAG’s verifiability, while creative applications often perform adequately with pure LLMs.</p>
</div>
</div>
<p class="normal">Development teams should consider RAG when their applications require:</p>
<ul>
<li class="b lletList">Access to current information not available in LLM training data</li>
<li class="b lletList">Domain-specific knowledge integration</li>
<li class="b lletList">Verifiable responses with source attribution</li>
<li class="b lletList">Processing of specialized data formats</li>
<li class="b lletList">High precision in regulated industries</li>
</ul>
<p class="normal">With that, let’s explore the implementation details, optimization strategies, and production deployment considerations for each RAG <a id="_idTextAnchor159"/>component.</p>
<h1 class="heading-1" id="_idParaDest-92"><a id="_idTextAnchor160"/>From embeddings to search</h1>
<p class="normal">As mentioned, a RAG system comprises a retriever that finds relevant information, an augmentation mechanism that integrates this information, and a generator that produces the final output. When building AI <a id="_idIndexMarker295"/>applications with LLMs, we often focus on the exciting parts – prompts, chains, and model outputs. However, the foundation of any robust RAG system lies in how we store and retrieve our vector embeddings. Think of it like building a library – before we can efficiently find books (vector search), we need both a building to store them (vector storage) and an organization system to find them (vector indexing). In this section, we introduce the core components of a RAG system: vector embeddings, vector stores, and indexing strategies to optimize retrieval.</p>
<p class="normal">To make RAG work, we first need to solve a fundamental challenge: how do we help computers understand the meaning of text so they can find relevant information? This is where embeddin<a id="_idTextAnchor161"/>gs come in.</p>
<div><h2 class="heading-2" id="_idParaDest-93"><a id="_idTextAnchor162"/>Embeddings</h2>
<p class="normal">Embeddings are numerical representations of text that capture semantic meaning. When we create an embedding, we’re converting words or <a id="_idIndexMarker296"/>chunks of text into vectors (lists of numbers) that computers can process. These vectors can be either sparse (mostly zeros with few non-zero values) or dense (most values are non-zero), with modern LLM systems typically using dense embeddings.</p>
<p class="normal">What makes embeddings powerful is that texts with similar meanings have similar numerical representations, enabling semantic search through nearest neighbor algorithms.</p>
<p class="normal">In other words, the embedding model transforms text into numerical vectors. The same model is used for both documents as well as queries to ensure consistency in the vector space. Here’s how you’d use embeddings in LangChain:</p>
<pre>from langchain_openai import OpenAIEmbeddings
# Initialize the embeddings model
embeddings_model = OpenAIEmbeddings()
# Create embeddings for the original example sentences
text1 = "The cat sat on the mat"
text2 = "A feline rested on the carpet"
text3 = "Python is a programming language"
# Get embeddings using LangChain
embeddings = embeddings_model.embed_documents([text1, text2, text3])
# These similar sentences will have similar embeddings
embedding1 = embeddings[0] # Embedding for "The cat sat on the mat"
embedding2 = embeddings[1] # Embedding for "A feline rested on the
carpet"
embedding3 = embeddings[2] # Embedding for "Python is a programming
language"
# Output shows 3 documents with their embedding dimensions
print(f"Number of documents: {len(embeddings)}")
print(f"Dimensions per embedding: {len(embeddings[0])}")
# Typically 1536 dimensions with OpenAI's embeddings</pre>
<div><p class="normal">Once we have these OpenAI <a id="_idIndexMarker297"/>embeddings (the 1536-dimensional vectors we generated for our example sentences above), we need a purpose-built system to store them. Unlike regular database values, these high-dimensional vectors require specialized storage solutions.</p>
<div><div><p class="normal"> The <code class="inlineCode">Embeddings</code> class in LangChain provides a standard interface for all embedding models from various providers (OpenAI, Cohere, Hugging Face, and others). It exposes two primary methods:</p>
<ul>
<li class="bulletList"><code class="inlineCode">embed_documents</code>: Takes multiple texts and returns embeddings for each</li>
<li class="bulletList"><code class="inlineCode">embed_query</code>: Takes a single text (your search query) and returns its embedding</li>
</ul>
<p class="normal">Some providers use different embedding methods for documents versus queries, which is why these are separate methods in the API.</p>
</div>
</div>
<p class="normal">This brings us to vector stores – specialized databases optimized for similarity searches in high-dimens<a id="_idTextAnchor163"/>ional spaces.</p>
<h2 class="heading-2" id="_idParaDest-94"><a id="_idTextAnchor164"/>Vector stores</h2>
<p class="normal">Vector stores are specialized databases designed to store, manage, and efficiently search vector embeddings. As we’ve seen, embeddings <a id="_idIndexMarker298"/>convert text (or other data) into numerical vectors that capture semantic meaning.</p>
<p class="normal">Vector stores solve the fundamental challenge of how to persistently and efficiently search through these high-dimensional vectors. Please note that the vector database operates as an independent system that can be:</p>
<ul>
<li class="b lletList">Scaled independently of the RAG components</li>
<li class="b lletList">Maintained and optimized separately</li>
<li class="b lletList">Potentially shared across multiple RAG applications</li>
<li class="b lletList">Hosted as a dedicated service</li>
</ul>
<p class="normal">When working with embeddings, several <a id="_idIndexMarker299"/>challenges arise:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Scale</strong>: Applications often need to store millions of embeddings</li>
<li class="b lletList"><strong class="keyWord">Dimensionality</strong>: Each embedding might have hundreds or thousands of dimensions</li>
<li class="b lletList"><strong class="keyWord">Search performance</strong>: Finding similar vectors quickly becomes computationally intensive</li>
<li class="b lletList"><strong class="keyWord">Associated data</strong>: We need to maintain connections between vectors and their source documents</li>
</ul>
<div><p class="normal">Consider a real-world example of what we need to store:</p>
<pre># Example of data that needs efficient storage in a vector store
document_data = {
 "id": "doc_42",
 "text": "LangChain is a framework for developing applications powered by language models.",
 "embedding": [0.123, -0.456, 0.789, ...],  # 1536 dimensions for OpenAI embeddings
 "metadata": {
 "source": "documentation.pdf",
 "page": 7,
 "created_at": "2023-06-15"
    }
}</pre>
<p class="normal">At their core, vector stores combine two essential components:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Vector storage</strong>: The actual database that persists vectors and metadata</li>
<li class="b lletList"><strong class="keyWord">Vector index</strong>: A specialized data structure that enables efficient similarity search</li>
</ul>
<p class="normal">The efficiency challenge comes from the <em class="italic">curse of dimensionality</em> – as vector dimensions increase, computing similarities <a id="_idIndexMarker300"/>becomes increasingly expensive, requiring O(dN) operations for d dimensions and N vectors. This makes naive similarity search impractical for large-scale applications.</p>
<p class="normal">Vector stores enable similarity-based search through distance calculations in high-dimensional space. While traditional databases excel at exact matching, vector embeddings allow for semantic search and <strong class="keyWord">approximate nearest neighbor</strong> (<strong class="keyWord">ANN</strong>) retrieval.</p>
<p class="normal">The key difference from traditional <a id="_idIndexMarker301"/>databases is how vector stores handle searches.</p>
<p class="normal"><strong class="keyWord">Traditional database search</strong>:</p>
<ul>
<li class="bulletList">Uses exact matching (equality, ranges)</li>
<li class="bulletList">Optimized for structured data (for example, “find all customers with age &gt; 30”)</li>
<li class="bulletList">Usually utilizes B-trees or hash-based indexes</li>
</ul>
<p class="normal"><strong class="keyWord">Vector store search:</strong></p>
<ul>
<li class="bulletList">Uses similarity metrics (cosine similarity, Euclidean distance)</li>
<li class="bulletList">Optimized for high-dimensional vector spaces</li>
<li class="bulletList">Employs Approximate Nearest Neighbor<a id="_idTextAnchor165"/> (ANN) algorithms</li>
</ul>
<div><h3 class="heading-3" id="_idParaDest-95"><a id="_idTextAnchor166"/>Vector stores comparison</h3>
<p class="normal">Vector stores manage high-dimensional <a id="_idIndexMarker302"/>embeddings for retrieval. The following table compares popular vector stores across key attributes to help you select the most appropriate solution for your specific needs:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Database</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Deployment options</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">License</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Notable features</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Pinecone</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud-only</p>
</td>
<td class="No-Table-Style">
<p class="normal">Commercial</p>
</td>
<td class="No-Table-Style">
<p class="normal">Auto-scaling, enterprise security, monitoring</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Milvus</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud, Self-hosted</p>
</td>
<td class="No-Table-Style">
<p class="normal">Apache 2.0</p>
</td>
<td class="No-Table-Style">
<p class="normal">HNSW/IVF indexing, multi-modal support, CRUD operations</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Weaviate</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud, Self-hosted</p>
</td>
<td class="No-Table-Style">
<p class="normal">BSD 3-Clause</p>
</td>
<td class="No-Table-Style">
<p class="normal">Graph-like structure, multi-modal support</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Qdrant</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud, Self-hosted</p>
</td>
<td class="No-Table-Style">
<p class="normal">Apache 2.0</p>
</td>
<td class="No-Table-Style">
<p class="normal">HNSW indexing, filtering optimization, JSON metadata</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">ChromaDB</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud, Self-hosted</p>
</td>
<td class="No-Table-Style">
<p class="normal">Apache 2.0</p>
</td>
<td class="No-Table-Style">
<p class="normal">Lightweight, easy setup</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">AnalyticDB-V</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud-only</p>
</td>
<td class="No-Table-Style">
<p class="normal">Commercial</p>
</td>
<td class="No-Table-Style">
<p class="normal">OLAP integration, SQL support, enterprise features</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">pg_vector</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud, Self-hosted</p>
</td>
<td class="No-Table-Style">
<p class="normal">OSS</p>
</td>
<td class="No-Table-Style">
<p class="normal">SQL support, PostgreSQL integration</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Vertex Vector Search</p>
</td>
<td class="No-Table-Style">
<p class="normal">Cloud-only</p>
</td>
<td class="No-Table-Style">
<p class="normal">Commercial</p>
</td>
<td class="No-Table-Style">
<p class="normal">Easy setup, low latency, high scalability</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 4.1: Vector store comparison by deployment options, licensing, and key features</p>
<p class="normal">Each vector store offers different tradeoffs in terms of deployment flexibility, licensing, and specialized capabilities. For production RAG systems, consider factors such as:</p>
<ul>
<li class="b lletList">Whether you<a id="_idIndexMarker303"/> need cloud-managed or self-hosted deployment</li>
<li class="b lletList">The need for specific features like SQL integration or multi-modal support</li>
<li class="b lletList">The complexity of setup and maintenance</li>
<li class="b lletList">Scaling requirements for your expected embedding volume</li>
</ul>
<div><p class="normal">For many applications starting with RAG, lightweight options like ChromaDB provide an excellent balance of simplicity and functionality, while enterprise deployments might benefit from the advanced features of Pinecone or AnalyticDB-V. Modern vector stores support several search patterns:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Exact search</strong>: Returns precise nearest neighbors but becomes computationally prohibitive with large vector <a id="_idIndexMarker304"/>collections</li>
<li class="b lletList"><strong class="keyWord">Approximate search</strong>: Trades accuracy for speed using techniques like LSH, HNSW, or quantization; measured by recall (the percentage of true nearest neighbors retrieved)</li>
<li class="b lletList"><strong class="keyWord">Hybrid search</strong>: Combines vector similarity with text-based search (like keyword matching or BM25) in a single query</li>
<li class="b lletList"><strong class="keyWord">Filtered vector search</strong>: Applies traditional database filters (for example, metadata constraints) alongside vector similarity search</li>
</ul>
<p class="normal">Vector stores also handle different types of embeddings:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Dense vector search</strong>: Uses<a id="_idIndexMarker305"/> continuous embeddings where most dimensions have non-zero values, typically from neural models (like BERT, OpenAI embeddings)</li>
<li class="b lletList"><strong class="keyWord">Sparse vector search</strong>: Uses high-dimensional vectors where most values are zero, resembling traditional TF-IDF or BM25 representations</li>
<li class="b lletList"><strong class="keyWord">Sparse-dense hybrid</strong>: Combines both approaches to leverage semantic similarity (dense) and keyword precision (sparse)</li>
</ul>
<p class="normal">They also often give a<a id="_idIndexMarker306"/> choice of multiple similarity measures, for example:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Inner product</strong>: Useful for comparing semantic directions</li>
<li class="b lletList"><strong class="keyWord">Cosine similarity</strong>: Normalizes for vector magnitude</li>
<li class="b lletList"><strong class="keyWord">Euclidean distance</strong>: Measures the L2 distance in vector space (note: with normalized embeddings, this becomes functionally equivalent to the dot product)</li>
<li class="b lletList"><strong class="keyWord">Hamming distance</strong>: For binary vector representations</li>
</ul>
<p class="normal">When implementing vector storage for RAG applications, one of the first architectural decisions is whether to use local storage or a cloud-based solution. Let’s explore the tradeoffs and considerations for each approach.</p>
<div><ul>
<li class="b lletList">Choose local storage when you need maximum control, have strict privacy requirements, or operate at a smaller scale with predictable workloads.</li>
<li class="b lletList">Choose cloud storage when you need elastic scaling, prefer managed services, or operate distributed applications with variable workloads.</li>
<li class="b lletList">Consider hybrid storage architecture when you want to balance performance and scalability, combining local caching with cloud-based persistence.</li>
</ul>
<h3 class="heading-3" id="_idParaDest-96"><a id="_idTextAnchor167"/>Hardware considerations for vector stores</h3>
<p class="normal">Regardless of your deployment approach, understanding<a id="_idIndexMarker307"/> the hardware requirements is crucial for optimal performance:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Memory requirements</strong>: Vector databases are memory-intensive, with production systems often requiring 16-64GB RAM for millions of embeddings. Local deployments should plan for sufficient memory headroom to accommodate index growth.</li>
<li class="b lletList"><strong class="keyWord">CPU vs. GPU</strong>: While basic vector operations work on CPUs, GPU acceleration significantly improves performance for large-scale similarity searches. For high-throughput applications, GPU support can provide 10-50x speed improvements.</li>
<li class="b lletList"><strong class="keyWord">Storage speed</strong>: SSD storage is strongly recommended over HDD for production vector stores, as index loading and search performance depend heavily on I/O speed. This is especially critical for local deployments.</li>
<li class="b lletList"><strong class="keyWord">Network bandwidth</strong>: For cloud-based or distributed setups, network latency and bandwidth become critical factors that can impact query response times.</li>
</ul>
<p class="normal">For development and testing, most<a id="_idIndexMarker308"/> vector stores can run on standard laptops with 8GB+ RAM, but production deployments should consider dedicated infrastructure or cloud-based vector store services that handle these resource considerations automatic<a id="_idTextAnchor168"/>ally.</p>
<h3 class="heading-3" id="_idParaDest-97"><a id="_idTextAnchor169"/>Vector store interface in LangChain</h3>
<p class="normal">Now that we’ve explored the role of vector stores and compared some common options, let’s look at how LangChain simplifies <a id="_idIndexMarker309"/>working with them. LangChain provides a standardized interface for working with vector stores, allowing you to easily switch between different implementations:</p>
<pre>from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
# Initialize with an embedding model
embeddings = OpenAIEmbeddings()
vector_store = Chroma(embedding_function=embeddings)</pre>
<div><p class="normal">The <code class="inlineCode">vectorstore</code> base class in LangChain provides these essential operations:</p>
<ol>
<li class="numberedList" value="1">Adding documents:<pre>docs = [Document(page_content="Content 1"), Document(page_
content="Content 2")]
ids = vector_store.add_documents(docs)</pre></li>
<li class="numberedList">Similarity search:<pre>results = vector_store.similarity_search("How does LangChain work?", k=3)</pre></li>
<li class="numberedList">Deletion:<pre>vector_store.delete(ids=["doc_1", "doc_2"])</pre></li>
<li class="numberedList">Maximum marginal relevance search:<pre># Find relevant BUT diverse documents (reduce redundancy)
results = vector_store.max_marginal_relevance_search(
 "How does LangChain work?",
 k=3,
 fetch_k=10,
 lambda_mult=0.5  # Controls diversity (0=max diversity, 1=max relevance)
)</pre></li>
</ol>
<p class="normal">It’s important to also briefly highlight applications of vector stores apart from RAG:</p>
<ul>
<li class="bulletList">Anomaly detection in large datasets</li>
<li class="bulletList">Personalization and recommendation systems</li>
<li class="bulletList">NLP tasks</li>
<li class="bulletList">Fraud detection</li>
<li class="bulletList">Network security monitoring</li>
</ul>
<p class="normal">Storing vectors isn’t enough, however. We need to find similar vectors quickly when processing queries. Without <a id="_idIndexMarker310"/>proper indexing, searching through vectors would be like trying to find a book in a library with no organization system – you’d have to check every singl<a id="_idTextAnchor170"/>e book.</p>
<div><h2 class="heading-2" id="_idParaDest-98"><a id="_idTextAnchor171"/>Vector indexing strategies</h2>
<p class="normal">Vector indexing is a critical component that makes vector databases practical for real-world applications. At its core, indexing <a id="_idIndexMarker311"/>solves a fundamental performance challenge: how to efficiently find similar vectors without comparing against every single vector in the database (brute force approach), which is computationally prohibitive for even medium-sized data volumes.</p>
<p class="normal">Vector indexes are specialized data structures that organize vectors in ways that allow the system to quickly identify which sections of the vector space are most likely to contain similar vectors. Instead of checking every vector, the system can focus on promising regions first. </p>
<p class="normal">Some common indexing approaches include:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Tree-based structures</strong> that hierarchically divide the vector space</li>
<li class="b lletList"><strong class="keyWord">Graph-based methods</strong> like <strong class="keyWord">Hierarchical Navigable Small World</strong> (<strong class="keyWord">HNSW</strong>) that create navigable networks <a id="_idIndexMarker312"/>of connected vectors</li>
<li class="b lletList"><strong class="keyWord">Hashing techniques</strong> that map similar vectors to the same “buckets”</li>
</ul>
<p class="normal">Each of the preceding approaches offers different trade-offs between:</p>
<ul>
<li class="b lletList">Search speed</li>
<li class="b lletList">Accuracy of results</li>
<li class="b lletList">Memory usage</li>
<li class="b lletList">Update efficiency (how quickly new vectors can be added)</li>
</ul>
<p class="normal">When using a vector store in LangChain, the indexing strategy is typically handled by the underlying implementation. For example, when you create a FAISS index or use Pinecone, those systems automatically apply appropriate indexing strategies based on your configuration.</p>
<p class="normal">The key takeaway is that proper indexing transforms vector search from an O(n) operation (where n is the number of vectors) to something much more efficient (often closer to O(log n)), making it possible to <a id="_idIndexMarker313"/>search through millions of vectors in milliseconds rather than seconds or minutes.</p>
<div><p class="normal">Here’s a table to provide an overview of different strategies:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Strategy</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Core algorithm</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Complexity</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Memory usage</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Best for</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Notes</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Exact Search (Brute Force)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Compares query vector with every vector in database</p>
</td>
<td class="No-Table-Style">
<p class="normal">Search: O(DN)</p>
<p class="normal">Build: O(1)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Low – only stores raw vectors</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Small datasets</li>
<li class="bulletList">When 100% recall needed</li>
<li class="bulletList">Testing/baseline</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Easiest to implement</li>
<li class="bulletList">Good baseline for testing</li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">HNSW (Hierarchical Navigable Small World)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Creates layered graph with decreasing connectivity from bottom to top</p>
</td>
<td class="No-Table-Style">
<p class="normal">Search: O(log N)</p>
<p class="normal">Build: O(N log N)</p>
</td>
<td class="No-Table-Style">
<p class="normal">High – stores graph connections plus vectors</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Production systems</li>
<li class="bulletList">When high accuracy needed</li>
<li class="bulletList">Large-scale search</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Industry standard</li>
<li class="bulletList">Requires careful tuning of M (connections) and ef (search depth)</li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">LSH (Locality Sensitive Hashing)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Uses hash functions that map similar vectors to the same buckets</p>
</td>
<td class="No-Table-Style">
<p class="normal">Search: O(N<img alt="" src="img/Icon_2.png"/>)</p>
<p class="normal">Build: O(N)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Medium – stores multiple hash tables</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Streaming data</li>
<li class="bulletList">When updates frequent</li>
<li class="bulletList">Approximate search OK</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Good for dynamic data</li>
<li class="bulletList">Tunable accuracy vs speed</li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">IVF (Inverted File Index)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Clusters vectors and searches within relevant clusters</p>
</td>
<td class="No-Table-Style">
<p class="normal">Search: O(DN/k)</p>
<p class="normal">Build: O(kN)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Low – stores cluster assignments</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Limited memory</li>
<li class="bulletList">Balance of speed/accuracy</li>
<li class="bulletList">Simple implementation</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">k = number of clusters</li>
<li class="bulletList">Often combined with other methods</li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Product Quantization (PQ)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Compresses vectors by splitting into subspaces and quantizing</p>
</td>
<td class="No-Table-Style">
<p class="normal">Search: varies</p>
<p class="normal">Build: O(N)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Very Low – compressed vectors</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Memory-constrained systems</li>
<li class="bulletList">Massive datasets</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Often combined with IVF</li>
<li class="bulletList">Requires training codebooks</li>
<li class="bulletList">Complex implementation</li>
</ul>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<div><p class="normal">Tree-Based (KD-Tree, Ball Tree)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Recursively partitions space into regions</p>
</td>
<td class="No-Table-Style">
<p class="normal">Search: O(D log N) best case</p>
<p class="normal">Build: O(N log N)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Medium – tree structure</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Low dimensional data</li>
<li class="bulletList">Static datasets</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Works well for D &lt; 100</li>
<li class="bulletList">Expensive updates</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 4.2: Vector store comparison by deployment options, licensing, and key features</p>
<p class="normal">When selecting an indexing strategy for your RAG system, consider these practical tradeoffs:</p>
<ul>
<li class="b lletList"><strong class="keyWord">For maximum accuracy with small datasets</strong> (&lt;100K vectors): Exact Search provides perfect recall but becomes prohibitively expensive as your dataset grows.</li>
<li class="b lletList"><strong class="keyWord">For production systems with millions of vectors</strong>: HNSW offers the best balance of search speed and accuracy, making<a id="_idIndexMarker314"/> it the industry standard for large-scale applications. While it requires more memory than other approaches, its logarithmic search complexity delivers consistent performance even as your dataset scales.</li>
<li class="b lletList"><strong class="keyWord">For memory-constrained environments</strong>: IVF+PQ (Inverted File Index with Product Quantization) dramatically reduces memory requirements—often by 10-20x compared to raw vectors—with a modest accuracy tradeoff. This combination is particularly valuable for edge deployments or when embedding billions of documents.</li>
<li class="b lletList"><strong class="keyWord">For frequently updated collections</strong>: LSH provides efficient updates without rebuilding the entire index, making it suitable for streaming data applications where documents are continuously added or removed.</li>
</ul>
<p class="normal">Most modern vector databases default to HNSW for good reason, but understanding these tradeoffs allows you to optimize for your specific constraints when necessary. To illustrate the practical difference between indexing strategies, let’s compare the performance and accuracy of exact search versus HNSW indexing using FAISS:</p>
<pre>import numpy as np
import faiss
import time
# Create sample data - 10,000 vectors with 128 dimensions
dimension = 128
num_vectors = 10000
vectors = np.random.random((num_vectors, dimension)).astype('float32')
query = np.random.random((1, dimension)).astype('float32')</pre>
<div><pre># Exact search index
exact_index = faiss.IndexFlatL2(dimension)
exact_index.add(vectors)
# HNSW index (approximate but faster)
hnsw_index = faiss.IndexHNSWFlat(dimension, 32)  # 32 connections per node
hnsw_index.add(vectors)
# Compare search times
start_time = time.time()
exact_D, exact_I = exact_index.search(query, k=10)  # Search for 10 nearest neighbors
exact_time = time.time() - start_time
start_time = time.time()
hnsw_D, hnsw_I = hnsw_index.search(query, k=10)
hnsw_time = time.time() - start_time
# Calculate overlap (how many of the same results were found)
overlap = len(set(exact_I[0]).intersection(set(hnsw_I[0])))
overlap_percentage = overlap * 100 / 10
print(f"Exact search time: {exact_time:.6f} seconds")
print(f"HNSW search time: {hnsw_time:.6f} seconds")
print(f"Speed improvement: {exact_time/hnsw_time:.2f}x faster")
print(f"Result overlap: {overlap_percentage:.1f}%")
Running this code typically produces results like:
Exact search time: 0.003210 seconds
HNSW search time: 0.000412 seconds
Speed improvement: 7.79x faster
Result overlap: 90.0%</pre>
<div><p class="normal">This example <a id="_idIndexMarker315"/>demonstrates the fundamental tradeoff in vector indexing: exact search guarantees finding the true nearest neighbors but takes longer, while HNSW provides approximate results significantly faster. The overlap percentage shows how many of the same nearest neighbors were found by both methods.</p>
<p class="normal">For small datasets like this example (10,000 vectors), the absolute time difference is minimal. However, as your dataset grows<a id="_idIndexMarker316"/> to millions or billions of vectors, exact search becomes prohibitively expensive, while HNSW maintains logarithmic scaling—making approximate indexing methods essential for production RAG systems.</p>
<p class="normal">Here’s a diagram that can help developers choose the right indexing strategy based on their requirements:</p>
<figure class="mediaobject"><img alt="Figure 4.2: Choosing an indexing strategy" src="img/B32363_04_02.png"/></figure>
<p class="packt_figref">Figure 4.2: Choosing an indexing strategy</p>
<div><p class="normal">The preceding figure<a id="_idIndexMarker317"/> illustrates a decision tree for selecting the appropriate indexing strategy based on your deployment constraints. The flowchart helps you navigate key decision points:</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Start by assessing your dataset size</strong>: For small collections (under 100K vectors), exact search remains viable and provides perfect accuracy.</li>
<li class="numberedList"><strong class="keyWord">Consider your memory constraints</strong>: If memory is limited, follow the left branch toward compression<a id="_idIndexMarker318"/> techniques like <strong class="keyWord">Product Quantization</strong> (<strong class="keyWord">PQ</strong>).</li>
<li class="numberedList"><strong class="keyWord">Evaluate update frequency</strong>: If your application requires frequent index updates, prioritize methods like LSH that support efficient updates.</li>
<li class="numberedList"><strong class="keyWord">Assess search speed requirements</strong>: For applications demanding ultra-low latency, HNSW typically provides the fastest search times once built.</li>
<li class="numberedList"><strong class="keyWord">Balance with accuracy needs</strong>: As you move downward in the flowchart, consider the accuracy-efficiency tradeoff based on your application’s tolerance for approximate results.</li>
</ol>
<p class="normal">For most production RAG applications, you’ll likely end up with HNSW or a combined approach like IVF+HNSW, which clusters vectors first (IVF) and then builds efficient graph structures (HNSW) within each cluster. This combination delivers excellent performance across a wide range of scenarios.</p>
<p class="normal">To improve retrieval, documents must be processed and structured effectively. The next section explores loading various<a id="_idIndexMarker319"/> document types and handling multi-modal cont<a id="_idTextAnchor172"/>ent.</p>
<p class="normal">Vector libraries, like Facebook (Meta) Faiss or Spotify Annoy, provide functionality for working with vector data. They typically offer different<a id="_idIndexMarker320"/> implementations of the <strong class="keyWord">ANN</strong> algorithm, such as clustering or tree-based methods, and allow users to perform vector similarity searches for various applications. Let’s quickly go through a few of the most popular ones:</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Faiss</strong> is a library developed <a id="_idIndexMarker321"/>by Meta (previously Facebook) that provides efficient similarity search and clustering of dense vectors. It offers various indexing algorithms, including PQ, LSH, and HNSW. Faiss is widely used for large-scale vector search tasks and supports both CPU and GPU acceleration.</li>
<li class="b lletList"><strong class="keyWord">Annoy</strong> is a C++ library for approximate nearest neighbor search in high-dimensional spaces maintained <a id="_idIndexMarker322"/>and developed by Spotify, implementing the Annoy algorithm based on a forest of random projection trees.</li>
<li class="b lletList"><strong class="keyWord">hnswlib</strong> is a C++ library for <a id="_idIndexMarker323"/>approximate nearest-neighbor search using the HNSW algorithm.</li>
<li class="b lletList"><strong class="keyWord">Non-Metric Space Library </strong>(<strong class="keyWord">nmslib</strong>) supports <a id="_idIndexMarker324"/>various indexing algorithms like HNSW, SW-graph, and SPTAG.</li>
<li class="b lletList"><strong class="keyWord">SPTAG </strong>by Microsoft implements a <a id="_idIndexMarker325"/>distributed ANN. It comes with a k-d tree and relative neighborhood graph (SPTAG-KDT), and a balanced k-means tree and relative neighborhood graph (SPTAG-BKT).</li>
</ul>
<div><div><p class="normal">There are a lot more vector search libraries you can choose from. You can get a complete overview at <a href="https://github.com/erikbern/ann-benchmarks">https://github.com/erikbern/ann-benchmarks</a>.</p>
</div>
</div>
<p class="normal">When implementing vector storage solutions, consider:</p>
<ul>
<li class="b lletList">The tradeoff between exact and approximate search</li>
<li class="b lletList">Memory constraints and scaling requirements</li>
<li class="b lletList">The need for hybrid search capabilities combining vector and traditional search</li>
<li class="b lletList">Multi-modal data support requirements</li>
<li class="b lletList">Integration costs and maintenance complexity</li>
</ul>
<p class="normal">For many applications, a hybrid approach combining vector search with traditional database capabilities provides the<a id="_idIndexMarker326"/> most flexibl<a id="_idTextAnchor173"/>e solution.</p>
<h1 class="heading-1" id="_idParaDest-99"><a id="_idTextAnchor174"/>Breaking down the RAG pipeline</h1>
<p class="normal">Think of the RAG pipeline as an assembly line in a library, where raw materials (documents) get transformed into a searchable knowledge base that can answer questions. Let us walk through how each component <a id="_idIndexMarker327"/>plays its part.</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Document processing – the foundation</strong></li>
</ol>
<p class="normal-one">Document processing is like preparing books for a library. When documents first enter the system, they need to be:</p>
<ul>
<li class="bulletList level-2">Loaded using document loaders appropriate for their format (PDF, HTML, text, etc.)</li>
<li class="bulletList level-2">Transformed into a standard format that the system can work with</li>
<li class="bulletList level-2">Split into smaller, meaningful chunks that are easier to process and retrieve</li>
</ul>
<p class="normal-one">For example, when processing a textbook, we might break it into chapter-sized or paragraph-sized chunks while preserving important context in metadata.</p>
<div><ol>
<li class="numberedList" value="2"><strong class="keyWord">Vector indexing – creating the card catalog</strong></li>
</ol>
<p class="normal-one">Once documents are processed, we need a way to make them searchable. This is where vector indexing comes in. Here’s how it works:</p>
<ul>
<li class="bulletList level-2">An embedding model converts each document chunk into a vector (think of it as capturing the document’s meaning in a list of numbers)</li>
<li class="bulletList level-2">These vectors are organized in a special data structure (the vector store) that makes them easy to search</li>
<li class="bulletList level-2">The vector store also maintains connections between these vectors and their original documents</li>
</ul>
<p class="normal-one">This is similar to how a library’s card catalog organizes books by subject, making it easy to find related materials.</p>
<ol>
<li class="numberedList" value="3"><strong class="keyWord">Vector stores – the organized shelves</strong></li>
</ol>
<p class="normal-one">Vector stores are like the organized shelves in our library. They:</p>
<ul>
<li class="bulletList level-2">Store both the document vectors and the original document content</li>
<li class="bulletList level-2">Provide efficient ways to search through the vectors</li>
<li class="bulletList level-2">Offer different organization methods (like HNSW or IVF) that balance speed and accuracy</li>
</ul>
<p class="normal-one">For example, using FAISS (a popular vector store), we might organize our vectors in a hierarchical structure that lets us quickly narrow down which documents to examine in detail.</p>
<ol>
<li class="numberedList" value="4"><strong class="keyWord">Retrieval – finding the right books</strong></li>
</ol>
<p class="normal-one">Retrieval is where everything comes together. When a question comes in:</p>
<ul>
<li class="bulletList level-2">The question gets converted into a vector using the same embedding model</li>
<li class="bulletList level-2">The vector store finds documents whose vectors are most similar to the question vector</li>
</ul>
<p class="normal-one">The retriever might apply additional logic, like:</p>
<ul>
<li class="bulletList level-2">Removing duplicate information</li>
<li class="bulletList level-2">Balancing relevance and diversity</li>
<li class="bulletList level-2">Combining results from different search methods</li>
</ul>
<div><p class="normal">A basic RAG implementation<a id="_idIndexMarker328"/> looks like this:</p>
<pre># For query transformation
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
# For basic RAG implementation
from langchain_community.document_loaders import JSONLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
# 1. Load documents
loader = JSONLoader(
    file_path="knowledge_base.json",
    jq_schema=".[].content",  # This extracts the content field from each array item
    text_content=True
)
documents = loader.load()
# 2. Convert to vectors
embedder = OpenAIEmbeddings()
embeddings = embedder.embed_documents([doc.page_content for doc in documents])
# 3. Store in vector database
vector_db = FAISS.from_documents(documents, embedder)
# 4. Retrieve similar docs
query = "What are the effects of climate change?"</pre>
<p class="normal">results = vector_db.similarity_search(query)This implementation covers the core RAG workflow: document loading, embedding, storage, and retrieval.</p>
<p class="normal">Building a RAG system with LangChain requires understanding two fundamental building blocks, which we should discuss a bit more in detail: <strong class="keyWord">document loaders</strong> and <strong class="keyWord">retrievers</strong>. Let’s explore how these components <a id="_idIndexMarker329"/>work together to create effective retr<a id="_idTextAnchor175"/>ieval systems.</p>
<div><h2 class="heading-2" id="_idParaDest-100"><a id="_idTextAnchor176"/>Document processing</h2>
<p class="normal">LangChain provides a comprehensive<a id="_idIndexMarker330"/> system for loading documents from various sources through document loaders. A document loader is a component in LangChain that transforms various data sources into a standardized document format that can be used throughout the LangChain ecosystem. Each document contains the actual content and associated metadata.</p>
<p class="normal">Document loaders serve as the foundation for RAG systems by:</p>
<ul>
<li class="b lletList">Converting diverse data sources into a uniform format</li>
<li class="b lletList">Extracting text and metadata from files</li>
<li class="b lletList">Preparing documents for further processing (like chunking or embedding)</li>
</ul>
<p class="normal">LangChain supports loading documents from a wide range of document types and sources through specialized loaders, for example:</p>
<ul>
<li class="b lletList"><strong class="keyWord">PDFs</strong>: Using PyPDFLoader</li>
<li class="b lletList"><strong class="keyWord">HTML</strong>: WebBaseLoader for extracting web page text</li>
<li class="b lletList"><strong class="keyWord">Plain text</strong>: TextLoader for raw text inputs</li>
<li class="b lletList"><strong class="keyWord">WebBaseLoader </strong>for web page content extraction</li>
<li class="b lletList"><strong class="keyWord">ArxivLoader </strong>for scientific papers</li>
<li class="b lletList"><strong class="keyWord">WikipediaLoader </strong>for encyclopedia entries</li>
<li class="b lletList"><strong class="keyWord">YoutubeLoader </strong>for video transcripts</li>
<li class="b lletList"><strong class="keyWord">ImageCaptionLoader </strong>for image content</li>
</ul>
<p class="normal">You may have noticed some non-text content types in the preceding list. Advanced RAG systems can handle non-text data; for example, image embeddings or audio transcripts.</p>
<p class="normal">The following table organizes LangChain document loaders into a comprehensive table:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table003-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Category</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Description</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Notable Examples</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Common Use Cases</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">File Systems</p>
</td>
<td class="No-Table-Style">
<p class="normal">Load from local files</p>
</td>
<td class="No-Table-Style">
<p class="normal">TextLoader, CSVLoader, PDFLoader</p>
</td>
<td class="No-Table-Style">
<p class="normal">Processing local documents, data files</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Web Content</p>
</td>
<td class="No-Table-Style">
<p class="normal">Extract from online sources</p>
</td>
<td class="No-Table-Style">
<p class="normal">WebBaseLoader, RecursiveURLLoader, SitemapLoader</p>
</td>
<td class="No-Table-Style">
<p class="normal">Web scraping, content aggregation</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<div><p class="normal">Cloud Storage</p>
</td>
<td class="No-Table-Style">
<p class="normal">Access cloud-hosted files</p>
</td>
<td class="No-Table-Style">
<p class="normal">S3DirectoryLoader, GCSFileLoader, DropboxLoader</p>
</td>
<td class="No-Table-Style">
<p class="normal">Enterprise data integration</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Databases</p>
</td>
<td class="No-Table-Style">
<p class="normal">Load from structured data stores</p>
</td>
<td class="No-Table-Style">
<p class="normal">MongoDBLoader, SnowflakeLoader, BigQueryLoader</p>
</td>
<td class="No-Table-Style">
<p class="normal">Business intelligence, data analysis</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Social Media</p>
</td>
<td class="No-Table-Style">
<p class="normal">Import social platform content</p>
</td>
<td class="No-Table-Style">
<p class="normal">TwitterTweetLoader, RedditPostsLoader, DiscordChatLoader</p>
</td>
<td class="No-Table-Style">
<p class="normal">Social media analysis</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Productivity Tools</p>
</td>
<td class="No-Table-Style">
<p class="normal">Access workspace documents</p>
</td>
<td class="No-Table-Style">
<p class="normal">NotionDirectoryLoader, SlackDirectoryLoader, TrelloLoader</p>
</td>
<td class="No-Table-Style">
<p class="normal">Knowledge base creation</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Scientific Sources</p>
</td>
<td class="No-Table-Style">
<p class="normal">Load academic content</p>
</td>
<td class="No-Table-Style">
<p class="normal">ArxivLoader, PubMedLoader</p>
</td>
<td class="No-Table-Style">
<p class="normal">Research applications</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 4.3: Document loaders in LangChain</p>
<p class="normal">Finally, modern document<a id="_idIndexMarker331"/> loaders offer several sophisticated capabilities:</p>
<ul>
<li class="b lletList">Concurrent loading for better performance</li>
<li class="b lletList">Metadata extraction and preservation</li>
<li class="b lletList">Format-specific parsing (like table extraction from PDFs)</li>
<li class="b lletList">Error handling and validation</li>
<li class="b lletList">Integration with transformation pipelines</li>
</ul>
<p class="normal">Let’s go through an example of loading a JSON file. Here’s a typical pattern for using a document loader:</p>
<pre>from langchain_community.document_loaders import JSONLoader
# Load a json file
loader = JSONLoader(
 file_path="knowledge_base.json",
 jq_schema=".[].content",  # This extracts the content field from each array item
 text_content=True
)
documents = loader.load()
print(documents)</pre>
<div><p class="normal">Document loaders come with a standard <code class="inlineCode">.load()</code> method interface that returns documents in LangChain’s document format. The initialization is source-specific. After loading, documents often need processing before<a id="_idIndexMarker332"/> storage and retrieval, and selecting the right chunking strategy determines the relevance and diversity of AI-generated responses<a id="_idTextAnchor177"/>.</p>
<h3 class="heading-3" id="_idParaDest-101"><a id="_idTextAnchor178"/>Chunking strategies</h3>
<p class="normal">Chunking—how you divide documents into smaller pieces—can dramatically impact your RAG system’s performance. Poor chunking <a id="_idIndexMarker333"/>can break apart related concepts, lose critical context, and ultimately lead to<a id="_idIndexMarker334"/> irrelevant retrieval results. The way you chunk documents affects:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Retrieval accuracy</strong>: Well-formed chunks maintain semantic coherence, making them easier to match with relevant queries</li>
<li class="b lletList"><strong class="keyWord">Context preservation</strong>: Poor chunking can split related information, causing knowledge gaps</li>
<li class="b lletList"><strong class="keyWord">Response quality</strong>: When the LLM receives fragmented or irrelevant chunks, it generates less accurate responses</li>
</ul>
<p class="normal">Let’s explore a hierarchy of chunking approaches, from simple to sophisticated, to help you implement the most effective strategy for your specific use ca<a id="_idTextAnchor179"/>se.</p>
<h4 class="heading-4">Fixed-size chunking</h4>
<p class="normal">The most basic approach divides text into<a id="_idIndexMarker335"/> chunks of a specified length without considering content structure:</p>
<pre>from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter(
 separator=" ",   # Split on spaces to avoid breaking words
 chunk_size=200,
 chunk_overlap=20
)
chunks = text_splitter.split_documents(documents)
print(f"Generated {len(chunks)} chunks from document")</pre>
<p class="normal">Fixed-size chunking is good for quick prototyping or when document structure is relatively uniform, however, it often splits text at <a id="_idIndexMarker336"/>awkward positions, breaking sentences, paragraphs, or logical u<a id="_idTextAnchor180"/>nits.</p>
<div><h4 class="heading-4">Recursive character chunking</h4>
<p class="normal">This method respects natural text <a id="_idIndexMarker337"/>boundaries by recursively applying different separators:</p>
<pre>from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    separators=["\n\n", "\n", ". ", " ", ""],
    chunk_size=150,
    chunk_overlap=20
)
document = """
document = """# Introduction to RAG
Retrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.
It helps address hallucinations by grounding responses in retrieved information.
## Key Components
RAG consists of several components:
1. Document processing
2. Vector embedding
3. Retrieval
4. Augmentation
5. Generation
### Document Processing
This step involves loading and chunking documents appropriately.
"""
chunks = text_splitter.split_text(document)
print(chunks)</pre>
<p class="normal">Here are the chunks:</p>
<pre>['# Introduction to RAG\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models.', 'It helps address hallucinations by grounding responses in retrieved information.', '## Key Components\nRAG consists of several components:\n1. Document processing\n2. Vector embedding\n3. Retrieval\n4. Augmentation\n5. Generation', '### Document Processing\nThis step involves loading and chunking documents appropriately.']</pre>
<div><p class="normal">How it works is that the splitter first attempts to divide text at paragraph breaks (<code class="inlineCode">\n\n</code>). If the resulting chunks are still too large, it tries the next separator (<code class="inlineCode">\n</code>), and so on. This approach preserves natural text boundaries while maintaining reasonable chunk sizes.</p>
<p class="normal">Recursive character chunking<a id="_idIndexMarker338"/> is the recommended default strategy for most applications. It works well for a wide range of document types and provides a good balance between preserving context and maintaining manageable chunk<a id="_idTextAnchor181"/> sizes.</p>
<h4 class="heading-4">Document-specific chunking</h4>
<p class="normal">Different document types have different structures. Document-specific chunking adapts to these structures. An implementation <a id="_idIndexMarker339"/>could involve using different specialized splitters based on document type using <code class="inlineCode">if</code> statements. For example, we could be using a <code class="inlineCode">MarkdownTextSplitter</code>, <code class="inlineCode">PythonCodeTextSplitter</code>, or <code class="inlineCode">HTMLHeaderTextSplitter</code> depending on the content type being markdown, Python, or HTML.</p>
<p class="normal">This can be useful when working with specialized document formats where structure matters – code repositories, technical documentation, markdown articles, or similar. Its advantage is that it preserves logical document structure, maintains functional units together (like code functions, markdown sections), and improves retrieval relevance for domain-specific <a id="_idTextAnchor182"/>queries.</p>
<h4 class="heading-4">Semantic chunking</h4>
<p class="normal">Unlike previous approaches<a id="_idIndexMarker340"/> that rely on textual separators, semantic chunking analyzes the meaning of content to determine chunk boundaries.</p>
<pre>from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()
text_splitter = SemanticChunker(
    embeddings=embeddings,
    add_start_index=True  # Include position metadata
)
chunks = text_splitter.split_text(document)</pre>
<div><p class="normal">These are the chunks:</p>
<pre>['# Introduction to RAG\nRetrieval-Augmented Generation (RAG) combines retrieval systems with generative AI models. It helps address hallucinations by grounding responses in retrieved information. ## Key Components\nRAG consists of several components:\n1. Document processing\n2. Vector embedding\n3. Retrieval\n4.',
 'Augmentation\n5. Generation\n\n### Document Processing\nThis step involves loading and chunking documents appropriately. ']</pre>
<p class="normal">Here’s how the <code class="inlineCode">SemanticChunker</code> works:</p>
<ol>
<li class="numberedList" value="1">Splits text into sentences</li>
<li class="numberedList">Creates embeddings for groups of sentences (determined by <code class="inlineCode">buffer_size</code>)</li>
<li class="numberedList">Measures semantic similarity between adjacent groups</li>
<li class="numberedList">Identifies natural breakpoints where topics or concepts change</li>
<li class="numberedList">Creates chunks that preserve semantic coherence</li>
</ol>
<p class="normal">You may use semantic chunking<a id="_idIndexMarker341"/> for complex technical documents where semantic cohesion is crucial for accurate retrieval and when you’re willing to spend additional compute/costs on embedding generation.</p>
<p class="normal">Benefits include chunk creation based on actual meaning rather than superficial text features and keeping related concepts together even when they span traditional separator b<a id="_idTextAnchor183"/>oundaries.</p>
<h4 class="heading-4">Agent-based chunking</h4>
<p class="normal">This experimental approach uses LLMs <a id="_idIndexMarker342"/>to intelligently divide text based on semantic analysis and content understanding in the following manner:</p>
<ol>
<li class="numberedList" value="1">Analyze the document’s structure and content</li>
<li class="numberedList">Identify natural breakpoints based on topic shifts</li>
<li class="numberedList">Determine optimal chunk boundaries that preserve meaning</li>
<li class="numberedList">Return a list of starting positions for creating chunks</li>
</ol>
<p class="normal">This type of chunking can be useful for exceptionally complex documents where standard splitting methods fail to preserve critical relationships between concepts. This approach is particularly useful when:</p>
<div><ul>
<li class="b lletList">Documents contain intricate logical flows that need to be preserved</li>
<li class="b lletList">Content requires domain-specific understanding to chunk appropriately</li>
<li class="b lletList">Maximum retrieval accuracy justifies the additional expense of LLM-based processing</li>
</ul>
<p class="normal">The limitations are that it comes with a higher computational cost and latency, and that chunk sizes are less p<a id="_idTextAnchor184"/>redictable.</p>
<h4 class="heading-4">Multi-modal chunking</h4>
<p class="normal">Modern documents often contain a <a id="_idIndexMarker343"/>mix of text, tables, images, and code. Multi-modal chunking handles these different content types appropriately.</p>
<p class="normal">We can imagine the following process for multi-modal content:</p>
<ol>
<li class="numberedList" value="1">Extract text, images, and tables separately</li>
<li class="numberedList">Process text with appropriate text chunker</li>
<li class="numberedList">Process tables to preserve structure</li>
<li class="numberedList">For images: generate captions or extract text via OCR or a vision LLM</li>
<li class="numberedList">Create metadata linking related elements</li>
<li class="numberedList">Embed each element appropriately</li>
</ol>
<p class="normal">In practice, you would use specialized libraries such as unstructured for document parsing, vision models for image<a id="_idIndexMarker344"/> understanding, and table extraction tools for str<a id="_idTextAnchor185"/>uctured data.</p>
<h4 class="heading-4">Choosing the right chunking strategy</h4>
<p class="normal">Your chunking strategy should <a id="_idIndexMarker345"/>be guided by document characteristics, retrieval needs, and computational resources as the following table illustrates:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table004">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Factor</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Condition</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Recommended Strategy</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Document Characteristics</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Highly structured documents (markdown, code)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Document-specific chunking</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p class="normal">Complex technical content</p>
</td>
<td class="No-Table-Style">
<p class="normal">Semantic chunking</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p class="normal">Mixed media</p>
</td>
<td class="No-Table-Style">
<p class="normal">Multi-modal approaches</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Retrieval Needs</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Fact-based QA</p>
</td>
<td class="No-Table-Style">
<p class="normal">Smaller chunks (100-300 tokens)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p class="normal">Complex reasoning</p>
</td>
<td class="No-Table-Style">
<p class="normal">Larger chunks (500-1000 tokens)</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<div><p class="normal">Context-heavy answers</p>
</td>
<td class="No-Table-Style">
<p class="normal">Sliding window with significant overlap</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Computational Resources</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal">Limited API budget</p>
</td>
<td class="No-Table-Style">
<p class="normal">Basic recursive chunking</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p class="normal">Performance-critical</p>
</td>
<td class="No-Table-Style">
<p class="normal">Pre-computed semantic chunks</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 4.4: Comparison of chunking strategies</p>
<p class="normal">We recommend starting with Level 2 (Recursive Character Chunking) as your baseline, then experiment with more advanced strategies if retrieval quality needs improvement.</p>
<p class="normal">For most RAG applications, the <code class="inlineCode">RecursiveCharacterTextSplitter</code> with appropriate chunk size and overlap settings provides an excellent balance of simplicity, performance, and retrieval quality. As your system matures, you can evaluate whether more sophisticated chunking strategies deliver meaningful improvements.</p>
<p class="normal">However, it is often critical to performance to experiment with different chunk sizes specific to your use case and document types. Please refer to <a href="E_Chapter_8.xhtml#_idTextAnchor390"><em class="italic">Chapter 8</em></a> for testing and benchmarking strategies.</p>
<p class="normal">The next section covers <a id="_idIndexMarker346"/>semantic search, hybrid methods, and advanced ranking techniques.<a id="_idTextAnchor186"/></p>
<h3 class="heading-3" id="_idParaDest-102"><a id="_idTextAnchor187"/>Retrieval</h3>
<p class="normal">Retrieval integrates a vector store with other LangChain components for simplified querying and compatibility. Retrieval<a id="_idIndexMarker347"/> systems form a crucial bridge between unstructured queries and relevant documents.</p>
<p class="normal">In LangChain, a retriever is fundamentally an interface that accepts natural language queries and returns relevant documents. Let’s explore how this works in detail.</p>
<p class="normal">At its heart, a retriever in LangChain follows a<a id="_idIndexMarker348"/> simple yet powerful pattern:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Input</strong>: Takes a query as a string</li>
<li class="b lletList"><strong class="keyWord">Processing</strong>: Applies retrieval logic specific to the implementation</li>
<li class="b lletList"><strong class="keyWord">Output</strong>: Returns a list of document objects, each containing:<ul><li class="bulletList level-2"><code class="inlineCode">page_content</code>: The actual document content</li>
<li class="bulletList level-2"><code class="inlineCode">metadata</code>: Associated information like document ID or source</li>
</ul></li>
</ul>
<div><p class="normal">This diagram (from the LangChain documentation) illustrates this relationship.</p>
<figure class="mediaobject"><img alt="Figure 4.3: The relationship between query, retriever, and documents" src="img/B32363_04_03.png"/></figure>
<p class="packt_figref">Figure 4.3: The relationship between query, retriever, and documents</p>
<p class="normal">LangChain offers a rich ecosystem of retrievers, each designed to solve specific information retrieval challenges<a id="_idTextAnchor188"/>.</p>
<h4 class="heading-4">LangChain retrievers</h4>
<p class="normal">The retrievers can be broadly<a id="_idIndexMarker349"/> categorized into a few key groups that serve different use cases and implementation needs:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Core infrastructure retrievers</strong> include both self-hosted options like ElasticsearchRetriever and cloud-based <a id="_idIndexMarker350"/>solutions from major providers like Amazon, Google, and Microsoft.</li>
<li class="b lletList"><strong class="keyWord">External knowledge retrievers </strong>tap<a id="_idIndexMarker351"/> into external and established knowledge bases. ArxivRetriever, WikipediaRetriever, and TavilySearchAPI stand out here, offering direct access to academic papers, encyclopedia entries, and web content respectively.</li>
<li class="b lletList"><strong class="keyWord">Algorithmic retrievers</strong> include several classic information retrieval methods. The BM25 and TF-IDF retrievers excel at lexical search, while kNN retrievers handle semantic similarity searches. Each <a id="_idIndexMarker352"/>of these algorithms brings its own strengths – BM25 for keyword precision, TF-IDF for document classification, and kNN for similarity matching.</li>
<li class="b lletList"><strong class="keyWord">Advanced/Specialized retrievers</strong> often address specific performance requirements or resource constraints that<a id="_idIndexMarker353"/> may arise in production environments. LangChain offers specialized retrievers with unique capabilities. NeuralDB provides CPU-optimized retrieval, while LLMLingua focuses on document compression.</li>
<li class="b lletList"><strong class="keyWord">Integration retrievers</strong> connect <a id="_idIndexMarker354"/>with popular platforms and services. These retrievers, like those for Google Drive or Outline, make it easier to incorporate existing document repositories into your RAG application.</li>
</ul>
<div><p class="normal">Here’s a basic example of retriever usage:</p>
<pre># Basic retriever interaction
docs = retriever.invoke("What is machine learning?")</pre>
<p class="normal">LangChain supports several sophisticated approaches to ret<a id="_idTextAnchor189"/>rieval:</p>
<h4 class="heading-4">Vector store retrievers</h4>
<p class="normal">Vector stores serve as the foundation for semantic search, converting documents and queries into embeddings for similarity <a id="_idIndexMarker355"/>matching. Any vector store can become a retriever<a id="_idIndexMarker356"/> through the <code class="inlineCode">as_retriever()</code> method:</p>
<pre>from langchain_community.retrievers import KNNRetriever
from langchain_openai import OpenAIEmbeddings
retriever = KNNRetriever.from_documents(documents, OpenAIEmbeddings())
results = retriever.invoke("query")</pre>
<p class="normal">These are the retrievers most relevant for RAG systems.</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Search API retrievers</strong>: These<a id="_idIndexMarker357"/> retrievers interface with external search services without storing documents locally. For example:<pre>from langchain_community.retrievers.pubmed import PubMedRetriever
retriever = PubMedRetriever()
results = retriever.invoke("COVID research")</pre></li>
<li class="numberedList"><strong class="keyWord">Database retrievers</strong>: These connect<a id="_idIndexMarker358"/> to structured data sources, translating natural language queries into database queries:<ul><li class="bulletList level-2">SQL databases using text-to-SQL conversion</li>
<li class="bulletList level-2">Graph databases using text-to-Cypher translation</li>
<li class="bulletList level-2">Document databases with specialized query interfaces</li>
</ul></li>
<li class="numberedList"><strong class="keyWord">Lexical search retrievers</strong>: These<a id="_idIndexMarker359"/> implement traditional text-matching algorithms:<ul><li class="bulletList level-2">BM25 for probabilistic ranking</li>
<li class="bulletList level-2">TF-IDF for term frequency analysis</li>
<li class="bulletList level-2">Elasticsearch integration for scalable text search</li>
</ul></li>
</ol>
<div><p class="normal">Modern retrieval systems<a id="_idIndexMarker360"/> often combine multiple approaches for better results:</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Hybrid search</strong>: Combines semantic and lexical search to leverage:<ul><li class="bulletList level-2">Vector similarity for semantic understanding</li>
<li class="bulletList level-2">Keyword matching for precise terminology</li>
<li class="bulletList level-2">Weighted combinations for optimal results</li>
</ul></li>
<li class="numberedList"><strong class="keyWord">Maximal Marginal Relevance (MMR)</strong>: Optimizes<a id="_idIndexMarker361"/> for both relevance and diversity by:<ul><li class="bulletList level-2">Selecting documents similar to the query</li>
<li class="bulletList level-2">Ensuring retrieved documents are distinct from each other</li>
<li class="bulletList level-2">Balancing exploration and exploitation</li>
</ul></li>
<li class="numberedList"><strong class="keyWord">Custom retrieval logic</strong>: LangChain allows the creation of specialized retrievers by implementing the <code class="inlineCode">BaseRe<a id="_idTextAnchor190"/>triever</code> class.</li>
</ol>
<h2 class="heading-2" id="_idParaDest-103"><a id="_idTextAnchor191"/>Advanced RAG techniques</h2>
<p class="normal">When building production<a id="_idIndexMarker362"/> RAG systems, a simple vector similarity search often isn’t enough. Modern applications need more sophisticated approaches to find and validate relevant information. Let’s explore how to enhance a basic RAG system with advanced techniques that dramatically improve result quality.</p>
<p class="normal">A standard vector search has several limitations:</p>
<ul>
<li class="b lletList">It might miss contextually relevant documents that use different terminology</li>
<li class="b lletList">It can’t distinguish between authoritative and less reliable sources</li>
<li class="b lletList">It might return redundant or contradictory information</li>
<li class="b lletList">It has no way to verify if generated responses accurately reflect the source material</li>
</ul>
<p class="normal">Modern retrieval systems often employ multiple complementary techniques to improve result quality. Two particularly powerful approaches are hybrid retrieval <a id="_idTextAnchor192"/>and re-ranking.</p>
<h3 class="heading-3" id="_idParaDest-104"><a id="_idTextAnchor193"/>Hybrid retrieval: Combining semantic and keyword search</h3>
<p class="normal">Hybrid retrieval combines two retrieval <a id="_idIndexMarker363"/>methods in parallel and the results are fused to leverage the strengths of both approaches:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Dense retrieval</strong>: Uses vector embeddings <a id="_idIndexMarker364"/>for semantic understanding</li>
<li class="b lletList"><strong class="keyWord">Sparse retrieval</strong>: Employs lexical <a id="_idIndexMarker365"/>methods like BM25 for keyword precision</li>
</ul>
<div><p class="normal">For example, a hybrid retriever might use vector similarity to find semantically related documents while simultaneously running a keyword search to catch exact terminology matches, then combine the results using rank <a id="_idTextAnchor194"/>fusion algorithms.</p>
<pre>from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from langchain.vectorstores import FAISS
# Setup semantic retriever
vector_retriever = vector_store.as_retriever(search_kwargs={"k": 5})
# Setup lexical retriever
bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 5
# Combine retrievers
hybrid_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, bm25_retriever],
    weights=[0.7, 0.3]  # Weight semantic search higher than keyword search
)
results = hybrid_retriever.get_relevant_documents("climate change impacts")</pre>
<h3 class="heading-3" id="_idParaDest-105"><a id="_idTextAnchor195"/>Re-ranking</h3>
<p class="normal">Re-ranking is a post-processing <a id="_idIndexMarker366"/>step that can follow any retrieval method, including hybrid retrieval:</p>
<ol>
<li class="numberedList" value="1">First, retrieve a larger set of candidate documents</li>
<li class="numberedList">Apply a more sophisticated model to re-score documents</li>
<li class="numberedList">Reorder based on these more precise relevance scores</li>
</ol>
<p class="normal">Re-ranking follows three main paradigms:</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Pointwise rerankers</strong>: Score each document independently (for example, on a scale of 1-10) and sort the resulting array of<a id="_idIndexMarker367"/> documents accordingly</li>
<li class="b lletList"><strong class="keyWord">Pairwise rerankers</strong>: Compare <a id="_idIndexMarker368"/>document pairs to determine preferences, then construct a final ordering by ranking documents based on their win/loss record across all comparisons</li>
<li class="b lletList"><strong class="keyWord">Listwise rerankers</strong>: The re-ranking <a id="_idIndexMarker369"/>model processes the entire list of documents (and the original query) holistically to determine optimal order by optimizing NDCG or MAP</li>
</ul>
<p class="normal">LangChain offers several re-ranking implementations:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Cohere rerank</strong>: Commercial<a id="_idIndexMarker370"/> API-based solution with excellent quality:<pre># Complete document compressor example
from langchain.retrievers.document_compressors import CohereRerank
from langchain.retrievers import ContextualCompressionRetriever
# Initialize the compressor
compressor = CohereRerank(top_n=3)
# Create a compression retriever
compression_retriever = ContextualCompressionRetriever(
 base_compressor=compressor,
 base_retriever=base_retriever
)
# Original documents
print("Original documents:")
original_docs = base_retriever.get_relevant_documents("How do transformers work?")
for i, doc in enumerate(original_docs):
 print(f"Doc {i}: {doc.page_content[:100]}...")
# Compressed documents
print("\nCompressed documents:")
compressed_docs = compression_retriever.get_relevant_documents("How do transformers work?")
for i, doc in enumerate(compressed_docs):
 print(f"Doc {i}: {doc.page_content[:100]}...")</pre></li>
<li class="b lletList"><strong class="keyWord">RankLLM</strong>: Library<a id="_idIndexMarker371"/> supporting open-source LLMs fine-tuned specifically <a id="_idIndexMarker372"/>for re-ranking:<pre>from langchain_community.document_compressors.rankllm_rerank import RankLLMRerank
compressor = RankLLMRerank(top_n=3, model="zephyr")</pre></li>
<li class="b lletList"><strong class="keyWord">LLM-based custom rerankers</strong>: Using<a id="_idIndexMarker373"/> any LLM to score document relevance:<pre># Simplified example - LangChain provides more streamlined implementations
relevance_score_chain = ChatPromptTemplate.from_template(
 "Rate relevance of document to query on scale of 1-10: {document}"
) | llm | StrOutputParser()</pre></li>
</ul>
<div><p class="normal">Please note that while Hybrid retrieval focuses on how documents are retrieved, re-ranking focuses on how they’re ordered after retrieval. These approaches can, and often should, be used together in a pipeline. When evaluating re-rankers, use position-aware metrics like Recall@k, which measures how effectively the re-ranker surfaces all relevant documents in the top positions.</p>
<p class="normal">Cross-encoder re-ranking typically improves these metrics by 10-20% over initial retrieval, especia<a id="_idTextAnchor196"/>lly for the top positions.</p>
<h3 class="heading-3" id="_idParaDest-106"><a id="_idTextAnchor197"/>Query transformation: Improving retrieval through better queries</h3>
<p class="normal">Even the best retrieval system can struggle with poorly formulated queries. Query transformation techniques address this <a id="_idIndexMarker374"/>challenge by enhancing or reformulating the original query to improve retrieval results.</p>
<p class="normal">Query expansion generates multiple variations of the original query to capture different aspects or phrasings. This helps bridge the vocabulary gap between users and documents:</p>
<pre>from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
expansion_template = """Given the user question: {question}</pre>
<p class="normal">Generate three alternative versions that express the same information need but with different wording:</p>
<pre>1."""
expansion_prompt = PromptTemplate(
    input_variables=["question"],
    template=expansion_template
)
llm = ChatOpenAI(temperature=0.7)
expansion_chain = expansion_prompt | llm | StrOutputParser()</pre>
<div><p class="normal">Let’s see this in practice:</p>
<pre># Generate expanded queries
original_query = "What are the effects of climate change?"
expanded_queries = expansion_chain.invoke(original_query)
print(expanded_queries)</pre>
<p class="normal">We should be getting<a id="_idIndexMarker375"/> something like this:</p>
<pre>What impacts does climate change have?
2. How does climate change affect the environment?
3. What are the consequences of climate change?</pre>
<p class="normal">A more advanced approach is <strong class="keyWord">Hypothetical<a id="_idTextAnchor198"/> Document Embeddings</strong> (<strong class="keyWord">HyDE</strong>).</p>
<h4 class="heading-4">Hypothetical Document Embeddings (HyDE)</h4>
<p class="normal">HyDE uses an LLM to generate a <a id="_idIndexMarker376"/>hypothetical answer document based on the query, and then uses that document’s embedding for retrieval. This technique is especially powerful for complex queries where the semantic gap between query and document language is significant:</p>
<pre>from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# Create prompt for generating hypothetical document
hyde_template = """Based on the question: {question}
Write a passage that could contain the answer to this question:"""
hyde_prompt = PromptTemplate(
    input_variables=["question"],
    template=hyde_template
)
llm = ChatOpenAI(temperature=0.2)
hyde_chain = hyde_prompt | llm | StrOutputParser()
# Generate hypothetical document
query = "What dietary changes can reduce carbon footprint?"
hypothetical_doc = hyde_chain.invoke(query)
# Use the hypothetical document for retrieval
embeddings = OpenAIEmbeddings()
embedded_query = embeddings.embed_query(hypothetical_doc)
results = vector_db.similarity_search_by_vector(embedded_query, k=3)</pre>
<div><p class="normal">Query transformation techniques are particularly useful when dealing with ambiguous queries, questions formulated by non-experts, or situations where terminology mismatches between queries and documents are common. They do add computational overhead but can dramatically improve<a id="_idIndexMarker377"/> retrieval quality, especially for complex o<a id="_idTextAnchor199"/>r poorly formulated questions.</p>
<h3 class="heading-3" id="_idParaDest-107"><a id="_idTextAnchor200"/>Context processing: maximizing retrieved information value</h3>
<p class="normal">Once documents are retrieved, context <a id="_idIndexMarker378"/>processing techniques help distill and <a id="_idIndexMarker379"/>organize the information to maximize it<a id="_idTextAnchor201"/>s value in the generation phase.</p>
<h4 class="heading-4">Contextual compression</h4>
<p class="normal">Contextual compression<a id="_idIndexMarker380"/> extracts only the most relevant parts of retrieved documents, removing irrelevant content that might distract the generator:</p>
<pre>from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(temperature=0)
compressor = LLMChainExtractor.from_llm(llm)
# Create a basic retriever from the vector store
base_retriever = vector_db.as_retriever(search_kwargs={"k": 3})
compression_retriever = ContextualCompressionRetriever(
 base_compressor=compressor,
 base_retriever=base_retriever
)
compressed_docs = compression_retriever.invoke("How do transformers work?")</pre>
<p class="normal">Here are our compressed documents:</p>
<pre>[Document(metadata={'source': 'Neural Network Review 2021', 'page': 42}, page_content="The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017."),
 Document(metadata={'source': 'Large Language Models Survey', 'page': 89}, page_content='GPT models are autoregressive transformers that predict the next t<a id="_idTextAnchor202"/>oken based on previous tokens.')]</pre>
<div><h4 class="heading-4">Maximum marginal relevance</h4>
<p class="normal">Another powerful approach is <strong class="keyWord">Maximum Marginal Relevance</strong> (<strong class="keyWord">MMR</strong>), which balances document relevance with <a id="_idIndexMarker381"/>diversity, ensuring that the retrieved set contains varied perspectives rather than redundant information:</p>
<pre>from langchain_community.vectorstores import FAISS
vector_store = FAISS.from_documents(documents, embeddings)
mmr_results = vector_store.max_marginal_relevance_search(
 query="What are transformer models?",
 k=5,            # Number of documents to return
 fetch_k=20,     # Number of documents to initially fetch
 lambda_mult=0.5  # Diversity parameter (0 = max diversity, 1 = max relevance)
)</pre>
<p class="normal">Context processing techniques are especially valuable when dealing with lengthy documents where only portions are relevant, or when providing comprehensive coverage of a topic requires diverse viewpoints. They <a id="_idIndexMarker382"/>help reduce noise in the generator’s input and ensure that the most valuable information is prioritized.</p>
<p class="normal">The final area for RAG enhancement focuses on improving the generated response itself, ensuring it’s<a id="_idTextAnchor203"/> accurate, trustworthy, and useful.</p>
<h3 class="heading-3" id="_idParaDest-108"><a id="_idTextAnchor204"/>Response enhancement: Improving generator output</h3>
<p class="normal">These response enhancement techniques are particularly important in applications where accuracy and transparency are <a id="_idIndexMarker383"/>paramount, such as educational resources, healthcare information, or legal advice. They help build user trust by making AI-generated<a id="_idIndexMarker384"/> content more verifiable and reliable.</p>
<p class="normal">Let’s first assume we have <a id="_idTextAnchor205"/>some documents as our knowledge base:</p>
<pre>from langchain_core.documents import Document
# Example documents
documents = [
    Document(
 page_content="The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.",</pre>
<div><pre>        metadata={"source": "Neural Network Review 2021", "page": 42}
    ),
    Document(
 page_content="BERT uses bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks.",
        metadata={"source": "Introduction to NLP", "page": 137}
    ),
    Document(
 page_content="GPT models are autoregressive transformers that predict the next token based on previous tokens.",
        metadata={"source": "Large Language Models Survey", "page": 89}
    )
]</pre>
<h4 class="heading-4">Source attribution</h4>
<p class="normal">Source attribution explicitly connects generated information to the retrieved sources, helping users verify facts and understand where information comes from. Let’s set up our foundation for source attribution. We’ll initialize a <a id="_idIndexMarker385"/>vector store with our documents and create a retriever configured to fetch the top 3 most relevant documents for each query. The attribution prompt template instructs the model to use citations for each claim and include a reference list:</p>
<pre>from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
# Create a vector store and retriever
embeddings = OpenAIEmbeddings()
vector_store = FAISS.from_documents(documents, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 3})
# Source attribution prompt template
attribution_prompt = ChatPromptTemplate.from_template("""
You are a precise AI assistant that provides well-sourced information.
Answer the following question based ONLY on the provided sources. For each fact or claim in your answer,</pre>
<div><pre>include a citation using [1], [2], etc. that refers to the source. Include a numbered reference list at the end.
Question: {question}
Sources:
{sources}
Your answer:
""")</pre>
<p class="normal">Next, we’ll need helper functions to format the sources with citation numbers and generate attributed responses:</p>
<pre># Create a source-formatted string from documents
def format_sources_with_citations(docs):
    formatted_sources = []
 for i, doc in enumerate(docs, 1):
        source_info = f"[{i}] {doc.metadata.get('source', 'Unknown source')}"
 if doc.metadata.get('page'):
            source_info += f", page {doc.metadata['page']}"
        formatted_sources.append(f"{source_info}\n{doc.page_content}")
 return "\n\n".join(formatted_sources)
# Build the RAG chain with source attribution
def generate_attributed_response(question):
 # Retrieve relevant documents
    retrieved_docs = retriever.invoke(question)
 
 # Format sources with citation numbers
    sources_formatted = format_sources_with_citations(retrieved_docs)
 
 # Create the attribution chain using LCEL
    attribution_chain = (
        attribution_prompt
        | ChatOpenAI(temperature=0)
        | StrOutputParser()</pre>
<div><pre>    )
 
 # Generate the response with citations
    response = attribution_chain.invoke({
 "question": question,
 "sources": sources_formatted
    })
 
 return response</pre>
<p class="normal">This example implements<a id="_idIndexMarker386"/> source attribution by:</p>
<ol>
<li class="numberedList" value="1">Retrieving relevant documents for a query</li>
<li class="numberedList">Formatting each document with a citation number</li>
<li class="numberedList">Using a prompt that explicitly requests citations for each fact</li>
<li class="numberedList">Generating a response that includes inline citations ([1], [2], etc.)</li>
<li class="numberedList">Adding a references section that links each citation to its source</li>
</ol>
<p class="normal">The key advantages of this approach are transparency and verifiability – users can trace each claim back to its source, which is especially important for academic, medical, or legal applications.</p>
<p class="normal">Let’s see what we get when we execute this with a query:</p>
<pre># Example usage
question = "How do transformer models work and what are some examples?"
attributed_answer = generate_attributed_response(question)
attributed_answer
We should be getting a response like this:
Transformer models work by utilizing self-attention mechanisms to weigh the importance of different input tokens when making predictions. This architecture was first introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017 [1].
One example of a transformer model is BERT, which employs bidirectional training of the Transformer, masked language modeling, and next sentence prediction tasks [2]. Another example is GPT (Generative Pre-trained Transformer) models, which are autoregressive transformers that predict the next token based on previous tokens [3].
Reference List:</pre>
<div><pre>[1] Neural Network Review 2021, page 42
[2] Introduction to NLP, page 137
[3] Large Language Models Survey, page 89</pre>
<p class="normal">Self-consistency checking compares the generated response against the retrieved context to verify accuracy<a id="_idTextAnchor206"/> and identify<a id="_idIndexMarker387"/> potential hallucinations.</p>
<h4 class="heading-4">Self-consistency checking: ensuring factual accuracy</h4>
<p class="normal">Self-consistency checking verifies that generated responses accurately reflect the information in retrieved documents, providing a <a id="_idIndexMarker388"/>crucial layer of protection against hallucinations. We can  use LCEL to create streamlined verification pipelines:</p>
<pre>from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from typing import List, Dict
from langchain_core.documents import Document
def verify_response_accuracy(
    retrieved_docs: List[Document],
    generated_answer: str,
    llm: ChatOpenAI = None
) -&gt; Dict:
 """
    Verify if a generated answer is fully supported by the retrieved documents.
    Args:
        retrieved_docs: List of documents used to generate the answer
        generated_answer: The answer produced by the RAG system
        llm: Language model to use for verification
    Returns:
        Dictionary containing verification results and any identified issues
    """
 if llm is None:
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
 
 # Create context from retrieved documents</pre>
<div><pre>    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
 </pre>
<p class="normal">The function above begins our verification process by accepting the retrieved documents and generated answers as inputs. It initializes a language model for verification if one isn’t provided and combines all<a id="_idIndexMarker389"/> document content into a single context string. Next, we’ll define the verification prompt that instructs the LLM to perform a detailed fact-checking analysis:</p>
<pre> # Define verification prompt - fixed to avoid JSON formatting issues in the template
    verification_prompt = ChatPromptTemplate.from_template("""
    As a fact-checking assistant, verify whether the following answer is fully supported
    by the provided context. Identify any statements that are not supported or contradict the context.
 
    Context:
    {context}
 
    Answer to verify:
    {answer}
 
    Perform a detailed analysis with the following structure:
    1. List any factual claims in the answer
    2. For each claim, indicate whether it is:
       - Fully supported (provide the supporting text from context)
       - Partially supported (explain what parts lack support)
       - Contradicted (identify the contradiction)
       - Not mentioned in context
    3. Overall assessment: Is the answer fully grounded in the context?
 
    Return your analysis in JSON format with the following structure:
    {{
      "claims": [
        {{
          "claim": "The factual claim",
          "status": "fully_supported|partially_supported|contradicted|not_mentioned",
          "evidence": "Supporting or contradicting text from context",</pre>
<div><pre>          "explanation": "Your explanation"
        }}
      ],
      "fully_grounded": true|false,
      "issues_identified": ["List any specific issues"]
    }}
    """)</pre>
<p class="normal">The verification prompt is structured to perform a comprehensive fact check. It instructs the model to break down each claim in the answer and categorize it based on how well it’s supported by the provided <a id="_idIndexMarker390"/>context. The prompt also requests the output in a structured JSON format that can be easily processed programmatically.</p>
<p class="normal">Finally, we’ll complete the function with the verification chain and example usage:</p>
<pre>    # Create verification chain using LCEL
    verification_chain = (
        verification_prompt
        | llm
        | StrOutputParser()
    )
 
    # Run verification
    result = verification_chain.invoke({
 "context": context,
 "answer": generated_answer
    })
 
    return result
# Example usage
retrieved_docs = [
    Document(page_content="The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017. It relies on self-attention mechanisms instead of recurrent or convolutional neural networks."),
    Document(page_content="BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.")
]</pre>
<div><pre>generated_answer = "The transformer architecture was introduced by OpenAI in 2018 and uses recurrent neural networks. BERT is a transformer model developed by Google."
verification_result = verify_response_accuracy(retrieved_docs, generated_answer)
print(verification_result)</pre>
<p class="normal">We should get a response like this:</p>
<pre>{
    "claims": [
        {
            "claim": "The transformer architecture was introduced by OpenAI in 2018",
            "status": "contradicted",
            "evidence": "The transformer architecture was introduced in the paper 'Attention is All You Need' by Vaswani et al. in 2017.",
            "explanation": "The claim is contradicted by the fact that the transformer architecture was introduced in 2017 by Vaswani et al., not by OpenAI in 2018."
        },
        {
            "claim": "The transformer architecture uses recurrent neural networks",
            "status": "contradicted",
            "evidence": "It relies on self-attention mechanisms instead of recurrent or convolutional neural networks.",
            "explanation": "The claim is contradicted by the fact that the transformer architecture does not use recurrent neural networks but relies on self-attention mechanisms."
        },
        {
            "claim": "BERT is a transformer model developed by Google",
            "status": "fully_supported",
            "evidence": "BERT is a transformer-based model developed by Google that uses masked language modeling and next sentence prediction as pre-training objectives.",</pre>
<div><pre>            "explanation": "This claim is fully supported by the provided context."
        }
    ],
    "fully_grounded": false,
    "issues_identified": ["The answer contains incorrect information about the introduction of the transformer architecture and its use of recurrent neural networks."]
}</pre>
<p class="normal">Based on the verification <a id="_idIndexMarker391"/>result, you can:</p>
<ol>
<li class="numberedList" value="1">Regenerate the answer if issues are found</li>
<li class="numberedList">Add qualifying statements to indicate uncertainty</li>
<li class="numberedList">Filter out unsupported claims</li>
<li class="numberedList">Include confidence indicators for different parts of the response</li>
</ol>
<p class="normal">This approach systematically analyzes generated responses against source documents, identifying specific unsupported claims rather than just providing a binary assessment. For each factual assertion, it determines whether it’s fully supported, partially supported, contradicted, or not mentioned in<a id="_idIndexMarker392"/> the context.</p>
<p class="normal">Self-consistency checking is essential for applications where trustworthiness is paramount, such as medical information, financial advice, or educational content. Detecting and addressing hallucinations before they reach users significantly improves the reliability of RAG systems.</p>
<p class="normal">The verification can be further enhanced by:</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Granular claim extraction</strong>: Breaking down complex responses into atomic factual claims</li>
<li class="numberedList"><strong class="keyWord">Evidence linking</strong>: Explicitly connecting each claim to specific supporting text</li>
<li class="numberedList"><strong class="keyWord">Confidence scoring</strong>: Assigning numerical confidence scores to different parts of the response</li>
<li class="numberedList"><strong class="keyWord">Selective regeneration</strong>: Regenerating only the unsupported portions of responses</li>
</ol>
<p class="normal">These techniques create a verification layer that substantially reduces the risk of presenting incorrect information to users while maintaining the fluency and coherence of generated responses.</p>
<p class="normal">While the techniques we’ve discussed enhance individual components of the RAG pipeline, corrective RAG represents a more holistic approach that addresses fundamental retrieval quality issues at a systemic level.</p>
<div><h3 class="heading-3" id="_idParaDest-109"><a id="_idTextAnchor207"/>Corrective RAG</h3>
<p class="normal">The techniques we’ve explored so <a id="_idIndexMarker393"/>far mostly assume that our retrieval mechanism returns relevant, accurate documents. But what happens when it doesn’t? In real-world applications, retrieval systems often return irrelevant, insufficient, or even misleading content. This “garbage in, garbage out” problem represents a critical vulnerability in standard RAG systems. <strong class="keyWord">Corrective Retrieval-Augmented Generation</strong> (<strong class="keyWord">CRAG</strong>) directly addresses this challenge by<a id="_idIndexMarker394"/> introducing explicit evaluation and correction mechanisms into the RAG pipeline.</p>
<p class="normal">CRAG extends the standard RAG pipeline with evaluation and conditional branching:</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Initial retrieval:</strong> Standard document retrieval from the vector store based on the query.</li>
<li class="numberedList"><strong class="keyWord">Retrieval evaluation:</strong> A retrieval evaluator component assesses each document’s relevance and quality.</li>
<li class="numberedList"><strong class="keyWord">Conditional correction:</strong><ol><li class="alphabeticList level-2" value="1"><strong class="keyWord">Relevant documents:</strong> Pass high-quality documents directly to the generator.</li>
<li class="alphabeticList level-2"><strong class="keyWord">Irrelevant documents:</strong> Filter out low-quality documents to prevent noise.</li>
<li class="alphabeticList level-2"><strong class="keyWord">Insufficient/Ambiguous results:</strong> Trigger alternative information-seeking strategies (like web search) when internal knowledge is inadequate.</li>
</ol></li>
<li class="numberedList"><strong class="keyWord">Generation:</strong> Produce the final response using the filtered or augmented context.</li>
</ol>
<p class="normal">This workflow transforms<a id="_idIndexMarker395"/> RAG from a static pipeline into a more dynamic, self-correcting system capable of seeking additional information when needed.</p>
<figure class="mediaobject"><img alt="Figure 4.4: Corrective RAG workflow showing evaluation and conditional branching" src="img/B32363_04_04.png"/></figure>
<p class="packt_figref">Figure 4.4: Corrective RAG workflow showing evaluation and conditional branching</p>
<div><p class="normal">The retrieval evaluator is the cornerstone of CRAG. Its job is to analyze the relationship between retrieved documents and the query, determining which documents are truly relevant. Implementations typically use an LLM with a carefully crafted prompt:</p>
<pre>from pydantic import BaseModel, Field
class DocumentRelevanceScore(BaseModel):
 """Binary relevance score for document evaluation."""
 is_relevant: bool = Field(description="Whether the document contains information relevant to the query")
    reasoning: str = Field(description="Explanation for the relevance decision")
def evaluate_document(document, query, llm):
 """Evaluate if a document is relevant to a query."""
    prompt = f""" You are an expert document evaluator. Your task is to determine if the following document contains information relevant to the given query.
Query: {query}
Document content:
{document.page_content}
Analyze whether this document contains information that helps answer the query.
"""
    Evaluation = llm.with_structured_output(DocumentRelevanceScore).invoke(prompt)
 return evaluation</pre>
<p class="normal">By evaluating each document independently, CRAG can make fine-grained decisions about which content to include, exclude, or supplement, substantially improving the quality of the final context provided to the<a id="_idIndexMarker396"/> generator.</p>
<p class="normal">Since the CRAG implementation builds on concepts we’ll introduce in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, we’ll not be showing the complete code here, but you can find the implementation in the book’s companion repository. Please note that LangGraph is particularly well-suited for implementing CRAG because it allows for conditional branching based on document evaluation.</p>
<div><p class="normal">While CRAG enhances RAG by adding evaluation and correction mechanisms to the retrieval pipeline, Agentic RAG represents a more fundamental paradigm shift by introducing autonomous AI agents to orchestrate the entire RAG process.</p>
<h3 class="heading-3" id="_idParaDest-110"><a id="_idTextAnchor208"/>Agentic RAG</h3>
<p class="normal">Agentic RAG employs AI agents—autonomous <a id="_idIndexMarker397"/>systems capable of planning, reasoning, and <a id="_idIndexMarker398"/>decision-making—to dynamically manage information retrieval and generation. Unlike traditional RAG or even CRAG, which follow relatively structured workflows, agentic RAG uses agents to:</p>
<ul>
<li class="b lletList">Analyze queries and decompose complex questions into manageable sub-questions</li>
<li class="b lletList">Plan information-gathering strategies based on the specific task requirements</li>
<li class="b lletList">Select appropriate tools (retrievers, web search, calculators, APIs, etc.)</li>
<li class="b lletList">Execute multi-step processes, potentially involving multiple rounds of retrieval and reasoning</li>
<li class="b lletList">Reflect on intermediate results and adapt strategies accordingly</li>
</ul>
<p class="normal">The key distinction between CRAG and agentic RAG lies in their focus: CRAG primarily enhances data quality through evaluation and correction, while agentic RAG focuses on process intelligence through autonomous planning and orchestration.</p>
<p class="normal">Agentic RAG is particularly valuable for complex use cases that require:</p>
<ul>
<li class="b lletList">Multi-step reasoning across multiple information sources</li>
<li class="b lletList">Dynamic tool selection based on query analysis</li>
<li class="b lletList">Persistent task execution with intermediate reflection</li>
<li class="b lletList">Integration with various external systems and APIs</li>
</ul>
<p class="normal">However, agentic RAG introduces significant complexity in implementation, potentially higher latency due to multiple reasoning steps, and increased computational costs from multiple LLM calls for planning and reflection.</p>
<p class="normal">In <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, we’ll explore the implementation of agent-based systems in depth, including patterns that can be <a id="_idIndexMarker399"/>applied to create agentic RAG systems. The core techniques—tool integration, planning, reflection, and orchestration—are fundamental to both general agent systems and agentic RAG specifically.</p>
<p class="normal">By understanding both CRAG <a id="_idIndexMarker400"/>and agentic RAG approaches, you’ll be equipped to select the most appropriate RAG architecture based on your specific requirements, balancing<a id="_idTextAnchor209"/> accuracy, flexibility, complexity, and performance.</p>
<div><h3 class="heading-3" id="_idParaDest-111"><a id="_idTextAnchor210"/>Choosing the right techniques</h3>
<p class="normal">When implementing <a id="_idIndexMarker401"/>advanced RAG techniques, consider the specific requirements and constraints of your application. To guide your decision-making process, the following table provides a comprehensive comparison of RAG approaches discussed throughout this chapter:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table005">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">RAG Approach</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Chapter Section</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Core Mechanism</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Key Strengths</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Key Weaknesses</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Primary Use Cases</strong></p>
</td>
<td class="No-Table-Style">
<p class="normal"><strong class="keyWord">Relative Complexity</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Naive RAG</p>
</td>
<td class="No-Table-Style">
<p class="normal">Breaking down the RAG pipeline</p>
</td>
<td class="No-Table-Style">
<p class="normal">Basic index <img alt="" src="img/Icon.png"/> retrieve <img alt="" src="img/Icon.png"/> generate workflow with single retrieval step</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Simple implementation</li>
<li class="bulletList"> Low initial resource usage</li>
<li class="bulletList">Straightforward debugging</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Limited retrieval quality</li>
<li class="bulletList">Vulnerability to hallucinations</li>
<li class="bulletList">No handling of retrieval failures</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Simple Q&amp;A systems</li>
<li class="bulletList">Basic document lookup</li>
<li class="bulletList">Prototyping</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">Low</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Hybrid Retrieval</p>
</td>
<td class="No-Table-Style">
<p class="normal">Advanced RAG techniques – hybrid retrieval</p>
</td>
<td class="No-Table-Style">
<p class="normal">Combines sparse (BM25) and dense (vector) retrieval methods</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Balances keyword precision with semantic understanding</li>
<li class="bulletList">Handles vocabulary mismatch</li>
<li class="bulletList">Improves recall without sacrificing precision</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Increased system complexity</li>
<li class="bulletList">Challenge in optimizing fusion weights</li>
<li class="bulletList">Higher computational overhead</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Technical documentation</li>
<li class="bulletList">Content with specialized terminology</li>
<li class="bulletList">Multi-domain knowledge bases</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">Medium</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Re-ranking</p>
</td>
<td class="No-Table-Style">
<p class="normal">Advanced RAG techniques – re-ranking</p>
</td>
<td class="No-Table-Style">
<p class="normal">Post-processes initial retrieval results with more sophisticated relevance models</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Improves result ordering</li>
<li class="bulletList">Captures nuanced relevance signals</li>
<li class="bulletList">Can be applied to any retrieval method</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Additional computation layer</li>
<li class="bulletList">May create bottlenecks for large result sets</li>
<li class="bulletList">Requires training or configuring re-rankers</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">When retrieval quality is critical</li>
<li class="bulletList">For handling ambiguous queries</li>
<li class="bulletList">High-value information needs</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">Medium</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Query Transformation (HyDE)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Advanced RAG techniques – query transformation</p>
</td>
<td class="No-Table-Style">
<p class="normal">Generates hypothetical document from query for improved retrieval</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Bridges query-document semantic gap</li>
<li class="bulletList">Improves retrieval for complex queries</li>
<li class="bulletList">Handles implicit information needs</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Additional LLM generation step</li>
<li class="bulletList">Depends on hypothetical document quality</li>
<li class="bulletList">Potential for query drift</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Complex or ambiguous queries</li>
<li class="bulletList">Users with unclear information needs</li>
<li class="bulletList">Domain-specific search</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">Medium</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<div><p class="normal">Context Processing</p>
</td>
<td class="No-Table-Style">
<p class="normal">Advanced RAG techniques - context processing</p>
</td>
<td class="No-Table-Style">
<p class="normal">Optimizes retrieved documents before sending to the generator (compression, MMR)</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Maximizes context window utilization</li>
<li class="bulletList">Reduces redundancy Focuses on most relevant information</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Risk of removing important context</li>
<li class="bulletList">Processing adds latency</li>
<li class="bulletList">May lose document coherence</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Large documents</li>
<li class="bulletList">When context window is limited</li>
<li class="bulletList">Redundant information sources</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">Medium</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Response Enhancement</p>
</td>
<td class="No-Table-Style">
<p class="normal">Advanced RAG techniques – response enhancement</p>
</td>
<td class="No-Table-Style">
<p class="normal">Improves generated output with source attribution and consistency checking</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Increases output trustworthiness</li>
<li class="bulletList">Provides verification mechanisms</li>
<li class="bulletList">Enhances user confidence</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">May reduce fluency or conciseness</li>
<li class="bulletList">Additional post-processing overhead</li>
<li class="bulletList">Complex implementation logic</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Educational or research content</li>
<li class="bulletList">Legal or medical information</li>
<li class="bulletList">When attribution is required</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">Medium-High</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Corrective RAG (CRAG)</p>
</td>
<td class="No-Table-Style">
<p class="normal">Advanced RAG techniques – corrective RAG</p>
</td>
<td class="No-Table-Style">
<p class="normal">Evaluates retrieved documents and takes corrective actions (filtering, web search)</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Explicitly handles poor retrieval results</li>
<li class="bulletList">Improves robustness</li>
<li class="bulletList">Can dynamically supplement knowledge</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Increased latency from evaluation</li>
<li class="bulletList">Depends on evaluator accuracy</li>
<li class="bulletList">More complex conditional logic</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">High-reliability requirements</li>
<li class="bulletList">Systems needing factual accuracy</li>
<li class="bulletList">Applications with potential knowledge gaps</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">High</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p class="normal">Agentic RAG</p>
</td>
<td class="No-Table-Style">
<p class="normal">Advanced RAG techniques – agentic RAG</p>
</td>
<td class="No-Table-Style">
<p class="normal">Uses autonomous AI agents to orchestrate information gathering and synthesis</p>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Highly adaptable to complex tasks</li>
<li class="bulletList">Can use diverse tools beyond retrieval</li>
<li class="bulletList">Multi-step reasoning capabilities</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Significant implementation complexity</li>
<li class="bulletList">Higher cost and latency</li>
<li class="bulletList">Challenging to debug and control</li>
</ul>
</td>
<td class="No-Table-Style">
<ul>
<li class="bulletList">Complex multi-step information tasks</li>
<li class="bulletList">Research applications</li>
<li class="bulletList">Systems integrating multiple data sources</li>
</ul>
</td>
<td class="No-Table-Style">
<p class="normal">Very High</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 4.5: Comparing RAG techniques</p>
<div><p class="normal">For technical or specialized domains with complex terminology, hybrid retrieval provides a strong foundation by capturing both semantic relationships and exact terminology. When dealing with lengthy documents where only portions are relevant, add contextual compression to extract the most pertinent sections.</p>
<p class="normal">For applications where accuracy and transparency are critical, implement source attribution and self-consistency <a id="_idIndexMarker402"/>checking to ensure that generated responses are faithful to the retrieved information. If users frequently submit ambiguous or poorly formulated queries, query transformation techniques can help bridge the gap between user language and document terminology.</p>
<p class="normal">So when should you choose each approach?</p>
<ul>
<li class="b lletList">Start with naive RAG for quick prototyping and simple question-answering</li>
<li class="b lletList">Add hybrid retrieval when facing vocabulary mismatch issues or mixed content types</li>
<li class="b lletList">Implement re-ranking when the initial retrieval quality needs refinement</li>
<li class="b lletList">Use query transformation for complex queries or when users struggle to articulate information needs</li>
<li class="b lletList">Apply context processing when dealing with limited context windows or redundant information</li>
<li class="b lletList">Add response enhancement for applications requiring high trustworthiness and attribution</li>
<li class="b lletList">Consider CRAG when reliability and factual accuracy are mission-critical</li>
</ul>
<div><div><p class="normal">Explore agentic RAG (covered more in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>) for complex, multi-step information tasks requiring reasoning</p>
</div>
</div>
<p class="normal">In practice, production RAG systems often combine multiple approaches. For example, a robust enterprise system might use hybrid retrieval with query transformation, apply context processing to optimize the retrieved information, enhance responses with source attribution, and implement CRAG’s evaluation layer for critical applications.</p>
<p class="normal">Start with implementing one or two key techniques that address your most pressing challenges, then measure their impact on performance metrics like relevance, accuracy, and user satisfaction. Add additional techniques incrementally as needed, always considering the tradeoff between improved results and increased computational costs.</p>
<p class="normal">To demonstrate a RAG<a id="_idIndexMarker403"/> system in practice, in the next section, we’ll walk through the implementation of a chatbot that retrieves and integrates external knowledge into responses.</p>
<div><h1 class="heading-1" id="_idParaDest-112"><a id="_idTextAnchor211"/>Developing a corporate documentation chatbot</h1>
<p class="normal">In this section, we will build a corporate<a id="_idIndexMarker404"/> documentation chatbot that leverages LangChain for LLM interactions and LangGraph for state management and workflow orchestration. LangGraph complements the implementation in several critical ways:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Explicit state management</strong>: Unlike basic RAG pipelines that operate as linear sequences, LangGraph maintains a formal state object containing all relevant information (queries, retrieved documents, intermediate results, etc.).</li>
<li class="b lletList"><strong class="keyWord">Conditional processing</strong>: LangGraph enables conditional branching based on the quality of retrieved documents or other<a id="_idIndexMarker405"/> evaluation criteria—essential for ensuring reliable output.</li>
<li class="b lletList"><strong class="keyWord">Multi-step reasoning</strong>: For complex documentation tasks, LangGraph allows breaking the process into discrete steps (retrieval, generation, validation, refinement) while maintaining context throughout.</li>
<li class="b lletList"><strong class="keyWord">Human-in-the-loop integration</strong>: When document quality or compliance cannot be automatically verified, LangGraph facilitates seamless integration of human feedback.</li>
</ul>
<p class="normal">With the <strong class="keyWord">Corporate Documentation Manager</strong> tool we <a id="_idIndexMarker406"/>built, you can generate, validate, and refine project documentation while incorporating human feedback to ensure compliance with corporate standards. In many organizations, maintaining up-to-date project documentation is critical. Our pipeline leverages LLMs to:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Generate documentation</strong>: Produce detailed project documentation from a user’s prompt</li>
<li class="b lletList"><strong class="keyWord">Conduct compliance checks</strong>: Analyze the generated document for adherence to corporate standards and best practices</li>
<li class="b lletList"><strong class="keyWord">Handle human feedback</strong>: Solicit expert feedback if compliance issues are detected</li>
<li class="b lletList"><strong class="keyWord">Finalize documentation</strong>: Revise the document based on feedback to ensure it is both accurate and compliant</li>
</ul>
<p class="normal">The idea is that this process not only streamlines documentation creation but also introduces a safety net by involving human-in-the-loop validation. The code is split into several modules, each handling a specific part of the pipeline, and a Streamlit app ties everything together for a web-based interface.</p>
<p class="normal">The code will demonstrate the following key features:</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Modular pipeline design</strong>: Defines a clear state and uses nodes for documentation generation, compliance analysis, human feedback, and finalization</li>
<li class="b lletList"><strong class="keyWord">Interactive interface</strong>: Integrates the pipeline with Gradio for real-time user interactions</li>
</ul>
<div><div><p class="normal">While this chapter provides a brief overview of performance measurements and evaluation metrics, an in-depth discussion of performance and observability will be covered in <a href="E_Chapter_8.xhtml#_idTextAnchor390"><em class="italic">Chapter 8</em></a>. Please make sure you have installed all the dependencies needed for this book, as explained in <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a>. Otherwise, you might run into issues.</p>
<p class="normal">Additionally, given the pace of the field and the development of the LangChain library, we are making an effort to keep the GitHub repository up to date. Please see <a href="https://github.com/benman1/generative_ai_with_langchain">https://github.com/benman1/generative_ai_with_langchain</a>.</p>
<p class="normal">For any questions, or if you have any trouble running the code, please create an issue on GitHub or join the discussion on Discord: <a href="https://packt.link/lang">https://packt.link/lang</a>.</p>
</div>
</div>
<p class="normal">Let’s get started! Each file in the project serves a specific role in the overall documentation chatbot. Let’s first look at <a id="_idIndexMarker407"/>document loa<a id="_idTextAnchor212"/>ding.</p>
<h2 class="heading-2" id="_idParaDest-113"><a id="_idTextAnchor213"/>Document loading</h2>
<p class="normal">The main purpose of this <a id="_idIndexMarker408"/>module is to give an interface to read different document formats.</p>
<div><div><p class="normal">The <code class="inlineCode">Document</code> class in LangChain is a fundamental data structure for storing and manipulating text content along with associated metadata. It stores text content through its required <code class="inlineCode">page_content</code> parameter along with optional metadata stored as a dictionary.</p>
<p class="normal">The class also supports an optional <code class="inlineCode">id</code> parameter that ideally should be formatted as a UUID to uniquely identify documents across collections, though this isn’t strictly enforced. Documents can be created by simply passing content and metadata, as in this example:</p>
<pre>Document(page_content="Hello, world!", metadata={"source": "https://example.com"})</pre>
<p class="normal">This interface serves as the standard representation of text data throughout LangChain’s document processing pipelines, enabling consistent handling during loading, splitting, transformation, and retrieval operations. </p>
</div>
</div>
<div><p class="normal">This module is responsible for loading documents in various formats. It defines:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Custom Loader classes</strong>: The <code class="inlineCode">EpubReader</code> class inherits from <code class="inlineCode">UnstructuredEPubLoader</code> and configures it to work in “fast” mode using element extraction, optimizing it for EPUB document processing.</li>
<li class="b lletList"><strong class="keyWord">DocumentLoader class</strong>: A central class that manages document loading across different file formats by maintaining a mapping between file extensions and their appropriate loader classes.</li>
<li class="b lletList"><strong class="keyWord">load_document function</strong>: A utility function that accepts a file path, determines its extension, instantiates the appropriate loader class from the <code class="inlineCode">DocumentLoader</code>'s mapping, and returns the loaded content as a list of <code class="inlineCode">Document</code> objects.</li>
</ul>
<p class="normal">Let’s get the imports out of the way:</p>
<pre>import logging
import os
import pathlib
import tempfile
from typing import Any
from langchain_community.document_loaders.epub import UnstructuredEPubLoader
from langchain_community.document_loaders.pdf import PyPDFLoader
from langchain_community.document_loaders.text import TextLoader
from langchain_community.document_loaders.word_document import (
 UnstructuredWordDocumentLoader
)
from langchain_core.documents import Document
from streamlit.logger import get_logger
logging.basicConfig(encoding="utf-8", level=logging.INFO)
LOGGER = get_logger(__name__)</pre>
<p class="normal">This module first defines a custom class, <code class="inlineCode">EpubReader</code>, that inherits from <code class="inlineCode">UnstructuredEPubLoader</code>. This class is responsible for loading documents with supported extensions. The <code class="inlineCode">supported_extentions</code> dictionary maps file extensions to their corresponding document loader <a id="_idIndexMarker409"/>classes. This gives us interfaces to read PDF, text, EPUB, and Word documents with different extensions.</p>
<div><p class="normal">The <code class="inlineCode">EpubReader</code> class inherits from an EPUB loader and configures it to work in <code class="inlineCode">"fast"</code> mode using element extraction:</p>
<pre>class EpubReader(UnstructuredEPubLoader):
 def __init__(self, file_path: str | list[str], **unstructured_kwargs: Any):
 super().__init__(file_path, **unstructured_kwargs, mode="elements", strategy="fast")
class DocumentLoaderException(Exception):
 pass
class DocumentLoader(object):
 """Loads in a document with a supported extension."""
    supported_extensions = {
 ".pdf": PyPDFLoader,
 ".txt": TextLoader,
 ".epub": EpubReader,
 ".docx": UnstructuredWordDocumentLoader,
 ".doc": UnstructuredWordDocumentLoader,
    }</pre>
<p class="normal">Our <code class="inlineCode">DocumentLoader</code> maintains a mapping (<code class="inlineCode">supported_extensions</code>) of file extensions (for example, .pdf, .txt, .epub, .docx, .doc) to their respective loader classes. But we’ll also need one more function:</p>
<pre>def load_document(temp_filepath: str) -&gt; list[Document]:
 """Load a file and return it as a list of documents."""
    ext = pathlib.Path(temp_filepath).suffix
    loader = DocumentLoader.supported_extensions.get(ext)
 if not loader:
 raise DocumentLoaderException(
 f"Invalid extension type {ext}, cannot load this type of file"
        )
    loaded = loader(temp_filepath)
    docs = loaded.load()</pre>
<div><pre>    logging.info(docs)
 return docs</pre>
<p class="normal">The <code class="inlineCode">load_document</code> function defined above takes a file path, determines its extension, selects the appropriate loader from the <code class="inlineCode">supported_extensions</code> dictionary, and returns a list of <code class="inlineCode">Document</code> objects. If the file <a id="_idIndexMarker410"/>extension isn’t supported, it raises a <code class="inlineCode">DocumentLoaderException</code> to alert the user that the file type cannot be pr<a id="_idTextAnchor214"/>ocessed.</p>
<h2 class="heading-2" id="_idParaDest-114"><a id="_idTextAnchor215"/>Language model setup</h2>
<p class="normal">The <code class="inlineCode">llms.py</code> module sets<a id="_idIndexMarker411"/> up the LLM and embeddings for the application. First, the imports and loading the API keys as environment variables – please see <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a> for details if you skipped that part.</p>
<pre>from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_groq import ChatGroq
from langchain_openai import OpenAIEmbeddings
from config import set_environment
set_environment()</pre>
<p class="normal">Let’s initialize the LangChain <code class="inlineCode">ChatGroq</code> interface using the API key from environment variables:</p>
<pre>chat_model = ChatGroq(
 model="deepseek-r1-distill-llama-70b",
 temperature=0,
 max_tokens=None,
 timeout=None,
 max_retries=2,
)</pre>
<p class="normal">This uses <code class="inlineCode">ChatGroq</code> (configured with a specific model, temperature, and retries) for generating documentation drafts and revisions. The configured model is the DeepSeek 70B R1 model.</p>
<p class="normal">We’ll then use <code class="inlineCode">OpenAIEmbeddings</code> to convert text into vector representations:</p>
<pre>store = LocalFileStore("./cache/")
underlying_embeddings = OpenAIEmbeddings(</pre>
<div><pre> model="text-embedding-3-large",
)
# Avoiding unnecessary costs by caching the embeddings.
EMBEDDINGS = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings, store, namespace=underlying_embeddings.model
)</pre>
<p class="normal">To reduce API costs and <a id="_idIndexMarker412"/>speed up repeated queries, it wraps the embeddings with a caching mechanism (<code class="inlineCode">CacheBackedEmbeddings</code>) that stores vectors locally in a file-based store (<code class="inlineCode">LocalF<a id="_idTextAnchor216"/>ileStore</code>).</p>
<h2 class="heading-2" id="_idParaDest-115"><a id="_idTextAnchor217"/>Document retrieval</h2>
<p class="normal">The <code class="inlineCode">rag.py</code> module implements <a id="_idIndexMarker413"/>document retrieval based on semantic similarity. We have these main components:</p>
<ul>
<li class="b lletList">Text splitting</li>
<li class="b lletList">In-memory vector store</li>
<li class="b lletList"><code class="inlineCode">DocumentRetriever</code> class</li>
</ul>
<p class="normal">Let’s start with the imports again:</p>
<pre>import os
import tempfile
from typing import List, Any
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from chapter4.document_loader import load_document
from chapter4.llms import EMBEDDINGS</pre>
<p class="normal">We need to set up a vector store for the retriever to use:</p>
<pre>VECTOR_STORE = InMemoryVectorStore(embedding=EMBEDDINGS)</pre>
<p class="normal">The document chunks are stored in an <code class="inlineCode">InMemoryVectorStore</code> using the cached embeddings, allowing for fast similarity searches. The module uses <code class="inlineCode">RecursiveCharacterTextSplitter</code> to break documents into smaller chunks, which makes them more manageable for retrieval:</p>
<div><pre>def split_documents(docs: List[Document]) -&gt; list[Document]:
 """Split each document."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500, chunk_overlap=200
    )
 return text_splitter.split_documents(docs)</pre>
<p class="normal">This custom retriever inherits <a id="_idIndexMarker414"/>from a base retriever and manages an internal list of documents:</p>
<pre>class DocumentRetriever(BaseRetriever):
 """A retriever that contains the top k documents that contain the user query."""
    documents: List[Document] = []
    k: int = 5
 def model_post_init(self, ctx: Any) -&gt; None:
 self.store_documents(self.documents)
    @staticmethod
 def store_documents(docs: List[Document]) -&gt; None:
 """Add documents to the vector store."""
        splits = split_documents(docs)
        VECTOR_STORE.add_documents(splits)
 def add_uploaded_docs(self, uploaded_files):
 """Add uploaded documents."""
        docs = []
        temp_dir = tempfile.TemporaryDirectory()
 for file in uploaded_files:
            temp_filepath = os.path.join(temp_dir.name, file.name)
 with open(temp_filepath, "wb") as f:
                f.write(file.getvalue())
                docs.extend(load_document(temp_filepath))
 self.documents.extend(docs)
 self.store_documents(docs)
 def _get_relevant_documents(
            self, query: str, *, run_manager: CallbackManagerForRetrieverRun</pre>
<div><pre> ) -&gt; List[Document]:
 """Sync implementations for retriever."""
 if len(self.documents) == 0:
 return []
 return VECTOR_STORE.similarity_search(query="", k=self.k)</pre>
<p class="normal">There are a few methods that we should explain:</p>
<ul>
<li class="b lletList"><code class="inlineCode">store_documents()</code> splits the documents and adds them to the vector store.</li>
<li class="b lletList"><code class="inlineCode">add_uploaded_docs()</code> processes files uploaded by the user, stores them temporarily, loads them as documents, and adds them to the vector store.</li>
<li class="b lletList"><code class="inlineCode">_get_relevant_documents()</code> returns the top k documents related to a given query from the vector <a id="_idIndexMarker415"/>store. This is the similarity search th<a id="_idTextAnchor218"/>at we’ll use.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-116"><a id="_idTextAnchor219"/>Designing the state graph</h2>
<p class="normal">The <code class="inlineCode">rag.py</code> module implements the RAG pipeline that ties together document retrieval with LLM-based generation:</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">System prompt</strong>: A template <a id="_idIndexMarker416"/>prompt instructs the AI on how to use the provided document snippets when generating a response. This prompt sets the context and provides guidance on how to utilize the retrieved information.</li>
<li class="b lletList"><strong class="keyWord">State definition</strong>: A <code class="inlineCode">TypedDict</code> class defines the structure of our graph’s state, tracking key information like the user’s question, retrieved context documents, generated answers, issues reports, and the conversation’s message history. This state object flows through each node in our pipeline and gets updated at each step.</li>
<li class="b lletList"><strong class="keyWord">Pipeline steps</strong>: The module defines several key functions that serve as processing nodes in our graph:<ul><li class="bulletList level-2"><strong class="keyWord">Retrieve function</strong>: Fetches relevant documents based on the user’s query</li>
<li class="bulletList level-2"><strong class="keyWord">generate function</strong>: Creates a draft answer using the retrieved documents and query</li>
<li class="bulletList level-2"><strong class="keyWord">double_check function</strong>: Evaluates the generated content for compliance with corporate standards</li>
<li class="bulletList level-2"><strong class="keyWord">doc_finalizer function</strong>: Either returns the original answer if no issues were found or revises it based on the feedback from the checker</li>
</ul></li>
<li class="b lletList"><strong class="keyWord">Graph compilation</strong>: Uses a state graph (via LangGraph’s <code class="inlineCode">StateGraph</code>) to define the sequence of steps. The pipeline is then compiled into a runnable graph that can process queries through the complete workflow.</li>
</ul>
<p class="normal">Let’s get the imports out of the way:</p>
<pre>from typing import Annotated
from langchain_core.documents import Document
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.checkpoint.memory import MemorySaver
from langgraph.constants import END
from langgraph.graph import START, StateGraph, add_messages
from typing_extensions import List, TypedDict
from chapter4.llms import chat_model
from chapter4.retriever import DocumentRetriever</pre>
<p class="normal">As we mentioned earlier, the system prompt template instructs the AI on how to use the provided document snippets <a id="_idIndexMarker417"/>when generating a response:</p>
<pre>system_prompt = (
 "You're a helpful AI assistant. Given a user question "
 "and some corporate document snippets, write documentation."
 "If none of the documents is relevant to the question, "
 "mention that there's no relevant document, and then "
 "answer the question to the best of your knowledge."
 "\n\nHere are the corporate documents: "
 "{context}"
)</pre>
<p class="normal">We’ll then instantiate a <code class="inlineCode">DocumentRetriever</code> and a <code class="inlineCode">prompt</code>:</p>
<pre>retriever = DocumentRetriever()
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{question}"),
    ]
)</pre>
<div><p class="normal">We then have to define the state of the graph. A <code class="inlineCode">TypedDict</code> state is used to hold the current state of the application (for example, question, context documents, answer, issues report):</p>
<pre>class State(TypedDict):
    question: str
    context: List[Document]
    answer: str
    issues_report: str
    issues_detected: bool
    messages: Annotated[list, add_messages]</pre>
<p class="normal">Each of these fields corresponds to a node in the graph that we’ll define with LangGraph. We have the following processing in the nodes:</p>
<ul>
<li class="b lletList"><code class="inlineCode">retrieve</code> function: Uses the retriever to get relevant documents based on the most recent message</li>
<li class="b lletList"><code class="inlineCode">generate</code> function: Creates a draft answer by combining the retrieved document content with the user question using the chat prompt</li>
<li class="b lletList"><code class="inlineCode">double_check</code> function: Reviews the generated draft for compliance with corporate standards. It checks the draft and sets flags if issues are detected</li>
<li class="b lletList"><code class="inlineCode">doc_finalizer</code> function: If issues are found, it revises the document based on the provided feedback; otherwise, it returns<a id="_idIndexMarker418"/> the original answer</li>
</ul>
<p class="normal">Let’s start with the retrieval:</p>
<pre>def retrieve(state: State):
    retrieved_docs = retriever.invoke(state["messages"][-1].content)
 print(retrieved_docs)
 return {"context": retrieved_docs}
def generate(state: State):
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    messages = prompt.invoke(
        {"question": state["messages"][-1].content, "context": docs_content}
    )
    response = chat_model.invoke(messages)
 print(response.content)
 return {"answer": response.content}</pre>
<div><p class="normal">We’ll also implement a content validation check as a critical quality assurance step in our RAG pipeline. Please note that this is the simplest implementation possible. In a production environment, we could have implemented a human-in-the-loop review process or more sophisticated guardrails. Here, we’re using an LLM to analyze the generated content for any issues:</p>
<pre>def double_check(state: State):
    result = chat_model.invoke(
        [{
 "role": "user",
 "content": (
 f"Review the following project documentation for compliance with our corporate standards. "
 f"Return 'ISSUES FOUND' followed by any issues detected or 'NO ISSUES': {state['answer']}"
            )
        }]
    )
 if "ISSUES FOUND" in result.content:
 print("issues detected")
 return {
 "issues_report": result.split("ISSUES FOUND", 1)[1].strip(),
 "issues_detected": True
        }
 print("no issues detected")
 return {
 "issues_report": "",
 "issues_detected": False
    }</pre>
<p class="normal">The final node integrates any<a id="_idIndexMarker419"/> feedback to produce the finalized, compliant document:</p>
<pre>def doc_finalizer(state: State):
 """Finalize documentation by integrating feedback."""
 if "issues_detected" in state and state["issues_detected"]:
        response = chat_model.invoke(
            messages=[{
 "role": "user",
 "content": (</pre>
<div><pre> f"Revise the following documentation to address these feedback points: {state['issues_report']}\n"
 f"Original Document: {state['answer']}\n"
 f"Always return the full revised document, even if no changes are needed."
                )
            }]
        )
 return {
 "messages": [AIMessage(response.content)]
        }
 return {
 "messages": [AIMessage(state["answer"])]
    }</pre>
<p class="normal">With our nodes defined, we construct the state graph:</p>
<pre>graph_builder = StateGraph(State).add_sequence(
 [retrieve, generate, double_check, doc_finalizer]
)
graph_builder.add_edge(START, "retrieve")
graph_builder.add_edge("doc_finalizer", END)
memory = MemorySaver()
graph = graph_builder.compile(checkpointer=memory)
config = {"configurable": {"thread_id": "abc123"}}
We can visualize this graph from a Jupyter notebook:
from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))</pre>
<div><p class="normal">This is what the sequential flow from document retrieval to generation, validation, and finalization looks like:</p>
<figure class="mediaobject"><img alt="Figure 4.5:  State graph of the corporate documentation pipeline" src="img/B32363_04_05.png"/></figure>
<p class="packt_figref">Figure 4.5:  State graph of the corporate documentation pipeline</p>
<p class="normal">Before building a user interface, it’s important to test our RAG pipeline to ensure it functions correctly. Let’s examine how we can do this programmatically:</p>
<pre>from langchain_core.messages import HumanMessage
input_messages = [HumanMessage("What's the square root of 10?")]
response = graph.invoke({"messages": input_messages}, config=config</pre>
<p class="normal">The execution time varies <a id="_idIndexMarker420"/>depending on the complexity of the query and how extensively the model needs to reason about its response. Each step in our graph may involve API calls to the LLM, which contributes to the overall processing time. Once the pipeline completes, we can extract the final response from the returned object:</p>
<pre>print(response["messages"][-1].content)</pre>
<div><p class="normal">The response object contains the complete state of our workflow, including all intermediate results. By accessing <code class="inlineCode">response["messages"][-1].content</code>, we’re retrieving the content of the last message, which contains the finalized answer generated by our RAG pipeline.</p>
<p class="normal">Now that we’ve confirmed our pipeline works as expected, we can create a user-friendly interface. While there are several Python frameworks available for building interactive interfaces (such as Gradio, Dash, and Taipy), we’ll use Streamlit due to its popularity, simplicity, and strong integration with data science workflows. Let’s explore how to create a comprehensive user interface for our<a id="_idTextAnchor220"/> RAG application!</p>
<h2 class="heading-2" id="_idParaDest-117"><a id="_idTextAnchor221"/>Integrating with Streamlit for a user interface</h2>
<p class="normal">We integrate our pipeline with <a id="_idIndexMarker421"/>Streamlit to enable interactive documentation generation. This interface lets users submit documentation requests and view the process in real time:</p>
<pre>import streamlit as st
from langchain_core.messages import HumanMessage
from chapter4.document_loader import DocumentLoader
from chapter4.rag import graph, config, retriever</pre>
<p class="normal">We’ll configure the Streamlit page with a title and wide layout for better readability:</p>
<pre>st.set_page_config(page_title="Corporate Documentation Manager", layout="wide")</pre>
<p class="normal">We’ll initialize the session state for chat history and file management:</p>
<pre>if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if 'uploaded_files' not in st.session_state:
    st.session_state.uploaded_files = []</pre>
<p class="normal">Every time we reload the app, we display chat messages from the history on the app rerun:</p>
<pre>for message in st.session_state.chat_history:
 print(f"message: {message}")
    with st.chat_message(message["role"]):
        st.markdown(message["content"])</pre>
<div><p class="normal">The retriever processes all uploaded files and embeds them for semantic search:</p>
<pre>docs = retriever.add_uploaded_docs(st.session_state.uploaded_files)</pre>
<div><div><p class="normal">Please remember to avoid repeated calls for the same documents, we’re using a cache.</p>
</div>
</div>
<p class="normal">We need a function next to invoke the graph and return a string:</p>
<pre>def process_message(message):
 """Assistant response."""
    response = graph.invoke({"messages": HumanMessage(message)}, config=config)
 return response["messages"][-1].content</pre>
<p class="normal">This ignores the previous messages. We could change the prompt to provide previous messages to the LLM. We can then show a project description using markdown. Just briefly:</p>
<pre>st.markdown("""
# <img alt="" src="img/Icon3.png"/> Corporate Documentation Manager with Citations
""")</pre>
<p class="normal">Next, we present our UI in<a id="_idIndexMarker422"/> two columns, one for chat and one for file management:</p>
<pre>col1, col2 = st.columns([2, 1])</pre>
<p class="normal">Column 1 looks like this:</p>
<pre>with col1:
 st.subheader("Chat Interface")
    # React to user input
 if user_message := st.chat_input("Enter your message:"):
        # Display user message in chat message container
        with st.chat_message("User"):
 st.markdown(user_message)
        # Add user message to chat history
 st.session_state.chat_history.append({"role": "User", "content": user_message})
        response = process_message(user_message)</pre>
<div><pre>        with st.chat_message("Assistant"):
 st.markdown(response)
        # Add response to chat history
 st.session_state.chat_history.append(
            {"role": "Assistant", "content": response}
        )</pre>
<p class="normal">Column 2 takes the files and gives them to the retriever:</p>
<pre>with col2:
 st.subheader("Document Management")
    # File uploader
    uploaded_files = st.file_uploader(
 "Upload Documents",
 type=list(DocumentLoader.supported_extensions),
        accept_multiple_files=True
    )
 if uploaded_files:
 for file in uploaded_files:
 if file.name not in st.session_state.uploaded_files:
 st.session_state.uploaded_files.append(file)</pre>
<p class="normal">To run our Corporate Documentation Manager application on Linux or macOS, follow these steps:</p>
<ol>
<li class="numberedList" value="1">Open your terminal and change directory to where your project files are. This ensures that the <code class="inlineCode">chapter4/</code> directory is accessible.</li>
<li class="numberedList">Set <code class="inlineCode">PYTHONPATH</code> and run Streamlit. The imports within the project rely on the current directory being in the Python module search path. Therefore, we’ll set <code class="inlineCode">PYTHONPATH</code> when we run Streamlit:<pre>PYTHONPATH=. streamlit run chapter4/streamlit_app.py</pre></li>
</ol>
<p class="normal-one">The preceding command tells Python to look in the current directory for modules, allowing it to find the <code class="inlineCode">chapter4</code> package.</p>
<ol>
<li class="numberedList" value="3">Once the <a id="_idIndexMarker423"/>command runs successfully, Streamlit will start a web server. Open your web browser and navigate to <code class="inlineCode">http://localhost:8501</code> to use the application.</li>
</ol>
<div><div><div><p class="normal"><strong class="keyWord">Troubleshooting tips</strong></p>
<ul>
<li class="bulletList">Please make sure you’ve installed all required packages. You can ensure you have Python installed on your system by using pip or other package managers as explained in <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a>.</li>
<li class="bulletList">If you encounter import errors, verify that you’re in the correct directory and that <code class="inlineCode">PYTHONPATH</code> is set correctly.</li>
</ul>
<p class="normal">By following these steps, you should be able to run the application and use it to generate, check, and finalize corporate docum<a id="_idTextAnchor222"/>entation with ease.</p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-118"><a id="_idTextAnchor223"/>Evaluation and performance considerations</h2>
<p class="normal">In <a href="E_Chapter_3.xhtml#_idTextAnchor107"><em class="italic">Chapter 3</em></a>, we explored implementing RAG with citations in the Corporate Documentation Manager example. To further enhance<a id="_idIndexMarker424"/> reliability, additional mechanisms can be incorporated into the pipeline. One improvement is to integrate a robust retrieval system such as FAISS, Pinecone, or Elasticsearch to fetch real-time sources. This is complemented by scoring mechanisms like precision, recall, and mean reciprocal rank to evaluate retrieval quality. Another enhancement involves assessing answer accuracy by comparing generated responses against ground-truth data or curated references and incorporating human-in-the-loop validation to ensure the outputs are both correct and useful.</p>
<p class="normal">It is also important to implement robust error-handling routines within each node. For example, if a citation retrieval fails, the system might fall back to default sources or note that citations could not be retrieved. Building observability into the pipeline by logging API calls, node execution times, and retrieval performance is essential for scaling up and maintaining reliability in production. Optimizing API use by leveraging local models when possible, caching common queries, and managing memory efficiently when handling large-scale embeddings further supports cost optimization and scalability.</p>
<p class="normal">Evaluating and optimizing our documentation chatbot is vital for ensuring both accuracy and efficiency. Modern benchmarks focus on whether the documentation meets corporate standards and how accurately it addresses the original request. Retrieval quality metrics such as precision, recall, and mean reciprocal rank measure the effectiveness of retrieving relevant content during compliance checks. Comparing the AI-generated documentation against ground-truth or manually curated examples provides a basis for assessing answer accuracy. Performance can be improved by fine-tuning search parameters for faster retrieval, optimizing memory management for large-scale embeddings, and reducing API costs by using local models for inference when applicable.</p>
<div><p class="normal">These strategies build a<a id="_idIndexMarker425"/> more reliable, transparent, and production-ready RAG application that not only generates content but also explains its sources. Further performance and observability strategies will be covered in <a href="E_Chapter_8.xhtml#_idTextAnchor390"><em class="italic">Chapter 8</em></a>.</p>
<p class="normal">Building an effective RAG system means understanding its common failure points and addressing them with quantitative and testing-based strategies. In the next section, we’ll explore the typical failure points and best practices in relation to RAG systems.</p>
<h1 class="heading-1" id="_idParaDest-119"><a id="_idTextAnchor224"/>Troubleshooting RAG systems</h1>
<p class="normal">Barnett and colleagues in their paper <em class="italic">Seven Failure Points When Engineering a Retrieval Augmented Generation System</em> (2024), and Li <a id="_idIndexMarker426"/>and colleagues in their paper <em class="italic">Enhancing Retrieval-Augmented Generation: A Study of Best Practices</em> (2025) emphasize the importance of both robust design and continuous system calibration:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Foundational setup</strong>: Ensure comprehensive and high-quality document collections, clear prompt formulations, and effective retrieval techniques that enhance precision and relevance.</li>
<li class="b lletList"><strong class="keyWord">Continuous calibration</strong>: Regular monitoring, user feedback, and updates to the knowledge base help identify emerging issues during operation.</li>
</ul>
<p class="normal">By implementing these practices early in development, many common RAG failures can be prevented. However, even well-designed systems encounter issues. The following sections explore the seven most common failure points identified by Barnett and colleagues (2024) and provide targeted solutions informed by empirical research.</p>
<p class="normal">A few common failure points and their remedies are as follows:</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Missing content</strong>: Failure occurs when the system lacks relevant documents. Prevent this by validating content during ingestion and adding domain-specific resources. Use explicit signals to indicate when information is unavailable.</li>
<li class="b lletList"><strong class="keyWord">Missed top-ranked documents</strong>: Even with relevant documents available, poor ranking can lead to their exclusion. Improve this with advanced embedding models, hybrid semantic-lexical searches, and sentence-level retrieval.</li>
<li class="b lletList"><strong class="keyWord">Context window limitations</strong>: When key information is spread across documents that exceed the model’s context limit, it may be truncated. Mitigate this by optimizing document chunking and extracting the most relevant sentences.</li>
<li class="b lletList"><strong class="keyWord">Information extraction failure</strong>: Sometimes, the LLM fails to synthesize the available context properly. This can be resolved by refining prompt design—using explicit instructions and contrastive examples enhances extraction accuracy.</li>
<li class="b lletList"><strong class="keyWord">Format compliance issues</strong>: Answers may be correct but delivered in the wrong format (e.g., incorrect table or JSON structure). Enforce structured output with parsers, precise format examples, and post-processing validation.</li>
<li class="b lletList"><strong class="keyWord">Specificity mismatch</strong>: The output may be too general or too detailed. Address this by using query expansion techniques and tailoring prompts based on the user’s expertise level.</li>
<li class="b lletList"><strong class="keyWord">Incomplete information</strong>: Answers might capture only a portion of the necessary details. Increase retrieval diversity (e.g., using maximum marginal relevance) and refine query transformation methods to cover all aspects of the query.</li>
</ul>
<p class="normal">Integrating focused retrieval<a id="_idIndexMarker427"/> methods, such as retrieving documents first and then extracting key sentences, has been shown to improve performance—even bridging some gaps caused by smaller model sizes. Continuous testing and prompt engineering remain essential to maintaining system quality as operati<a id="_idTextAnchor225"/>onal conditions evolve.</p>
<h1 class="heading-1" id="_idParaDest-120"><a id="_idTextAnchor226"/>Summary</h1>
<p class="normal">In this chapter, we explored the key aspects of RAG, including vector storage, document processing, retrieval strategies, and implementation. Following this, we built a comprehensive RAG chatbot that leverages LangChain for LLM interactions and LangGraph for state management and workflow orchestration. This is a prime example of how you can design modular, maintainable, and user-friendly LLM applications that not only generate creative outputs but also incorporate dynamic feedback loops.</p>
<p class="normal">This foundation opens the door to more advanced RAG systems, whether you’re retrieving documents, enhancing context, or tailoring outputs to meet specific user needs. As you continue to develop production-ready LLM applications, consider how these patterns can be adapted and extended to suit your requirements. In <a href="E_Chapter_8.xhtml#_idTextAnchor390"><em class="italic">Chapter 8</em></a>, we’ll be discussing how to benchmark and quantify the performance of RAG systems to ensure performance is up to requirements.</p>
<p class="normal">In the next chapter, we will build on this foundation by introducing intelligent agents that can utilize tools for enhanced interactions. We will cover various tool integration strategies, structured tool output generation, and agent architectures such as ReACT. This will allow us to develop more capable AI systems that can dynamically interact w<a id="_idTextAnchor227"/>ith external resources.</p>
<div><h1 class="heading-1" id="_idParaDest-121"><a id="_idTextAnchor228"/>Questions</h1>
<ol>
<li class="numberedList" value="1">What are the key benefits of using vector embeddings in RAG?</li>
<li class="numberedList">How does MMR improve document retrieval?</li>
<li class="numberedList">Why is chunking necessary for effective document retrieval?</li>
<li class="numberedList">What strategies can be used to mitigate hallucinations in RAG implementations?</li>
<li class="numberedList">How do hybrid search techniques enhance the retrieval process?</li>
<li class="numberedList">What are the key components of a chatbot utilizing RAG principles?</li>
<li class="numberedList">Why is performance evaluation critical in RAG-based systems?</li>
<li class="numberedList">What are the different retrieval methods in RAG systems?</li>
<li class="numberedList">How does contextual compression refine retrieved information before LLM processing?</li>
</ol>
<h1 class="heading-1" id="_idParaDest-122"><a id="_idTextAnchor229"/><a id="_idTextAnchor230"/>Subscribe to our weekly newsletter</h1>
<p class="normal">Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers, and innovators, at <a href="E_Chapter_4.xhtml">https://packt.link/Q5UyU</a>.</p>
<p class="normal"><img alt="" src="img/Newsletter_QRcode1.jpg"/></p>
</div>
</body></html>