- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Training an Entity Linker Model with spaCy
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spaCy训练实体链接器模型
- en: '**Entity linking** is the NLP task that maps textual mentions to unique identifiers
    in external knowledge bases. This chapter explores how to train an entity linking
    model using spaCy and the best practices on how to create good datasets for NLP
    training. We will also learn how to use a custom corpus reader to train a spaCy
    component. With this knowledge, you can customize any of the spaCy components
    to use while training your models.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**实体链接**是将文本提及映射到外部知识库中唯一标识符的NLP任务。本章将探讨如何使用spaCy训练实体链接模型，以及如何创建用于NLP训练的高质量数据集的最佳实践。我们还将学习如何使用自定义语料库读取器来训练spaCy组件。有了这些知识，你可以自定义任何spaCy组件，以便在训练模型时使用。'
- en: 'We will cover the following in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Understanding the concept and importance of entity linking in NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解实体链接在NLP中的概念和重要性
- en: Best practices for creating high-quality datasets for NLP training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于NLP训练的高质量数据集的最佳实践
- en: Training an **EntityLinker** component with spaCy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用spaCy训练**实体链接器**组件
- en: Utilizing a custom corpus reader to train a spaCy component
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用自定义语料库读取器来训练spaCy组件
- en: By the end of this chapter, you will be able to develop NLP models that integrate
    with external knowledge bases, enhancing accuracy and applicability in real-world
    scenarios.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够开发与外部知识库集成的NLP模型，从而提高在现实世界场景中的准确性和适用性。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the data and the code for this chapter can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有数据和代码都可以在[https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)找到。
- en: Understanding the entity linking task
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解实体链接任务
- en: 'Entity linking is the task of identifying the entity mentioned and linking
    it to the corresponding entry in each knowledge base. For example, the **Washington**
    entity can refer to the person George Washington or the US state. With entity
    linking or entity resolution, our goal is to map the entity to the correct real-world
    representation. As spaCy’s documentation says, the **EntityLinker** spaCy architecture
    requires three main components:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实体链接是将提到的实体与其在各个知识库中的对应条目进行关联的任务。例如，**华盛顿**实体可以指代人物乔治·华盛顿或美国的一个州。通过实体链接或实体解析，我们的目标是把实体映射到正确的现实世界表示。正如spaCy的文档所说，spaCy的**实体链接器**架构需要三个主要组件：
- en: A **knowledge base** ( **KB** ) to store the unique identifiers, synonyms, and
    prior probabilities
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于存储唯一标识符、同义词和先验概率的**知识库**（**KB**）
- en: A **candidate generation step** to produce the likely identifiers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于生成可能标识符的**候选生成步骤**
- en: A **machine learning model** to select the most likely ID from the list of candidates
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于从候选列表中选择最可能ID的**机器学习模型**
- en: In KB, each textual mention (alias) is represented as a **Candidate** object
    that may or may not be linked to an entity. A prior probability is assigned to
    each candidate **(alias,** **entity)** pair.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在KB中，每个文本提及（别名）都表示为一个可能或可能未链接到实体的**候选**对象。每个候选（别名，实体）对都分配一个先验概率。
- en: In the spaCy **EntityLinker** architecture, first, we initialize a KB with the
    language shared **Vocab** and the length of the fixed-size entity vectors. Then,
    we need to set the model. The **spacy.EntityLinker.v2** class uses the **spacy.HashEmbedCNN.v2**
    model as the default model architecture, which is spaCy’s standard **tok2vec**
    layer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在spaCy的**实体链接器**架构中，首先，我们使用共享的语言**词汇表**和固定大小实体向量的长度初始化一个KB。然后，我们需要设置模型。`spacy.EntityLinker.v2`类使用`spacy.HashEmbedCNN.v2`模型作为默认的模型架构，这是spaCy的标准**tok2vec**层。
- en: The **spacy.HashEmbedCNN.v2** architecture is defined by a **MultiHashEmbed**
    embedding layer and a **MaxoutWindowEncoder** encoding layer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`spacy.HashEmbedCNN.v2`架构由一个**多哈希嵌入**嵌入层和一个**最大输出窗口编码器**编码层定义。'
- en: The **MultiHashEmbed** embedding layer uses subword features and constructs
    an embedding layer that separately embeds lexical attributes ( **"NORM"** , **"PREFIX"**
    , **"SUFFIX"** , etc.) using hash embedding. The hash embedding is intended to
    solve the problem of embedding size. In the paper *Hash Embeddings for Efficient
    Word Representations* ( [https://arxiv.org/pdf/1709.03933](https://arxiv.org/pdf/1709.03933)
    ), Svenstrup, Hansen, and Winther point out that even for moderately small embedding
    sizes (300 dimensions), if the vocabulary is large (3 million words and phrases
    as in Google Word2Vec), the total number of parameters is close to 1 billion.
    Using hash embedding is memory-efficient because it’s a compact data structure,
    requiring less storage space than bag-of-words representation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**MultiHashEmbed** 嵌入层使用子词特征，并构建一个嵌入层，该层使用哈希嵌入分别嵌入词汇属性（**"NORM"**、**"PREFIX"**、**"SUFFIX"**
    等）。哈希嵌入旨在解决嵌入大小的问题。在论文《用于高效词表示的哈希嵌入》([https://arxiv.org/pdf/1709.03933](https://arxiv.org/pdf/1709.03933)
    )中，Svenstrup、Hansen 和 Winther 指出，即使是中等大小的嵌入尺寸（300 维度），如果词汇量很大（如 Google Word2Vec
    中的 300 万个单词和短语），总参数数接近 10 亿。使用哈希嵌入是内存高效的，因为它是一个紧凑的数据结构，所需的存储空间比词袋表示少。'
- en: The **MaxoutWindowEncoder** encoding layer encodes context using convolutions.
    The two main parameters this layer takes are **window_size** and **depth** . The
    **window_size** parameter sets the number of words to concatenate around each
    token to construct the convolution. The **depth** parameter sets the number of
    convolutional layers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**MaxoutWindowEncoder** 编码层使用卷积来编码上下文。该层接受的两个主要参数是 **window_size** 和 **depth**。**window_size**
    参数设置围绕每个标记连接的单词数量，以构建卷积。**depth** 参数设置卷积层的数量。'
- en: '**spacy.HashEmbedCNN.v2** and all the other spaCy layers are defined using
    the Thinc API. Let’s see the code that defines these layers (for learning purposes
    only since we just need to point to this definition in our **config.cfg** file):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**spacy.HashEmbedCNN.v2** 和所有其他 spaCy 层都是使用 Thinc API 定义的。让我们看看定义这些层的代码（仅用于学习目的，因为我们只需要在
    **config.cfg** 文件中指向这个定义）：'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can see that the layer is using **MultiHashEmbed** to embed the text and
    the **MaxoutWindowEncoder** layer to encode the embeddings providing a final linear
    output for the model. Next, let’s see the code for the **EntityLinker.v2** architecture
    itself:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，该层正在使用 **MultiHashEmbed** 来嵌入文本，并使用 **MaxoutWindowEncoder** 层来编码嵌入，为模型提供最终的线性输出。接下来，让我们看看
    **EntityLinker.v2** 架构本身的代码：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The parameters of the **build_nel_encoder()** method are the **tok2vec** model
    and the output dimension, **nO** , determined by the length of the vectors encoding
    each entity in the KB. When we don’t set **nO** , it’s automatically set when
    **initialize** is called. We will define the **tok2vec** model in the **config.cfg**
    file like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**build_nel_encoder()** 方法的参数是 **tok2vec** 模型和由 KB 中每个实体的编码向量的长度确定的输出维度 **nO**。当我们不设置
    **nO** 时，它在调用 **initialize** 时会自动设置。我们将在 **config.cfg** 文件中定义 **tok2vec** 模型，如下所示：'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now you have an idea of how spaCy and Thinc interact under the hood to create
    the **EntityLinker** model architecture. If you ever need to change the parameters
    or try different models, you have an idea of where to go to change these settings.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了 spaCy 和 Thinc 在底层如何交互以创建 **EntityLinker** 模型架构。如果您需要更改参数或尝试不同的模型，您知道去哪里更改这些设置。
- en: In business settings, we usually start with no datasets available, forcing us
    to create our own datasets. Having good quality datasets is a key component to
    training models with good performance, so it’s important to learn the fundamentals
    for creating good datasets. Let’s talk about this in the next section.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业环境中，我们通常从没有可用的数据集开始，迫使我们创建自己的数据集。拥有高质量的数据集是训练性能良好的模型的关键组成部分，因此学习创建良好数据集的基本知识非常重要。让我们在下一节中讨论这个问题。
- en: Best practices for creating a good NLP corpus
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建良好 NLP 语料库的最佳实践
- en: Due to fine-tuning techniques, fortunately, we don’t need huge amounts of data
    to train models today. However, good datasets are still very important because
    they are critical to guaranteeing and evaluating the performance of our NLP systems.
    For instance, biases and cultural nuances deeply embedded within language can
    inadvertently shape AI outputs if not carefully curated. In the article *How to
    make a racist AI without really trying* ( [https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/](https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)
    ), the researcher Robyn Speer provides a great tutorial on sentiment analysis
    and shows us that we can produce racist AI solutions by simply reusing embeddings
    that were trained on biased data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微调技术的存在，幸运的是，我们今天不需要大量的数据来训练模型。然而，好的数据集仍然非常重要，因为它们对于保证和评估我们的NLP系统的性能至关重要。例如，如果未经仔细策划，语言中深深嵌入的偏见和文化细微差别可能会无意中塑造AI的输出。在文章《如何在不经意间制造出有种族歧视倾向的AI》中（[https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/](https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/)），研究人员罗宾·斯皮尔提供了一个关于情感分析的精彩教程，并展示了我们如何通过简单地重复使用在偏见数据上训练的嵌入来产生有种族歧视倾向的AI解决方案。
- en: 'Usually, the annotation process for NLP tasks consists mainly of the following
    steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，NLP任务的标注过程主要包括以下步骤：
- en: Define the problem or the task. What problem are we trying to solve? This question
    will guide us on how to select samples for labeling, how to label the data consistently,
    and so on.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义问题或任务。我们试图解决什么问题？这个问题将指导我们如何选择用于标注的样本，如何一致性地标注数据，等等。
- en: Identify and prepare a selection of the representative texts as starting material
    for the corpus.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定并准备一组代表性文本，作为语料库的起始材料。
- en: Define an annotation scheme and annotate a fragment of the corpus to determine
    the feasibility of the data to solve the task.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个标注方案，并对语料库的一部分进行标注，以确定数据解决任务的可行性。
- en: Annotate a large portion of the corpus.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标注语料库的大部分内容。
- en: It’s always a good practice to define the annotation instructions to ensure
    the dataset is annotated consistently. This is important because we need this
    consistency so the machine learning algorithms can generalize effectively. Andrew
    Ng tells an example of the importance of consistency in the annotation process
    in the course *Machine Learning in Production* ( [https://www.coursera.org/learn/introduction-to-machine-learning-in-production](https://www.coursera.org/learn/introduction-to-machine-learning-in-production)
    ), where a computer vision model was not achieving the performance they were expecting,
    and after running some error analysis, they found out that it was due to inconsistencies
    in the labeling process. The task was to find flaws in steel sheet images, and
    some annotations labeled a whole area with flaws while others labeled the flaws
    individually.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总是定义标注说明是一个好习惯，以确保数据集的标注一致性。这很重要，因为我们需要这种一致性，以便机器学习算法能够有效地泛化。安德鲁·吴在课程《生产中的机器学习》中讲述了标注过程中一致性的重要性（[https://www.coursera.org/learn/introduction-to-machine-learning-in-production](https://www.coursera.org/learn/introduction-to-machine-learning-in-production)），其中，一个计算机视觉模型没有达到预期的性能，经过一些错误分析后，他们发现是由于标注过程中的不一致性。任务是寻找钢板图像中的缺陷，一些标注将整个有缺陷的区域标注为缺陷，而另一些则单独标注缺陷。
- en: To build good quality NLP datasets, we can borrow some principles from data
    quality dimensions. The first characteristic of a good labeled dataset is **consistency**
    . The data should have a consistent structure and unity in terms of formatting,
    labeling, and categorization.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建高质量的NLP数据集，我们可以从数据质量维度借用一些原则。一个好的标注数据集的第一个特征是**一致性**。数据在格式、标注和分类方面应该具有一致的结构和统一性。
- en: A good dataset should also be **representative** of the target application domain.
    If we’re working on a pipeline to extract information from the business documentation
    of a company, it may not be a good idea to use legal texts to train an embedding
    model for that.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的数据集也应该代表目标应用领域。如果我们正在构建一个从公司业务文档中提取信息的管道，那么使用法律文本来训练嵌入模型可能不是一个好主意。
- en: The third and final characteristic of a good dataset is that it’s **well-documented**
    . We should be explicit and transparent about how we gathered and selected the
    data, what the labeling instructions were, and who annotated the data. This documentation
    is very important because it allows for transparency, reproducibility, and bias
    management.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的数据集的第三个也是最后一个特征是它**有良好的文档记录**。我们应该明确和透明地说明我们如何收集和选择数据，标签说明是什么，以及谁标注了数据。这种文档记录非常重要，因为它允许透明度、可重复性和偏差管理。
- en: In the upcoming section we create a small dataset, we’ve created a small dataset
    with sentences from news websites that mention the entities we want to disambiguate
    (Taylor Swift, Taylor Lautner, and Taylor Fritz). The labeling task was to label
    the **Taylor** mentions in the news sentences to each of the persons mentioned.
    Unfortunately, in real life, we usually don’t deal with easy scenarios like this,
    and these are the moments when following the consistency, representativeness,
    and well-documented principles to build the datasets will become much more important.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们创建了一个小数据集，我们创建了一个包含来自新闻网站提及我们想要消歧的实体（Taylor Swift、Taylor Lautner和Taylor
    Fritz）的句子的数据集。标签任务是标记新闻句子中的**Taylor**提及到每个提及的人。不幸的是，在现实生活中，我们通常不会遇到这样的简单场景，这些是我们遵循一致性、代表性和良好文档原则来构建数据集变得更为重要的时刻。
- en: Now that we know how to create a corpus, let’s get back to spaCy and learn how
    to train our **EntityLinker** component.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何创建语料库，让我们回到spaCy，学习如何训练我们的**实体链接器**组件。
- en: Training an EntityLinker component with spaCy
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用spaCy训练实体链接器组件
- en: 'The first step to train the model is to create the KB. We want to create a
    pipeline that will detect whether a reference to *Taylor* means a reference to
    Taylor Swift (singer), Taylor Lautner (actor), or Taylor Fritz (tennis player).
    Each of them has its own page and identifier on Wikidata so we will use Wikidata
    as our KB source. To create the KB, we need to create an instance of the **InMemoryLookupKB**
    class passing the shared **Vocab** object and the size of the embeddings that
    we`ll use to encode the entities. Let’s create our KB:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的第一个步骤是创建知识库。我们想要创建一个管道，用于检测对*Taylor*的引用是指Taylor Swift（歌手）、Taylor Lautner（演员）还是Taylor
    Fritz（网球运动员）。他们每个人在Wikidata上都有自己的页面和标识符，因此我们将使用Wikidata作为我们的知识库来源。要创建知识库，我们需要创建一个**InMemoryLookupKB**类的实例，传递共享的**Vocab**对象和我们将要用来编码实体的嵌入向量的大小。让我们创建我们的知识库：
- en: 'First, we will choose the **Language** object ( **en_core_web_md** ) and add
    a **SpanRuler** component to match all the **taylor** mentions (this will be used
    to create the corpus):'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将选择**语言**对象（**en_core_web_md**）并添加一个**SpanRuler**组件来匹配所有的**taylor**提及（这将用于创建语料库）：
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will save both the KB and the model to disk, so let’s define these files:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把知识库和模型保存到磁盘上，所以让我们定义这些文件：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And finally, we can start the KB creation. First, we instantiate the kb object
    passing **Vocab** and the size of the vectors. The **en_core_web_md** model has
    vectors of 300 dimensions so we set this size for the entity vectors:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以开始创建知识库。首先，我们实例化kb对象，传递**Vocab**和向量的大小。**en_core_web_md**模型有300维的向量，所以我们为此设置实体向量的大小：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have the KB, we can use the **add_entity()** method to add the
    entities and the **add_alias()** method to add the mention (in our case, the mention
    will be **Taylor** ) with the prior probabilities for each entity. Let’s create
    the code to do all of that:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了知识库，我们可以使用**add_entity()**方法添加实体，使用**add_alias()**方法添加提及（在我们的情况下，提及将是**Taylor**）以及每个实体的先验概率。让我们创建完成所有这些的代码：
- en: 'First, we create two dictionaries, one with Wikidata IDs for each of our entities
    and another with their descriptions:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建两个字典，一个包含我们每个实体的Wikidata ID，另一个包含它们的描述：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, it’s time to add the entities to the KB. Each Wikidata QID will have a
    vector representation of the entity’s description. To add the entity, we use the
    **add_entity()** method and, for now, we can set an arbitrary value for the **freq**
    parameter (we will tell spaCy to ignore this frequency on the **config.cfg** file
    we’ll use to train the model):'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候将实体添加到知识库中。每个Wikidata QID将有一个实体描述的向量表示。要添加实体，我们使用**add_entity()**方法，目前我们可以为**freq**参数设置一个任意值（我们将告诉spaCy在训练模型的**config.cfg**文件中忽略这个频率）：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can add the mentions ( **alias** ) to the KB. If the **Taylor Swift**
    entity appears in the text, we have no doubt it’s referring to Taylor Swift the
    singer. Similarly, if the **Taylor Lautner** entity appears in the text, we have
    no doubt it’s referring to the actor Taylor Lautner. We add this information to
    the KB by setting the probabilities of these entities to **1** :'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将提及（**alias**）添加到知识库中。如果文本中出现**Taylor Swift**实体，我们毫无疑问它指的是歌手泰勒·斯威夫特。同样，如果文本中出现**Taylor
    Lautner**实体，我们毫无疑问它指的是演员泰勒·洛特纳。我们将通过将这些实体的概率设置为**1**来添加此信息到知识库中：
- en: '[PRE8]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When the name and the surname of the entities are present, we have no doubt
    about who is who, but what if the text mentions only **Taylor** ? We will set
    the initial probabilities to be equal for all our three entities (30% for each
    since the sum of the probabilities must not exceed 100%):'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当实体的名字和姓氏都存在时，我们毫无疑问知道是谁，但如果文本只提到**Taylor**呢？我们将为所有三个实体设置初始概率相等（每个实体30%，因为概率之和不能超过100%）：
- en: '[PRE9]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Our KB is not all set. Let’s print the entities and the aliases to check whether
    everything is okay:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的KB还没有完全设置好。让我们打印实体和别名来检查是否一切正常：
- en: '[PRE10]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, it’s time to save the KB and the **nlp** model to disk so we can use them
    later:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候将知识库和**nlp**模型保存到磁盘上了，这样我们以后就可以使用它们：
- en: '[PRE11]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With **InMemoryLookupKB** all set, we can now prepare the training data for
    spaCy. The CSV file we’ll work with has columns called **text** (the sentence),
    **person** and **label** (the name of the entity), **ent_start** and **ent_end**
    (the position of the tokens in the sentence), and the Wikidata **QID** . The data
    has 49 sentences and we will use 80% for training (we will split this into **train**
    and **dev** sets) and 20% for testing. We will prepare the data in two steps:
    first, creating the **Doc** objects and then adding them to the **DocBin** train
    and **dev** objects.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在**InMemoryLookupKB**配置完成后，我们现在可以为spaCy准备训练数据。我们将处理的CSV文件包含名为**text**（句子）、**person**和**label**（实体的名称）、**ent_start**和**ent_end**（标记在句子中的位置）以及Wikidata的**QID**这些列。数据包含49个句子，我们将使用80%进行训练（我们将将其分为**train**和**dev**集）和20%进行测试。我们将分两步准备数据：首先，创建**Doc**对象，然后将其添加到**DocBin**的**train**和**dev**对象中。
- en: 'To create the **Doc** objects, we will wrap each sentence in a **Doc** object
    and create **Span** objects using the data from the CSV file. This **Span** object
    has a **kb_id** parameter we will use to set the Wikidata QID of the entity. Let’s
    go ahead and do that:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建**Doc**对象，我们将每个句子包装在一个**Doc**对象中，并使用CSV文件中的数据创建**Span**对象。这个**Span**对象有一个**kb_id**参数，我们将用它来设置实体的Wikidata
    QID。让我们继续这样做：
- en: 'First, we load the CSV file and get 80% of the rows for training and the rest
    for testing:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载CSV文件，并获取80%的行用于训练，其余的用于测试：
- en: '[PRE12]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we instantiate the pipeline we were working with to create the KB, wrap
    the sentences in it to create the **Doc** objects, and create the **Span** objects
    for the entities. We will use two lists, one to store **docs** and the other to
    store **QIDs** :'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们实例化我们之前用于创建知识库的管道，将句子包装在它里面以创建**Doc**对象，并为实体创建**Span**对象。我们将使用两个列表，一个用于存储**docs**，另一个用于存储**QIDs**：
- en: '[PRE13]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will use the **QIDs** list to split the sentences into **train** and **dev**
    **DocBin** objects. To do that, we will get the indexes of each QID, use the first
    eight sentences for **train** , and leave the rest for **dev** . Let’s do that
    in the code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**QIDs**列表将句子分割成**train**和**dev**的**DocBin**对象。为此，我们将获取每个QID的索引，使用前八个句子进行**train**，其余的留给**dev**。让我们在代码中这样做：
- en: 'First, we import the objects and create our empty **DocBin** objects:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入对象并创建我们的空**DocBin**对象：
- en: '[PRE14]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we loop through each entity QID, get the indexes of their sentences, and
    add them to each **DocBin** object:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们遍历每个实体QID，获取它们句子的索引，并将它们添加到每个**DocBin**对象中：
- en: '[PRE15]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And now we can save the **DocBin** files to disk:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将**DocBin**文件保存到磁盘上：
- en: '[PRE16]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With the **train** and **dev** sets ready, we can go ahead and train the model.
    We will use the same configuration file that spaCy’s Nel Emerson tutorial uses
    ( [https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson](https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson)
    ). You can get this file in the Github repository here: [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/tree/main/chapter_10](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/tree/main/chapter_10)
    .'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当准备好**train**和**dev**集后，我们可以继续训练模型。我们将使用spaCy的Nel Emerson教程中使用的相同配置文件（[https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson](https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson)）。您可以在GitHub仓库中获取此文件：[https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/tree/main/chapter_10](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/tree/main/chapter_10)。
- en: If you need to refresh your understanding of spaCy’s training process, you can
    refer to [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) . One new thing we will
    need to do to train the **EntityLinker** component is to use a custom file with
    additional code that should be used for training. We’ll do this because we need
    to create **Example** objects the way **EntityLinker** needs. Let’s talk more
    about this in the next section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要刷新对spaCy训练过程的了解，可以参考[*第6章*](B22441_06.xhtml#_idTextAnchor087)。为了训练**EntityLinker**组件，我们需要做一些新的事情，即使用一个包含用于训练的额外代码的自定义文件。我们将这样做，因为我们需要以**EntityLinker**需要的方式创建**Example**对象。让我们在下一节中更多关于这一点进行讨论。
- en: Training with a custom corpus reader
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义语料库读取器进行训练
- en: 'spaCy’s **Corpus** class manages annotated corpora for data loading during
    training. The default corpus reader ( **spacy.Corpus.v1** ) creates the **Example**
    objects using the **make_doc()** method of the **Language** class. This method
    only tokenizes the text. To train the **EntityLinker** component, it needs to
    have the entities available in the doc. That’s why we will create our own corpus
    reader in a file called **custom_functions.py** . The reader should receive as
    parameters the path to the **DocBin** file and the **nlp** object. Inside the
    method, we will loop through each **Doc** to create the examples. Let’s go ahead
    and create this method:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的**Corpus**类管理用于训练期间数据加载的标注语料库。默认的语料库读取器（**spacy.Corpus.v1**）使用**Language**类的**make_doc()**方法创建**Example**对象。此方法仅对文本进行分词。为了训练**EntityLinker**组件，它需要在文档中具有可用的实体。这就是为什么我们将创建自己的语料库读取器，并将其保存为名为**custom_functions.py**的文件。读取器应接收**DocBin**文件的路径和**nlp**对象作为参数。在方法内部，我们将遍历每个**Doc**以创建示例。让我们继续创建这个方法：
- en: 'First, we disable the **EntityLinker** component of the pipeline and then get
    all the docs from the **DocBin** file:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们禁用管道中的**EntityLinker**组件，然后从**DocBin**文件中获取所有文档：
- en: '[PRE17]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we will create the **Example** objects for each doc. The first parameter
    of **Example** is the text processed by the **nlp** object and the second is the
    **doc** with the annotated entities:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将为每个文档创建**Example**对象。**Example**的第一个参数是经过**nlp**对象处理的文本，第二个参数是带有标注实体的**doc**：
- en: '[PRE18]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To reference this reader in the **config.cfg** file, we need to register it
    using the **@spacy.registry** decorator. Let’s import the libraries we will need
    and register the reader:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在**config.cfg**文件中引用此读取器，我们需要使用**@spacy.registry**装饰器进行注册。让我们导入我们将需要的库并注册读取器：
- en: '[PRE19]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We use the **partial** function so that when the spaCy code uses the reader
    internally, we only need to pass the **nlp** object. Now, we can reference this
    **MyCorpus.v1** reader in the **config.cfg** file like this:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用**partial**函数，这样当spaCy代码在内部使用读取器时，我们只需要传递**nlp**对象。现在，我们可以在**config.cfg**文件中像这样引用这个**MyCorpus.v1**读取器：
- en: '[PRE20]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the full source code of our **custom_functions.py** file:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是我们的**custom_functions.py**文件的完整源代码：
- en: '[PRE21]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To make the reader available when we call the **train** CLI command, we need
    to provide the source code that contains the Python code we’ve created. We do
    this using the **code** parameter. First, we get the **config.cfg** file:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了在调用**train** CLI命令时使读取器可用，我们需要提供包含我们创建的Python代码的源代码。我们使用**code**参数来完成此操作。首先，我们获取**config.cfg**文件：
- en: '[PRE22]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we can run the full command to train our **EntityLinker** pipeline:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以运行完整的命令来训练我们的**EntityLinker**管道：
- en: '[PRE23]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now that we have the trained model, we need to test it. Let’s do that in the
    next section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练好的模型，我们需要对其进行测试。让我们在下一节中这样做。
- en: Testing the entity linking model
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试实体链接模型
- en: 'To test the model, we will load it from the path we saved it on using the **train**
    command and call the **nlp** object on the doc we want to disambiguate the entities.
    The entity linker model adds **kb_id** to the entities and we can use it to see
    which of the *Taylors* the model predicted. Let’s use some of the **df_test**
    sentences to evaluate the model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试模型，我们将使用 **train** 命令从我们保存它的路径加载它，并在我们想要消歧义实体的文档上调用 **nlp** 对象。实体链接器模型将
    **kb_id** 添加到实体中，我们可以用它来查看模型预测了哪个 *Taylors*。让我们使用一些 **df_test** 句子来评估模型：
- en: 'First, we load the model :'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载模型：
- en: '[PRE24]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we process the sentences. The **doc.ents** entities have the **kb_id**
    attribute with the hash of the entity and the **kb_id_** attribute with the plain
    text QID. Let’s process the text:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们处理句子。**doc.ents** 实体具有包含实体哈希值的 **kb_id** 属性，以及包含纯文本 QID 的 **kb_id_** 属性。让我们处理文本：
- en: '[PRE25]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we can show the entities using **displacy** :'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 **displacy** 显示实体：
- en: '[PRE26]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Figure 10* *.1* shows the result. **Q26876** is the Wikidata ID for Taylor
    Swift, so the model was right this time.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*图 10.1* 显示了结果。**Q26876** 是泰勒·斯威夫特的 Wikidata ID，所以这次模型也是正确的。'
- en: '![Figure 10.1 – The Taylor Swift entity disambiguated correctly by the model](img/B22441_10_01.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 模型正确地消歧义了泰勒·斯威夫特的实体](img/B22441_10_01.jpg)'
- en: Figure 10.1 – The Taylor Swift entity disambiguated correctly by the model
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 模型正确地消歧义了泰勒·斯威夫特的实体
- en: 'Let’s test the model with another sentence:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用另一个句子测试模型：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*Figure 10* *.2* shows the result. **Q23359** is the Wikidata ID for Taylor
    Lauter, so the model was also right on this one.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*图 10.2* 显示了结果。**Q23359** 是泰勒·洛特纳的 Wikidata ID，所以模型在这点上也是正确的。'
- en: '![Figure 10.2 – The Taylor Lautner entity disambiguated correctly by the model](img/B22441_10_02.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 模型正确地消歧义了泰勒·洛特纳的实体](img/B22441_10_02.jpg)'
- en: Figure 10.2 – The Taylor Lautner entity disambiguated correctly by the model
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 模型正确地消歧义了泰勒·洛特纳的实体
- en: We’re taking it easy on the model by only evaluating these simple sentences
    because the goal here was only to demonstrate how to obtain the results from the
    model. Training this component was quite a journey; congratulations!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型采取轻松的态度，只评估这些简单的句子，因为这里的目的是仅展示如何从模型中获取结果。训练这个组件是一次相当漫长的旅程；恭喜！
- en: Summary
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to train an **EntityLinker** component using
    spaCy. We saw some implementation details to learn more about the **HashEmbedCNN.v2**
    layer and the **EntityLinker.v2** architecture.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用 spaCy 训练 **EntityLinker** 组件。我们看到了一些实现细节，以了解更多关于 **HashEmbedCNN.v2**
    层和 **EntityLinker.v2** 架构的信息。
- en: We also discussed some characteristics of high-quality datasets for NLP training,
    stressing the importance of consistency, representativeness, and thorough documentation.
    Finally, we saw how to create a custom corpus reader used to train the entity
    linking model. With this knowledge, you can customize any other spaCy component.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了用于 NLP 训练的高质量数据集的一些特征，强调了一致性、代表性和详尽文档的重要性。最后，我们看到了如何创建用于训练实体链接模型的定制语料库读取器。有了这些知识，你可以定制任何其他
    spaCy 组件。
- en: In the next and final chapter, you will learn how to combine spaCy with other
    cool open source libraries to create great NLP applications. See you there!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章和最后一章中，你将学习如何将 spaCy 与其他酷炫的开源库结合使用，以创建出色的 NLP 应用程序。那里见！
