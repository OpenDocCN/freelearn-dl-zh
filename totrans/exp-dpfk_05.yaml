- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extracting Faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will start our hands-on activities with the code. To begin
    with, we’ll cover the process of extracting faces.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting faces is a series of steps in a process that involves many different
    stages, but it’s the first discrete stage in creating a deepfake. This chapter
    will first talk about how to run the face extraction script, then we will go hands-on
    with the code, explaining what each part does.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following key sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting image files from a video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running extract on frame images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting hands-on with the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To proceed, we recommend that you download the code from the GitHub repo at
    [https://github.com/PacktPublishing/Exploring-Deepfakes](https://github.com/PacktPublishing/Exploring-Deepfakes)
    and follow the instructions in the `readme.md` file to install Anaconda and to
    create a virtual environment with all required libraries to run the scripts. The
    repo will also contain any errata or updates that have happened since the book
    was published, so please check there for any updates.
  prefs: []
  type: TYPE_NORMAL
- en: Getting image files from a video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Videos are not designed for frame-by-frame access and can cause problems when
    processed out of order. Accessing a video file is a complicated process and not
    good for a beginner-level chapter like this. For this reason, the first task is
    to convert any videos that you want to extract from into individual frames. The
    best way to do this is to use **FFmpeg**. If you followed the installation instructions
    in the *Technical requirements* section, you will have FFmpeg installed and ready
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin the process.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: When you see code or command examples such as those present here with text inside
    of curly brackets, you should replace that text with the information that is explained
    in the brackets. For example, if it says `cd {Folder with video}` and the video
    is in the `c:\Videos\` folder, then you should enter `cd c:\Videos`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Place the video into a folder, then open an Anaconda prompt and enter the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will fill the `frames` folder with numbered images containing the exported
    frames.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of options you can control with FFmpeg, from the resolution
    of the output image to the number of frames to be skipped. These features are
    beyond the scope of this book but have been covered extensively elsewhere. We
    advise searching for a guide to ffmpeg’s command line options if you’re going
    to do much more than the basics.
  prefs: []
  type: TYPE_NORMAL
- en: Running extract on frame images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the extract process on a video, you can run the extract program from
    the cloned `git` repository folder. To do this, simply run the following in an
    Anaconda prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This will run the face extraction process on each of the images in the folder
    and put the extracted images into the `face_images` folder, which (by default)
    will be inside the folder of frame images. This folder will contain three types
    of files for each face detected, along with a file containing all alignments.
  prefs: []
  type: TYPE_NORMAL
- en: face_alignments.json
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There will just be one of these files. It is a JSON-formatted file containing
    the landmark positions and warp matrix for every face found in the images. This
    file is human-readable like any JSON file and can be read or edited (though it’s
    probably not something that you’d do manually).
  prefs: []
  type: TYPE_NORMAL
- en: face_landmarks_{filename}_{face number}.png
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a copy of the original image with a bounding box drawn around the face
    and five **landmark** points written out. Landmarks are common points on the face.
    We’ll cover the usage of these landmark points later, but the ones we’re most
    interested in are the eyes, nose, and corners of the mouth.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Example of face_landmarks_bryan_0.png](img/B17535_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Example of face_landmarks_bryan_0.png
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: All of the images of people used for practical demonstration in this section
    are of the authors.
  prefs: []
  type: TYPE_NORMAL
- en: face_bbox_{filename}_{face number}.png
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This image represents the original **bounding box** (the smallest box that surrounds
    the detected face) found in the original image. It will be in the full original
    size and angle of the face found in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Example of face_bbox_bryan_0.png](img/B17535_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Example of face_bbox_bryan_0.png
  prefs: []
  type: TYPE_NORMAL
- en: face_aligned_{filename}_{face number}.png
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This image will be a smaller size (the default is 256x256) image of the face.
    This is an **aligned** face image, where the face has been lined up according
    to the landmarks. This image should generally have the face centered in the image
    and lined up vertically. If the face in the bounding box image was crooked or
    angled in the box, it should be straightened in the aligned image. There may also
    be a black cutout where the edge of the original frame cuts off the data.
  prefs: []
  type: TYPE_NORMAL
- en: This image is the most important image and is the one that will be used for
    training the model. It is critical for quality training that the aligned face
    is a good-quality image. This is the data that you’ll want to clean to get a successful
    deepfake.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Example of face_aligned_bryan_0.png](img/B17535_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Example of face_aligned_bryan_0.png
  prefs: []
  type: TYPE_NORMAL
- en: face_mask_{filename}_{face number}.png
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This image matches the size of the aligned image. In fact, it matches the aligned
    image in the crop, rotation, and size. The mask is an AI-predicted outline of
    the face that will be used later to ensure that the face is trained properly and
    help swap the final face back onto the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Example of face_mask_bryan_0.png](img/B17535_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Example of face_mask_bryan_0.png
  prefs: []
  type: TYPE_NORMAL
- en: The mask should line up with the aligned face perfectly, showing where the face
    and edges are. You can see how the mask overlays onto the face in *Figure 5**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Example of mask overlayed on aligned face](img/B17535_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Example of mask overlayed on aligned face
  prefs: []
  type: TYPE_NORMAL
- en: So, now that you know how to run the face detector and all of its output, let’s
    get into the hands-on section and determine exactly what it’s doing.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it’s time to get into the code. We’ll go over exactly what `C5-face_detection.py`
    does and why each option was chosen. There are five main parts to the code: initialization,
    image preparation, face detection, face landmarking/aligning, and masking.'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: Formatting for easy reading in a book requires modifying the spacing in the
    samples. Python, however, is whitespace sensitive and uses spacing as a part of
    the language syntax. This means that copying code from this book will almost definitely
    contain the wrong spacing. For this reason, we highly recommend pulling the code
    from the Git repository for the book at [https://github.com/PacktPublishing/Exploring-Deepfakes](https://github.com/PacktPublishing/Exploring-Deepfakes)
    if you plan on running it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we’re importing all the important libraries and functions that
    we will use. These are all used in the code and will be explained when they’re
    used, but it’s a good idea to get familiar with the imports for any project since
    it lets you understand the functions being used and where they came from.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these imports are from the Python standard library, and if you’re interested
    in reading more about them, you can find their documentation online. We’ll be
    explaining those that aren’t part of the standard library as we come to them,
    including where you can find documentation on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s skip to the end of the code for a moment, where we’ll look at argument
    parsing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we define the options available in the script. It lets you
    change many options without needing to modify the code at all. Most of these will
    be fine with the defaults, but the argument parser included in the Python standard
    libraries gives us an easy-to-use method to make those changes at runtime. The
    documents and guides for the argument parser are part of the standard library,
    so we’ll skip over the basics, except to state that we use `parser.add_argument`
    for each argument that we want, and all the arguments get put into the `opt` variable.
  prefs: []
  type: TYPE_NORMAL
- en: One special thing we’re doing here is to change the `export_path` variable after
    it is defined, replacing the `$path` string in the variable to put the output
    folder into the input folder. If the user overrides the default, there will be
    nothing to replace (or the user could use `$path` to specify a different subfolder
    in the input folder if desired).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Python norms require that this section is at the end of the file to work. However,
    it’s very important to at least look at this section so that you know what it
    is doing before we go through the rest of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we return to the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This section begins with the main function we’re using to do all the actual
    work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we do is make sure the output folder exists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This creates the output folder if it doesn’t already exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we enable **graphics processing unit** (**GPU**) acceleration if it’s
    available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this part, we begin with the `PyTorch` for CUDA availability and save the
    device variable for later use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll initialize the face detector and aligner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code defines the face detector and aligner that we’ll be using; it specifies
    what devices we’re going to use and tells the aligner to use a model trained for
    `2D` face landmarks. These classes come from the `face_alignment` library available
    at [https://github.com/1adrianb/face-alignment](https://github.com/1adrianb/face-alignment).
    Both the detector and the aligner are AI models that have been trained for their
    specific uses. The detector finds any faces in a given image, returning their
    position through the use of a bounding box. The aligner takes those detected faces
    and finds landmarks that we can use to align the face to a known position.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define our masker and prepare it. We load the pre-trained weights
    and set it to use CUDA if NVIDIA support is enabled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the `desired_segments` variable to define which of the masker segments
    we want to use. In our current masker, this gives us the face itself and discards
    background, hair, and clothing so that our model will only need to learn the information
    that we want to swap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we get a list of files in the input folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code first prepares an empty dictionary to store the alignment data. This
    dictionary will store all the data that we want to store and save with the face
    data we’ll use to convert with later.
  prefs: []
  type: TYPE_NORMAL
- en: Next, it will get a list of all files in the folder. This assumes that each
    file is an image that we want to import any faces from. If there are any non-image
    files, it will fail, so it’s important to keep extra files out of the folder.
  prefs: []
  type: TYPE_NORMAL
- en: Next, files in the directory are listed and checked to ensure they’re files
    before being stored in a list.
  prefs: []
  type: TYPE_NORMAL
- en: Image preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The image preparation is the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'This loads the images and gets them ready to be processed by the rest of the
    tools:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is the start of a loop that will go over every file in the directory to
    process them. The `tqdm` library creates a nice readable status bar, including
    predictions for how long the process will take. This one line is enough to get
    the basics, but there are a lot more features that it can provide. You can see
    the full documentation at [https://github.com/tqdm/tqdm#documentation](https://github.com/tqdm/tqdm#documentation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we load the image and convert it to RGB color order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: OpenCV is a library of tools for operating on images. You can find documentation
    for it at [https://docs.opencv.org/](https://docs.opencv.org/). In this code,
    we use it to open images and handle various image tasks, which we’ll explain as
    they come up.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV loads the images in a **blue, green, red** (**BGR**) color order, which
    is unusual, so we must convert it to **red, green, blue** (**RGB**) color order
    for later processing since the other libraries expect the files in that color
    order. If we don’t convert it, all the colors will be off, and many tools will
    provide the wrong results.
  prefs: []
  type: TYPE_NORMAL
- en: Then we get the image’s shape; this will give us the height and width of the
    image, as well as the number of color channels (this will be three since we loaded
    them as color images).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to check whether we need to resize the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you’re running images through AI models, the image’s dimensions can drastically
    change the amount of memory used. This can cause an error if it exceeds the memory
    available for AI to use. In this code, we’re getting a maximum size from the options,
    then finding what adjustment we need to make to resize the image down to the maximum
    size and keep track of that change so that the results from the face detector
    can be restored to work on the original image. This process allows us to use a
    smaller image for the face detection AI while still using the face from the full-sized
    image, giving us the best resolution and details.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now it’s time to detect the faces within the image. This process goes through
    each image to detect any faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we check the resized image for any faces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This runs the face detector to find any faces in the image. The library makes
    this a very easy process. We send the resized image through and get back a list
    of all the faces found as well as their bounding boxes. These boxes are all relative
    to the smaller, resized image to ensure we have enough memory to handle it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll iterate over each face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As input into the loop we use the built-in Python function `enumerate`, which
    gives us a count of each of the faces as it finds them. We use this number later
    to identify the face by number.Then, we break out the bounding box size and divide
    it by the adjustment size. This restores the face bounding box that was detected
    to match the original image instead of the smaller resized image. We store the
    adjusted face detection in variables that we round to integers so we can keep
    it lined up with the individual pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the confidence level of the face detection AI is used to skip any faces
    that fall below the confidence level, which is set up in the options.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The confidence level from the face detection AI varies from `0` (no face) to
    `1` (absolute certainty of a face). Most AI uses the range of `0` to `1` since
    it’s easier for AI to work with a known range. If you are doing AI work and get
    results you don’t expect, you might want to check whether it’s been restricted
    down to the range of `0` to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we make sure the face is big enough to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code finds the face height and width, then uses that to find the overall
    size of the face, and compares it against a minimum face size, skipping any faces
    that are too small. It uses the original size of the frame to filter out faces
    that aren’t the main focus more easily. This is set low but can be useful if you
    have lots of faces in a single image, such as in a crowd scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we write out the bounding box as an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code creates an image containing just the parts of the image that fit inside
    the face bounding box of the face and saves it as a `.png` file into the output
    folder. This lets you see the face that is detected; however, it’s not actually
    needed for any future step, so it can be removed if you don’t want to save this
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Face landmarking/aligning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to detect landmarks of the face and align them to a known position.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will get alignments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the same library we used for face detection, passing the full image
    and the adjusted bounding boxes in order to get landmarks for the face. The library
    returns 68 landmark positions, which are based on specific points on the face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we draw a box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we generate a new copy of the original image in BGR color format so we
    can draw onto the image without damaging our original copy. We then use OpenCV
    to draw a rectangle for the detected face. Its `thickness` is set to `10` pixels
    and drawn in black.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the landmarks we’re going to use for alignment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code, we split the 68 landmarks down to just 5\. We use the average
    of the landmarks around each eye to find the average eye position, and then get
    the tip of the nose and the corners of the mouth and save them into a new array.
    This reduced set of landmarks helps to keep alignment consistent since the 68
    landmarks contain a lot of noisy edge points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will draw the landmarks onto the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we define a set of colors, then use those to draw individual dots for
    each landmark position. We save them as a `.png` image file in the output folder.
    These images are for demonstration and debugging purposes and aren’t ever used
    later, so you can remove them (and this save call) if you don’t need those debug
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the mean face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we define another array; this is based on the average face locations of
    the different landmarks. We use this in the next part to align the image. These
    numbers are based on where we want the face to be but could be any numbers that
    can be reliably detected on the face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we generate the transformation matrix to align the face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a bit complicated to understand, so we’ll go into depth here. We use
    an algorithm to align two point sets created by Shinji Umeyama as implemented
    in the `SciKit-Image` library. This takes two sets of points, one a known set
    (in this case, `MEAN_FACE`, which we defined earlier) and the other an unknown
    set (in this case, the five landmark points from the detected face saved in `limited_landmarks`),
    and aligns them. Next, we multiply the landmarks by the size that we want the
    image to end up being and add a border around the face so that it is centered
    with some extra space around the edges.
  prefs: []
  type: TYPE_NORMAL
- en: The `umeyama` algorithm creates a matrix that we save as `warp_matrix`, which
    encodes the translations necessary to create an aligned face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we add the landmarks and `warp_matrix` to the list of alignment data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we create and write the aligned face image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the code creates a new copy of the original image and then uses the `warpAffine`
    function from the OpenCV library to apply the `warp_matrix` that was generated
    by the `umeyama` algorithm. The matrix includes all the information – translation
    (moving the image side to side or up and down), scaling (resizing to fit), and
    rotation – to align the face with the pre-defined landmarks. Finally, it saves
    that newly aligned image as a file.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: While OpenCV does all image processes in BGR color order, it’s fine to do any
    tasks that don’t depend on the color order, such as this `cv2.warpAffine()` step
    here. If you do ignore the color order, you must be careful since it can get easy
    to forget which color order you are using, leading to complicated bugs where the
    colors are all wrong. In this case, since the next step will be to write the image
    out as a file using `cv2.imwrite()`, we are fine using the BGR color order image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we save the data we’ll need to later reconstruct the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We save the landmarks and the warp matrix into a dictionary, which we’ll later
    save as a JSON file. This information is important to save for later processing
    steps, so we must make sure to save it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’re going to create a mask image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The masker is another AI that has specific requirements for the image that it
    is given. To meet these requirements, we must first process the face image in
    certain ways. The first is that the masker AI expects images that are 512x512
    pixels. Since our aligned faces can be different sizes, we need to make a copy
    of the image that is in the expected 512x512 size.
  prefs: []
  type: TYPE_NORMAL
- en: We then convert it to a `PyTorch` Tensor instead of a `Numpy` array and then
    `unsqueeze` the tensor. This adds an additional dimension since the masker works
    on an array containing one or more images; even though we’re only feeding it one,
    we still need to give it that extra dimension containing our single image to match
    the shape expected.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the masker expects the channels to be in a different order than we have
    them. To do this, we `permute` the array into the correct order. Also, traditionally
    images are stored in the range of `0`-`255`, which allows for `256` variations
    of each separate color, but the masker expects the image colors to be a float
    in the range of 0-1\. We divide the range by `255` to get into the expected range.
  prefs: []
  type: TYPE_NORMAL
- en: Next, if NVIDIA support is enabled, we convert the image into a CUDA variable,
    which converts it into a format that can be used by the GPU as well as handle
    being moved to the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we run the masker AI on the image that we’ve prepared, saving the mask
    output to a new variable. The masker outputs multiple arrays of information, but
    only the first array is useful to us now, so we save only that one and discard
    all the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we process the masker output and save it to an image file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that the result has been returned from the masker, we still need to process
    it to get something useful. First, we use `softmax` to convert the result from
    absolute to relative values. This lets us look at the mask as an array of likelihoods
    that each pixel belongs to a particular class instead of the raw values from the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use `interpolate`, which is a `Pytorch` method, to resize the data
    back to the original face image size. We have to do this because, like the input,
    the output of the masker model is 512x512\. We use `bicubic` because it gives
    the smoothest results, but other options could be chosen instead.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use `sum` and `where` to `255` or `0`. We also use `desired_segments`
    to remove the segments that aren’t useful to us. We’re using `.7` as a threshold
    here, so if we’re 70% sure that a given pixel should be in the mask, we keep it,
    but if it’s below that 70% cutoff, we throw that pixel out.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we move the data back to the CPU (if it was already on the CPU, then nothing
    changes) and convert it to a `Numpy` array.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we save the mask image as a `.png` file so we can use it later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step of the entire extract process is to write the alignment data
    as a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once each image is processed, the last step of the file is to save the file
    containing all the landmark data for later use. Here we use the `json_tricks`
    library, which has some useful functionality for writing out `Numpy` arrays as
    a JSON file. The library handles everything for writing and reading back the JSON
    file as `Numpy` arrays, so we can simply pass the full `dictionary` of arrays
    without manually converting them to lists or other default Python types for the
    Python standard library JSON to handle. For full documentation on this, please
    visit their documentation page at [https://json-tricks.readthedocs.io/en/latest/](https://json-tricks.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’ve extracted all the faces from a folder full of images. We’ve
    run them through multiple AI to get the data we need and formatted all that data
    for later use. This data is now ready for training, which will be covered in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extraction is the first step of the training process. In this chapter, we examined
    the data that we will need in later steps as well as the process of extracting
    the required training data from the source images. We went hands-on with the process,
    using multiple AIs to detect and landmark faces and generate a mask, as well as
    the necessary steps to process and save that data.
  prefs: []
  type: TYPE_NORMAL
- en: The `C5-face_detection.py` file can process a directory of images. So, we covered
    how to convert a video into a directory of images and how to process that directory
    through the script. The script creates all the files you need for training and
    some interesting debug images that let you visualize each of the processes the
    detector uses to process the images. We then looked at the entire process, line
    by line, so that we knew exactly what was going on inside that script, learning
    not just what was being output, but exactly how that output was created.
  prefs: []
  type: TYPE_NORMAL
- en: 'After finishing the detection process, you can go through data cleaning, as
    talked about in [*Chapter 3*](B17535_03.xhtml#_idTextAnchor054), *Mastering Data*,
    to make sure your data is ready for the subject of the next chapter: training.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used pre-existing libraries for face detection, landmarking, and aligning
    landmarks. There are other libraries that offer similar functionality. Not all
    libraries work the same way, and implementing the differences is an extremely
    useful exercise. Try replacing the `face_alignment` library with another library
    for detecting faces, such as [https://github.com/timesler/facenet-pytorch](https://github.com/timesler/facenet-pytorch)
    or [https://github.com/serengil/deepface](https://github.com/serengil/deepface).
    Open source has lots of useful libraries but learning the differences and when
    to use one over another can be difficult, and converting between them can be a
    useful practice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We used `2D` landmarks for alignment in this chapter, but there may be a need
    for `3D` landmarks instead. Try replacing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: and adjust the rest of the process accordingly. You will also need to modify
    `MEAN_FACE` to account for the third dimension.
  prefs: []
  type: TYPE_NORMAL
- en: What other problems do `3D` landmarks include? What do you gain by using them?
  prefs: []
  type: TYPE_NORMAL
- en: 'In deepfakes, we’re most interested in faces, so this process uses techniques
    specific to faces. Imagine what you’d need to do to extract images of different
    objects. Watches, hats, or sunglasses, for example. The repo at [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)
    has a pre-trained model that can detect hundreds of different objects. Try extracting
    a different object instead of faces. Think in particular about how to do an alignment:
    can you utilize edge detection or color patterns to find points to which you can
    align?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Umeyama’s method treats every point that it aligns with equal importance, but
    what happens if you try to align with all 68 landmarks instead of just the `5`?
    What about 2 points? Can you find a better method? A faster method? A more accurate
    one? Try modifying the script to output all 68 landmark points in the `face_landmarks`
    `.png` file so you can visualize the process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
