["```py\npip install opencv-python\n```", "```py\n    pip install diffusers==0.23.0\n    ```", "```py\n    pip install -U diffusers\n    ```", "```py\n    from diffusers import MotionAdapter\n    ```", "```py\n    adapter = MotionAdapter.from_pretrained(\n    ```", "```py\n        \"guoyww/animatediff-motion-adapter-v1-5-2\"\n    ```", "```py\n    )\n    ```", "```py\n    from diffusers import AnimateDiffPipeline\n    ```", "```py\n    pipe = AnimateDiffPipeline.from_pretrained(\n    ```", "```py\n        \"digiplay/majicMIX_realistic_v6\",\n    ```", "```py\n        motion_adapter    = adapterm,\n    ```", "```py\n        safety_checker    = None\n    ```", "```py\n    )\n    ```", "```py\n    from diffusers import EulerAncestralDiscreteScheduler\n    ```", "```py\n    scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\n    ```", "```py\n        model_path,\n    ```", "```py\n        subfolder         = \"scheduler\",\n    ```", "```py\n        clip_sample       = False,\n    ```", "```py\n        timestep_spacing  = \"linspace\",\n    ```", "```py\n        steps_offset      = 1\n    ```", "```py\n    )\n    ```", "```py\n    pipe.scheduler = scheduler\n    ```", "```py\n    pipe.enable_vae_slicing()\n    ```", "```py\n    pipe.enable_model_cpu_offload()\n    ```", "```py\n    import torch\n    ```", "```py\n    from diffusers.utils import export_to_gif, export_to_video\n    ```", "```py\n    prompt = \"\"\"photorealistic, 1girl, dramatic lighting\"\"\"\n    ```", "```py\n    neg_prompt = \"\"\"worst quality, low quality, normal quality, lowres, bad anatomy, bad hands, monochrome, grayscale watermark, moles\"\"\"\n    ```", "```py\n    #pipe.to(\"cuda:0\")\n    ```", "```py\n    output = pipe(\n    ```", "```py\n        prompt = prompt,\n    ```", "```py\n        negative_prompt = neg_prompt,\n    ```", "```py\n        height = 256,\n    ```", "```py\n        width = 256,\n    ```", "```py\n        num_frames = 16,\n    ```", "```py\n        num_inference_steps = 30,\n    ```", "```py\n        guidance_scale= 8.5,\n    ```", "```py\n        generator = torch.Generator(\"cuda\").manual_seed(7)\n    ```", "```py\n    )\n    ```", "```py\n    frames = output.frames[0]\n    ```", "```py\n    torch.cuda.empty_cache()\n    ```", "```py\n    export_to_gif(frames, \"animation_origin_256_wo_lora.gif\")\n    ```", "```py\n    export_to_video(frames, \"animation_origin_256_wo_lora.mp4\")\n    ```", "```py\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", \n    adapter_name=\"zoom-in\")\n```", "```py\nimport torch\nfrom diffusers.utils import export_to_gif, export_to_video\nprompt = \"\"\"\nphotorealistic, 1girl, dramatic lighting\n\"\"\"\nneg_prompt = \"\"\"\nworst quality, low quality, normal quality, lowres, bad anatomy, bad hands\n, monochrome, grayscale watermark, moles\n\"\"\"\npipe.to(\"cuda:0\")\npipe.load_lora_weights(\"guoyww/animatediff-motion-lora-zoom-in\", adapter_name=\"zoom-in\")\noutput = pipe(\n    prompt = prompt,\n    negative_prompt = neg_prompt,\n    height = 256,\n    width = 256,\n    num_frames = 16,\n    num_inference_steps = 40,\n    guidance_scale = 8.5,\n    generator = torch.Generator(\"cuda\").manual_seed(123)\n)\nframes = output.frames[0]\npipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nexport_to_gif(frames, \"animation_origin_256_w_lora_zoom_in.gif\")\nexport_to_video(frames, \"animation_origin_256_w_lora_zoom_in.mp4\")\n```"]