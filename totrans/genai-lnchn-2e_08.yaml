- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software Development and Data Analysis Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explores how natural language—our everyday English or whatever
    language you prefer to interact in with an LLM—has emerged as a powerful interface
    for programming, a paradigm shift that, when taken to its extreme, is called *vibe
    coding*. Instead of learning acquiring new programming languages or frameworks,
    developers can now articulate their intent in natural language, leaving it to
    advanced LLMs and frameworks such as LangChain to translate these ideas into robust,
    production-ready code. Moreover, while traditional programming languages remain
    essential for production systems, LLMs are creating new workflows that complement
    existing practices and potentially increase accessibility This evolution represents
    a significant shift from earlier attempts at code generation and automation.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll specifically discuss LLMs’ place in software development and the state
    of the art of performance, models, and applications. We’ll see how to use LLM
    chains and agents to help in code generation and data analysis, training ML models,
    and extracting predictions. We’ll cover writing code with LLMs, giving examples
    with different models be it on Google’s generative AI services, Hugging Face,
    or Anthropic. After this, we’ll move on to more advanced approaches with agents
    and RAG for documentation or a code repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also be applying LLM agents to data science: we’ll first train a model
    on a dataset, then we’ll analyze and visualize a dataset. Whether you’re a developer,
    a data scientist, or a technical decision-maker, this chapter will equip you with
    a clear understanding of how LLMs are reshaping software development and data
    analysis while maintaining the essential role of conventional programming languages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs in software development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing code with LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying LLM agents for data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs in software development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The relationship between natural language and programming is undergoing a significant
    transformation. Traditional programming languages remain essential in software
    development—C++ and Rust for performance-critical applications, Java and C# for
    enterprise systems, and Python for rapid development, data analysis, and ML workflows.
    However, natural language, particularly English, now serves as a powerful interface
    to streamline software development and data science tasks, complementing rather
    than replacing these specialized programming tools.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced AI assistants let you build software by simply staying “in the vibe”
    of what you want, without ever writing or even picturing a line of code. This
    style of development, known as vibe coding, was popularized by Andrej Karpathy
    in early 2025\. Instead of framing tasks in programming terms or wrestling with
    syntax, you describe desired behaviors, user flows or outcomes in plain conversation.
    The model then orchestrates data structures, logic and integration behind the
    scenes. With vibe coding you don’t debug—you re-vibe. This means, you iterate
    by restating or refining requirements in natural language, and let the assistant
    reshape the system. The result is a pure, intuitive design-first workflow that
    completely abstracts away all coding details.
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as Cursor, Windsurf (formerly Codeium), OpenHands, and Amazon Q Developer
    have emerged to support this development approach, each offering different capabilities
    for AI-assisted coding. In practice, these interfaces are democratizing software
    creation while freeing experienced engineers from repetitive tasks. However, balancing
    speed with code quality and security remains critical, especially for production
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: The software development landscape has long sought to make programming more
    accessible through various abstraction layers. Early efforts included fourth-generation
    languages that aimed to simplify syntax, allowing developers to express logic
    with fewer lines of code. This evolution continued with modern low-code platforms,
    which introduced visual programming with pre-built components to democratize application
    development beyond traditional coding experts. The latest and perhaps most transformative
    evolution features natural language programming through LLMs, which interpret
    human intentions expressed in plain language and translate them into functional
    code.
  prefs: []
  type: TYPE_NORMAL
- en: What makes this current evolution particularly distinctive is its fundamental
    departure from previous approaches. Rather than creating new artificial languages
    for humans to learn, we’re adapting intelligent tools to understand natural human
    communication, significantly lowering the barrier to entry. Unlike traditional
    low-code platforms that often result in proprietary implementations, natural language
    programming generates standard code without vendor lock-in, preserving developer
    freedom and compatibility with existing ecosystems. Perhaps most importantly,
    this approach offers unprecedented flexibility across the spectrum, from simple
    tasks to complex applications, serving both novices seeking quick solutions and
    experienced developers looking to accelerate their workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The future of development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analysts at International Data Corporation (IDC) project that, by 2028, natural
    language will be used to create 70% of new digital solutions (IDC FutureScape,
    *Worldwide Developer and DevOps 2025 Predictions*). However, this doesn’t mean
    traditional programming will disappear; rather, it’s evolving into a two-tier
    system where natural language serves as a high-level interface while traditional
    programming languages handle precise implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: However, this evolution does not spell the end for traditional programming languages.
    While natural language can streamline the design phase and accelerate prototyping,
    the precision and determinism of languages like Python remain essential for building
    reliable, production-ready systems. In other words, rather than replacing code
    entirely, English (or Mandarin, or whichever natural language best suits our cognitive
    process) is augmenting it—acting as a high-level layer that bridges human intent
    with executable logic.
  prefs: []
  type: TYPE_NORMAL
- en: For software developers, data scientists, and technical decision-makers, this
    shift means embracing a hybrid workflow where natural language directives, powered
    by LLMs and frameworks such as LangChain, coexist with conventional code. This
    integrated approach paves the way for faster innovation, personalized software
    solutions, and, ultimately, a more accessible development process.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For production environments, the current evolution manifests in several ways
    that are transforming how development teams operate. Natural language interfaces
    enable faster prototyping and reduce time spent on boilerplate code, while traditional
    programming remains essential for the optimization and implementation of complex
    features. However, recent independent research shows significant limitations in
    current AI coding capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The 2025 OpenAI *SWE-Lancer* benchmark study found that even the top-performing
    model completed only 26.2% of individual engineering tasks drawn from real-world
    freelance projects. The research identified specific challenges including surface-level
    problem-solving, limited context understanding across multiple files, inadequate
    testing, and poor edge case handling.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, many organizations report productivity gains when
    using AI coding assistants in targeted ways. The most effective approach appears
    to be collaboration—using AI to accelerate routine tasks while applying human
    expertise to areas where AI still struggles, such as architectural decisions,
    comprehensive testing, and understanding business requirements in context. As
    the technology matures, the successful integration of natural language and traditional
    programming will likely depend on clearly defining where each excels rather than
    assuming AI can autonomously handle complex software engineering challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code maintenance has evolved through AI-assisted approaches where developers
    use natural language to understand and modify codebases. While GitHub reports
    Copilot users completed specific coding tasks 55% faster in controlled experiments,
    independent field studies show more modest productivity gains ranging from 4–22%,
    depending on context and measurement approach. Similarly, Salesforce reports their
    internal CodeGenie tool contributes to productivity improvements, including automating
    aspects of code review and security scanning. Beyond raw speed improvements, research
    consistently shows AI coding assistants reduce developer cognitive load and improve
    satisfaction, particularly for repetitive tasks. However, studies also highlight
    important limitations: generated code often requires significant human verification
    and rework, with some independent research reporting higher bug rates in AI-assisted
    code. The evidence suggests these tools are valuable assistants that streamline
    development workflows while still requiring human expertise for quality and security
    assurance.'
  prefs: []
  type: TYPE_NORMAL
- en: The field of code debugging has been enhanced as natural language queries help
    developers identify and resolve issues faster by explaining error messages, suggesting
    potential fixes, and providing context for unexpected behavior. AXA’s deployment
    of “AXA Secure GPT,” trained on internal policies and code repositories, has significantly
    reduced routine task turnaround times, allowing development teams to focus on
    more strategic work (AXA, *AXA offers secure Generative AI to employees*).
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to understanding complex systems, developers can use LLMs to generate
    explanations and visualizations of intricate architectures, legacy codebases,
    or third-party dependencies, accelerating onboarding and system comprehension.
    For example, Salesforce’s system landscape diagrams show how their LLM-integrated
    platforms connect across various services, though recent earnings reports indicate
    these AI initiatives have yet to significantly impact their financial results.
  prefs: []
  type: TYPE_NORMAL
- en: System architecture itself is evolving as applications increasingly need to
    be designed with natural language interfaces in mind, both for development and
    potential user interaction. BMW reported implementing a platform that uses generative
    AI to produce real-time insights via chat interfaces, reducing the time from data
    ingestion to actionable recommendations from days to minutes. However, this architectural
    transformation reflects a broader industry pattern where consulting firms have
    become major financial beneficiaries of the generative AI boom. Recent industry
    analysis shows that consulting giants such as Accenture are generating more revenue
    from generative AI services ($3.6 billion in annualized bookings) than most generative
    AI startups combined, raising important questions about value delivery and implementation
    effectiveness that organizations must consider when planning their AI architecture
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: For software developers, data scientists, and decision-makers, this integration
    means faster iteration, lower costs, and a smoother transition from idea to deployment.
    While LLMs help generate boilerplate code and automate routine tasks, human oversight
    remains critical for system architecture, security, and performance. As the case
    studies demonstrate, companies integrating natural language interfaces into development
    and operational pipelines are already realizing tangible business value while
    maintaining necessary human guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of code LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The development of code-specialized LLMs has followed a rapid trajectory since
    their inception, progressing through three distinct phases that have transformed
    software development practices. The first *Foundation phase* (2021 to early 2022)
    introduced the first viable code generation models that proved the concept was
    feasible. This was followed by the *Expansion phase* (late 2022 to early 2023),
    which brought significant improvements in reasoning capabilities and contextual
    understanding. Most recently, the *Diversification phase* (mid-2023 to 2024) has
    seen the emergence of both advanced commercial offerings and increasingly capable
    open-source alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: This evolution has been characterized by parallel development tracks in both
    proprietary and open-source ecosystems. Initially, commercial models dominated
    the landscape, but open-source alternatives have gained substantial momentum more
    recently. Throughout this progression, several key milestones have marked transformative
    shifts in capabilities, opening new possibilities for AI-assisted development
    across different programming languages and tasks. The historical context of this
    evolution provides important insights for understanding implementation approaches
    with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: Evolution of code LLMs (2021–2024)](img/B32363_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Evolution of code LLMs (2021–2024)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.1* illustrates the progression of code-specialized language models
    across commercial (upper track) and open-source (lower track) ecosystems. Key
    milestones are highlighted, showing the transition from early proof-of-concept
    models to increasingly specialized solutions. The timeline spans from early commercial
    models such as Codex to recent advancements such as Google’s Gemini 2.5 Pro (March
    2025) and specialized code models such as Mistral AI’s Codestral series.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, we’ve witnessed an explosion of LLMs fine-tuned specifically
    tailored for coding—commonly known as code LLMs. These models are rapidly evolving,
    each with its own set of strengths and limitations, and are reshaping the software
    development landscape. They offer the promise of accelerating development workflows
    across a broad spectrum of software engineering tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code generation:** Transforming natural language requirements into code snippets
    or full functions. For instance, developers can generate boilerplate code or entire
    modules based on project specifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test generation:** Creating unit tests from descriptions of expected behavior
    to improve code reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code documentation**: Automatically generating docstrings, comments, and
    technical documentation from existing code or specifications. This significantly
    reduces the documentation burden that often gets deprioritized in fast-paced development
    environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code editing and refactoring:** Automatically suggesting improvements, fixing
    bugs, and restructuring code for maintainability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code translation:** Converting code between different programming languages
    or frameworks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Debugging and automated program repair**: Identifying bugs within large codebases
    and generating patches to resolve issues. For example, tools such as SWE-agent,
    AutoCodeRover, and RepoUnderstander iteratively refine code by navigating repositories,
    analyzing abstract syntax trees, and applying targeted changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The landscape of code-specialized LLMs has grown increasingly diverse and complex.
    This evolution raises critical questions for developers implementing these models
    in production environments: Which model is most suitable for specific programming
    tasks? How do different models compare in terms of code quality, accuracy, and
    reasoning capabilities? What are the trade-offs between open-source and commercial
    options? This is where benchmarks become essential tools for evaluation and selection.'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarks for code LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Objective benchmarks provide standardized methods to compare model performance
    across a variety of coding tasks, languages, and complexity levels. They help
    quantify capabilities that would otherwise remain subjective impressions, allowing
    for data-driven implementation decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For LangChain developers specifically, understanding benchmark results offers
    several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Informed model selection:** Choosing the optimal model for specific use cases
    based on quantifiable performance metrics rather than marketing claims or incomplete
    testing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Appropriate tooling**: Designing LangChain pipelines that incorporate the
    right balance of model capabilities and augmentation techniques based on known
    model strengths and limitations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-benefit analysis:** Evaluating whether premium commercial models justify
    their expense compared to free or self-hosted alternatives for particular applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance expectations:** Setting realistic expectations about what different
    models can achieve when integrated into larger systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code-generating LLMs demonstrate varying capabilities across established benchmarks,
    with performance characteristics directly impacting their effectiveness in LangChain
    implementations. Recent evaluations of leading models, including OpenAI’s GPT-4o
    (2024), Anthropic’s Claude 3.5 Sonnet (2025), and open-source models such as Llama
    3, show significant advancements in standard benchmarks. For instance, OpenAI’s
    o1 achieves 92.4% pass@1 on HumanEval (*A Survey On Large Language Models For
    Code Generation*, 2025), while Claude 3 Opus reaches 84.9% on the same benchmark
    (*The Claude 3 Model Family: Opus, Sonnet, Haiku*, 2024). However, performance
    metrics reveal important distinctions between controlled benchmark environments
    and the complex requirements of production LangChain applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard benchmarks provide useful but limited insights into model capabilities
    for LangChain implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HumanEval**: This benchmark evaluates functional correctness through 164
    Python programming problems. HumanEval primarily tests isolated function-level
    generation rather than the complex, multi-component systems typical in LangChain
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MBPP** (**Mostly Basic Programming Problems**): This contains approximately
    974 entry-level Python tasks. These problems lack the dependencies and contextual
    complexity found in production environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ClassEval**: This newer benchmark tests class-level code generation, addressing
    some limitations of function-level testing. Recent research by Liu et al. (*Evaluating
    Large Language Models in Class-Level Code Generation*, 2024) shows performance
    degradation of 15–30% compared to function-level tasks, highlighting challenges
    in maintaining contextual dependencies across methods—a critical consideration
    for LangChain components that manage state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SWE-bench**: More representative of real-world development, this benchmark
    evaluates models on bug-fixing tasks from actual GitHub repositories. Even top-performing
    models achieve only 40–65% success rates, as found by Jimenez et al. (*SWE-bench:
    Can Language Models Resolve Real-World GitHub Issues?*, 2023), demonstrating the
    significant gap between synthetic benchmarks and authentic coding challenges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM-based software engineering approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When implementing code-generating LLMs within LangChain frameworks, several
    key challenges emerge.
  prefs: []
  type: TYPE_NORMAL
- en: Repository-level problems that require understanding multiple files, dependencies,
    and context present significant challenges. Research using the ClassEval benchmark
    (Xueying Du and colleagues, *Evaluating Large Language Models in Class-Level Code
    Generation*, 2024) demonstrated that LLMs find class-level code generation “significantly
    more challenging than generating standalone functions,” with performance consistently
    lower when managing dependencies between methods compared to function-level benchmarks
    such as HumanEval.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs can be leveraged to understand repository-level code context despite the
    inherent challenges. The following implementation demonstrates a practical approach
    to analyzing multi-file Python codebases with LangChain, loading repository files
    as context for the model to consider when implementing new features. This pattern
    helps address the context limitations by directly providing a repository structure
    to the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This implementation uses GPT-4o to generate code while considering the context
    of entire repositories by pulling in relevant Python files to understand dependencies.
    This approach addresses context limitations but requires careful document chunking
    and retrieval strategies for large codebases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generated code often appears superficially correct but contains subtle bugs
    or security vulnerabilities that evade initial detection. The Uplevel Data Labs
    study (*Can GenAI Actually Improve Developer Productivity?*) analyzing nearly
    800 developers found a “significantly higher bug rate” in code produced by developers
    with access to AI coding assistants compared to those without. This is further
    supported by BlueOptima’s comprehensive analysis in 2024 of over 218,000 developers
    (*Debunking GitHub’s Claims: A Data-Driven Critique of Their Copilot Study*),
    which revealed that 88% of professionals needed to substantially rework AI-generated
    code before it was production-ready, often due to “aberrant coding patterns” that
    weren’t immediately apparent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Security researchers have identified a persistent risk where AI models inadvertently
    introduce security flaws by replicating insecure patterns from their training
    data, with these vulnerabilities frequently escaping detection during initial
    syntax and compilation checks (*Evaluating Large Language Models through Role-Guide
    and Self-Reflection: A Comparative Study*, 2024, and *HalluLens: LLM Hallucination
    Benchmark*, 2024). These findings emphasize the critical importance of thorough
    human review and testing of AI-generated code before production deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how to create a specialized validation chain
    that systematically analyzes generated code for common issues, serving as a first
    line of defense against subtle bugs and vulnerabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]python'
  prefs: []
  type: TYPE_NORMAL
- en: '{generated_code}'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a detailed analysis with specific issues and recommended fixes. """
  prefs: []
  type: TYPE_NORMAL
- en: validation_prompt = PromptTemplate( input_variables=["generated_code"], template=validation_template
    )
  prefs: []
  type: TYPE_NORMAL
- en: validation_chain = validation_prompt | llm
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: from typing import List
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_core.output_parsers import PydanticOutputParser
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_core.prompts import PromptTemplate
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_openai import ChatOpenAI
  prefs: []
  type: TYPE_NORMAL
- en: from pydantic import BaseModel, Field
  prefs: []
  type: TYPE_NORMAL
- en: Define the Pydantic model for structured output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'class SecurityAnalysis(BaseModel):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Security analysis results for generated code."""'
  prefs: []
  type: TYPE_NORMAL
- en: 'vulnerabilities: List[str] = Field(description="List of identified security
    vulnerabilities")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'mitigation_suggestions: List[str] = Field(description="Suggested fixes for
    each vulnerability")'
  prefs: []
  type: TYPE_NORMAL
- en: 'risk_level: str = Field(description="Overall risk assessment: Low, Medium,
    High, Critical")'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Initialize the output parser with the Pydantic model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: parser = PydanticOutputParser(pydantic_object=SecurityAnalysis)
  prefs: []
  type: TYPE_NORMAL
- en: Create the prompt template with format instructions from the parser
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: security_prompt = PromptTemplate.from_template(
  prefs: []
  type: TYPE_NORMAL
- en: 'template="""Analyze the following code for security vulnerabilities: {code}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Consider:'
  prefs: []
  type: TYPE_NORMAL
- en: SQL injection vulnerabilities
  prefs: []
  type: TYPE_NORMAL
- en: Cross-site scripting (XSS) risks
  prefs: []
  type: TYPE_NORMAL
- en: Insecure direct object references
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and authorization weaknesses
  prefs: []
  type: TYPE_NORMAL
- en: Sensitive data exposure
  prefs: []
  type: TYPE_NORMAL
- en: Missing input validation
  prefs: []
  type: TYPE_NORMAL
- en: Command injection opportunities
  prefs: []
  type: TYPE_NORMAL
- en: Insecure dependency usage
  prefs: []
  type: TYPE_NORMAL
- en: '{format_instructions}""",'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: input_variables=["code"],
  prefs: []
  type: TYPE_NORMAL
- en: 'partial_variables={"format_instructions": parser.get_format_instructions()}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the language model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: llm = ChatOpenAI(model="gpt-4", temperature=0)
  prefs: []
  type: TYPE_NORMAL
- en: Compose the chain using LCEL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: security_chain = security_prompt | llm | parser
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_google_genai import ChatGoogleGenerativeAI
  prefs: []
  type: TYPE_NORMAL
- en: question = """
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an integer n, return a string array answer (1-indexed) where:'
  prefs: []
  type: TYPE_NORMAL
- en: answer[i] == "FizzBuzz" if i is divisible by 3 and 5.
  prefs: []
  type: TYPE_NORMAL
- en: answer[i] == "Fizz" if i is divisible by 3.
  prefs: []
  type: TYPE_NORMAL
- en: answer[i] == "Buzz" if i is divisible by 5.
  prefs: []
  type: TYPE_NORMAL
- en: answer[i] == i (as a string) if none of the above conditions are true.
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro")
  prefs: []
  type: TYPE_NORMAL
- en: print(llm.invoke(question).content)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: from langchain.llms import HuggingFacePipeline
  prefs: []
  type: TYPE_NORMAL
- en: from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Choose a more up-to-date model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: checkpoint = "google/codegemma-2b"
  prefs: []
  type: TYPE_NORMAL
- en: Load the model and tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: model = AutoModelForCausalLM.from_pretrained(checkpoint)
  prefs: []
  type: TYPE_NORMAL
- en: tokenizer = AutoTokenizer.from_pretrained(checkpoint)
  prefs: []
  type: TYPE_NORMAL
- en: Create a text generation pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: pipe = pipeline(
  prefs: []
  type: TYPE_NORMAL
- en: task="text-generation",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: model=model,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: tokenizer=tokenizer,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: max_new_tokens=500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Integrate the pipeline with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: llm = HuggingFacePipeline(pipeline=pipe)
  prefs: []
  type: TYPE_NORMAL
- en: Define the input text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: text = """
  prefs: []
  type: TYPE_NORMAL
- en: 'def calculate_primes(n):'
  prefs: []
  type: TYPE_NORMAL
- en: \"\"\"Create a list of consecutive integers from 2 up to N.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '>>> calculate_primes(20)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output: [2, 3, 5, 7, 11, 13, 17, 19]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \"\"\"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: Use the LangChain LLM to generate text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: output = llm(text)
  prefs: []
  type: TYPE_NORMAL
- en: print(output)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'def calculate_primes(n):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Create a list of consecutive integers from 2 up to N.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '>>> calculate_primes(20)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Output: [2, 3, 5, 7, 11, 13, 17, 19]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"""'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: primes = []
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for i in range(2, n + 1):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if is_prime(i):'
  prefs: []
  type: TYPE_NORMAL
- en: primes.append(i)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return primes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'def is_prime(n):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Return True if n is prime."""'
  prefs: []
  type: TYPE_NORMAL
- en: 'if n < 2:'
  prefs: []
  type: TYPE_NORMAL
- en: return False
  prefs: []
  type: TYPE_NORMAL
- en: 'for i in range(2, int(n ** 0.5) + 1):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if n % i == 0:'
  prefs: []
  type: TYPE_NORMAL
- en: return False
  prefs: []
  type: TYPE_NORMAL
- en: return True
  prefs: []
  type: TYPE_NORMAL
- en: 'def main():'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Get user input and print the list of primes."""'
  prefs: []
  type: TYPE_NORMAL
- en: 'n = int(input("Enter a number: "))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: primes = calculate_primes(n)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: print(primes)
  prefs: []
  type: TYPE_NORMAL
- en: 'if __name__ == "__main__":'
  prefs: []
  type: TYPE_NORMAL
- en: main()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <|file_separator|>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: from langchain.llms import HuggingFaceHub
  prefs: []
  type: TYPE_NORMAL
- en: Choose a lightweight model good for code generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: repo_id = "bigcode/starcoder"
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the HuggingFaceHub LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: llm = HuggingFaceHub(
  prefs: []
  type: TYPE_NORMAL
- en: repo_id=repo_id,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: task="text-generation",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: model_kwargs={
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '"temperature": 0.5,'
  prefs: []
  type: TYPE_NORMAL
- en: '"max_length": 1000'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Use the LangChain LLM to generate text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: output = llm.invoke(text)
  prefs: []
  type: TYPE_NORMAL
- en: print(output)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_anthropic import ChatAnthropic
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_core.prompts.prompt import PromptTemplate
  prefs: []
  type: TYPE_NORMAL
- en: 'template = """Question: {question}'
  prefs: []
  type: TYPE_NORMAL
- en: Let's think step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '"""'
  prefs: []
  type: TYPE_NORMAL
- en: prompt = PromptTemplate(template=template, input_variables=["question"])
  prefs: []
  type: TYPE_NORMAL
- en: llm = ChatAnthropic(model='claude-3-opus-20240229')
  prefs: []
  type: TYPE_NORMAL
- en: llm_chain = prompt | llm
  prefs: []
  type: TYPE_NORMAL
- en: print(llm_chain.invoke(text))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the prime numbers up to a given number N, we can follow the Sieve
    of Eratosthenes algorithm. Here''s a step-by-step approach to solve the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Step-by-step explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a list called `numbers` that contains consecutive integers from 2
    to N using the `range()` function.
  prefs: []
  type: TYPE_NORMAL
- en: We initialize an empty list called `primes` to store the prime numbers that
    we will calculate.
  prefs: []
  type: TYPE_NORMAL
- en: We start a loop that continues as long as there are numbers in the `numbers`
    list.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the loop, we take the first number from the `numbers` list and consider
    it as a prime number. We append this prime number to the `primes` list.
  prefs: []
  type: TYPE_NORMAL
- en: We remove all the multiples of the prime number from the `numbers` list using
    a list comprehension. The list comprehension creates a new list containing only
    the numbers that are not divisible by the current prime number.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We repeat steps 4 and 5 until the `numbers` list becomes empty. This ensures
    that we have considered all the numbers up to N and identified the prime numbers
    among them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we return the `primes` list, which contains all the prime numbers up
    to N.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_openai import ChatOpenAI
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.agents import load_tools, initialize_agent, AgentType
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_experimental.tools import PythonREPLTool
  prefs: []
  type: TYPE_NORMAL
- en: 'tools = [PythonREPLTool()]   # Gives agent ability to run Python code'
  prefs: []
  type: TYPE_NORMAL
- en: llm = ChatOpenAI()
  prefs: []
  type: TYPE_NORMAL
- en: Set up the agent with necessary tools and model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: agent = initialize_agent(
  prefs: []
  type: TYPE_NORMAL
- en: tools,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'llm,  # Language model to power the agent'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'verbose=True # Shows agent''s thinking process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ')  # Agent makes decisions without examples'
  prefs: []
  type: TYPE_NORMAL
- en: result = agent("What are the prime numbers until 20?")
  prefs: []
  type: TYPE_NORMAL
- en: print(result)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Entering new AgentExecutor chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I can write a Python script to find the prime numbers up to 20.
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: Python_REPL'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Input: def is_prime(n):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if n <= 1:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: return False
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for i in range(2, int(n**0.5) + 1):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if n % i == 0:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return False
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: primes = [num for num in range(2, 21) if is_prime(num)]
  prefs: []
  type: TYPE_NORMAL
- en: print(primes)
  prefs: []
  type: TYPE_NORMAL
- en: 'Observation: [2, 3, 5, 7, 11, 13, 17, 19]'
  prefs: []
  type: TYPE_NORMAL
- en: I now know the final answer
  prefs: []
  type: TYPE_NORMAL
- en: 'Final Answer: [2, 3, 5, 7, 11, 13, 17, 19]'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '{''input'': ''What are the prime numbers until 20?'', ''output'': ''[2, 3,
    5, 7, 11, 13, 17, 19]''}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_community.document_loaders import DocusaurusLoader
  prefs: []
  type: TYPE_NORMAL
- en: import nest_asyncio
  prefs: []
  type: TYPE_NORMAL
- en: nest_asyncio.apply()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Load all pages from LangChain docs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: loader = DocusaurusLoader("https://python.langchain.com")
  prefs: []
  type: TYPE_NORMAL
- en: documents[0]
  prefs: []
  type: TYPE_NORMAL
- en: nest_asyncio.apply() enables async operations in Jupyter notebooks. The loader
    gets all pages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: from langchain.embeddings import CacheBackedEmbeddings
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_openai import OpenAIEmbeddings
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.storage import LocalFileStore
  prefs: []
  type: TYPE_NORMAL
- en: Cache embeddings locally to avoid redundant API calls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: store = LocalFileStore("./cache/")
  prefs: []
  type: TYPE_NORMAL
- en: underlying_embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
  prefs: []
  type: TYPE_NORMAL
- en: embeddings = CacheBackedEmbeddings.from_bytes_store(
  prefs: []
  type: TYPE_NORMAL
- en: underlying_embeddings, store, namespace=underlying_embeddings.model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_text_splitters import RecursiveCharacterTextSplitter
  prefs: []
  type: TYPE_NORMAL
- en: text_splitter = RecursiveCharacterTextSplitter(
  prefs: []
  type: TYPE_NORMAL
- en: chunk_size=1000,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: chunk_overlap=20,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: length_function=len,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: is_separator_regex=False,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: splits = text_splitter.split_documents(documents)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_chroma import Chroma
  prefs: []
  type: TYPE_NORMAL
- en: Store document embeddings for efficient retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_google_vertexai import VertexAI
  prefs: []
  type: TYPE_NORMAL
- en: llm = VertexAI(model_name="gemini-pro")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: from langchain import hub
  prefs: []
  type: TYPE_NORMAL
- en: retriever = vectorstore.as_retriever()
  prefs: []
  type: TYPE_NORMAL
- en: Use community-created RAG prompt template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: prompt = hub.pull("rlm/rag-prompt")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_core.runnables import RunnablePassthrough
  prefs: []
  type: TYPE_NORMAL
- en: 'def format_docs(docs):'
  prefs: []
  type: TYPE_NORMAL
- en: return "\n\n".join(doc.page_content for doc in docs)
  prefs: []
  type: TYPE_NORMAL
- en: Chain combines context retrieval, prompting, and response generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: rag_chain = (
  prefs: []
  type: TYPE_NORMAL
- en: '{"context": retriever | format_docs, "question": RunnablePassthrough()}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| prompt'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| llm'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| StrOutputParser()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: response = rag_chain.invoke("What is Task Decomposition?")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: import os
  prefs: []
  type: TYPE_NORMAL
- en: from git import Repo
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_community.document_loaders.generic import GenericLoader
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_community.document_loaders.parsers import LanguageParser
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_text_splitters import Language, RecursiveCharacterTextSplitter
  prefs: []
  type: TYPE_NORMAL
- en: Clone the book repository from GitHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'repo_path = os.path.expanduser("~/Downloads/generative_ai_with_langchain")  #
    this directory should not exist yet!'
  prefs: []
  type: TYPE_NORMAL
- en: repo = Repo.clone_from("https://github.com/benman1/generative_ai_with_langchain",
    to_path=repo_path)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: loader = GenericLoader.from_filesystem(
  prefs: []
  type: TYPE_NORMAL
- en: repo_path,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: glob="**/*",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: suffixes=[".py"],
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: documents = loader.load()
  prefs: []
  type: TYPE_NORMAL
- en: python_splitter = RecursiveCharacterTextSplitter.from_language(
  prefs: []
  type: TYPE_NORMAL
- en: language=Language.PYTHON, chunk_size=50, chunk_overlap=0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Split the Document into chunks for embedding and vector storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: texts = python_splitter.split_documents(documents)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Create vector store and retriever
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: db = Chroma.from_documents(texts, OpenAIEmbeddings())
  prefs: []
  type: TYPE_NORMAL
- en: retriever = db.as_retriever(
  prefs: []
  type: TYPE_NORMAL
- en: 'search_type="mmr",  # Maximal Marginal Relevance for diverse results'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'search_kwargs={"k": 8}  # Return 8 most relevant chunks'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: Set up Q&A chain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: prompt = ChatPromptTemplate.from_messages([
  prefs: []
  type: TYPE_NORMAL
- en: ("system", "Answer based on context:\n\n{context}"),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ("placeholder", "{chat_history}"),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ("user", "{input}"),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '])'
  prefs: []
  type: TYPE_NORMAL
- en: Create chain components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: document_chain = create_stuff_documents_chain(ChatOpenAI(), prompt)
  prefs: []
  type: TYPE_NORMAL
- en: qa = create_retrieval_chain(retriever, document_chain)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: question = "What examples are in the code related to software development?"
  prefs: []
  type: TYPE_NORMAL
- en: 'result = qa.invoke({"input": question})'
  prefs: []
  type: TYPE_NORMAL
- en: print(result["answer"])
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of the code related to software development in the given
    context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '1\. Task planner and executor for software development: This indicates that
    the code includes functionality for planning and executing tasks related to software
    development.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. debug your code: This suggests that there is a recommendation to debug
    the code if an error occurs during software development.'
  prefs: []
  type: TYPE_NORMAL
- en: These examples provide insights into the software development process described
    in the context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_experimental.agents.agent_toolkits.python.base import create_python_agent
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_experimental.tools.python.tool import PythonREPLTool
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_anthropic import ChatAnthropic
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.agents.agent_types import AgentType
  prefs: []
  type: TYPE_NORMAL
- en: agent_executor = create_python_agent(
  prefs: []
  type: TYPE_NORMAL
- en: llm=ChatAnthropic(model='claude-3-opus-20240229'),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: tool=PythonREPLTool(),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: verbose=True,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: result = agent_executor.run(
  prefs: []
  type: TYPE_NORMAL
- en: '"""Understand, write a single neuron neural network in PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: Take synthetic data for y=2x. Train for 1000 epochs and print every 100 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Return prediction for x = 5"""
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: print(result)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Entering new AgentExecutor chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Here is a single neuron neural network in PyTorch that trains on synthetic
    data for y=2x, prints the loss every 100 epochs, and returns the prediction for
    x=5:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: Python_REPL'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Input:'
  prefs: []
  type: TYPE_NORMAL
- en: import torch
  prefs: []
  type: TYPE_NORMAL
- en: import torch.nn as nn
  prefs: []
  type: TYPE_NORMAL
- en: Create synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
  prefs: []
  type: TYPE_NORMAL
- en: y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])
  prefs: []
  type: TYPE_NORMAL
- en: Define the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[...] # Code for creating the model omitted for brevity'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Epoch [100/1000], Loss: 0.0529'
  prefs: []
  type: TYPE_NORMAL
- en: '[...] # Training progress for epochs 200-900 omitted for brevity'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Epoch [1000/1000], Loss: 0.0004'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction for x=5: 9.9659'
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: '- I created a single neuron neural network model in PyTorch using nn.Linear(1,
    1)'
  prefs: []
  type: TYPE_NORMAL
- en: '- I generated synthetic data where y=2x for training'
  prefs: []
  type: TYPE_NORMAL
- en: '- I defined the MSE loss function and SGD optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: '- I trained the model for 1000 epochs, printing the loss every 100 epochs'
  prefs: []
  type: TYPE_NORMAL
- en: '- After training, I made a prediction for x=5'
  prefs: []
  type: TYPE_NORMAL
- en: The final prediction for x=5 is 9.9659, which is very close to the expected
    value of 10 (since y=2x).
  prefs: []
  type: TYPE_NORMAL
- en: So in conclusion, I was able to train a simple single neuron PyTorch model to
    fit the synthetic y=2x data well and make an accurate prediction for a new input
    x=5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Final Answer: The trained single neuron PyTorch model predicts a value of 9.9659
    for x=5.'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The final output confirms that our agent successfully built and trained a model
    that learned the y=2x relationship. The prediction for x=5 is approximately 9.97,
    which is very close to the expected value of 10.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: from sklearn.datasets import load_iris
  prefs: []
  type: TYPE_NORMAL
- en: df = load_iris(as_frame=True)["data"]
  prefs: []
  type: TYPE_NORMAL
- en: df.to_csv("iris.csv", index=False)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: from langchain_experimental.agents.agent_toolkits.pandas.base import
  prefs: []
  type: TYPE_NORMAL
- en: create_pandas_dataframe_agent
  prefs: []
  type: TYPE_NORMAL
- en: from langchain import PromptTemplate
  prefs: []
  type: TYPE_NORMAL
- en: PROMPT = (
  prefs: []
  type: TYPE_NORMAL
- en: '"If you do not know the answer, say you don''t know.\n"'
  prefs: []
  type: TYPE_NORMAL
- en: '"Think step by step.\n"'
  prefs: []
  type: TYPE_NORMAL
- en: '"\n"'
  prefs: []
  type: TYPE_NORMAL
- en: '"Below is the query.\n"'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '"Query: {query}\n"'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: prompt = PromptTemplate(template=PROMPT, input_variables=["query"])
  prefs: []
  type: TYPE_NORMAL
- en: llm = OpenAI()
  prefs: []
  type: TYPE_NORMAL
- en: agent = create_pandas_dataframe_agent(
  prefs: []
  type: TYPE_NORMAL
- en: llm, df, verbose=True, allow_dangerous_code=True
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: agent.run(prompt.format(query="What's this dataset about?"))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: Entering new AgentExecutor chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Thought: I need to understand the structure and contents of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: python_repl_ast'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Input: print(df.head())'
  prefs: []
  type: TYPE_NORMAL
- en: sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
  prefs: []
  type: TYPE_NORMAL
- en: 0                5.1               3.5                1.4               0.2
  prefs: []
  type: TYPE_NORMAL
- en: 1                4.9               3.0                1.4               0.2
  prefs: []
  type: TYPE_NORMAL
- en: 2                4.7               3.2                1.3               0.2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 3                4.6               3.1                1.5               0.2
  prefs: []
  type: TYPE_NORMAL
- en: 4                5.0               3.6                1.4               0.2
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains four features (sepal length, sepal width, petal length,
    and petal width) and 150 entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Final Answer: Based on the observation, this dataset is likely about measurements
    of flower characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '''Based on the observation, this dataset is likely about measurements of flower
    characteristics.'''
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: agent.run(prompt.format(query="Which row has the biggest difference between
    petal length and petal width?"))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Entering new AgentExecutor chain...
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Thought: First, we need to find the difference between petal length and petal
    width for each row. Then, we need to find the row with the maximum difference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: python_repl_ast'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Input: df[''petal_diff''] = df[''petal length (cm)''] - df[''petal width
    (cm)'']'
  prefs: []
  type: TYPE_NORMAL
- en: df['petal_diff'].max()
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Observation: 4.7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action: python_repl_ast'
  prefs: []
  type: TYPE_NORMAL
- en: 'Action Input: df[''petal_diff''].idxmax()'
  prefs: []
  type: TYPE_NORMAL
- en: 'Observation: 122'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Final Answer: Row 122 has the biggest difference between petal length and petal
    width.'
  prefs: []
  type: TYPE_NORMAL
- en: Finished chain.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '''Row 122 has the biggest difference between petal length and petal width.'''
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: agent.run(prompt.format(query="Show the distributions for each column visually!"))
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: For this visualization query, the agent generates code to create appropriate
    plots for each measurement column. The agent decides to use histograms to show
    the distribution of each feature in the dataset, providing visual insights that
    complement the numerical analyses from previous queries. This demonstrates how
    our agent can generate code for creating informative data visualizations that
    help understand the dataset’s characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: These three examples showcase the versatility of our data analysis agent in
    handling different types of analytical tasks. By progressively increasing the
    complexity of our queries—from basic exploration to statistical analysis and visualization—we
    can see how the agent uses its tools effectively to provide meaningful insights
    about the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'When designing your own data analysis agents, consider providing them with
    a variety of analysis tools that cover the full spectrum of data science workflows:
    exploration, preprocessing, analysis, visualization, and interpretation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B32363_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Our LLM agent visualizing the well-known Iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: In the repository, you can see a UI that wraps a data science agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data science agents represent a powerful application of LangChain’s capabilities.
    These agents can:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate and execute Python code for data analysis and machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build and train models based on simple natural language instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer complex questions about datasets through analysis and visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automate repetitive data science tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While these agents aren’t yet ready to replace human data scientists, they can
    significantly accelerate workflows by handling routine tasks and providing quick
    insights from data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s conclude the chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has examined how LLMs are reshaping software development and data
    analysis practices through natural language interfaces. We traced the evolution
    from early code generation models to today’s sophisticated systems, analyzing
    benchmarks that reveal both capabilities and limitations. Independent research
    suggests that while 55% productivity gains in controlled settings don’t fully
    translate to production environments, meaningful improvements of 4-22% are still
    being realized, particularly when human expertise guides LLM implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Our practical demonstrations illustrated diverse approaches to LLM integration
    through LangChain. We used multiple models to generate code solutions, built RAG
    systems to augment LLMs with documentation and repository knowledge, and created
    agents capable of training neural networks and analyzing datasets with minimal
    human intervention. Throughout these implementations, we looked at critical security
    considerations, providing validation frameworks and risk mitigation strategies
    essential for production deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the capabilities and integration strategies for LLMs in software
    and data workflows, we now turn our attention to ensuring these solutions work
    reliably in production. In [*Chapter 8*](E_Chapter_8.xhtml#_idTextAnchor390),
    we’ll delve into evaluation and testing methodologies that help validate AI-generated
    code and safeguard system performance, setting the stage for building truly production-ready
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is vibe coding, and how does it change the traditional approach to writing
    and maintaining code?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What key differences exist between traditional low-code platforms and LLM-based
    development approaches?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do independent research findings on productivity gains from AI coding assistants
    differ from vendor claims, and what factors might explain this discrepancy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What specific benchmark metrics show that LLMs struggle more with class-level
    code generation compared to function-level tasks, and why is this distinction
    important for practical implementations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe the validation framework presented in the chapter for LLM-generated
    code. What are the six key areas of assessment, and why is each important for
    production systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the repository RAG example from the chapter, explain how you would modify
    the implementation to better handle large codebases with thousands of files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What patterns emerged in the dataset analysis examples that demonstrate how
    LLMs perform in structured data analysis tasks versus unstructured text processing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the agentic approach to data science, as demonstrated in the neural
    network training example, differ from traditional programming workflows? What
    advantages and limitations did this approach reveal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do LLM integrations in LangChain enable more effective software development
    and data analysis?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What critical factors should organizations consider when implementing LLM-based
    development or analysis tools?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
