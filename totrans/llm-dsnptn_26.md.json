["```py\npip install google-search-results sentence-transformers openai\n```", "```py\n    from serpapi import GoogleSearch\n    from sentence_transformers import SentenceTransformer, util\n    import torch\n    import openai\n    SERPAPI_KEY = \"YOUR_SERPAPI_KEY\"  # Replace with your SerpAPI key\n    OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"  # Replace with your OpenAI key\n    openai.api_key = OPENAI_API_KEY\n    ```", "```py\n        def search(query):\n        params = {\n            \"q\": query,\n            \"hl\": \"en\",\n            \"gl\": \"us\",\n            \"google_domain\": \"google.com\",\n            \"api_key\": SERPAPI_KEY,\n        }\n        search = GoogleSearch(params)\n        results = search.get_dict()\n        return results\n    model = SentenceTransformer('all-mpnet-base-v2')\n    ```", "```py\n       def retrieve_snippets(query, results, top_k=3):\n        snippets = [\n            result.get(\"snippet\", \"\")\n            for result in results.get(\"organic_results\", [])\n        ]\n        if not snippets:\n            return []\n        query_embedding = model.encode(query,\n            convert_to_tensor=True)\n        snippet_embeddings = model.encode(snippets,\n            convert_to_tensor=True)\n        cosine_scores = util.pytorch_cos_sim(\n            query_embedding, snippet_embeddings\n        )[0]\n        top_results = torch.topk(cosine_scores, k=top_k)\n        return [snippets[i] for i in top_results.indices]\n    ```", "```py\n        def generate_answer(query, context):\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a knowledgeable expert. Answer the user's query based only on the information provided in the context. \"\n                           \"If the answer is not in the context, say 'I couldn't find an answer to your question in the provided context.'\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Context: {context}\\n\\nQuery: {query}\",\n            },\n        ]\n        response = openai.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n            temperature=0.7,\n            max_tokens=256\n        )\n        return response.choices[0].message.content\n    ```", "```py\n       def rag_system(query):\n        search_results = search(query)\n        relevant_snippets = retrieve_snippets(query, search_results)\n        if not relevant_snippets:\n            return \"Could not find any information related to your query\"\n        context = \" \".join(relevant_snippets)\n        answer = generate_answer(query, context)\n        return answer\n    # Example usage\n    query = \"What are the latest advancements in quantum computing?\"\n    answer = rag_system(query)\n    print(answer)\n    ```", "```py\npip install faiss-cpu sentence-transformers\n# Use faiss-gpu if you have a compatible GPU\n```", "```py\n    from sentence_transformers import SentenceTransformer\n    import faiss\n    import numpy as np\n    # Load the SentenceTransformer model\n    model = SentenceTransformer('all-mpnet-base-v2')\n    ```", "```py\n    # Sample sentences\n    text_data = [\n        \"A man is walking his dog in the park.\",\n        \"Children are playing with toys indoors.\",\n        \"An artist is painting a landscape on canvas.\",\n        \"The sun sets behind the mountain ridge.\",\n        \"Birds are singing outside the window.\"\n    ]\n    # Generate vector representations using a SentenceTransformer model\n    import numpy as np\n    from sentence_transformers import SentenceTransformer\n    model = SentenceTransformer('all-MiniLM-L6-v2')  # Replace with your model if different\n    vectors = model.encode(text_data, convert_to_tensor=True)\n    # Ensure compatibility with Faiss by converting to 32-bit floating point and moving to CPU\n    vectors = vectors.detach().cpu().numpy().astype(np.float32)\n    ```", "```py\n    # Get the dimensionality of the embeddings\n    dimension = embeddings.shape[1]\n    # Create a Faiss index (flat L2 distance)\n    index = faiss.IndexFlatL2(dimension)\n    # Add the embeddings to the index\n    index.add(embeddings)\n    ```", "```py\n    # Define a search query\n    query = \"What is the dog doing?\"\n    # Encode the query\n    query_embedding = model.encode(query, convert_to_tensor=True)\n    query_embedding = \\\n        query_embedding.cpu().numpy().astype('float32')\n    ```", "```py\n    # Search for the k nearest neighbors\n    k = 2\n    distances, indices = index.search(query_embedding, k)\n    # Print the results\n    print(\"Nearest neighbors:\")\n    for i, idx in enumerate(indices[0]):\n        print(f\"  Index: {idx}, Distance: {distances[0][i]},\n        Sentence: {sentences[idx]}\")\n    ```", "```py\nNearest neighbors:\n  Index: 0, Distance: 0.634912312, Sentence: A man is walking his dog in the park.\n  Index: 1, Distance: 1.237844944, Sentence: Children are playing with toys indoors.\n```", "```py\nfrom transformers import pipeline\nclass QueryExpansionRAG(AdvancedRAG):\n    def __init__(\n        self, model_name, knowledge_base,\n        query_expansion_model=\"t5-small\"\n    ):\n        super().__init__(model_name, knowledge_base)\n        self.query_expander = pipeline(\n            \"text2text-generation\", model=query_expansion_model\n        )\n    def expand_query(self, query):\n        expanded = self.query_expander(\n            f\"expand query: {query}\", max_length=50,\n            num_return_sequences=3\n        )\n        return [query] + [e['generated_text'] for e in expanded]\n    def retrieve(self, query, k=5):\n        expanded_queries = self.expand_query(query)\n        all_retrieved = []\n        for q in expanded_queries:\n            all_retrieved.extend(super().retrieve(q, k))\n        # Remove duplicates and return top k\n        unique_retrieved = list(dict.fromkeys(all_retrieved))\n        return unique_retrieved[:k]\n# Example usage\nrag_system = QueryExpansionRAG(model_name, knowledge_base)\nretrieved_docs = rag_system.retrieve(query)\nprint(\"Retrieved documents:\", retrieved_docs)\n```", "```py\nfrom transformers import AutoModelForCausalLM\nclass GenerativeRAG(QueryExpansionRAG):\n    def __init__(\n        self, retriever_model, generator_model, knowledge_base\n    ):\n        super().__init__(retriever_model, knowledge_base)\n        self.generator = \\\n            AutoModelForCausalLM.from_pretrained(generator_model)\n        self.generator_tokenizer = \\\n            AutoTokenizer.from_pretrained(generator_model)\n    def generate_response(self, query, max_length=100):\n        retrieved_docs = self.retrieve(query)\n        context = \"\\n\".join(retrieved_docs)\n        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n        inputs = self.generator_tokenizer(prompt, return_tensors=\"pt\")\n        outputs = self.generator.generate(inputs,\n            max_length=max_length)\n        return self.generator_tokenizer.decode(\n            outputs[0], skip_special_tokens=True)\n# Example usage\nretriever_model = \"all-MiniLM-L6-v2\"\ngenerator_model = \"gpt2-medium\"\nrag_system = GenerativeRAG(\n    retriever_model, generator_model, knowledge_base\n)\nresponse = rag_system.generate_response(query)\nprint(\"Generated response:\", response)\n```", "```py\nclass ShardedRAG(GenerativeRAG):\n    def __init__(\n        self, retriever_model, generator_model,\n        knowledge_base, num_shards=5\n    ):\n        super().__init__(retriever_model, generator_model,\n            knowledge_base)\n        self.num_shards = num_shards\n        self.sharded_indexes = self.build_sharded_index()\n    def build_sharded_index(self):\n        embeddings = self.get_embeddings(self.knowledge_base)\n        sharded_indexes = []\n        shard_size = len(embeddings) // self.num_shards\n        for i in range(self.num_shards):\n            start = i * shard_size\n            end = start + shard_size if i < self.num_shards - 1\n                else len(embeddings)\n            shard_index = faiss.IndexFlatL2(embeddings.shape[1])\n            shard_index.add(embeddings[start:end])\n            sharded_indexes.append(shard_index)\n        return sharded_indexes\n    def retrieve(self, query, k=5):\n        query_embedding = self.get_embeddings([query])[0]\n        all_retrieved = []\n        for shard_index in self.sharded_indexes:\n            _, indices = shard_index.search(\n                np.array([query_embedding]), k)\n            all_retrieved.extend([self.knowledge_base[i]\n                for i in indices[0]])\n        # Remove duplicates and return top k\n        unique_retrieved = list(dict.fromkeys(all_retrieved))\n        return unique_retrieved[:k]\n# Example usage\nsharded_rag = ShardedRAG(retriever_model, generator_model,\n    knowledge_base)\nresponse = sharded_rag.generate_response(query)\nprint(\"Generated response:\", response)\n```"]